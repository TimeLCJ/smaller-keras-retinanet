 1057/10000 [==>...........................] - ETA: 1:18:20 - loss: 3.5341 - regression_loss: 2.5582 - classification_lo 1058/10000 [==>...........................] - ETA: 1:18:20 - loss: 3.5336 - regression_loss: 2.5580 - classification_lo 1059/10000 [==>...........................] - ETA: 1:18:18 - loss: 3.5324 - regression_loss: 2.5572 - classification_lo 1060/10000 [==>...........................] - ETA: 1:18:17 - loss: 3.5312 - regression_loss: 2.5563 - classification_lo 1061/10000 [==>...........................] - ETA: 1:18:16 - loss: 3.5307 - regression_loss: 2.5562 - classification_lo 1062/10000 [==>...........................] - ETA: 1:18:15 - loss: 3.5293 - regression_loss: 2.5552 - classification_lo 1063/10000 [==>...........................] - ETA: 1:18:14 - loss: 3.5289 - regression_loss: 2.5551 - classification_lo 1064/10000 [==>...........................] - ETA: 1:18:13 - loss: 3.5286 - regression_loss: 2.5550 - classification_lo 1065/10000 [==>...........................] - ETA: 1:18:12 - loss: 3.5285 - regression_loss: 2.5551 - classification_lo 1066/10000 [==>...........................] - ETA: 1:18:15 - loss: 3.5287 - regression_loss: 2.5551 - classification_lo 1067/10000 [==>...........................] - ETA: 1:18:17 - loss: 3.5301 - regression_loss: 2.5563 - classification_lo 1068/10000 [==>...........................] - ETA: 1:18:16 - loss: 3.5300 - regression_loss: 2.5565 - classification_lo 1069/10000 [==>...........................] - ETA: 1:18:19 - loss: 3.5288 - regression_loss: 2.5558 - classification_lo 1070/10000 [==>...........................] - ETA: 1:18:17 - loss: 3.5285 - regression_loss: 2.5556 - classification_lo 1071/10000 [==>...........................] - ETA: 1:18:16 - loss: 3.5281 - regression_loss: 2.5555 - classification_lo 1072/10000 [==>...........................] - ETA: 1:18:18 - loss: 3.5272 - regression_loss: 2.5551 - classification_lo 1073/10000 [==>...........................] - ETA: 1:18:17 - loss: 3.5258 - regression_loss: 2.5541 - classification_lo 1074/10000 [==>...........................] - ETA: 1:18:16 - loss: 3.5261 - regression_loss: 2.5546 - classification_lo 1075/10000 [==>...........................] - ETA: 1:18:15 - loss: 3.5261 - regression_loss: 2.5547 - classification_lo 1076/10000 [==>...........................] - ETA: 1:18:14 - loss: 3.5260 - regression_loss: 2.5547 - classification_lo 1077/10000 [==>...........................] - ETA: 1:18:13 - loss: 3.5257 - regression_loss: 2.5544 - classification_lo 1078/10000 [==>...........................] - ETA: 1:18:12 - loss: 3.5254 - regression_loss: 2.5544 - classification_lo 1079/10000 [==>...........................] - ETA: 1:18:10 - loss: 3.5246 - regression_loss: 2.5538 - classification_lo 1080/10000 [==>...........................] - ETA: 1:18:09 - loss: 3.5242 - regression_loss: 2.5535 - classification_lo 1081/10000 [==>...........................] - ETA: 1:18:08 - loss: 3.5239 - regression_loss: 2.5535 - classification_lo 1082/10000 [==>...........................] - ETA: 1:18:07 - loss: 3.5250 - regression_loss: 2.5511 - classification_lo 1083/10000 [==>...........................] - ETA: 1:18:06 - loss: 3.5248 - regression_loss: 2.5511 - classification_lo 1084/10000 [==>...........................] - ETA: 1:18:05 - loss: 3.5248 - regression_loss: 2.5513 - classification_lo 1085/10000 [==>...........................] - ETA: 1:18:04 - loss: 3.5240 - regression_loss: 2.5506 - classification_lo 1086/10000 [==>...........................] - ETA: 1:18:03 - loss: 3.5241 - regression_loss: 2.5508 - classification_lo 1087/10000 [==>...........................] - ETA: 1:18:02 - loss: 3.5228 - regression_loss: 2.5498 - classification_lo 1088/10000 [==>...........................] - ETA: 1:18:00 - loss: 3.5230 - regression_loss: 2.5501 - classification_lo 1089/10000 [==>...........................] - ETA: 1:18:00 - loss: 3.5241 - regression_loss: 2.5507 - classification_lo 1090/10000 [==>...........................] - ETA: 1:17:59 - loss: 3.5240 - regression_loss: 2.5510 - classification_lo 1091/10000 [==>...........................] - ETA: 1:17:58 - loss: 3.5240 - regression_loss: 2.5509 - classification_lo 1092/10000 [==>...........................] - ETA: 1:17:56 - loss: 3.5232 - regression_loss: 2.5504 - classification_lo 1093/10000 [==>...........................] - ETA: 1:17:55 - loss: 3.5230 - regression_loss: 2.5506 - classification_lo 1094/10000 [==>...........................] - ETA: 1:17:54 - loss: 3.5231 - regression_loss: 2.5507 - classification_lo 1095/10000 [==>...........................] - ETA: 1:17:53 - loss: 3.5228 - regression_loss: 2.5507 - classification_lo 1096/10000 [==>...........................] - ETA: 1:17:52 - loss: 3.5228 - regression_loss: 2.5503 - classification_lo 1097/10000 [==>...........................] - ETA: 1:17:51 - loss: 3.5216 - regression_loss: 2.5493 - classification_lo 1098/10000 [==>...........................] - ETA: 1:17:50 - loss: 3.5213 - regression_loss: 2.5492 - classification_lo 1099/10000 [==>...........................] - ETA: 1:17:49 - loss: 3.5201 - regression_loss: 2.5481 - classification_lo 1100/10000 [==>...........................] - ETA: 1:17:48 - loss: 3.5201 - regression_loss: 2.5480 - classification_lo 1101/10000 [==>...........................] - ETA: 1:17:46 - loss: 3.5196 - regression_loss: 2.5478 - classification_lo 1102/10000 [==>...........................] - ETA: 1:17:45 - loss: 3.5193 - regression_loss: 2.5477 - classification_lo 1103/10000 [==>...........................] - ETA: 1:17:44 - loss: 3.5198 - regression_loss: 2.5480 - classification_lo 1104/10000 [==>...........................] - ETA: 1:17:43 - loss: 3.5188 - regression_loss: 2.5474 - classification_lo 1105/10000 [==>...........................] - ETA: 1:17:45 - loss: 3.5180 - regression_loss: 2.5468 - classification_lo 1106/10000 [==>...........................] - ETA: 1:17:44 - loss: 3.5177 - regression_loss: 2.5466 - classification_lo 1107/10000 [==>...........................] - ETA: 1:17:48 - loss: 3.5172 - regression_loss: 2.5464 - classification_lo 1108/10000 [==>...........................] - ETA: 1:17:46 - loss: 3.5171 - regression_loss: 2.5464 - classification_lo 1109/10000 [==>...........................] - ETA: 1:17:45 - loss: 3.5175 - regression_loss: 2.5470 - classification_lo 1110/10000 [==>...........................] - ETA: 1:17:44 - loss: 3.5168 - regression_loss: 2.5466 - classification_lo 1111/10000 [==>...........................] - ETA: 1:17:43 - loss: 3.5163 - regression_loss: 2.5463 - classification_lo 1112/10000 [==>...........................] - ETA: 1:17:42 - loss: 3.5160 - regression_loss: 2.5463 - classification_lo 1113/10000 [==>...........................] - ETA: 1:17:41 - loss: 3.5156 - regression_loss: 2.5462 - classification_lo 1114/10000 [==>...........................] - ETA: 1:17:40 - loss: 3.5151 - regression_loss: 2.5461 - classification_lo 1115/10000 [==>...........................] - ETA: 1:17:39 - loss: 3.5145 - regression_loss: 2.5457 - classification_lo 1116/10000 [==>...........................] - ETA: 1:17:38 - loss: 3.5143 - regression_loss: 2.5455 - classification_lo 1117/10000 [==>...........................] - ETA: 1:17:37 - loss: 3.5138 - regression_loss: 2.5452 - classification_lo 1118/10000 [==>...........................] - ETA: 1:17:36 - loss: 3.5132 - regression_loss: 2.5449 - classification_lo 1119/10000 [==>...........................] - ETA: 1:17:35 - loss: 3.5134 - regression_loss: 2.5448 - classification_lo 1120/10000 [==>...........................] - ETA: 1:17:34 - loss: 3.5129 - regression_loss: 2.5446 - classification_lo 1121/10000 [==>...........................] - ETA: 1:17:33 - loss: 3.5120 - regression_loss: 2.5440 - classification_lo 1122/10000 [==>...........................] - ETA: 1:17:32 - loss: 3.5117 - regression_loss: 2.5438 - classification_lo 1123/10000 [==>...........................] - ETA: 1:17:31 - loss: 3.5114 - regression_loss: 2.5437 - classification_lo 1124/10000 [==>...........................] - ETA: 1:17:29 - loss: 3.5113 - regression_loss: 2.5438 - classification_lo 1125/10000 [==>...........................] - ETA: 1:17:28 - loss: 3.5113 - regression_loss: 2.5438 - classification_lo 1126/10000 [==>...........................] - ETA: 1:17:27 - loss: 3.5111 - regression_loss: 2.5437 - classification_lo 1127/10000 [==>...........................] - ETA: 1:17:26 - loss: 3.5101 - regression_loss: 2.5430 - classification_lo 1128/10000 [==>...........................] - ETA: 1:17:25 - loss: 3.5098 - regression_loss: 2.5427 - classification_lo 1129/10000 [==>...........................] - ETA: 1:17:24 - loss: 3.5093 - regression_loss: 2.5424 - classification_lo 1130/10000 [==>...........................] - ETA: 1:17:23 - loss: 3.5088 - regression_loss: 2.5421 - classification_lo 1131/10000 [==>...........................] - ETA: 1:17:22 - loss: 3.5079 - regression_loss: 2.5413 - classification_lo 1132/10000 [==>...........................] - ETA: 1:17:22 - loss: 3.5072 - regression_loss: 2.5410 - classification_lo 1133/10000 [==>...........................] - ETA: 1:17:21 - loss: 3.5069 - regression_loss: 2.5411 - classification_lo 1134/10000 [==>...........................] - ETA: 1:17:19 - loss: 3.5062 - regression_loss: 2.5405 - classification_lo 1135/10000 [==>...........................] - ETA: 1:17:19 - loss: 3.5062 - regression_loss: 2.5405 - classification_lo 1136/10000 [==>...........................] - ETA: 1:17:18 - loss: 3.5060 - regression_loss: 2.5406 - classification_lo 1137/10000 [==>...........................] - ETA: 1:17:17 - loss: 3.5064 - regression_loss: 2.5407 - classification_lo 1138/10000 [==>...........................] - ETA: 1:17:16 - loss: 3.5052 - regression_loss: 2.5400 - classification_lo 1139/10000 [==>...........................] - ETA: 1:17:15 - loss: 3.5048 - regression_loss: 2.5399 - classification_lo 1140/10000 [==>...........................] - ETA: 1:17:14 - loss: 3.5041 - regression_loss: 2.5394 - classification_lo 1141/10000 [==>...........................] - ETA: 1:17:13 - loss: 3.5040 - regression_loss: 2.5395 - classification_lo 1142/10000 [==>...........................] - ETA: 1:17:12 - loss: 3.5038 - regression_loss: 2.5395 - classification_lo 1143/10000 [==>...........................] - ETA: 1:17:11 - loss: 3.5031 - regression_loss: 2.5393 - classification_lo 1144/10000 [==>...........................] - ETA: 1:17:10 - loss: 3.5022 - regression_loss: 2.5387 - classification_lo 1145/10000 [==>...........................] - ETA: 1:17:09 - loss: 3.5012 - regression_loss: 2.5380 - classification_lo 1146/10000 [==>...........................] - ETA: 1:17:08 - loss: 3.5012 - regression_loss: 2.5381 - classification_lo 1147/10000 [==>...........................] - ETA: 1:17:07 - loss: 3.5005 - regression_loss: 2.5377 - classification_lo 1148/10000 [==>...........................] - ETA: 1:17:07 - loss: 3.4994 - regression_loss: 2.5368 - classification_lo 1149/10000 [==>...........................] - ETA: 1:17:05 - loss: 3.4994 - regression_loss: 2.5370 - classification_lo 1150/10000 [==>...........................] - ETA: 1:17:04 - loss: 3.4990 - regression_loss: 2.5366 - classification_lo 1151/10000 [==>...........................] - ETA: 1:17:03 - loss: 3.4992 - regression_loss: 2.5367 - classification_lo 1152/10000 [==>...........................] - ETA: 1:17:02 - loss: 3.4990 - regression_loss: 2.5368 - classification_lo 1153/10000 [==>...........................] - ETA: 1:17:01 - loss: 3.4990 - regression_loss: 2.5370 - classification_lo 1154/10000 [==>...........................] - ETA: 1:17:00 - loss: 3.4989 - regression_loss: 2.5370 - classification_lo 1155/10000 [==>...........................] - ETA: 1:16:59 - loss: 3.4984 - regression_loss: 2.5366 - classification_lo 1156/10000 [==>...........................] - ETA: 1:16:58 - loss: 3.4976 - regression_loss: 2.5360 - classification_lo 1157/10000 [==>...........................] - ETA: 1:17:01 - loss: 3.4969 - regression_loss: 2.5353 - classification_lo 1158/10000 [==>...........................] - ETA: 1:17:00 - loss: 3.4967 - regression_loss: 2.5353 - classification_lo 1159/10000 [==>...........................] - ETA: 1:16:59 - loss: 3.4959 - regression_loss: 2.5350 - classification_lo 1160/10000 [==>...........................] - ETA: 1:16:58 - loss: 3.4957 - regression_loss: 2.5349 - classification_lo 1161/10000 [==>...........................] - ETA: 1:16:57 - loss: 3.4945 - regression_loss: 2.5339 - classification_lo 1162/10000 [==>...........................] - ETA: 1:16:56 - loss: 3.4957 - regression_loss: 2.5349 - classification_lo 1163/10000 [==>...........................] - ETA: 1:16:55 - loss: 3.4962 - regression_loss: 2.5354 - classification_lo 1164/10000 [==>...........................] - ETA: 1:16:54 - loss: 3.4956 - regression_loss: 2.5351 - classification_lo 1165/10000 [==>...........................] - ETA: 1:16:52 - loss: 3.4954 - regression_loss: 2.5350 - classification_lo 1166/10000 [==>...........................] - ETA: 1:16:51 - loss: 3.4944 - regression_loss: 2.5342 - classification_lo 1167/10000 [==>...........................] - ETA: 1:16:50 - loss: 3.4945 - regression_loss: 2.5344 - classification_lo 1168/10000 [==>...........................] - ETA: 1:16:49 - loss: 3.4942 - regression_loss: 2.5344 - classification_lo 1169/10000 [==>...........................] - ETA: 1:16:54 - loss: 3.4942 - regression_loss: 2.5345 - classification_lo 1170/10000 [==>...........................] - ETA: 1:16:53 - loss: 3.4945 - regression_loss: 2.5347 - classification_lo 1171/10000 [==>...........................] - ETA: 1:16:52 - loss: 3.4941 - regression_loss: 2.5346 - classification_lo 1172/10000 [==>...........................] - ETA: 1:16:51 - loss: 3.4942 - regression_loss: 2.5346 - classification_lo 1173/10000 [==>...........................] - ETA: 1:16:50 - loss: 3.4934 - regression_loss: 2.5342 - classification_lo 1174/10000 [==>...........................] - ETA: 1:16:49 - loss: 3.4928 - regression_loss: 2.5339 - classification_lo 1175/10000 [==>...........................] - ETA: 1:16:48 - loss: 3.4922 - regression_loss: 2.5335 - classification_lo 1176/10000 [==>...........................] - ETA: 1:16:47 - loss: 3.4920 - regression_loss: 2.5336 - classification_lo 1177/10000 [==>...........................] - ETA: 1:16:46 - loss: 3.4917 - regression_loss: 2.5335 - classification_lo 1178/10000 [==>...........................] - ETA: 1:16:45 - loss: 3.4911 - regression_loss: 2.5331 - classification_lo 1179/10000 [==>...........................] - ETA: 1:16:44 - loss: 3.4902 - regression_loss: 2.5322 - classification_lo 1180/10000 [==>...........................] - ETA: 1:16:44 - loss: 3.4900 - regression_loss: 2.5322 - classification_lo 1181/10000 [==>...........................] - ETA: 1:16:43 - loss: 3.4906 - regression_loss: 2.5326 - classification_lo 1182/10000 [==>...........................] - ETA: 1:16:43 - loss: 3.4903 - regression_loss: 2.5326 - classification_lo 1183/10000 [==>...........................] - ETA: 1:16:42 - loss: 3.4901 - regression_loss: 2.5325 - classification_lo 1184/10000 [==>...........................] - ETA: 1:16:41 - loss: 3.4902 - regression_loss: 2.5328 - classification_lo 1185/10000 [==>...........................] - ETA: 1:16:40 - loss: 3.4896 - regression_loss: 2.5324 - classification_lo 1186/10000 [==>...........................] - ETA: 1:16:39 - loss: 3.4892 - regression_loss: 2.5322 - classification_lo 1187/10000 [==>...........................] - ETA: 1:16:41 - loss: 3.4889 - regression_loss: 2.5320 - classification_lo 1188/10000 [==>...........................] - ETA: 1:16:40 - loss: 3.4887 - regression_loss: 2.5320 - classification_lo 1189/10000 [==>...........................] - ETA: 1:16:39 - loss: 3.4902 - regression_loss: 2.5332 - classification_lo 1190/10000 [==>...........................] - ETA: 1:16:38 - loss: 3.4898 - regression_loss: 2.5331 - classification_lo 1191/10000 [==>...........................] - ETA: 1:16:37 - loss: 3.4889 - regression_loss: 2.5322 - classification_lo 1192/10000 [==>...........................] - ETA: 1:16:36 - loss: 3.4884 - regression_loss: 2.5318 - classification_lo 1193/10000 [==>...........................] - ETA: 1:16:35 - loss: 3.4899 - regression_loss: 2.5330 - classification_lo 1194/10000 [==>...........................] - ETA: 1:16:34 - loss: 3.4899 - regression_loss: 2.5330 - classification_lo 1195/10000 [==>...........................] - ETA: 1:16:33 - loss: 3.4898 - regression_loss: 2.5329 - classification_lo 1196/10000 [==>...........................] - ETA: 1:16:32 - loss: 3.4894 - regression_loss: 2.5326 - classification_lo 1197/10000 [==>...........................] - ETA: 1:16:31 - loss: 3.4892 - regression_loss: 2.5324 - classification_lo 1198/10000 [==>...........................] - ETA: 1:16:30 - loss: 3.4890 - regression_loss: 2.5323 - classification_lo 1199/10000 [==>...........................] - ETA: 1:16:29 - loss: 3.4887 - regression_loss: 2.5322 - classification_lo 1200/10000 [==>...........................] - ETA: 1:16:28 - loss: 3.4882 - regression_loss: 2.5317 - classification_lo 1201/10000 [==>...........................] - ETA: 1:16:27 - loss: 3.4882 - regression_loss: 2.5318 - classification_lo 1202/10000 [==>...........................] - ETA: 1:16:26 - loss: 3.4902 - regression_loss: 2.5330 - classification_lo 1203/10000 [==>...........................] - ETA: 1:16:26 - loss: 3.4900 - regression_loss: 2.5328 - classification_lo 1204/10000 [==>...........................] - ETA: 1:16:25 - loss: 3.4901 - regression_loss: 2.5329 - classification_lo 1205/10000 [==>...........................] - ETA: 1:16:24 - loss: 3.4894 - regression_loss: 2.5323 - classification_lo 1206/10000 [==>...........................] - ETA: 1:16:23 - loss: 3.4885 - regression_loss: 2.5317 - classification_lo 1207/10000 [==>...........................] - ETA: 1:16:22 - loss: 3.4879 - regression_loss: 2.5312 - classification_lo 1208/10000 [==>...........................] - ETA: 1:16:22 - loss: 3.4872 - regression_loss: 2.5308 - classification_lo 1209/10000 [==>...........................] - ETA: 1:16:21 - loss: 3.4867 - regression_loss: 2.5304 - classification_lo 1210/10000 [==>...........................] - ETA: 1:16:20 - loss: 3.4865 - regression_loss: 2.5304 - classification_lo 1211/10000 [==>...........................] - ETA: 1:16:19 - loss: 3.4853 - regression_loss: 2.5296 - classification_lo 1212/10000 [==>...........................] - ETA: 1:16:17 - loss: 3.4841 - regression_loss: 2.5288 - classification_lo 1213/10000 [==>...........................] - ETA: 1:16:16 - loss: 3.4840 - regression_loss: 2.5289 - classification_lo 1214/10000 [==>...........................] - ETA: 1:16:15 - loss: 3.4846 - regression_loss: 2.5293 - classification_lo 1215/10000 [==>...........................] - ETA: 1:16:18 - loss: 3.4845 - regression_loss: 2.5293 - classification_lo 1216/10000 [==>...........................] - ETA: 1:16:17 - loss: 3.4839 - regression_loss: 2.5287 - classification_lo 1217/10000 [==>...........................] - ETA: 1:16:17 - loss: 3.4837 - regression_loss: 2.5286 - classification_lo 1218/10000 [==>...........................] - ETA: 1:16:16 - loss: 3.4835 - regression_loss: 2.5286 - classification_lo 1219/10000 [==>...........................] - ETA: 1:16:17 - loss: 3.4821 - regression_loss: 2.5265 - classification_lo 1220/10000 [==>...........................] - ETA: 1:16:16 - loss: 3.4816 - regression_loss: 2.5262 - classification_lo 1221/10000 [==>...........................] - ETA: 1:16:15 - loss: 3.4812 - regression_loss: 2.5262 - classification_lo 1222/10000 [==>...........................] - ETA: 1:16:14 - loss: 3.4814 - regression_loss: 2.5265 - classification_lo 1223/10000 [==>...........................] - ETA: 1:16:13 - loss: 3.4815 - regression_loss: 2.5266 - classification_lo 1224/10000 [==>...........................] - ETA: 1:16:12 - loss: 3.4815 - regression_loss: 2.5265 - classification_lo 1225/10000 [==>...........................] - ETA: 1:16:11 - loss: 3.4805 - regression_loss: 2.5255 - classification_lo 1226/10000 [==>...........................] - ETA: 1:16:10 - loss: 3.4800 - regression_loss: 2.5254 - classification_lo 1227/10000 [==>...........................] - ETA: 1:16:09 - loss: 3.4798 - regression_loss: 2.5252 - classification_lo 1228/10000 [==>...........................] - ETA: 1:16:08 - loss: 3.4795 - regression_loss: 2.5251 - classification_lo 1229/10000 [==>...........................] - ETA: 1:16:08 - loss: 3.4793 - regression_loss: 2.5250 - classification_lo 1230/10000 [==>...........................] - ETA: 1:16:07 - loss: 3.4790 - regression_loss: 2.5250 - classification_lo 1231/10000 [==>...........................] - ETA: 1:16:06 - loss: 3.4785 - regression_loss: 2.5246 - classification_lo 1232/10000 [==>...........................] - ETA: 1:16:05 - loss: 3.4783 - regression_loss: 2.5248 - classification_lo 1233/10000 [==>...........................] - ETA: 1:16:04 - loss: 3.4787 - regression_loss: 2.5251 - classification_lo 1234/10000 [==>...........................] - ETA: 1:16:03 - loss: 3.4783 - regression_loss: 2.5249 - classification_lo 1235/10000 [==>...........................] - ETA: 1:16:02 - loss: 3.4780 - regression_loss: 2.5247 - classification_lo 1236/10000 [==>...........................] - ETA: 1:16:02 - loss: 3.4790 - regression_loss: 2.5256 - classification_lo 1237/10000 [==>...........................] - ETA: 1:16:01 - loss: 3.4786 - regression_loss: 2.5255 - classification_lo 1238/10000 [==>...........................] - ETA: 1:16:00 - loss: 3.4785 - regression_loss: 2.5256 - classification_lo 1239/10000 [==>...........................] - ETA: 1:15:59 - loss: 3.4785 - regression_loss: 2.5257 - classification_lo 1240/10000 [==>...........................] - ETA: 1:15:58 - loss: 3.4780 - regression_loss: 2.5254 - classification_lo 1241/10000 [==>...........................] - ETA: 1:15:57 - loss: 3.4776 - regression_loss: 2.5254 - classification_lo 1242/10000 [==>...........................] - ETA: 1:15:56 - loss: 3.4773 - regression_loss: 2.5250 - classification_lo 1243/10000 [==>...........................] - ETA: 1:15:57 - loss: 3.4775 - regression_loss: 2.5252 - classification_lo 1244/10000 [==>...........................] - ETA: 1:15:56 - loss: 3.4774 - regression_loss: 2.5252 - classification_lo 1245/10000 [==>...........................] - ETA: 1:15:56 - loss: 3.4773 - regression_loss: 2.5250 - classification_lo 1246/10000 [==>...........................] - ETA: 1:15:54 - loss: 3.4767 - regression_loss: 2.5246 - classification_lo 1247/10000 [==>...........................] - ETA: 1:15:53 - loss: 3.4757 - regression_loss: 2.5237 - classification_lo 1248/10000 [==>...........................] - ETA: 1:15:55 - loss: 3.4747 - regression_loss: 2.5231 - classification_lo 1249/10000 [==>...........................] - ETA: 1:15:54 - loss: 3.4739 - regression_loss: 2.5224 - classification_lo 1250/10000 [==>...........................] - ETA: 1:15:53 - loss: 3.4738 - regression_loss: 2.5224 - classification_lo 1251/10000 [==>...........................] - ETA: 1:15:52 - loss: 3.4730 - regression_loss: 2.5219 - classification_lo 1252/10000 [==>...........................] - ETA: 1:15:51 - loss: 3.4726 - regression_loss: 2.5215 - classification_lo 1253/10000 [==>...........................] - ETA: 1:15:50 - loss: 3.4716 - regression_loss: 2.5207 - classification_lo 1254/10000 [==>...........................] - ETA: 1:15:49 - loss: 3.4717 - regression_loss: 2.5209 - classification_lo 1255/10000 [==>...........................] - ETA: 1:15:48 - loss: 3.4717 - regression_loss: 2.5209 - classification_lo 1256/10000 [==>...........................] - ETA: 1:15:47 - loss: 3.4710 - regression_loss: 2.5205 - classification_lo 1257/10000 [==>...........................] - ETA: 1:15:49 - loss: 3.4708 - regression_loss: 2.5204 - classification_lo 1258/10000 [==>...........................] - ETA: 1:15:47 - loss: 3.4703 - regression_loss: 2.5200 - classification_lo 1259/10000 [==>...........................] - ETA: 1:15:47 - loss: 3.4694 - regression_loss: 2.5193 - classification_lo 1260/10000 [==>...........................] - ETA: 1:15:46 - loss: 3.4691 - regression_loss: 2.5192 - classification_lo 1261/10000 [==>...........................] - ETA: 1:15:45 - loss: 3.4682 - regression_loss: 2.5185 - classification_lo 1262/10000 [==>...........................] - ETA: 1:15:44 - loss: 3.4681 - regression_loss: 2.5185 - classification_lo 1263/10000 [==>...........................] - ETA: 1:15:43 - loss: 3.4671 - regression_loss: 2.5179 - classification_lo 1264/10000 [==>...........................] - ETA: 1:15:42 - loss: 3.4669 - regression_loss: 2.5178 - classification_lo 1265/10000 [==>...........................] - ETA: 1:15:42 - loss: 3.4666 - regression_loss: 2.5177 - classification_lo 1266/10000 [==>...........................] - ETA: 1:15:41 - loss: 3.4669 - regression_loss: 2.5182 - classification_lo 1267/10000 [==>...........................] - ETA: 1:15:43 - loss: 3.4665 - regression_loss: 2.5179 - classification_lo 1268/10000 [==>...........................] - ETA: 1:15:43 - loss: 3.4659 - regression_loss: 2.5177 - classification_lo 1269/10000 [==>...........................] - ETA: 1:15:41 - loss: 3.4656 - regression_loss: 2.5176 - classification_lo 1270/10000 [==>...........................] - ETA: 1:15:41 - loss: 3.4650 - regression_loss: 2.5173 - classification_lo 1271/10000 [==>...........................] - ETA: 1:15:40 - loss: 3.4645 - regression_loss: 2.5171 - classification_lo 1272/10000 [==>...........................] - ETA: 1:15:39 - loss: 3.4638 - regression_loss: 2.5165 - classification_lo 1273/10000 [==>...........................] - ETA: 1:15:38 - loss: 3.4634 - regression_loss: 2.5162 - classification_lo 1274/10000 [==>...........................] - ETA: 1:15:37 - loss: 3.4629 - regression_loss: 2.5159 - classification_lo 1275/10000 [==>...........................] - ETA: 1:15:36 - loss: 3.4625 - regression_loss: 2.5157 - classification_lo 1276/10000 [==>...........................] - ETA: 1:15:35 - loss: 3.4625 - regression_loss: 2.5159 - classification_lo 1277/10000 [==>...........................] - ETA: 1:15:34 - loss: 3.4619 - regression_loss: 2.5155 - classification_lo 1278/10000 [==>...........................] - ETA: 1:15:32 - loss: 3.4614 - regression_loss: 2.5154 - classification_lo 1279/10000 [==>...........................] - ETA: 1:15:32 - loss: 3.4608 - regression_loss: 2.5149 - classification_lo 1280/10000 [==>...........................] - ETA: 1:15:32 - loss: 3.4605 - regression_loss: 2.5147 - classification_lo 1281/10000 [==>...........................] - ETA: 1:15:32 - loss: 3.4599 - regression_loss: 2.5144 - classification_lo 1282/10000 [==>...........................] - ETA: 1:15:31 - loss: 3.4594 - regression_loss: 2.5141 - classification_lo 1283/10000 [==>...........................] - ETA: 1:15:30 - loss: 3.4590 - regression_loss: 2.5137 - classification_lo 1284/10000 [==>...........................] - ETA: 1:15:29 - loss: 3.4587 - regression_loss: 2.5136 - classification_lo 1285/10000 [==>...........................] - ETA: 1:15:28 - loss: 3.4586 - regression_loss: 2.5136 - classification_lo 1286/10000 [==>...........................] - ETA: 1:15:27 - loss: 3.4585 - regression_loss: 2.5134 - classification_lo 1287/10000 [==>...........................] - ETA: 1:15:26 - loss: 3.4584 - regression_loss: 2.5133 - classification_lo 1288/10000 [==>...........................] - ETA: 1:15:26 - loss: 3.4575 - regression_loss: 2.5126 - classification_lo 1289/10000 [==>...........................] - ETA: 1:15:25 - loss: 3.4574 - regression_loss: 2.5126 - classification_lo 1290/10000 [==>...........................] - ETA: 1:15:24 - loss: 3.4568 - regression_loss: 2.5121 - classification_lo 1291/10000 [==>...........................] - ETA: 1:15:23 - loss: 3.4565 - regression_loss: 2.5119 - classification_lo 1292/10000 [==>...........................] - ETA: 1:15:22 - loss: 3.4565 - regression_loss: 2.5119 - classification_lo 1293/10000 [==>...........................] - ETA: 1:15:21 - loss: 3.4564 - regression_loss: 2.5120 - classification_lo 1294/10000 [==>...........................] - ETA: 1:15:20 - loss: 3.4555 - regression_loss: 2.5114 - classification_lo 1295/10000 [==>...........................] - ETA: 1:15:19 - loss: 3.4582 - regression_loss: 2.5126 - classification_lo 1296/10000 [==>...........................] - ETA: 1:15:18 - loss: 3.4577 - regression_loss: 2.5122 - classification_lo 1297/10000 [==>...........................] - ETA: 1:15:17 - loss: 3.4571 - regression_loss: 2.5117 - classification_lo 1298/10000 [==>...........................] - ETA: 1:15:16 - loss: 3.4558 - regression_loss: 2.5108 - classification_lo 1299/10000 [==>...........................] - ETA: 1:15:15 - loss: 3.4557 - regression_loss: 2.5106 - classification_lo 1300/10000 [==>...........................] - ETA: 1:15:15 - loss: 3.4550 - regression_loss: 2.5101 - classification_lo 1301/10000 [==>...........................] - ETA: 1:15:14 - loss: 3.4549 - regression_loss: 2.5102 - classification_lo 1302/10000 [==>...........................] - ETA: 1:15:13 - loss: 3.4546 - regression_loss: 2.5099 - classification_lo 1303/10000 [==>...........................] - ETA: 1:15:12 - loss: 3.4544 - regression_loss: 2.5099 - classification_lo 1304/10000 [==>...........................] - ETA: 1:15:16 - loss: 3.4538 - regression_loss: 2.5095 - classification_lo 1305/10000 [==>...........................] - ETA: 1:15:16 - loss: 3.4536 - regression_loss: 2.5093 - classification_lo 1306/10000 [==>...........................] - ETA: 1:15:15 - loss: 3.4533 - regression_loss: 2.5091 - classification_lo 1307/10000 [==>...........................] - ETA: 1:15:14 - loss: 3.4530 - regression_loss: 2.5089 - classification_lo 1308/10000 [==>...........................] - ETA: 1:15:13 - loss: 3.4533 - regression_loss: 2.5092 - classification_lo 1309/10000 [==>...........................] - ETA: 1:15:12 - loss: 3.4531 - regression_loss: 2.5091 - classification_lo 1310/10000 [==>...........................] - ETA: 1:15:11 - loss: 3.4524 - regression_loss: 2.5087 - classification_lo 1311/10000 [==>...........................] - ETA: 1:15:13 - loss: 3.4519 - regression_loss: 2.5082 - classification_lo 1312/10000 [==>...........................] - ETA: 1:15:13 - loss: 3.4511 - regression_loss: 2.5077 - classification_lo 1313/10000 [==>...........................] - ETA: 1:15:12 - loss: 3.4512 - regression_loss: 2.5079 - classification_lo 1314/10000 [==>...........................] - ETA: 1:15:11 - loss: 3.4512 - regression_loss: 2.5078 - classification_lo 1315/10000 [==>...........................] - ETA: 1:15:10 - loss: 3.4509 - regression_loss: 2.5079 - classification_lo 1316/10000 [==>...........................] - ETA: 1:15:09 - loss: 3.4507 - regression_loss: 2.5078 - classification_lo 1317/10000 [==>...........................] - ETA: 1:15:08 - loss: 3.4501 - regression_loss: 2.5076 - classification_lo 1318/10000 [==>...........................] - ETA: 1:15:08 - loss: 3.4499 - regression_loss: 2.5075 - classification_lo 1319/10000 [==>...........................] - ETA: 1:15:07 - loss: 3.4494 - regression_loss: 2.5073 - classification_lo 1320/10000 [==>...........................] - ETA: 1:15:06 - loss: 3.4486 - regression_loss: 2.5066 - classification_lo 1321/10000 [==>...........................] - ETA: 1:15:05 - loss: 3.4482 - regression_loss: 2.5063 - classification_lo 1322/10000 [==>...........................] - ETA: 1:15:07 - loss: 3.4473 - regression_loss: 2.5055 - classification_lo 1323/10000 [==>...........................] - ETA: 1:15:06 - loss: 3.4465 - regression_loss: 2.5050 - classification_lo 1324/10000 [==>...........................] - ETA: 1:15:05 - loss: 3.4455 - regression_loss: 2.5043 - classification_lo 1325/10000 [==>...........................] - ETA: 1:15:04 - loss: 3.4447 - regression_loss: 2.5038 - classification_lo 1326/10000 [==>...........................] - ETA: 1:15:03 - loss: 3.4441 - regression_loss: 2.5032 - classification_lo 1327/10000 [==>...........................] - ETA: 1:15:02 - loss: 3.4440 - regression_loss: 2.5033 - classification_lo 1328/10000 [==>...........................] - ETA: 1:15:02 - loss: 3.4443 - regression_loss: 2.5034 - classification_lo 1329/10000 [==>...........................] - ETA: 1:15:04 - loss: 3.4442 - regression_loss: 2.5036 - classification_lo 1330/10000 [==>...........................] - ETA: 1:15:03 - loss: 3.4443 - regression_loss: 2.5038 - classification_lo 1331/10000 [==>...........................] - ETA: 1:15:02 - loss: 3.4444 - regression_loss: 2.5039 - classification_lo 1332/10000 [==>...........................] - ETA: 1:15:01 - loss: 3.4438 - regression_loss: 2.5035 - classification_lo 1333/10000 [==>...........................] - ETA: 1:15:00 - loss: 3.4432 - regression_loss: 2.5031 - classification_lo 1334/10000 [===>..........................] - ETA: 1:14:59 - loss: 3.4421 - regression_loss: 2.5022 - classification_lo 1335/10000 [===>..........................] - ETA: 1:14:58 - loss: 3.4428 - regression_loss: 2.5003 - classification_lo 1336/10000 [===>..........................] - ETA: 1:14:57 - loss: 3.4431 - regression_loss: 2.5007 - classification_lo 1337/10000 [===>..........................] - ETA: 1:14:56 - loss: 3.4426 - regression_loss: 2.5002 - classification_lo 1338/10000 [===>..........................] - ETA: 1:14:56 - loss: 3.4424 - regression_loss: 2.5002 - classification_lo 1339/10000 [===>..........................] - ETA: 1:14:55 - loss: 3.4428 - regression_loss: 2.5006 - classification_lo 1340/10000 [===>..........................] - ETA: 1:14:55 - loss: 3.4425 - regression_loss: 2.5005 - classification_lo 1341/10000 [===>..........................] - ETA: 1:14:54 - loss: 3.4428 - regression_loss: 2.5008 - classification_lo 1342/10000 [===>..........................] - ETA: 1:14:53 - loss: 3.4424 - regression_loss: 2.5007 - classification_lo 1343/10000 [===>..........................] - ETA: 1:14:52 - loss: 3.4423 - regression_loss: 2.5008 - classification_lo 1344/10000 [===>..........................] - ETA: 1:14:52 - loss: 3.4422 - regression_loss: 2.5009 - classification_lo 1345/10000 [===>..........................] - ETA: 1:14:51 - loss: 3.4421 - regression_loss: 2.5007 - classification_lo 1346/10000 [===>..........................] - ETA: 1:14:50 - loss: 3.4417 - regression_loss: 2.5005 - classification_lo 1347/10000 [===>..........................] - ETA: 1:14:49 - loss: 3.4411 - regression_loss: 2.5000 - classification_lo 1348/10000 [===>..........................] - ETA: 1:14:48 - loss: 3.4409 - regression_loss: 2.5000 - classification_lo 1349/10000 [===>..........................] - ETA: 1:14:47 - loss: 3.4406 - regression_loss: 2.4999 - classification_lo 1350/10000 [===>..........................] - ETA: 1:14:49 - loss: 3.4404 - regression_loss: 2.5000 - classification_lo 1351/10000 [===>..........................] - ETA: 1:14:48 - loss: 3.4402 - regression_loss: 2.4998 - classification_lo 1352/10000 [===>..........................] - ETA: 1:14:47 - loss: 3.4395 - regression_loss: 2.4995 - classification_lo 1353/10000 [===>..........................] - ETA: 1:14:46 - loss: 3.4395 - regression_loss: 2.4997 - classification_lo 1354/10000 [===>..........................] - ETA: 1:14:45 - loss: 3.4393 - regression_loss: 2.4997 - classification_lo 1355/10000 [===>..........................] - ETA: 1:14:44 - loss: 3.4384 - regression_loss: 2.4988 - classification_lo 1356/10000 [===>..........................] - ETA: 1:14:44 - loss: 3.4381 - regression_loss: 2.4986 - classification_lo 1357/10000 [===>..........................] - ETA: 1:14:43 - loss: 3.4379 - regression_loss: 2.4987 - classification_lo 1358/10000 [===>..........................] - ETA: 1:14:42 - loss: 3.4377 - regression_loss: 2.4988 - classification_lo 1359/10000 [===>..........................] - ETA: 1:14:41 - loss: 3.4373 - regression_loss: 2.4987 - classification_lo 1360/10000 [===>..........................] - ETA: 1:14:40 - loss: 3.4375 - regression_loss: 2.4988 - classification_lo 1361/10000 [===>..........................] - ETA: 1:14:39 - loss: 3.4371 - regression_loss: 2.4987 - classification_lo 1362/10000 [===>..........................] - ETA: 1:14:38 - loss: 3.4368 - regression_loss: 2.4983 - classification_lo 1363/10000 [===>..........................] - ETA: 1:14:37 - loss: 3.4366 - regression_loss: 2.4984 - classification_lo 1364/10000 [===>..........................] - ETA: 1:14:36 - loss: 3.4363 - regression_loss: 2.4983 - classification_lo 1365/10000 [===>..........................] - ETA: 1:14:35 - loss: 3.4369 - regression_loss: 2.4985 - classification_lo 1366/10000 [===>..........................] - ETA: 1:14:34 - loss: 3.4362 - regression_loss: 2.4980 - classification_lo 1367/10000 [===>..........................] - ETA: 1:14:34 - loss: 3.4351 - regression_loss: 2.4972 - classification_lo 1368/10000 [===>..........................] - ETA: 1:14:33 - loss: 3.4350 - regression_loss: 2.4972 - classification_lo 1369/10000 [===>..........................] - ETA: 1:14:32 - loss: 3.4344 - regression_loss: 2.4969 - classification_lo 1370/10000 [===>..........................] - ETA: 1:14:31 - loss: 3.4338 - regression_loss: 2.4966 - classification_lo 1371/10000 [===>..........................] - ETA: 1:14:34 - loss: 3.4332 - regression_loss: 2.4963 - classification_lo 1372/10000 [===>..........................] - ETA: 1:14:33 - loss: 3.4337 - regression_loss: 2.4970 - classification_lo 1373/10000 [===>..........................] - ETA: 1:14:32 - loss: 3.4335 - regression_loss: 2.4970 - classification_lo 1374/10000 [===>..........................] - ETA: 1:14:31 - loss: 3.4331 - regression_loss: 2.4967 - classification_lo 1375/10000 [===>..........................] - ETA: 1:14:31 - loss: 3.4326 - regression_loss: 2.4965 - classification_lo 1376/10000 [===>..........................] - ETA: 1:14:30 - loss: 3.4316 - regression_loss: 2.4957 - classification_lo 1377/10000 [===>..........................] - ETA: 1:14:29 - loss: 3.4309 - regression_loss: 2.4953 - classification_lo 1378/10000 [===>..........................] - ETA: 1:14:28 - loss: 3.4312 - regression_loss: 2.4955 - classification_lo 1379/10000 [===>..........................] - ETA: 1:14:27 - loss: 3.4307 - regression_loss: 2.4951 - classification_lo 1380/10000 [===>..........................] - ETA: 1:14:26 - loss: 3.4307 - regression_loss: 2.4953 - classification_lo 1381/10000 [===>..........................] - ETA: 1:14:25 - loss: 3.4304 - regression_loss: 2.4950 - classification_lo 1382/10000 [===>..........................] - ETA: 1:14:25 - loss: 3.4297 - regression_loss: 2.4944 - classification_lo 1383/10000 [===>..........................] - ETA: 1:14:24 - loss: 3.4287 - regression_loss: 2.4937 - classification_lo 1384/10000 [===>..........................] - ETA: 1:14:23 - loss: 3.4283 - regression_loss: 2.4934 - classification_lo 1385/10000 [===>..........................] - ETA: 1:14:22 - loss: 3.4281 - regression_loss: 2.4932 - classification_lo 1386/10000 [===>..........................] - ETA: 1:14:21 - loss: 3.4277 - regression_loss: 2.4930 - classification_lo 1387/10000 [===>..........................] - ETA: 1:14:20 - loss: 3.4275 - regression_loss: 2.4929 - classification_lo 1388/10000 [===>..........................] - ETA: 1:14:19 - loss: 3.4273 - regression_loss: 2.4928 - classification_lo 1389/10000 [===>..........................] - ETA: 1:14:18 - loss: 3.4267 - regression_loss: 2.4924 - classification_lo 1390/10000 [===>..........................] - ETA: 1:14:18 - loss: 3.4267 - regression_loss: 2.4924 - classification_lo 1391/10000 [===>..........................] - ETA: 1:14:17 - loss: 3.4264 - regression_loss: 2.4923 - classification_lo 1392/10000 [===>..........................] - ETA: 1:14:16 - loss: 3.4263 - regression_loss: 2.4925 - classification_lo 1393/10000 [===>..........................] - ETA: 1:14:15 - loss: 3.4262 - regression_loss: 2.4925 - classification_lo 1394/10000 [===>..........................] - ETA: 1:14:14 - loss: 3.4250 - regression_loss: 2.4918 - classification_lo 1395/10000 [===>..........................] - ETA: 1:14:13 - loss: 3.4250 - regression_loss: 2.4919 - classification_lo 1396/10000 [===>..........................] - ETA: 1:14:12 - loss: 3.4251 - regression_loss: 2.4920 - classification_lo 1397/10000 [===>..........................] - ETA: 1:14:12 - loss: 3.4250 - regression_loss: 2.4919 - classification_lo 1398/10000 [===>..........................] - ETA: 1:14:11 - loss: 3.4250 - regression_loss: 2.4918 - classification_lo 1399/10000 [===>..........................] - ETA: 1:14:10 - loss: 3.4245 - regression_loss: 2.4916 - classification_lo 1400/10000 [===>..........................] - ETA: 1:14:09 - loss: 3.4244 - regression_loss: 2.4915 - classification_lo 1401/10000 [===>..........................] - ETA: 1:14:13 - loss: 3.4239 - regression_loss: 2.4912 - classification_lo 1402/10000 [===>..........................] - ETA: 1:14:12 - loss: 3.4234 - regression_loss: 2.4908 - classification_lo 1403/10000 [===>..........................] - ETA: 1:14:11 - loss: 3.4235 - regression_loss: 2.4908 - classification_lo 1404/10000 [===>..........................] - ETA: 1:14:09 - loss: 3.4232 - regression_loss: 2.4907 - classification_lo 1405/10000 [===>..........................] - ETA: 1:14:09 - loss: 3.4227 - regression_loss: 2.4902 - classification_lo 1406/10000 [===>..........................] - ETA: 1:14:08 - loss: 3.4221 - regression_loss: 2.4898 - classification_lo 1407/10000 [===>..........................] - ETA: 1:14:07 - loss: 3.4216 - regression_loss: 2.4896 - classification_lo 1408/10000 [===>..........................] - ETA: 1:14:06 - loss: 3.4216 - regression_loss: 2.4897 - classification_lo 1409/10000 [===>..........................] - ETA: 1:14:05 - loss: 3.4215 - regression_loss: 2.4897 - classification_lo 1410/10000 [===>..........................] - ETA: 1:14:05 - loss: 3.4215 - regression_loss: 2.4897 - classification_lo 1411/10000 [===>..........................] - ETA: 1:14:04 - loss: 3.4224 - regression_loss: 2.4898 - classification_lo 1412/10000 [===>..........................] - ETA: 1:14:03 - loss: 3.4218 - regression_loss: 2.4893 - classification_lo 1413/10000 [===>..........................] - ETA: 1:14:02 - loss: 3.4209 - regression_loss: 2.4887 - classification_lo 1414/10000 [===>..........................] - ETA: 1:14:01 - loss: 3.4206 - regression_loss: 2.4886 - classification_lo 1415/10000 [===>..........................] - ETA: 1:14:03 - loss: 3.4205 - regression_loss: 2.4885 - classification_lo 1416/10000 [===>..........................] - ETA: 1:14:03 - loss: 3.4203 - regression_loss: 2.4884 - classification_lo 1417/10000 [===>..........................] - ETA: 1:14:02 - loss: 3.4202 - regression_loss: 2.4882 - classification_lo 1418/10000 [===>..........................] - ETA: 1:14:01 - loss: 3.4199 - regression_loss: 2.4880 - classification_lo 1419/10000 [===>..........................] - ETA: 1:14:00 - loss: 3.4196 - regression_loss: 2.4879 - classification_lo 1420/10000 [===>..........................] - ETA: 1:13:59 - loss: 3.4198 - regression_loss: 2.4880 - classification_lo 1421/10000 [===>..........................] - ETA: 1:13:58 - loss: 3.4196 - regression_loss: 2.4881 - classification_lo 1422/10000 [===>..........................] - ETA: 1:13:58 - loss: 3.4185 - regression_loss: 2.4873 - classification_lo 1423/10000 [===>..........................] - ETA: 1:13:57 - loss: 3.4184 - regression_loss: 2.4872 - classification_lo 1424/10000 [===>..........................] - ETA: 1:13:56 - loss: 3.4180 - regression_loss: 2.4870 - classification_lo 1425/10000 [===>..........................] - ETA: 1:13:55 - loss: 3.4177 - regression_loss: 2.4852 - classification_lo 1426/10000 [===>..........................] - ETA: 1:13:54 - loss: 3.4170 - regression_loss: 2.4848 - classification_lo 1427/10000 [===>..........................] - ETA: 1:13:53 - loss: 3.4169 - regression_loss: 2.4848 - classification_lo 1428/10000 [===>..........................] - ETA: 1:13:53 - loss: 3.4164 - regression_loss: 2.4845 - classification_lo 1429/10000 [===>..........................] - ETA: 1:13:52 - loss: 3.4166 - regression_loss: 2.4828 - classification_lo 1430/10000 [===>..........................] - ETA: 1:13:51 - loss: 3.4160 - regression_loss: 2.4825 - classification_lo 1431/10000 [===>..........................] - ETA: 1:13:50 - loss: 3.4158 - regression_loss: 2.4823 - classification_lo 1432/10000 [===>..........................] - ETA: 1:13:49 - loss: 3.4151 - regression_loss: 2.4820 - classification_lo 1433/10000 [===>..........................] - ETA: 1:13:48 - loss: 3.4146 - regression_loss: 2.4816 - classification_lo 1434/10000 [===>..........................] - ETA: 1:13:48 - loss: 3.4139 - regression_loss: 2.4811 - classification_lo 1435/10000 [===>..........................] - ETA: 1:13:47 - loss: 3.4138 - regression_loss: 2.4810 - classification_lo 1436/10000 [===>..........................] - ETA: 1:13:46 - loss: 3.4133 - regression_loss: 2.4808 - classification_lo 1437/10000 [===>..........................] - ETA: 1:13:45 - loss: 3.4134 - regression_loss: 2.4809 - classification_lo 1438/10000 [===>..........................] - ETA: 1:13:44 - loss: 3.4132 - regression_loss: 2.4806 - classification_lo 1439/10000 [===>..........................] - ETA: 1:13:43 - loss: 3.4127 - regression_loss: 2.4802 - classification_lo 1440/10000 [===>..........................] - ETA: 1:13:42 - loss: 3.4122 - regression_loss: 2.4799 - classification_lo 1441/10000 [===>..........................] - ETA: 1:13:41 - loss: 3.4117 - regression_loss: 2.4793 - classification_lo 1442/10000 [===>..........................] - ETA: 1:13:40 - loss: 3.4115 - regression_loss: 2.4791 - classification_lo 1443/10000 [===>..........................] - ETA: 1:13:40 - loss: 3.4111 - regression_loss: 2.4789 - classification_lo 1444/10000 [===>..........................] - ETA: 1:13:39 - loss: 3.4118 - regression_loss: 2.4796 - classification_lo 1445/10000 [===>..........................] - ETA: 1:13:38 - loss: 3.4114 - regression_loss: 2.4792 - classification_lo 1446/10000 [===>..........................] - ETA: 1:13:37 - loss: 3.4104 - regression_loss: 2.4784 - classification_lo 1447/10000 [===>..........................] - ETA: 1:13:36 - loss: 3.4102 - regression_loss: 2.4782 - classification_lo 1448/10000 [===>..........................] - ETA: 1:13:35 - loss: 3.4099 - regression_loss: 2.4781 - classification_lo 1449/10000 [===>..........................] - ETA: 1:13:35 - loss: 3.4097 - regression_loss: 2.4782 - classification_lo 1450/10000 [===>..........................] - ETA: 1:13:34 - loss: 3.4090 - regression_loss: 2.4776 - classification_lo 1451/10000 [===>..........................] - ETA: 1:13:33 - loss: 3.4083 - regression_loss: 2.4772 - classification_lo 1452/10000 [===>..........................] - ETA: 1:13:32 - loss: 3.4076 - regression_loss: 2.4767 - classification_lo 1453/10000 [===>..........................] - ETA: 1:13:31 - loss: 3.4082 - regression_loss: 2.4771 - classification_lo 1454/10000 [===>..........................] - ETA: 1:13:30 - loss: 3.4074 - regression_loss: 2.4765 - classification_lo 1455/10000 [===>..........................] - ETA: 1:13:29 - loss: 3.4074 - regression_loss: 2.4767 - classification_lo 1456/10000 [===>..........................] - ETA: 1:13:29 - loss: 3.4075 - regression_loss: 2.4769 - classification_lo 1457/10000 [===>..........................] - ETA: 1:13:28 - loss: 3.4074 - regression_loss: 2.4768 - classification_lo 1458/10000 [===>..........................] - ETA: 1:13:27 - loss: 3.4071 - regression_loss: 2.4767 - classification_lo 1459/10000 [===>..........................] - ETA: 1:13:26 - loss: 3.4070 - regression_loss: 2.4768 - classification_lo 1460/10000 [===>..........................] - ETA: 1:13:26 - loss: 3.4063 - regression_loss: 2.4763 - classification_lo 1461/10000 [===>..........................] - ETA: 1:13:25 - loss: 3.4055 - regression_loss: 2.4757 - classification_lo 1462/10000 [===>..........................] - ETA: 1:13:24 - loss: 3.4049 - regression_loss: 2.4752 - classification_lo 1463/10000 [===>..........................] - ETA: 1:13:23 - loss: 3.4045 - regression_loss: 2.4749 - classification_lo 1464/10000 [===>..........................] - ETA: 1:13:23 - loss: 3.4038 - regression_loss: 2.4745 - classification_lo 1465/10000 [===>..........................] - ETA: 1:13:22 - loss: 3.4030 - regression_loss: 2.4738 - classification_lo 1466/10000 [===>..........................] - ETA: 1:13:21 - loss: 3.4024 - regression_loss: 2.4734 - classification_lo 1467/10000 [===>..........................] - ETA: 1:13:20 - loss: 3.4016 - regression_loss: 2.4729 - classification_lo 1468/10000 [===>..........................] - ETA: 1:13:20 - loss: 3.4009 - regression_loss: 2.4724 - classification_lo 1469/10000 [===>..........................] - ETA: 1:13:19 - loss: 3.4000 - regression_loss: 2.4717 - classification_lo 1470/10000 [===>..........................] - ETA: 1:13:23 - loss: 3.3997 - regression_loss: 2.4714 - classification_lo 1471/10000 [===>..........................] - ETA: 1:13:22 - loss: 3.3988 - regression_loss: 2.4707 - classification_lo 1472/10000 [===>..........................] - ETA: 1:13:21 - loss: 3.3986 - regression_loss: 2.4707 - classification_lo 1473/10000 [===>..........................] - ETA: 1:13:20 - loss: 3.3985 - regression_loss: 2.4707 - classification_lo 1474/10000 [===>..........................] - ETA: 1:13:20 - loss: 3.3973 - regression_loss: 2.4698 - classification_lo 1475/10000 [===>..........................] - ETA: 1:13:19 - loss: 3.3969 - regression_loss: 2.4697 - classification_lo 1476/10000 [===>..........................] - ETA: 1:13:18 - loss: 3.3961 - regression_loss: 2.4690 - classification_lo 1477/10000 [===>..........................] - ETA: 1:13:17 - loss: 3.3960 - regression_loss: 2.4688 - classification_lo 1478/10000 [===>..........................] - ETA: 1:13:19 - loss: 3.3956 - regression_loss: 2.4687 - classification_lo 1479/10000 [===>..........................] - ETA: 1:13:18 - loss: 3.3945 - regression_loss: 2.4678 - classification_lo 1480/10000 [===>..........................] - ETA: 1:13:17 - loss: 3.3943 - regression_loss: 2.4678 - classification_lo 1481/10000 [===>..........................] - ETA: 1:13:16 - loss: 3.3942 - regression_loss: 2.4677 - classification_lo 1482/10000 [===>..........................] - ETA: 1:13:15 - loss: 3.3934 - regression_loss: 2.4672 - classification_lo 1483/10000 [===>..........................] - ETA: 1:13:14 - loss: 3.3933 - regression_loss: 2.4672 - classification_lo 1484/10000 [===>..........................] - ETA: 1:13:13 - loss: 3.3923 - regression_loss: 2.4664 - classification_lo 1485/10000 [===>..........................] - ETA: 1:13:13 - loss: 3.3919 - regression_loss: 2.4663 - classification_lo 1486/10000 [===>..........................] - ETA: 1:13:12 - loss: 3.3912 - regression_loss: 2.4657 - classification_lo 1487/10000 [===>..........................] - ETA: 1:13:11 - loss: 3.3900 - regression_loss: 2.4648 - classification_lo 1488/10000 [===>..........................] - ETA: 1:13:10 - loss: 3.3898 - regression_loss: 2.4648 - classification_lo 1489/10000 [===>..........................] - ETA: 1:13:10 - loss: 3.3891 - regression_loss: 2.4642 - classification_lo 1490/10000 [===>..........................] - ETA: 1:13:09 - loss: 3.3889 - regression_loss: 2.4641 - classification_lo 1491/10000 [===>..........................] - ETA: 1:13:08 - loss: 3.3885 - regression_loss: 2.4635 - classification_lo 1492/10000 [===>..........................] - ETA: 1:13:07 - loss: 3.3882 - regression_loss: 2.4634 - classification_lo 1493/10000 [===>..........................] - ETA: 1:13:07 - loss: 3.3883 - regression_loss: 2.4634 - classification_lo 1494/10000 [===>..........................] - ETA: 1:13:06 - loss: 3.3880 - regression_loss: 2.4632 - classification_lo 1495/10000 [===>..........................] - ETA: 1:13:05 - loss: 3.3880 - regression_loss: 2.4633 - classification_lo 1496/10000 [===>..........................] - ETA: 1:13:04 - loss: 3.3881 - regression_loss: 2.4635 - classification_lo 1497/10000 [===>..........................] - ETA: 1:13:04 - loss: 3.3877 - regression_loss: 2.4633 - classification_lo 1498/10000 [===>..........................] - ETA: 1:13:03 - loss: 3.3874 - regression_loss: 2.4631 - classification_lo 1499/10000 [===>..........................] - ETA: 1:13:02 - loss: 3.3874 - regression_loss: 2.4632 - classification_lo 1500/10000 [===>..........................] - ETA: 1:13:01 - loss: 3.3875 - regression_loss: 2.4632 - classification_lo 1501/10000 [===>..........................] - ETA: 1:13:00 - loss: 3.3867 - regression_loss: 2.4626 - classification_lo 1502/10000 [===>..........................] - ETA: 1:12:59 - loss: 3.3866 - regression_loss: 2.4627 - classification_lo 1503/10000 [===>..........................] - ETA: 1:12:59 - loss: 3.3865 - regression_loss: 2.4626 - classification_lo 1504/10000 [===>..........................] - ETA: 1:12:58 - loss: 3.3861 - regression_loss: 2.4624 - classification_lo 1505/10000 [===>..........................] - ETA: 1:13:00 - loss: 3.3856 - regression_loss: 2.4619 - classification_lo 1506/10000 [===>..........................] - ETA: 1:12:59 - loss: 3.3853 - regression_loss: 2.4615 - classification_lo 1507/10000 [===>..........................] - ETA: 1:12:58 - loss: 3.3851 - regression_loss: 2.4615 - classification_lo 1508/10000 [===>..........................] - ETA: 1:12:57 - loss: 3.3846 - regression_loss: 2.4612 - classification_lo 1509/10000 [===>..........................] - ETA: 1:12:56 - loss: 3.3845 - regression_loss: 2.4612 - classification_lo 1510/10000 [===>..........................] - ETA: 1:12:56 - loss: 3.3850 - regression_loss: 2.4616 - classification_lo 1511/10000 [===>..........................] - ETA: 1:12:55 - loss: 3.3853 - regression_loss: 2.4618 - classification_lo 1512/10000 [===>..........................] - ETA: 1:12:54 - loss: 3.3851 - regression_loss: 2.4617 - classification_lo 1513/10000 [===>..........................] - ETA: 1:12:54 - loss: 3.3848 - regression_loss: 2.4616 - classification_lo 1514/10000 [===>..........................] - ETA: 1:12:53 - loss: 3.3844 - regression_loss: 2.4613 - classification_lo 1515/10000 [===>..........................] - ETA: 1:12:52 - loss: 3.3844 - regression_loss: 2.4615 - classification_lo 1516/10000 [===>..........................] - ETA: 1:12:51 - loss: 3.3845 - regression_loss: 2.4616 - classification_lo 1517/10000 [===>..........................] - ETA: 1:12:51 - loss: 3.3847 - regression_loss: 2.4617 - classification_lo 1518/10000 [===>..........................] - ETA: 1:12:50 - loss: 3.3842 - regression_loss: 2.4614 - classification_lo 1519/10000 [===>..........................] - ETA: 1:12:49 - loss: 3.3842 - regression_loss: 2.4614 - classification_lo 1520/10000 [===>..........................] - ETA: 1:12:48 - loss: 3.3837 - regression_loss: 2.4611 - classification_lo 1521/10000 [===>..........................] - ETA: 1:12:47 - loss: 3.3832 - regression_loss: 2.4608 - classification_lo 1522/10000 [===>..........................] - ETA: 1:12:47 - loss: 3.3825 - regression_loss: 2.4605 - classification_lo 1523/10000 [===>..........................] - ETA: 1:12:46 - loss: 3.3818 - regression_loss: 2.4601 - classification_lo 1524/10000 [===>..........................] - ETA: 1:12:45 - loss: 3.3814 - regression_loss: 2.4597 - classification_lo 1525/10000 [===>..........................] - ETA: 1:12:44 - loss: 3.3812 - regression_loss: 2.4595 - classification_lo 1526/10000 [===>..........................] - ETA: 1:12:43 - loss: 3.3809 - regression_loss: 2.4594 - classification_lo 1527/10000 [===>..........................] - ETA: 1:12:42 - loss: 3.3805 - regression_loss: 2.4591 - classification_lo 1528/10000 [===>..........................] - ETA: 1:12:41 - loss: 3.3803 - regression_loss: 2.4591 - classification_lo 1529/10000 [===>..........................] - ETA: 1:12:41 - loss: 3.3796 - regression_loss: 2.4585 - classification_lo 1530/10000 [===>..........................] - ETA: 1:12:40 - loss: 3.3789 - regression_loss: 2.4581 - classification_lo 1531/10000 [===>..........................] - ETA: 1:12:40 - loss: 3.3787 - regression_loss: 2.4578 - classification_lo 1532/10000 [===>..........................] - ETA: 1:12:39 - loss: 3.3780 - regression_loss: 2.4572 - classification_lo 1533/10000 [===>..........................] - ETA: 1:12:38 - loss: 3.3774 - regression_loss: 2.4567 - classification_lo 1534/10000 [===>..........................] - ETA: 1:12:37 - loss: 3.3773 - regression_loss: 2.4564 - classification_lo 1535/10000 [===>..........................] - ETA: 1:12:37 - loss: 3.3770 - regression_loss: 2.4565 - classification_lo 1536/10000 [===>..........................] - ETA: 1:12:36 - loss: 3.3769 - regression_loss: 2.4565 - classification_lo 1537/10000 [===>..........................] - ETA: 1:12:35 - loss: 3.3765 - regression_loss: 2.4563 - classification_lo 1538/10000 [===>..........................] - ETA: 1:12:34 - loss: 3.3762 - regression_loss: 2.4561 - classification_lo 1539/10000 [===>..........................] - ETA: 1:12:33 - loss: 3.3756 - regression_loss: 2.4557 - classification_lo 1540/10000 [===>..........................] - ETA: 1:12:33 - loss: 3.3753 - regression_loss: 2.4556 - classification_lo 1541/10000 [===>..........................] - ETA: 1:12:32 - loss: 3.3752 - regression_loss: 2.4557 - classification_lo 1542/10000 [===>..........................] - ETA: 1:12:31 - loss: 3.3752 - regression_loss: 2.4559 - classification_lo 1543/10000 [===>..........................] - ETA: 1:12:30 - loss: 3.3749 - regression_loss: 2.4558 - classification_lo 1544/10000 [===>..........................] - ETA: 1:12:29 - loss: 3.3748 - regression_loss: 2.4557 - classification_lo 1545/10000 [===>..........................] - ETA: 1:12:30 - loss: 3.3748 - regression_loss: 2.4558 - classification_lo 1546/10000 [===>..........................] - ETA: 1:12:29 - loss: 3.3745 - regression_loss: 2.4555 - classification_lo 1547/10000 [===>..........................] - ETA: 1:12:28 - loss: 3.3735 - regression_loss: 2.4547 - classification_lo 1548/10000 [===>..........................] - ETA: 1:12:28 - loss: 3.3734 - regression_loss: 2.4546 - classification_lo 1549/10000 [===>..........................] - ETA: 1:12:27 - loss: 3.3733 - regression_loss: 2.4547 - classification_lo 1550/10000 [===>..........................] - ETA: 1:12:26 - loss: 3.3730 - regression_loss: 2.4545 - classification_lo 1551/10000 [===>..........................] - ETA: 1:12:25 - loss: 3.3731 - regression_loss: 2.4548 - classification_lo 1552/10000 [===>..........................] - ETA: 1:12:24 - loss: 3.3727 - regression_loss: 2.4546 - classification_lo 1553/10000 [===>..........................] - ETA: 1:12:25 - loss: 3.3719 - regression_loss: 2.4539 - classification_lo 1554/10000 [===>..........................] - ETA: 1:12:24 - loss: 3.3714 - regression_loss: 2.4535 - classification_lo 1555/10000 [===>..........................] - ETA: 1:12:23 - loss: 3.3710 - regression_loss: 2.4533 - classification_lo 1556/10000 [===>..........................] - ETA: 1:12:23 - loss: 3.3710 - regression_loss: 2.4534 - classification_lo 1557/10000 [===>..........................] - ETA: 1:12:22 - loss: 3.3708 - regression_loss: 2.4533 - classification_lo 1558/10000 [===>..........................] - ETA: 1:12:21 - loss: 3.3705 - regression_loss: 2.4529 - classification_lo 1559/10000 [===>..........................] - ETA: 1:12:20 - loss: 3.3711 - regression_loss: 2.4534 - classification_lo 1560/10000 [===>..........................] - ETA: 1:12:19 - loss: 3.3709 - regression_loss: 2.4532 - classification_lo 1561/10000 [===>..........................] - ETA: 1:12:19 - loss: 3.3706 - regression_loss: 2.4530 - classification_lo 1562/10000 [===>..........................] - ETA: 1:12:18 - loss: 3.3700 - regression_loss: 2.4526 - classification_lo 1563/10000 [===>..........................] - ETA: 1:12:17 - loss: 3.3700 - regression_loss: 2.4526 - classification_lo 1564/10000 [===>..........................] - ETA: 1:12:16 - loss: 3.3693 - regression_loss: 2.4520 - classification_lo 1565/10000 [===>..........................] - ETA: 1:12:16 - loss: 3.3692 - regression_loss: 2.4521 - classification_lo 1566/10000 [===>..........................] - ETA: 1:12:15 - loss: 3.3693 - regression_loss: 2.4524 - classification_lo 1567/10000 [===>..........................] - ETA: 1:12:14 - loss: 3.3693 - regression_loss: 2.4524 - classification_lo 1568/10000 [===>..........................] - ETA: 1:12:14 - loss: 3.3694 - regression_loss: 2.4525 - classification_lo 1569/10000 [===>..........................] - ETA: 1:12:13 - loss: 3.3692 - regression_loss: 2.4523 - classification_lo 1570/10000 [===>..........................] - ETA: 1:12:12 - loss: 3.3682 - regression_loss: 2.4516 - classification_lo 1571/10000 [===>..........................] - ETA: 1:12:12 - loss: 3.3681 - regression_loss: 2.4518 - classification_lo 1572/10000 [===>..........................] - ETA: 1:12:11 - loss: 3.3675 - regression_loss: 2.4513 - classification_lo 1573/10000 [===>..........................] - ETA: 1:12:10 - loss: 3.3676 - regression_loss: 2.4513 - classification_lo 1574/10000 [===>..........................] - ETA: 1:12:10 - loss: 3.3674 - regression_loss: 2.4512 - classification_lo 1575/10000 [===>..........................] - ETA: 1:12:09 - loss: 3.3669 - regression_loss: 2.4509 - classification_lo 1576/10000 [===>..........................] - ETA: 1:12:08 - loss: 3.3665 - regression_loss: 2.4506 - classification_lo 1577/10000 [===>..........................] - ETA: 1:12:07 - loss: 3.3660 - regression_loss: 2.4502 - classification_lo 1578/10000 [===>..........................] - ETA: 1:12:06 - loss: 3.3653 - regression_loss: 2.4499 - classification_lo 1579/10000 [===>..........................] - ETA: 1:12:05 - loss: 3.3654 - regression_loss: 2.4500 - classification_lo 1580/10000 [===>..........................] - ETA: 1:12:05 - loss: 3.3652 - regression_loss: 2.4499 - classification_lo 1581/10000 [===>..........................] - ETA: 1:12:04 - loss: 3.3650 - regression_loss: 2.4498 - classification_lo 1582/10000 [===>..........................] - ETA: 1:12:03 - loss: 3.3646 - regression_loss: 2.4496 - classification_lo 1583/10000 [===>..........................] - ETA: 1:12:02 - loss: 3.3639 - regression_loss: 2.4491 - classification_lo 1584/10000 [===>..........................] - ETA: 1:12:01 - loss: 3.3632 - regression_loss: 2.4486 - classification_lo 1585/10000 [===>..........................] - ETA: 1:12:00 - loss: 3.3633 - regression_loss: 2.4487 - classification_lo 1586/10000 [===>..........................] - ETA: 1:12:00 - loss: 3.3631 - regression_loss: 2.4486 - classification_lo 1587/10000 [===>..........................] - ETA: 1:11:59 - loss: 3.3631 - regression_loss: 2.4487 - classification_lo 1588/10000 [===>..........................] - ETA: 1:11:58 - loss: 3.3626 - regression_loss: 2.4482 - classification_lo 1589/10000 [===>..........................] - ETA: 1:11:57 - loss: 3.3619 - regression_loss: 2.4478 - classification_lo 1590/10000 [===>..........................] - ETA: 1:11:56 - loss: 3.3616 - regression_loss: 2.4476 - classification_lo 1591/10000 [===>..........................] - ETA: 1:11:55 - loss: 3.3617 - regression_loss: 2.4478 - classification_lo 1592/10000 [===>..........................] - ETA: 1:11:55 - loss: 3.3615 - regression_loss: 2.4478 - classification_lo 1593/10000 [===>..........................] - ETA: 1:11:54 - loss: 3.3610 - regression_loss: 2.4474 - classification_lo 1594/10000 [===>..........................] - ETA: 1:11:53 - loss: 3.3603 - regression_loss: 2.4470 - classification_lo 1595/10000 [===>..........................] - ETA: 1:11:52 - loss: 3.3596 - regression_loss: 2.4462 - classification_lo 1596/10000 [===>..........................] - ETA: 1:11:51 - loss: 3.3594 - regression_loss: 2.4462 - classification_lo 1597/10000 [===>..........................] - ETA: 1:11:51 - loss: 3.3590 - regression_loss: 2.4460 - classification_lo 1598/10000 [===>..........................] - ETA: 1:11:50 - loss: 3.3588 - regression_loss: 2.4459 - classification_lo 1599/10000 [===>..........................] - ETA: 1:11:49 - loss: 3.3585 - regression_loss: 2.4456 - classification_lo 1600/10000 [===>..........................] - ETA: 1:11:48 - loss: 3.3578 - regression_loss: 2.4451 - classification_lo 1601/10000 [===>..........................] - ETA: 1:11:48 - loss: 3.3571 - regression_loss: 2.4446 - classification_lo 1602/10000 [===>..........................] - ETA: 1:11:47 - loss: 3.3567 - regression_loss: 2.4443 - classification_lo 1603/10000 [===>..........................] - ETA: 1:11:46 - loss: 3.3563 - regression_loss: 2.4442 - classification_lo 1604/10000 [===>..........................] - ETA: 1:11:46 - loss: 3.3559 - regression_loss: 2.4440 - classification_lo 1605/10000 [===>..........................] - ETA: 1:11:45 - loss: 3.3557 - regression_loss: 2.4438 - classification_lo 1606/10000 [===>..........................] - ETA: 1:11:44 - loss: 3.3554 - regression_loss: 2.4437 - classification_lo 1607/10000 [===>..........................] - ETA: 1:11:43 - loss: 3.3548 - regression_loss: 2.4432 - classification_lo 1608/10000 [===>..........................] - ETA: 1:11:43 - loss: 3.3545 - regression_loss: 2.4429 - classification_lo 1609/10000 [===>..........................] - ETA: 1:11:42 - loss: 3.3538 - regression_loss: 2.4424 - classification_lo 1610/10000 [===>..........................] - ETA: 1:11:41 - loss: 3.3534 - regression_loss: 2.4422 - classification_lo 1611/10000 [===>..........................] - ETA: 1:11:41 - loss: 3.3531 - regression_loss: 2.4420 - classification_lo 1612/10000 [===>..........................] - ETA: 1:11:40 - loss: 3.3525 - regression_loss: 2.4417 - classification_lo 1613/10000 [===>..........................] - ETA: 1:11:40 - loss: 3.3526 - regression_loss: 2.4416 - classification_lo 1614/10000 [===>..........................] - ETA: 1:11:39 - loss: 3.3521 - regression_loss: 2.4413 - classification_lo 1615/10000 [===>..........................] - ETA: 1:11:38 - loss: 3.3519 - regression_loss: 2.4412 - classification_lo 1616/10000 [===>..........................] - ETA: 1:11:38 - loss: 3.3514 - regression_loss: 2.4410 - classification_lo 1617/10000 [===>..........................] - ETA: 1:11:37 - loss: 3.3512 - regression_loss: 2.4406 - classification_lo 1618/10000 [===>..........................] - ETA: 1:11:36 - loss: 3.3512 - regression_loss: 2.4405 - classification_lo 1619/10000 [===>..........................] - ETA: 1:11:36 - loss: 3.3505 - regression_loss: 2.4399 - classification_lo 1620/10000 [===>..........................] - ETA: 1:11:35 - loss: 3.3497 - regression_loss: 2.4393 - classification_lo 1621/10000 [===>..........................] - ETA: 1:11:34 - loss: 3.3492 - regression_loss: 2.4390 - classification_lo 1622/10000 [===>..........................] - ETA: 1:11:33 - loss: 3.3486 - regression_loss: 2.4386 - classification_lo 1623/10000 [===>..........................] - ETA: 1:11:33 - loss: 3.3475 - regression_loss: 2.4377 - classification_lo 1624/10000 [===>..........................] - ETA: 1:11:32 - loss: 3.3475 - regression_loss: 2.4377 - classification_lo 1625/10000 [===>..........................] - ETA: 1:11:32 - loss: 3.3475 - regression_loss: 2.4379 - classification_lo 1626/10000 [===>..........................] - ETA: 1:11:31 - loss: 3.3471 - regression_loss: 2.4377 - classification_lo 1627/10000 [===>..........................] - ETA: 1:11:31 - loss: 3.3471 - regression_loss: 2.4377 - classification_lo 1628/10000 [===>..........................] - ETA: 1:11:30 - loss: 3.3465 - regression_loss: 2.4372 - classification_lo 1629/10000 [===>..........................] - ETA: 1:11:29 - loss: 3.3464 - regression_loss: 2.4373 - classification_lo 1630/10000 [===>..........................] - ETA: 1:11:28 - loss: 3.3463 - regression_loss: 2.4373 - classification_lo 1631/10000 [===>..........................] - ETA: 1:11:28 - loss: 3.3461 - regression_loss: 2.4369 - classification_lo 1632/10000 [===>..........................] - ETA: 1:11:27 - loss: 3.3457 - regression_loss: 2.4367 - classification_lo 1633/10000 [===>..........................] - ETA: 1:11:26 - loss: 3.3454 - regression_loss: 2.4366 - classification_lo 1634/10000 [===>..........................] - ETA: 1:11:25 - loss: 3.3450 - regression_loss: 2.4364 - classification_lo 1635/10000 [===>..........................] - ETA: 1:11:24 - loss: 3.3447 - regression_loss: 2.4362 - classification_lo 1636/10000 [===>..........................] - ETA: 1:11:24 - loss: 3.3448 - regression_loss: 2.4363 - classification_lo 1637/10000 [===>..........................] - ETA: 1:11:23 - loss: 3.3442 - regression_loss: 2.4359 - classification_lo 1638/10000 [===>..........................] - ETA: 1:11:22 - loss: 3.3437 - regression_loss: 2.4355 - classification_lo 1639/10000 [===>..........................] - ETA: 1:11:21 - loss: 3.3432 - regression_loss: 2.4351 - classification_lo 1640/10000 [===>..........................] - ETA: 1:11:21 - loss: 3.3427 - regression_loss: 2.4346 - classification_lo 1641/10000 [===>..........................] - ETA: 1:11:20 - loss: 3.3424 - regression_loss: 2.4346 - classification_lo 1642/10000 [===>..........................] - ETA: 1:11:19 - loss: 3.3421 - regression_loss: 2.4345 - classification_lo 1643/10000 [===>..........................] - ETA: 1:11:18 - loss: 3.3418 - regression_loss: 2.4342 - classification_lo 1644/10000 [===>..........................] - ETA: 1:11:18 - loss: 3.3413 - regression_loss: 2.4338 - classification_lo 1645/10000 [===>..........................] - ETA: 1:11:17 - loss: 3.3406 - regression_loss: 2.4333 - classification_lo 1646/10000 [===>..........................] - ETA: 1:11:16 - loss: 3.3405 - regression_loss: 2.4333 - classification_lo 1647/10000 [===>..........................] - ETA: 1:11:15 - loss: 3.3402 - regression_loss: 2.4331 - classification_lo 1648/10000 [===>..........................] - ETA: 1:11:15 - loss: 3.3397 - regression_loss: 2.4328 - classification_lo 1649/10000 [===>..........................] - ETA: 1:11:16 - loss: 3.3392 - regression_loss: 2.4324 - classification_lo 1650/10000 [===>..........................] - ETA: 1:11:16 - loss: 3.3388 - regression_loss: 2.4322 - classification_lo 1651/10000 [===>..........................] - ETA: 1:11:15 - loss: 3.3386 - regression_loss: 2.4321 - classification_lo 1652/10000 [===>..........................] - ETA: 1:11:15 - loss: 3.3381 - regression_loss: 2.4319 - classification_lo 1653/10000 [===>..........................] - ETA: 1:11:14 - loss: 3.3381 - regression_loss: 2.4320 - classification_lo 1654/10000 [===>..........................] - ETA: 1:11:13 - loss: 3.3377 - regression_loss: 2.4317 - classification_lo 1655/10000 [===>..........................] - ETA: 1:11:12 - loss: 3.3374 - regression_loss: 2.4314 - classification_lo 1656/10000 [===>..........................] - ETA: 1:11:12 - loss: 3.3373 - regression_loss: 2.4314 - classification_lo 1657/10000 [===>..........................] - ETA: 1:11:11 - loss: 3.3368 - regression_loss: 2.4311 - classification_lo 1658/10000 [===>..........................] - ETA: 1:11:10 - loss: 3.3373 - regression_loss: 2.4315 - classification_lo 1659/10000 [===>..........................] - ETA: 1:11:09 - loss: 3.3369 - regression_loss: 2.4312 - classification_lo 1660/10000 [===>..........................] - ETA: 1:11:09 - loss: 3.3372 - regression_loss: 2.4316 - classification_lo 1661/10000 [===>..........................] - ETA: 1:11:08 - loss: 3.3365 - regression_loss: 2.4311 - classification_lo 1662/10000 [===>..........................] - ETA: 1:11:07 - loss: 3.3363 - regression_loss: 2.4309 - classification_lo 1663/10000 [===>..........................] - ETA: 1:11:06 - loss: 3.3357 - regression_loss: 2.4305 - classification_lo 1664/10000 [===>..........................] - ETA: 1:11:05 - loss: 3.3356 - regression_loss: 2.4305 - classification_lo 1665/10000 [===>..........................] - ETA: 1:11:05 - loss: 3.3353 - regression_loss: 2.4305 - classification_lo 1666/10000 [===>..........................] - ETA: 1:11:04 - loss: 3.3349 - regression_loss: 2.4303 - classification_lo 1667/10000 [====>.........................] - ETA: 1:11:03 - loss: 3.3347 - regression_loss: 2.4301 - classification_lo 1668/10000 [====>.........................] - ETA: 1:11:03 - loss: 3.3340 - regression_loss: 2.4297 - classification_lo 1669/10000 [====>.........................] - ETA: 1:11:02 - loss: 3.3345 - regression_loss: 2.4303 - classification_lo 1670/10000 [====>.........................] - ETA: 1:11:01 - loss: 3.3343 - regression_loss: 2.4303 - classification_lo 1671/10000 [====>.........................] - ETA: 1:11:02 - loss: 3.3342 - regression_loss: 2.4304 - classification_lo 1672/10000 [====>.........................] - ETA: 1:11:02 - loss: 3.3336 - regression_loss: 2.4299 - classification_lo 1673/10000 [====>.........................] - ETA: 1:11:02 - loss: 3.3331 - regression_loss: 2.4296 - classification_lo 1674/10000 [====>.........................] - ETA: 1:11:01 - loss: 3.3326 - regression_loss: 2.4294 - classification_lo 1675/10000 [====>.........................] - ETA: 1:11:00 - loss: 3.3326 - regression_loss: 2.4293 - classification_lo 1676/10000 [====>.........................] - ETA: 1:10:59 - loss: 3.3319 - regression_loss: 2.4290 - classification_lo 1677/10000 [====>.........................] - ETA: 1:10:58 - loss: 3.3320 - regression_loss: 2.4291 - classification_lo 1678/10000 [====>.........................] - ETA: 1:10:58 - loss: 3.3316 - regression_loss: 2.4287 - classification_lo 1679/10000 [====>.........................] - ETA: 1:10:57 - loss: 3.3309 - regression_loss: 2.4281 - classification_lo 1680/10000 [====>.........................] - ETA: 1:10:56 - loss: 3.3305 - regression_loss: 2.4278 - classification_lo 1681/10000 [====>.........................] - ETA: 1:10:55 - loss: 3.3306 - regression_loss: 2.4278 - classification_lo 1682/10000 [====>.........................] - ETA: 1:10:55 - loss: 3.3300 - regression_loss: 2.4273 - classification_lo 1683/10000 [====>.........................] - ETA: 1:10:54 - loss: 3.3297 - regression_loss: 2.4270 - classification_lo 1684/10000 [====>.........................] - ETA: 1:10:53 - loss: 3.3295 - regression_loss: 2.4269 - classification_lo 1685/10000 [====>.........................] - ETA: 1:10:53 - loss: 3.3287 - regression_loss: 2.4260 - classification_lo 1686/10000 [====>.........................] - ETA: 1:10:52 - loss: 3.3282 - regression_loss: 2.4257 - classification_lo 1687/10000 [====>.........................] - ETA: 1:10:51 - loss: 3.3277 - regression_loss: 2.4253 - classification_lo 1688/10000 [====>.........................] - ETA: 1:10:51 - loss: 3.3277 - regression_loss: 2.4253 - classification_lo 1689/10000 [====>.........................] - ETA: 1:10:50 - loss: 3.3278 - regression_loss: 2.4253 - classification_lo 1690/10000 [====>.........................] - ETA: 1:10:49 - loss: 3.3277 - regression_loss: 2.4254 - classification_lo 1691/10000 [====>.........................] - ETA: 1:10:48 - loss: 3.3275 - regression_loss: 2.4253 - classification_lo 1692/10000 [====>.........................] - ETA: 1:10:47 - loss: 3.3270 - regression_loss: 2.4250 - classification_lo 1693/10000 [====>.........................] - ETA: 1:10:46 - loss: 3.3271 - regression_loss: 2.4250 - classification_lo 1694/10000 [====>.........................] - ETA: 1:10:46 - loss: 3.3271 - regression_loss: 2.4251 - classification_lo 1695/10000 [====>.........................] - ETA: 1:10:45 - loss: 3.3272 - regression_loss: 2.4252 - classification_lo 1696/10000 [====>.........................] - ETA: 1:10:44 - loss: 3.3266 - regression_loss: 2.4249 - classification_lo 1697/10000 [====>.........................] - ETA: 1:10:44 - loss: 3.3262 - regression_loss: 2.4245 - classification_lo 1698/10000 [====>.........................] - ETA: 1:10:43 - loss: 3.3260 - regression_loss: 2.4245 - classification_lo 1699/10000 [====>.........................] - ETA: 1:10:43 - loss: 3.3269 - regression_loss: 2.4231 - classification_lo 1700/10000 [====>.........................] - ETA: 1:10:42 - loss: 3.3268 - regression_loss: 2.4230 - classification_lo 1701/10000 [====>.........................] - ETA: 1:10:41 - loss: 3.3270 - regression_loss: 2.4234 - classification_lo 1702/10000 [====>.........................] - ETA: 1:10:41 - loss: 3.3266 - regression_loss: 2.4232 - classification_lo 1703/10000 [====>.........................] - ETA: 1:10:40 - loss: 3.3264 - regression_loss: 2.4231 - classification_lo 1704/10000 [====>.........................] - ETA: 1:10:39 - loss: 3.3263 - regression_loss: 2.4230 - classification_lo 1705/10000 [====>.........................] - ETA: 1:10:39 - loss: 3.3261 - regression_loss: 2.4230 - classification_lo 1706/10000 [====>.........................] - ETA: 1:10:38 - loss: 3.3255 - regression_loss: 2.4227 - classification_lo 1707/10000 [====>.........................] - ETA: 1:10:37 - loss: 3.3252 - regression_loss: 2.4226 - classification_lo 1708/10000 [====>.........................] - ETA: 1:10:37 - loss: 3.3246 - regression_loss: 2.4221 - classification_lo 1709/10000 [====>.........................] - ETA: 1:10:36 - loss: 3.3244 - regression_loss: 2.4220 - classification_lo 1710/10000 [====>.........................] - ETA: 1:10:35 - loss: 3.3249 - regression_loss: 2.4225 - classification_lo 1711/10000 [====>.........................] - ETA: 1:10:34 - loss: 3.3245 - regression_loss: 2.4222 - classification_lo 1712/10000 [====>.........................] - ETA: 1:10:34 - loss: 3.3240 - regression_loss: 2.4218 - classification_lo 1713/10000 [====>.........................] - ETA: 1:10:33 - loss: 3.3237 - regression_loss: 2.4215 - classification_lo 1714/10000 [====>.........................] - ETA: 1:10:32 - loss: 3.3230 - regression_loss: 2.4210 - classification_lo 1715/10000 [====>.........................] - ETA: 1:10:31 - loss: 3.3228 - regression_loss: 2.4209 - classification_lo 1716/10000 [====>.........................] - ETA: 1:10:30 - loss: 3.3225 - regression_loss: 2.4207 - classification_lo 1717/10000 [====>.........................] - ETA: 1:10:30 - loss: 3.3221 - regression_loss: 2.4205 - classification_lo 1718/10000 [====>.........................] - ETA: 1:10:29 - loss: 3.3222 - regression_loss: 2.4206 - classification_lo 1719/10000 [====>.........................] - ETA: 1:10:28 - loss: 3.3219 - regression_loss: 2.4205 - classification_lo 1720/10000 [====>.........................] - ETA: 1:10:27 - loss: 3.3216 - regression_loss: 2.4203 - classification_lo 1721/10000 [====>.........................] - ETA: 1:10:27 - loss: 3.3216 - regression_loss: 2.4205 - classification_lo 1722/10000 [====>.........................] - ETA: 1:10:26 - loss: 3.3211 - regression_loss: 2.4201 - classification_lo 1723/10000 [====>.........................] - ETA: 1:10:25 - loss: 3.3211 - regression_loss: 2.4201 - classification_lo 1724/10000 [====>.........................] - ETA: 1:10:25 - loss: 3.3207 - regression_loss: 2.4198 - classification_lo 1725/10000 [====>.........................] - ETA: 1:10:24 - loss: 3.3202 - regression_loss: 2.4195 - classification_lo 1726/10000 [====>.........................] - ETA: 1:10:23 - loss: 3.3195 - regression_loss: 2.4190 - classification_lo 1727/10000 [====>.........................] - ETA: 1:10:22 - loss: 3.3192 - regression_loss: 2.4188 - classification_lo 1728/10000 [====>.........................] - ETA: 1:10:22 - loss: 3.3189 - regression_loss: 2.4186 - classification_lo 1729/10000 [====>.........................] - ETA: 1:10:21 - loss: 3.3185 - regression_loss: 2.4184 - classification_lo 1730/10000 [====>.........................] - ETA: 1:10:21 - loss: 3.3186 - regression_loss: 2.4185 - classification_lo 1731/10000 [====>.........................] - ETA: 1:10:20 - loss: 3.3179 - regression_loss: 2.4179 - classification_lo 1732/10000 [====>.........................] - ETA: 1:10:19 - loss: 3.3180 - regression_loss: 2.4180 - classification_lo 1733/10000 [====>.........................] - ETA: 1:10:18 - loss: 3.3174 - regression_loss: 2.4176 - classification_lo 1734/10000 [====>.........................] - ETA: 1:10:17 - loss: 3.3173 - regression_loss: 2.4176 - classification_lo 1735/10000 [====>.........................] - ETA: 1:10:16 - loss: 3.3168 - regression_loss: 2.4171 - classification_lo 1736/10000 [====>.........................] - ETA: 1:10:17 - loss: 3.3168 - regression_loss: 2.4171 - classification_lo 1737/10000 [====>.........................] - ETA: 1:10:17 - loss: 3.3167 - regression_loss: 2.4171 - classification_lo 1738/10000 [====>.........................] - ETA: 1:10:16 - loss: 3.3166 - regression_loss: 2.4170 - classification_lo 1739/10000 [====>.........................] - ETA: 1:10:15 - loss: 3.3165 - regression_loss: 2.4169 - classification_lo 1740/10000 [====>.........................] - ETA: 1:10:14 - loss: 3.3158 - regression_loss: 2.4164 - classification_lo 1741/10000 [====>.........................] - ETA: 1:10:14 - loss: 3.3156 - regression_loss: 2.4163 - classification_lo 1742/10000 [====>.........................] - ETA: 1:10:13 - loss: 3.3149 - regression_loss: 2.4159 - classification_lo 1743/10000 [====>.........................] - ETA: 1:10:13 - loss: 3.3144 - regression_loss: 2.4155 - classification_lo 1744/10000 [====>.........................] - ETA: 1:10:12 - loss: 3.3137 - regression_loss: 2.4150 - classification_lo 1745/10000 [====>.........................] - ETA: 1:10:11 - loss: 3.3138 - regression_loss: 2.4150 - classification_lo 1746/10000 [====>.........................] - ETA: 1:10:10 - loss: 3.3134 - regression_loss: 2.4147 - classification_lo 1747/10000 [====>.........................] - ETA: 1:10:09 - loss: 3.3127 - regression_loss: 2.4141 - classification_lo 1748/10000 [====>.........................] - ETA: 1:10:09 - loss: 3.3123 - regression_loss: 2.4139 - classification_lo 1749/10000 [====>.........................] - ETA: 1:10:08 - loss: 3.3124 - regression_loss: 2.4137 - classification_lo 1750/10000 [====>.........................] - ETA: 1:10:07 - loss: 3.3126 - regression_loss: 2.4135 - classification_lo 1751/10000 [====>.........................] - ETA: 1:10:06 - loss: 3.3122 - regression_loss: 2.4132 - classification_lo 1752/10000 [====>.........................] - ETA: 1:10:06 - loss: 3.3122 - regression_loss: 2.4132 - classification_lo 1753/10000 [====>.........................] - ETA: 1:10:05 - loss: 3.3120 - regression_loss: 2.4131 - classification_lo 1754/10000 [====>.........................] - ETA: 1:10:04 - loss: 3.3121 - regression_loss: 2.4132 - classification_lo 1755/10000 [====>.........................] - ETA: 1:10:04 - loss: 3.3114 - regression_loss: 2.4126 - classification_lo 1756/10000 [====>.........................] - ETA: 1:10:03 - loss: 3.3111 - regression_loss: 2.4125 - classification_lo 1757/10000 [====>.........................] - ETA: 1:10:02 - loss: 3.3110 - regression_loss: 2.4124 - classification_lo 1758/10000 [====>.........................] - ETA: 1:10:02 - loss: 3.3106 - regression_loss: 2.4122 - classification_lo 1759/10000 [====>.........................] - ETA: 1:10:01 - loss: 3.3106 - regression_loss: 2.4122 - classification_lo 1760/10000 [====>.........................] - ETA: 1:10:00 - loss: 3.3104 - regression_loss: 2.4121 - classification_lo 1761/10000 [====>.........................] - ETA: 1:10:00 - loss: 3.3101 - regression_loss: 2.4120 - classification_lo 1762/10000 [====>.........................] - ETA: 1:09:59 - loss: 3.3095 - regression_loss: 2.4114 - classification_lo 1763/10000 [====>.........................] - ETA: 1:09:58 - loss: 3.3097 - regression_loss: 2.4117 - classification_lo 1764/10000 [====>.........................] - ETA: 1:09:57 - loss: 3.3095 - regression_loss: 2.4116 - classification_lo 1765/10000 [====>.........................] - ETA: 1:09:56 - loss: 3.3093 - regression_loss: 2.4114 - classification_lo 1766/10000 [====>.........................] - ETA: 1:09:58 - loss: 3.3094 - regression_loss: 2.4114 - classification_lo 1767/10000 [====>.........................] - ETA: 1:09:58 - loss: 3.3092 - regression_loss: 2.4114 - classification_lo 1768/10000 [====>.........................] - ETA: 1:09:57 - loss: 3.3089 - regression_loss: 2.4112 - classification_lo 1769/10000 [====>.........................] - ETA: 1:09:56 - loss: 3.3087 - regression_loss: 2.4109 - classification_lo 1770/10000 [====>.........................] - ETA: 1:09:55 - loss: 3.3089 - regression_loss: 2.4110 - classification_lo 1771/10000 [====>.........................] - ETA: 1:09:55 - loss: 3.3083 - regression_loss: 2.4106 - classification_lo 1772/10000 [====>.........................] - ETA: 1:09:54 - loss: 3.3082 - regression_loss: 2.4106 - classification_lo 1773/10000 [====>.........................] - ETA: 1:09:53 - loss: 3.3077 - regression_loss: 2.4102 - classification_lo 1774/10000 [====>.........................] - ETA: 1:09:52 - loss: 3.3073 - regression_loss: 2.4099 - classification_lo 1775/10000 [====>.........................] - ETA: 1:09:52 - loss: 3.3074 - regression_loss: 2.4099 - classification_lo 1776/10000 [====>.........................] - ETA: 1:09:51 - loss: 3.3068 - regression_loss: 2.4095 - classification_lo 1777/10000 [====>.........................] - ETA: 1:09:50 - loss: 3.3061 - regression_loss: 2.4088 - classification_lo 1778/10000 [====>.........................] - ETA: 1:09:49 - loss: 3.3056 - regression_loss: 2.4085 - classification_lo 1779/10000 [====>.........................] - ETA: 1:09:49 - loss: 3.3052 - regression_loss: 2.4083 - classification_lo 1780/10000 [====>.........................] - ETA: 1:09:49 - loss: 3.3050 - regression_loss: 2.4083 - classification_lo 1781/10000 [====>.........................] - ETA: 1:09:48 - loss: 3.3043 - regression_loss: 2.4078 - classification_lo 1782/10000 [====>.........................] - ETA: 1:09:48 - loss: 3.3042 - regression_loss: 2.4078 - classification_lo 1783/10000 [====>.........................] - ETA: 1:09:47 - loss: 3.3039 - regression_loss: 2.4077 - classification_lo 1784/10000 [====>.........................] - ETA: 1:09:47 - loss: 3.3038 - regression_loss: 2.4077 - classification_lo 1785/10000 [====>.........................] - ETA: 1:09:46 - loss: 3.3034 - regression_loss: 2.4074 - classification_lo 1786/10000 [====>.........................] - ETA: 1:09:45 - loss: 3.3031 - regression_loss: 2.4072 - classification_lo 1787/10000 [====>.........................] - ETA: 1:09:44 - loss: 3.3027 - regression_loss: 2.4068 - classification_lo 1788/10000 [====>.........................] - ETA: 1:09:44 - loss: 3.3027 - regression_loss: 2.4068 - classification_lo 1789/10000 [====>.........................] - ETA: 1:09:43 - loss: 3.3026 - regression_loss: 2.4069 - classification_lo 1790/10000 [====>.........................] - ETA: 1:09:42 - loss: 3.3024 - regression_loss: 2.4066 - classification_lo 1791/10000 [====>.........................] - ETA: 1:09:41 - loss: 3.3017 - regression_loss: 2.4061 - classification_lo 1792/10000 [====>.........................] - ETA: 1:09:41 - loss: 3.3019 - regression_loss: 2.4062 - classification_lo 1793/10000 [====>.........................] - ETA: 1:09:40 - loss: 3.3017 - regression_loss: 2.4062 - classification_lo 1794/10000 [====>.........................] - ETA: 1:09:42 - loss: 3.3013 - regression_loss: 2.4058 - classification_lo 1795/10000 [====>.........................] - ETA: 1:09:41 - loss: 3.3010 - regression_loss: 2.4056 - classification_lo 1796/10000 [====>.........................] - ETA: 1:09:40 - loss: 3.3008 - regression_loss: 2.4055 - classification_lo 1797/10000 [====>.........................] - ETA: 1:09:40 - loss: 3.3005 - regression_loss: 2.4051 - classification_lo 1798/10000 [====>.........................] - ETA: 1:09:39 - loss: 3.3001 - regression_loss: 2.4047 - classification_lo 1799/10000 [====>.........................] - ETA: 1:09:38 - loss: 3.3000 - regression_loss: 2.4048 - classification_lo 1800/10000 [====>.........................] - ETA: 1:09:37 - loss: 3.2998 - regression_loss: 2.4048 - classification_lo 1801/10000 [====>.........................] - ETA: 1:09:37 - loss: 3.2992 - regression_loss: 2.4043 - classification_lo 1802/10000 [====>.........................] - ETA: 1:09:36 - loss: 3.2989 - regression_loss: 2.4042 - classification_lo 1803/10000 [====>.........................] - ETA: 1:09:35 - loss: 3.2980 - regression_loss: 2.4036 - classification_lo 1804/10000 [====>.........................] - ETA: 1:09:35 - loss: 3.2981 - regression_loss: 2.4037 - classification_lo 1805/10000 [====>.........................] - ETA: 1:09:34 - loss: 3.2980 - regression_loss: 2.4038 - classification_lo 1806/10000 [====>.........................] - ETA: 1:09:33 - loss: 3.2977 - regression_loss: 2.4035 - classification_lo 1807/10000 [====>.........................] - ETA: 1:09:32 - loss: 3.2971 - regression_loss: 2.4032 - classification_lo 1808/10000 [====>.........................] - ETA: 1:09:32 - loss: 3.2963 - regression_loss: 2.4025 - classification_lo 1809/10000 [====>.........................] - ETA: 1:09:31 - loss: 3.2964 - regression_loss: 2.4025 - classification_lo 1810/10000 [====>.........................] - ETA: 1:09:30 - loss: 3.2959 - regression_loss: 2.4023 - classification_lo 1811/10000 [====>.........................] - ETA: 1:09:29 - loss: 3.2958 - regression_loss: 2.4023 - classification_lo 1812/10000 [====>.........................] - ETA: 1:09:29 - loss: 3.2957 - regression_loss: 2.4022 - classification_lo 1813/10000 [====>.........................] - ETA: 1:09:28 - loss: 3.2955 - regression_loss: 2.4022 - classification_lo 1814/10000 [====>.........................] - ETA: 1:09:27 - loss: 3.2953 - regression_loss: 2.4022 - classification_lo 1815/10000 [====>.........................] - ETA: 1:09:27 - loss: 3.2951 - regression_loss: 2.4022 - classification_lo 1816/10000 [====>.........................] - ETA: 1:09:26 - loss: 3.2947 - regression_loss: 2.4020 - classification_lo 1817/10000 [====>.........................] - ETA: 1:09:25 - loss: 3.2944 - regression_loss: 2.4017 - classification_lo 1818/10000 [====>.........................] - ETA: 1:09:25 - loss: 3.2941 - regression_loss: 2.4016 - classification_lo 1819/10000 [====>.........................] - ETA: 1:09:24 - loss: 3.2940 - regression_loss: 2.4015 - classification_lo 1820/10000 [====>.........................] - ETA: 1:09:23 - loss: 3.2932 - regression_loss: 2.4010 - classification_lo 1821/10000 [====>.........................] - ETA: 1:09:22 - loss: 3.2929 - regression_loss: 2.4008 - classification_lo 1822/10000 [====>.........................] - ETA: 1:09:22 - loss: 3.2924 - regression_loss: 2.4004 - classification_lo 1823/10000 [====>.........................] - ETA: 1:09:21 - loss: 3.2925 - regression_loss: 2.4005 - classification_lo 1824/10000 [====>.........................] - ETA: 1:09:20 - loss: 3.2922 - regression_loss: 2.4004 - classification_lo 1825/10000 [====>.........................] - ETA: 1:09:20 - loss: 3.2926 - regression_loss: 2.4005 - classification_lo 1826/10000 [====>.........................] - ETA: 1:09:19 - loss: 3.2918 - regression_loss: 2.3999 - classification_lo 1827/10000 [====>.........................] - ETA: 1:09:18 - loss: 3.2918 - regression_loss: 2.4000 - classification_lo 1828/10000 [====>.........................] - ETA: 1:09:17 - loss: 3.2912 - regression_loss: 2.3995 - classification_lo 1829/10000 [====>.........................] - ETA: 1:09:17 - loss: 3.2913 - regression_loss: 2.3997 - classification_lo 1830/10000 [====>.........................] - ETA: 1:09:16 - loss: 3.2907 - regression_loss: 2.3992 - classification_lo 1831/10000 [====>.........................] - ETA: 1:09:15 - loss: 3.2906 - regression_loss: 2.3992 - classification_lo 1832/10000 [====>.........................] - ETA: 1:09:15 - loss: 3.2904 - regression_loss: 2.3992 - classification_lo 1833/10000 [====>.........................] - ETA: 1:09:14 - loss: 3.2903 - regression_loss: 2.3992 - classification_lo 1834/10000 [====>.........................] - ETA: 1:09:13 - loss: 3.2901 - regression_loss: 2.3991 - classification_lo 1835/10000 [====>.........................] - ETA: 1:09:12 - loss: 3.2900 - regression_loss: 2.3990 - classification_lo 1836/10000 [====>.........................] - ETA: 1:09:11 - loss: 3.2894 - regression_loss: 2.3985 - classification_lo 1837/10000 [====>.........................] - ETA: 1:09:11 - loss: 3.2892 - regression_loss: 2.3984 - classification_lo 1838/10000 [====>.........................] - ETA: 1:09:10 - loss: 3.2886 - regression_loss: 2.3979 - classification_lo 1839/10000 [====>.........................] - ETA: 1:09:09 - loss: 3.2892 - regression_loss: 2.3985 - classification_lo 1840/10000 [====>.........................] - ETA: 1:09:08 - loss: 3.2896 - regression_loss: 2.3990 - classification_lo 1841/10000 [====>.........................] - ETA: 1:09:08 - loss: 3.2896 - regression_loss: 2.3991 - classification_lo 1842/10000 [====>.........................] - ETA: 1:09:07 - loss: 3.2896 - regression_loss: 2.3991 - classification_lo 1843/10000 [====>.........................] - ETA: 1:09:06 - loss: 3.2895 - regression_loss: 2.3991 - classification_lo 1844/10000 [====>.........................] - ETA: 1:09:06 - loss: 3.2891 - regression_loss: 2.3987 - classification_lo 1845/10000 [====>.........................] - ETA: 1:09:07 - loss: 3.2886 - regression_loss: 2.3985 - classification_lo 1846/10000 [====>.........................] - ETA: 1:09:07 - loss: 3.2885 - regression_loss: 2.3984 - classification_lo 1847/10000 [====>.........................] - ETA: 1:09:06 - loss: 3.2884 - regression_loss: 2.3983 - classification_lo 1848/10000 [====>.........................] - ETA: 1:09:05 - loss: 3.2882 - regression_loss: 2.3981 - classification_lo 1849/10000 [====>.........................] - ETA: 1:09:05 - loss: 3.2878 - regression_loss: 2.3979 - classification_lo 1850/10000 [====>.........................] - ETA: 1:09:04 - loss: 3.2877 - regression_loss: 2.3979 - classification_lo 1851/10000 [====>.........................] - ETA: 1:09:03 - loss: 3.2871 - regression_loss: 2.3974 - classification_lo 1852/10000 [====>.........................] - ETA: 1:09:02 - loss: 3.2868 - regression_loss: 2.3972 - classification_lo 1853/10000 [====>.........................] - ETA: 1:09:02 - loss: 3.2867 - regression_loss: 2.3971 - classification_lo 1854/10000 [====>.........................] - ETA: 1:09:01 - loss: 3.2864 - regression_loss: 2.3969 - classification_lo 1855/10000 [====>.........................] - ETA: 1:09:00 - loss: 3.2865 - regression_loss: 2.3969 - classification_lo 1856/10000 [====>.........................] - ETA: 1:09:00 - loss: 3.2862 - regression_loss: 2.3968 - classification_lo 1857/10000 [====>.........................] - ETA: 1:08:59 - loss: 3.2861 - regression_loss: 2.3967 - classification_lo 1858/10000 [====>.........................] - ETA: 1:08:58 - loss: 3.2860 - regression_loss: 2.3967 - classification_lo 1859/10000 [====>.........................] - ETA: 1:08:58 - loss: 3.2858 - regression_loss: 2.3965 - classification_lo 1860/10000 [====>.........................] - ETA: 1:08:57 - loss: 3.2858 - regression_loss: 2.3965 - classification_lo 1861/10000 [====>.........................] - ETA: 1:08:56 - loss: 3.2854 - regression_loss: 2.3964 - classification_lo 1862/10000 [====>.........................] - ETA: 1:08:55 - loss: 3.2851 - regression_loss: 2.3963 - classification_lo 1863/10000 [====>.........................] - ETA: 1:08:55 - loss: 3.2851 - regression_loss: 2.3963 - classification_lo 1864/10000 [====>.........................] - ETA: 1:08:55 - loss: 3.2847 - regression_loss: 2.3961 - classification_lo 1865/10000 [====>.........................] - ETA: 1:08:54 - loss: 3.2844 - regression_loss: 2.3959 - classification_lo 1866/10000 [====>.........................] - ETA: 1:08:53 - loss: 3.2842 - regression_loss: 2.3957 - classification_lo 1867/10000 [====>.........................] - ETA: 1:08:52 - loss: 3.2841 - regression_loss: 2.3956 - classification_lo 1868/10000 [====>.........................] - ETA: 1:08:51 - loss: 3.2841 - regression_loss: 2.3957 - classification_lo 1869/10000 [====>.........................] - ETA: 1:08:51 - loss: 3.2839 - regression_loss: 2.3957 - classification_lo 1870/10000 [====>.........................] - ETA: 1:08:50 - loss: 3.2838 - regression_loss: 2.3957 - classification_lo 1871/10000 [====>.........................] - ETA: 1:08:49 - loss: 3.2839 - regression_loss: 2.3958 - classification_lo 1872/10000 [====>.........................] - ETA: 1:08:49 - loss: 3.2837 - regression_loss: 2.3957 - classification_lo 1873/10000 [====>.........................] - ETA: 1:08:48 - loss: 3.2834 - regression_loss: 2.3956 - classification_lo 1874/10000 [====>.........................] - ETA: 1:08:47 - loss: 3.2831 - regression_loss: 2.3954 - classification_lo 1875/10000 [====>.........................] - ETA: 1:08:47 - loss: 3.2831 - regression_loss: 2.3955 - classification_lo 1876/10000 [====>.........................] - ETA: 1:08:46 - loss: 3.2828 - regression_loss: 2.3954 - classification_lo 1877/10000 [====>.........................] - ETA: 1:08:45 - loss: 3.2825 - regression_loss: 2.3952 - classification_lo 1878/10000 [====>.........................] - ETA: 1:08:45 - loss: 3.2822 - regression_loss: 2.3951 - classification_lo 1879/10000 [====>.........................] - ETA: 1:08:44 - loss: 3.2820 - regression_loss: 2.3950 - classification_lo 1880/10000 [====>.........................] - ETA: 1:08:43 - loss: 3.2816 - regression_loss: 2.3947 - classification_lo 1881/10000 [====>.........................] - ETA: 1:08:43 - loss: 3.2816 - regression_loss: 2.3946 - classification_lo 1882/10000 [====>.........................] - ETA: 1:08:42 - loss: 3.2810 - regression_loss: 2.3941 - classification_lo 1883/10000 [====>.........................] - ETA: 1:08:41 - loss: 3.2806 - regression_loss: 2.3937 - classification_lo 1884/10000 [====>.........................] - ETA: 1:08:40 - loss: 3.2811 - regression_loss: 2.3942 - classification_lo 1885/10000 [====>.........................] - ETA: 1:08:40 - loss: 3.2807 - regression_loss: 2.3939 - classification_lo 1886/10000 [====>.........................] - ETA: 1:08:39 - loss: 3.2805 - regression_loss: 2.3939 - classification_lo 1887/10000 [====>.........................] - ETA: 1:08:38 - loss: 3.2798 - regression_loss: 2.3933 - classification_lo 1888/10000 [====>.........................] - ETA: 1:08:37 - loss: 3.2797 - regression_loss: 2.3934 - classification_lo 1889/10000 [====>.........................] - ETA: 1:08:37 - loss: 3.2798 - regression_loss: 2.3935 - classification_lo 1890/10000 [====>.........................] - ETA: 1:08:36 - loss: 3.2794 - regression_loss: 2.3933 - classification_lo 1891/10000 [====>.........................] - ETA: 1:08:35 - loss: 3.2789 - regression_loss: 2.3928 - classification_lo 1892/10000 [====>.........................] - ETA: 1:08:35 - loss: 3.2788 - regression_loss: 2.3928 - classification_lo 1893/10000 [====>.........................] - ETA: 1:08:34 - loss: 3.2787 - regression_loss: 2.3927 - classification_lo 1894/10000 [====>.........................] - ETA: 1:08:33 - loss: 3.2784 - regression_loss: 2.3927 - classification_lo 1895/10000 [====>.........................] - ETA: 1:08:32 - loss: 3.2784 - regression_loss: 2.3927 - classification_lo 1896/10000 [====>.........................] - ETA: 1:08:32 - loss: 3.2783 - regression_loss: 2.3926 - classification_lo 1897/10000 [====>.........................] - ETA: 1:08:31 - loss: 3.2780 - regression_loss: 2.3925 - classification_lo 1898/10000 [====>.........................] - ETA: 1:08:31 - loss: 3.2780 - regression_loss: 2.3926 - classification_lo 1899/10000 [====>.........................] - ETA: 1:08:30 - loss: 3.2776 - regression_loss: 2.3924 - classification_lo 1900/10000 [====>.........................] - ETA: 1:08:29 - loss: 3.2772 - regression_loss: 2.3920 - classification_lo 1901/10000 [====>.........................] - ETA: 1:08:28 - loss: 3.2770 - regression_loss: 2.3920 - classification_lo 1902/10000 [====>.........................] - ETA: 1:08:28 - loss: 3.2771 - regression_loss: 2.3920 - classification_lo 1903/10000 [====>.........................] - ETA: 1:08:27 - loss: 3.2770 - regression_loss: 2.3920 - classification_lo 1904/10000 [====>.........................] - ETA: 1:08:26 - loss: 3.2769 - regression_loss: 2.3921 - classification_lo 1905/10000 [====>.........................] - ETA: 1:08:26 - loss: 3.2763 - regression_loss: 2.3916 - classification_lo 1906/10000 [====>.........................] - ETA: 1:08:25 - loss: 3.2761 - regression_loss: 2.3916 - classification_lo 1907/10000 [====>.........................] - ETA: 1:08:24 - loss: 3.2757 - regression_loss: 2.3912 - classification_lo 1908/10000 [====>.........................] - ETA: 1:08:23 - loss: 3.2754 - regression_loss: 2.3910 - classification_lo 1909/10000 [====>.........................] - ETA: 1:08:23 - loss: 3.2751 - regression_loss: 2.3908 - classification_lo 1910/10000 [====>.........................] - ETA: 1:08:22 - loss: 3.2749 - regression_loss: 2.3906 - classification_lo 1911/10000 [====>.........................] - ETA: 1:08:21 - loss: 3.2751 - regression_loss: 2.3907 - classification_lo 1912/10000 [====>.........................] - ETA: 1:08:21 - loss: 3.2752 - regression_loss: 2.3908 - classification_lo 1913/10000 [====>.........................] - ETA: 1:08:20 - loss: 3.2752 - regression_loss: 2.3908 - classification_lo 1914/10000 [====>.........................] - ETA: 1:08:19 - loss: 3.2747 - regression_loss: 2.3906 - classification_lo 1915/10000 [====>.........................] - ETA: 1:08:19 - loss: 3.2742 - regression_loss: 2.3902 - classification_lo 1916/10000 [====>.........................] - ETA: 1:08:18 - loss: 3.2739 - regression_loss: 2.3900 - classification_lo 1917/10000 [====>.........................] - ETA: 1:08:17 - loss: 3.2734 - regression_loss: 2.3897 - classification_lo 1918/10000 [====>.........................] - ETA: 1:08:16 - loss: 3.2727 - regression_loss: 2.3891 - classification_lo 1919/10000 [====>.........................] - ETA: 1:08:16 - loss: 3.2722 - regression_loss: 2.3886 - classification_lo 1920/10000 [====>.........................] - ETA: 1:08:15 - loss: 3.2720 - regression_loss: 2.3886 - classification_lo 1921/10000 [====>.........................] - ETA: 1:08:15 - loss: 3.2717 - regression_loss: 2.3884 - classification_lo 1922/10000 [====>.........................] - ETA: 1:08:14 - loss: 3.2717 - regression_loss: 2.3884 - classification_lo 1923/10000 [====>.........................] - ETA: 1:08:13 - loss: 3.2717 - regression_loss: 2.3885 - classification_lo 1924/10000 [====>.........................] - ETA: 1:08:12 - loss: 3.2715 - regression_loss: 2.3884 - classification_lo 1925/10000 [====>.........................] - ETA: 1:08:12 - loss: 3.2713 - regression_loss: 2.3883 - classification_lo 1926/10000 [====>.........................] - ETA: 1:08:11 - loss: 3.2713 - regression_loss: 2.3883 - classification_lo 1927/10000 [====>.........................] - ETA: 1:08:10 - loss: 3.2709 - regression_loss: 2.3879 - classification_lo 1928/10000 [====>.........................] - ETA: 1:08:10 - loss: 3.2704 - regression_loss: 2.3874 - classification_lo 1929/10000 [====>.........................] - ETA: 1:08:09 - loss: 3.2699 - regression_loss: 2.3870 - classification_lo 1930/10000 [====>.........................] - ETA: 1:08:08 - loss: 3.2698 - regression_loss: 2.3870 - classification_lo 1931/10000 [====>.........................] - ETA: 1:08:07 - loss: 3.2697 - regression_loss: 2.3869 - classification_lo 1932/10000 [====>.........................] - ETA: 1:08:07 - loss: 3.2697 - regression_loss: 2.3868 - classification_lo 1933/10000 [====>.........................] - ETA: 1:08:06 - loss: 3.2690 - regression_loss: 2.3863 - classification_lo 1934/10000 [====>.........................] - ETA: 1:08:05 - loss: 3.2687 - regression_loss: 2.3863 - classification_lo 1935/10000 [====>.........................] - ETA: 1:08:05 - loss: 3.2686 - regression_loss: 2.3863 - classification_lo 1936/10000 [====>.........................] - ETA: 1:08:04 - loss: 3.2685 - regression_loss: 2.3862 - classification_lo 1937/10000 [====>.........................] - ETA: 1:08:03 - loss: 3.2682 - regression_loss: 2.3861 - classification_lo 1938/10000 [====>.........................] - ETA: 1:08:03 - loss: 3.2675 - regression_loss: 2.3855 - classification_lo 1939/10000 [====>.........................] - ETA: 1:08:02 - loss: 3.2672 - regression_loss: 2.3853 - classification_lo 1940/10000 [====>.........................] - ETA: 1:08:03 - loss: 3.2666 - regression_loss: 2.3847 - classification_lo 1941/10000 [====>.........................] - ETA: 1:08:02 - loss: 3.2665 - regression_loss: 2.3848 - classification_lo 1942/10000 [====>.........................] - ETA: 1:08:01 - loss: 3.2663 - regression_loss: 2.3846 - classification_lo 1943/10000 [====>.........................] - ETA: 1:08:01 - loss: 3.2658 - regression_loss: 2.3841 - classification_lo 1944/10000 [====>.........................] - ETA: 1:08:00 - loss: 3.2656 - regression_loss: 2.3841 - classification_lo 1945/10000 [====>.........................] - ETA: 1:07:59 - loss: 3.2655 - regression_loss: 2.3841 - classification_lo 1946/10000 [====>.........................] - ETA: 1:07:59 - loss: 3.2653 - regression_loss: 2.3839 - classification_lo 1947/10000 [====>.........................] - ETA: 1:07:58 - loss: 3.2651 - regression_loss: 2.3838 - classification_lo 1948/10000 [====>.........................] - ETA: 1:07:59 - loss: 3.2648 - regression_loss: 2.3837 - classification_lo 1949/10000 [====>.........................] - ETA: 1:07:59 - loss: 3.2647 - regression_loss: 2.3834 - classification_lo 1950/10000 [====>.........................] - ETA: 1:07:58 - loss: 3.2643 - regression_loss: 2.3831 - classification_lo 1951/10000 [====>.........................] - ETA: 1:07:58 - loss: 3.2642 - regression_loss: 2.3831 - classification_lo 1952/10000 [====>.........................] - ETA: 1:07:57 - loss: 3.2647 - regression_loss: 2.3835 - classification_lo 1953/10000 [====>.........................] - ETA: 1:07:56 - loss: 3.2648 - regression_loss: 2.3836 - classification_lo 1954/10000 [====>.........................] - ETA: 1:07:56 - loss: 3.2648 - regression_loss: 2.3836 - classification_lo 1955/10000 [====>.........................] - ETA: 1:07:55 - loss: 3.2648 - regression_loss: 2.3836 - classification_lo 1956/10000 [====>.........................] - ETA: 1:07:55 - loss: 3.2645 - regression_loss: 2.3833 - classification_lo 1957/10000 [====>.........................] - ETA: 1:07:54 - loss: 3.2641 - regression_loss: 2.3831 - classification_lo 1958/10000 [====>.........................] - ETA: 1:07:53 - loss: 3.2641 - regression_loss: 2.3832 - classification_lo 1959/10000 [====>.........................] - ETA: 1:07:52 - loss: 3.2639 - regression_loss: 2.3832 - classification_lo 1960/10000 [====>.........................] - ETA: 1:07:52 - loss: 3.2640 - regression_loss: 2.3833 - classification_lo 1961/10000 [====>.........................] - ETA: 1:07:51 - loss: 3.2638 - regression_loss: 2.3832 - classification_lo 1962/10000 [====>.........................] - ETA: 1:07:51 - loss: 3.2635 - regression_loss: 2.3830 - classification_lo 1963/10000 [====>.........................] - ETA: 1:07:50 - loss: 3.2635 - regression_loss: 2.3831 - classification_lo 1964/10000 [====>.........................] - ETA: 1:07:49 - loss: 3.2635 - regression_loss: 2.3831 - classification_lo 1965/10000 [====>.........................] - ETA: 1:07:49 - loss: 3.2635 - regression_loss: 2.3831 - classification_lo 1966/10000 [====>.........................] - ETA: 1:07:48 - loss: 3.2633 - regression_loss: 2.3831 - classification_lo 1967/10000 [====>.........................] - ETA: 1:07:47 - loss: 3.2630 - regression_loss: 2.3828 - classification_lo 1968/10000 [====>.........................] - ETA: 1:07:46 - loss: 3.2630 - regression_loss: 2.3829 - classification_lo 1969/10000 [====>.........................] - ETA: 1:07:46 - loss: 3.2627 - regression_loss: 2.3828 - classification_lo 1970/10000 [====>.........................] - ETA: 1:07:45 - loss: 3.2625 - regression_loss: 2.3826 - classification_lo 1971/10000 [====>.........................] - ETA: 1:07:45 - loss: 3.2623 - regression_loss: 2.3826 - classification_lo 1972/10000 [====>.........................] - ETA: 1:07:44 - loss: 3.2621 - regression_loss: 2.3825 - classification_lo 1973/10000 [====>.........................] - ETA: 1:07:43 - loss: 3.2618 - regression_loss: 2.3823 - classification_lo 1974/10000 [====>.........................] - ETA: 1:07:43 - loss: 3.2614 - regression_loss: 2.3820 - classification_lo 1975/10000 [====>.........................] - ETA: 1:07:42 - loss: 3.2612 - regression_loss: 2.3820 - classification_lo 1976/10000 [====>.........................] - ETA: 1:07:41 - loss: 3.2614 - regression_loss: 2.3820 - classification_lo 1977/10000 [====>.........................] - ETA: 1:07:41 - loss: 3.2611 - regression_loss: 2.3818 - classification_lo 1978/10000 [====>.........................] - ETA: 1:07:40 - loss: 3.2611 - regression_loss: 2.3817 - classification_lo 1979/10000 [====>.........................] - ETA: 1:07:39 - loss: 3.2609 - regression_loss: 2.3816 - classification_lo 1980/10000 [====>.........................] - ETA: 1:07:38 - loss: 3.2609 - regression_loss: 2.3816 - classification_lo 1981/10000 [====>.........................] - ETA: 1:07:38 - loss: 3.2608 - regression_loss: 2.3816 - classification_lo 1982/10000 [====>.........................] - ETA: 1:07:37 - loss: 3.2603 - regression_loss: 2.3812 - classification_lo 1983/10000 [====>.........................] - ETA: 1:07:37 - loss: 3.2609 - regression_loss: 2.3800 - classification_lo 1984/10000 [====>.........................] - ETA: 1:07:36 - loss: 3.2609 - regression_loss: 2.3801 - classification_lo 1985/10000 [====>.........................] - ETA: 1:07:35 - loss: 3.2605 - regression_loss: 2.3797 - classification_lo 1986/10000 [====>.........................] - ETA: 1:07:34 - loss: 3.2602 - regression_loss: 2.3796 - classification_lo 1987/10000 [====>.........................] - ETA: 1:07:34 - loss: 3.2600 - regression_loss: 2.3795 - classification_lo 1988/10000 [====>.........................] - ETA: 1:07:33 - loss: 3.2597 - regression_loss: 2.3792 - classification_lo 1989/10000 [====>.........................] - ETA: 1:07:33 - loss: 3.2592 - regression_loss: 2.3789 - classification_lo 1990/10000 [====>.........................] - ETA: 1:07:32 - loss: 3.2591 - regression_loss: 2.3788 - classification_lo 1991/10000 [====>.........................] - ETA: 1:07:31 - loss: 3.2588 - regression_loss: 2.3787 - classification_lo 1992/10000 [====>.........................] - ETA: 1:07:30 - loss: 3.2583 - regression_loss: 2.3784 - classification_lo 1993/10000 [====>.........................] - ETA: 1:07:30 - loss: 3.2579 - regression_loss: 2.3781 - classification_lo 1994/10000 [====>.........................] - ETA: 1:07:29 - loss: 3.2578 - regression_loss: 2.3779 - classification_lo 1995/10000 [====>.........................] - ETA: 1:07:30 - loss: 3.2580 - regression_loss: 2.3767 - classification_lo 1996/10000 [====>.........................] - ETA: 1:07:29 - loss: 3.2580 - regression_loss: 2.3766 - classification_lo 1997/10000 [====>.........................] - ETA: 1:07:29 - loss: 3.2580 - regression_loss: 2.3766 - classification_lo 1998/10000 [====>.........................] - ETA: 1:07:28 - loss: 3.2578 - regression_loss: 2.3765 - classification_lo 1999/10000 [====>.........................] - ETA: 1:07:27 - loss: 3.2573 - regression_loss: 2.3763 - classification_lo 2000/10000 [=====>........................] - ETA: 1:07:26 - loss: 3.2569 - regression_loss: 2.3759 - classification_lo 2001/10000 [=====>........................] - ETA: 1:07:26 - loss: 3.2563 - regression_loss: 2.3755 - classification_lo 2002/10000 [=====>........................] - ETA: 1:07:25 - loss: 3.2557 - regression_loss: 2.3750 - classification_lo 2003/10000 [=====>........................] - ETA: 1:07:24 - loss: 3.2562 - regression_loss: 2.3753 - classification_lo 2004/10000 [=====>........................] - ETA: 1:07:24 - loss: 3.2558 - regression_loss: 2.3750 - classification_lo 2005/10000 [=====>........................] - ETA: 1:07:23 - loss: 3.2555 - regression_loss: 2.3749 - classification_lo 2006/10000 [=====>........................] - ETA: 1:07:22 - loss: 3.2560 - regression_loss: 2.3753 - classification_lo 2007/10000 [=====>........................] - ETA: 1:07:21 - loss: 3.2559 - regression_loss: 2.3752 - classification_lo 2008/10000 [=====>........................] - ETA: 1:07:21 - loss: 3.2555 - regression_loss: 2.3751 - classification_lo 2009/10000 [=====>........................] - ETA: 1:07:20 - loss: 3.2553 - regression_loss: 2.3750 - classification_lo 2010/10000 [=====>........................] - ETA: 1:07:22 - loss: 3.2553 - regression_loss: 2.3751 - classification_lo 2011/10000 [=====>........................] - ETA: 1:07:22 - loss: 3.2549 - regression_loss: 2.3747 - classification_lo 2012/10000 [=====>........................] - ETA: 1:07:21 - loss: 3.2546 - regression_loss: 2.3745 - classification_lo 2013/10000 [=====>........................] - ETA: 1:07:20 - loss: 3.2542 - regression_loss: 2.3743 - classification_lo 2014/10000 [=====>........................] - ETA: 1:07:20 - loss: 3.2541 - regression_loss: 2.3732 - classification_lo 2015/10000 [=====>........................] - ETA: 1:07:19 - loss: 3.2536 - regression_loss: 2.3727 - classification_lo 2016/10000 [=====>........................] - ETA: 1:07:19 - loss: 3.2535 - regression_loss: 2.3726 - classification_lo 2017/10000 [=====>........................] - ETA: 1:07:18 - loss: 3.2536 - regression_loss: 2.3728 - classification_lo 2018/10000 [=====>........................] - ETA: 1:07:17 - loss: 3.2536 - regression_loss: 2.3727 - classification_lo 2019/10000 [=====>........................] - ETA: 1:07:17 - loss: 3.2533 - regression_loss: 2.3726 - classification_lo 2020/10000 [=====>........................] - ETA: 1:07:16 - loss: 3.2534 - regression_loss: 2.3726 - classification_lo 2021/10000 [=====>........................] - ETA: 1:07:16 - loss: 3.2529 - regression_loss: 2.3724 - classification_lo 2022/10000 [=====>........................] - ETA: 1:07:15 - loss: 3.2526 - regression_loss: 2.3721 - classification_lo 2023/10000 [=====>........................] - ETA: 1:07:14 - loss: 3.2525 - regression_loss: 2.3719 - classification_lo 2024/10000 [=====>........................] - ETA: 1:07:13 - loss: 3.2525 - regression_loss: 2.3720 - classification_lo 2025/10000 [=====>........................] - ETA: 1:07:13 - loss: 3.2525 - regression_loss: 2.3721 - classification_lo 2026/10000 [=====>........................] - ETA: 1:07:12 - loss: 3.2522 - regression_loss: 2.3718 - classification_lo 2027/10000 [=====>........................] - ETA: 1:07:11 - loss: 3.2520 - regression_loss: 2.3717 - classification_lo 2028/10000 [=====>........................] - ETA: 1:07:10 - loss: 3.2520 - regression_loss: 2.3718 - classification_lo 2029/10000 [=====>........................] - ETA: 1:07:10 - loss: 3.2519 - regression_loss: 2.3719 - classification_lo 2030/10000 [=====>........................] - ETA: 1:07:09 - loss: 3.2519 - regression_loss: 2.3719 - classification_lo 2031/10000 [=====>........................] - ETA: 1:07:08 - loss: 3.2517 - regression_loss: 2.3717 - classification_lo 2032/10000 [=====>........................] - ETA: 1:07:08 - loss: 3.2519 - regression_loss: 2.3720 - classification_lo 2033/10000 [=====>........................] - ETA: 1:07:07 - loss: 3.2514 - regression_loss: 2.3716 - classification_lo 2034/10000 [=====>........................] - ETA: 1:07:06 - loss: 3.2513 - regression_loss: 2.3716 - classification_lo 2035/10000 [=====>........................] - ETA: 1:07:06 - loss: 3.2507 - regression_loss: 2.3711 - classification_lo 2036/10000 [=====>........................] - ETA: 1:07:05 - loss: 3.2510 - regression_loss: 2.3714 - classification_lo 2037/10000 [=====>........................] - ETA: 1:07:04 - loss: 3.2512 - regression_loss: 2.3715 - classification_lo 2038/10000 [=====>........................] - ETA: 1:07:04 - loss: 3.2505 - regression_loss: 2.3709 - classification_lo 2039/10000 [=====>........................] - ETA: 1:07:03 - loss: 3.2499 - regression_loss: 2.3704 - classification_lo 2040/10000 [=====>........................] - ETA: 1:07:02 - loss: 3.2492 - regression_loss: 2.3697 - classification_lo 2041/10000 [=====>........................] - ETA: 1:07:01 - loss: 3.2490 - regression_loss: 2.3696 - classification_lo 2042/10000 [=====>........................] - ETA: 1:07:01 - loss: 3.2489 - regression_loss: 2.3696 - classification_lo 2043/10000 [=====>........................] - ETA: 1:07:01 - loss: 3.2484 - regression_loss: 2.3693 - classification_lo 2044/10000 [=====>........................] - ETA: 1:07:00 - loss: 3.2484 - regression_loss: 2.3694 - classification_lo 2045/10000 [=====>........................] - ETA: 1:06:59 - loss: 3.2481 - regression_loss: 2.3692 - classification_lo 2046/10000 [=====>........................] - ETA: 1:06:59 - loss: 3.2483 - regression_loss: 2.3691 - classification_lo 2047/10000 [=====>........................] - ETA: 1:06:58 - loss: 3.2476 - regression_loss: 2.3684 - classification_lo 2048/10000 [=====>........................] - ETA: 1:06:58 - loss: 3.2474 - regression_loss: 2.3685 - classification_lo 2049/10000 [=====>........................] - ETA: 1:06:57 - loss: 3.2473 - regression_loss: 2.3685 - classification_lo 2050/10000 [=====>........................] - ETA: 1:06:56 - loss: 3.2470 - regression_loss: 2.3684 - classification_lo 2051/10000 [=====>........................] - ETA: 1:06:56 - loss: 3.2468 - regression_loss: 2.3683 - classification_lo 2052/10000 [=====>........................] - ETA: 1:06:55 - loss: 3.2469 - regression_loss: 2.3683 - classification_lo 2053/10000 [=====>........................] - ETA: 1:06:54 - loss: 3.2468 - regression_loss: 2.3682 - classification_lo 2054/10000 [=====>........................] - ETA: 1:06:54 - loss: 3.2461 - regression_loss: 2.3677 - classification_lo 2055/10000 [=====>........................] - ETA: 1:06:53 - loss: 3.2459 - regression_loss: 2.3674 - classification_lo 2056/10000 [=====>........................] - ETA: 1:06:52 - loss: 3.2458 - regression_loss: 2.3674 - classification_lo 2057/10000 [=====>........................] - ETA: 1:06:52 - loss: 3.2457 - regression_loss: 2.3675 - classification_lo 2058/10000 [=====>........................] - ETA: 1:06:51 - loss: 3.2450 - regression_loss: 2.3669 - classification_lo 2059/10000 [=====>........................] - ETA: 1:06:51 - loss: 3.2444 - regression_loss: 2.3664 - classification_lo 2060/10000 [=====>........................] - ETA: 1:06:50 - loss: 3.2440 - regression_loss: 2.3662 - classification_lo 2061/10000 [=====>........................] - ETA: 1:06:50 - loss: 3.2434 - regression_loss: 2.3656 - classification_lo 2062/10000 [=====>........................] - ETA: 1:06:50 - loss: 3.2431 - regression_loss: 2.3653 - classification_lo 2063/10000 [=====>........................] - ETA: 1:06:49 - loss: 3.2430 - regression_loss: 2.3654 - classification_lo 2064/10000 [=====>........................] - ETA: 1:06:49 - loss: 3.2429 - regression_loss: 2.3653 - classification_lo 2065/10000 [=====>........................] - ETA: 1:06:48 - loss: 3.2425 - regression_loss: 2.3651 - classification_lo 2066/10000 [=====>........................] - ETA: 1:06:47 - loss: 3.2424 - regression_loss: 2.3650 - classification_lo 2067/10000 [=====>........................] - ETA: 1:06:47 - loss: 3.2420 - regression_loss: 2.3647 - classification_lo 2068/10000 [=====>........................] - ETA: 1:06:46 - loss: 3.2416 - regression_loss: 2.3643 - classification_lo 2069/10000 [=====>........................] - ETA: 1:06:45 - loss: 3.2413 - regression_loss: 2.3641 - classification_lo 2070/10000 [=====>........................] - ETA: 1:06:45 - loss: 3.2412 - regression_loss: 2.3641 - classification_lo 2071/10000 [=====>........................] - ETA: 1:06:44 - loss: 3.2412 - regression_loss: 2.3641 - classification_lo 2072/10000 [=====>........................] - ETA: 1:06:45 - loss: 3.2412 - regression_loss: 2.3642 - classification_lo 2073/10000 [=====>........................] - ETA: 1:06:45 - loss: 3.2411 - regression_loss: 2.3641 - classification_lo 2074/10000 [=====>........................] - ETA: 1:06:44 - loss: 3.2407 - regression_loss: 2.3638 - classification_lo 2075/10000 [=====>........................] - ETA: 1:06:43 - loss: 3.2400 - regression_loss: 2.3632 - classification_lo 2076/10000 [=====>........................] - ETA: 1:06:43 - loss: 3.2398 - regression_loss: 2.3631 - classification_lo 2077/10000 [=====>........................] - ETA: 1:06:42 - loss: 3.2396 - regression_loss: 2.3629 - classification_lo 2078/10000 [=====>........................] - ETA: 1:06:41 - loss: 3.2394 - regression_loss: 2.3627 - classification_lo 2079/10000 [=====>........................] - ETA: 1:06:40 - loss: 3.2393 - regression_loss: 2.3626 - classification_lo 2080/10000 [=====>........................] - ETA: 1:06:40 - loss: 3.2389 - regression_loss: 2.3623 - classification_lo 2081/10000 [=====>........................] - ETA: 1:06:39 - loss: 3.2383 - regression_loss: 2.3619 - classification_lo 2082/10000 [=====>........................] - ETA: 1:06:38 - loss: 3.2380 - regression_loss: 2.3618 - classification_lo 2083/10000 [=====>........................] - ETA: 1:06:38 - loss: 3.2375 - regression_loss: 2.3613 - classification_lo 2084/10000 [=====>........................] - ETA: 1:06:37 - loss: 3.2371 - regression_loss: 2.3610 - classification_lo 2085/10000 [=====>........................] - ETA: 1:06:37 - loss: 3.2369 - regression_loss: 2.3609 - classification_lo 2086/10000 [=====>........................] - ETA: 1:06:36 - loss: 3.2366 - regression_loss: 2.3607 - classification_lo 2087/10000 [=====>........................] - ETA: 1:06:35 - loss: 3.2370 - regression_loss: 2.3612 - classification_lo 2088/10000 [=====>........................] - ETA: 1:06:35 - loss: 3.2371 - regression_loss: 2.3614 - classification_lo 2089/10000 [=====>........................] - ETA: 1:06:34 - loss: 3.2366 - regression_loss: 2.3610 - classification_lo 2090/10000 [=====>........................] - ETA: 1:06:33 - loss: 3.2360 - regression_loss: 2.3605 - classification_lo 2091/10000 [=====>........................] - ETA: 1:06:33 - loss: 3.2358 - regression_loss: 2.3605 - classification_lo 2092/10000 [=====>........................] - ETA: 1:06:32 - loss: 3.2357 - regression_loss: 2.3605 - classification_lo 2093/10000 [=====>........................] - ETA: 1:06:31 - loss: 3.2354 - regression_loss: 2.3604 - classification_lo 2094/10000 [=====>........................] - ETA: 1:06:31 - loss: 3.2352 - regression_loss: 2.3603 - classification_lo 2095/10000 [=====>........................] - ETA: 1:06:30 - loss: 3.2351 - regression_loss: 2.3602 - classification_lo 2096/10000 [=====>........................] - ETA: 1:06:30 - loss: 3.2347 - regression_loss: 2.3599 - classification_lo 2097/10000 [=====>........................] - ETA: 1:06:29 - loss: 3.2343 - regression_loss: 2.3596 - classification_lo 2098/10000 [=====>........................] - ETA: 1:06:28 - loss: 3.2341 - regression_loss: 2.3595 - classification_lo 2099/10000 [=====>........................] - ETA: 1:06:28 - loss: 3.2338 - regression_loss: 2.3591 - classification_lo 2100/10000 [=====>........................] - ETA: 1:06:27 - loss: 3.2336 - regression_loss: 2.3591 - classification_lo 2101/10000 [=====>........................] - ETA: 1:06:26 - loss: 3.2334 - regression_loss: 2.3590 - classification_lo 2102/10000 [=====>........................] - ETA: 1:06:26 - loss: 3.2335 - regression_loss: 2.3591 - classification_lo 2103/10000 [=====>........................] - ETA: 1:06:25 - loss: 3.2332 - regression_loss: 2.3589 - classification_lo 2104/10000 [=====>........................] - ETA: 1:06:24 - loss: 3.2328 - regression_loss: 2.3585 - classification_lo 2105/10000 [=====>........................] - ETA: 1:06:24 - loss: 3.2328 - regression_loss: 2.3585 - classification_lo 2106/10000 [=====>........................] - ETA: 1:06:23 - loss: 3.2326 - regression_loss: 2.3583 - classification_lo 2107/10000 [=====>........................] - ETA: 1:06:22 - loss: 3.2323 - regression_loss: 2.3581 - classification_lo 2108/10000 [=====>........................] - ETA: 1:06:21 - loss: 3.2323 - regression_loss: 2.3581 - classification_lo 2109/10000 [=====>........................] - ETA: 1:06:21 - loss: 3.2324 - regression_loss: 2.3583 - classification_lo 2110/10000 [=====>........................] - ETA: 1:06:20 - loss: 3.2323 - regression_loss: 2.3583 - classification_lo 2111/10000 [=====>........................] - ETA: 1:06:20 - loss: 3.2319 - regression_loss: 2.3581 - classification_lo 2112/10000 [=====>........................] - ETA: 1:06:19 - loss: 3.2314 - regression_loss: 2.3576 - classification_lo 2113/10000 [=====>........................] - ETA: 1:06:18 - loss: 3.2314 - regression_loss: 2.3577 - classification_lo 2114/10000 [=====>........................] - ETA: 1:06:18 - loss: 3.2312 - regression_loss: 2.3577 - classification_lo 2115/10000 [=====>........................] - ETA: 1:06:17 - loss: 3.2311 - regression_loss: 2.3575 - classification_lo 2116/10000 [=====>........................] - ETA: 1:06:16 - loss: 3.2309 - regression_loss: 2.3573 - classification_lo 2117/10000 [=====>........................] - ETA: 1:06:16 - loss: 3.2307 - regression_loss: 2.3572 - classification_lo 2118/10000 [=====>........................] - ETA: 1:06:15 - loss: 3.2304 - regression_loss: 2.3569 - classification_lo 2119/10000 [=====>........................] - ETA: 1:06:14 - loss: 3.2303 - regression_loss: 2.3569 - classification_lo 2120/10000 [=====>........................] - ETA: 1:06:14 - loss: 3.2300 - regression_loss: 2.3568 - classification_lo 2121/10000 [=====>........................] - ETA: 1:06:13 - loss: 3.2301 - regression_loss: 2.3569 - classification_lo 2122/10000 [=====>........................] - ETA: 1:06:12 - loss: 3.2296 - regression_loss: 2.3564 - classification_lo 2123/10000 [=====>........................] - ETA: 1:06:12 - loss: 3.2293 - regression_loss: 2.3563 - classification_lo 2124/10000 [=====>........................] - ETA: 1:06:12 - loss: 3.2292 - regression_loss: 2.3563 - classification_lo 2125/10000 [=====>........................] - ETA: 1:06:11 - loss: 3.2287 - regression_loss: 2.3560 - classification_lo 2126/10000 [=====>........................] - ETA: 1:06:11 - loss: 3.2283 - regression_loss: 2.3557 - classification_lo 2127/10000 [=====>........................] - ETA: 1:06:10 - loss: 3.2279 - regression_loss: 2.3555 - classification_lo 2128/10000 [=====>........................] - ETA: 1:06:10 - loss: 3.2279 - regression_loss: 2.3555 - classification_lo 2129/10000 [=====>........................] - ETA: 1:06:09 - loss: 3.2275 - regression_loss: 2.3553 - classification_lo 2130/10000 [=====>........................] - ETA: 1:06:09 - loss: 3.2271 - regression_loss: 2.3550 - classification_lo 2131/10000 [=====>........................] - ETA: 1:06:08 - loss: 3.2277 - regression_loss: 2.3539 - classification_lo 2132/10000 [=====>........................] - ETA: 1:06:07 - loss: 3.2276 - regression_loss: 2.3539 - classification_lo 2133/10000 [=====>........................] - ETA: 1:06:06 - loss: 3.2275 - regression_loss: 2.3539 - classification_lo 2134/10000 [=====>........................] - ETA: 1:06:06 - loss: 3.2273 - regression_loss: 2.3537 - classification_lo 2135/10000 [=====>........................] - ETA: 1:06:05 - loss: 3.2269 - regression_loss: 2.3534 - classification_lo 2136/10000 [=====>........................] - ETA: 1:06:04 - loss: 3.2264 - regression_loss: 2.3530 - classification_lo 2137/10000 [=====>........................] - ETA: 1:06:04 - loss: 3.2262 - regression_loss: 2.3529 - classification_lo 2138/10000 [=====>........................] - ETA: 1:06:03 - loss: 3.2256 - regression_loss: 2.3525 - classification_lo 2139/10000 [=====>........................] - ETA: 1:06:03 - loss: 3.2253 - regression_loss: 2.3523 - classification_lo 2140/10000 [=====>........................] - ETA: 1:06:02 - loss: 3.2248 - regression_loss: 2.3519 - classification_lo 2141/10000 [=====>........................] - ETA: 1:06:01 - loss: 3.2246 - regression_loss: 2.3518 - classification_lo 2142/10000 [=====>........................] - ETA: 1:06:01 - loss: 3.2246 - regression_loss: 2.3519 - classification_lo 2143/10000 [=====>........................] - ETA: 1:06:00 - loss: 3.2243 - regression_loss: 2.3515 - classification_lo 2144/10000 [=====>........................] - ETA: 1:06:00 - loss: 3.2239 - regression_loss: 2.3512 - classification_lo 2145/10000 [=====>........................] - ETA: 1:05:59 - loss: 3.2233 - regression_loss: 2.3508 - classification_lo 2146/10000 [=====>........................] - ETA: 1:05:58 - loss: 3.2233 - regression_loss: 2.3508 - classification_lo 2147/10000 [=====>........................] - ETA: 1:05:58 - loss: 3.2228 - regression_loss: 2.3503 - classification_lo 2148/10000 [=====>........................] - ETA: 1:05:57 - loss: 3.2224 - regression_loss: 2.3500 - classification_lo 2149/10000 [=====>........................] - ETA: 1:05:56 - loss: 3.2223 - regression_loss: 2.3500 - classification_lo 2150/10000 [=====>........................] - ETA: 1:05:56 - loss: 3.2223 - regression_loss: 2.3501 - classification_lo 2151/10000 [=====>........................] - ETA: 1:05:55 - loss: 3.2221 - regression_loss: 2.3499 - classification_lo 2152/10000 [=====>........................] - ETA: 1:05:54 - loss: 3.2218 - regression_loss: 2.3496 - classification_lo 2153/10000 [=====>........................] - ETA: 1:05:54 - loss: 3.2212 - regression_loss: 2.3490 - classification_lo 2154/10000 [=====>........................] - ETA: 1:05:53 - loss: 3.2211 - regression_loss: 2.3490 - classification_lo 2155/10000 [=====>........................] - ETA: 1:05:52 - loss: 3.2207 - regression_loss: 2.3488 - classification_lo 2156/10000 [=====>........................] - ETA: 1:05:52 - loss: 3.2205 - regression_loss: 2.3486 - classification_lo 2157/10000 [=====>........................] - ETA: 1:05:51 - loss: 3.2205 - regression_loss: 2.3486 - classification_lo 2158/10000 [=====>........................] - ETA: 1:05:53 - loss: 3.2210 - regression_loss: 2.3491 - classification_lo 2159/10000 [=====>........................] - ETA: 1:05:52 - loss: 3.2205 - regression_loss: 2.3487 - classification_lo 2160/10000 [=====>........................] - ETA: 1:05:53 - loss: 3.2204 - regression_loss: 2.3487 - classification_lo 2161/10000 [=====>........................] - ETA: 1:05:52 - loss: 3.2203 - regression_loss: 2.3487 - classification_lo 2162/10000 [=====>........................] - ETA: 1:05:52 - loss: 3.2200 - regression_loss: 2.3484 - classification_lo 2163/10000 [=====>........................] - ETA: 1:05:51 - loss: 3.2195 - regression_loss: 2.3481 - classification_lo 2164/10000 [=====>........................] - ETA: 1:05:50 - loss: 3.2193 - regression_loss: 2.3479 - classification_lo 2165/10000 [=====>........................] - ETA: 1:05:50 - loss: 3.2187 - regression_loss: 2.3474 - classification_lo 2166/10000 [=====>........................] - ETA: 1:05:49 - loss: 3.2183 - regression_loss: 2.3471 - classification_lo 2167/10000 [=====>........................] - ETA: 1:05:49 - loss: 3.2179 - regression_loss: 2.3468 - classification_lo 2168/10000 [=====>........................] - ETA: 1:05:48 - loss: 3.2179 - regression_loss: 2.3467 - classification_lo 2169/10000 [=====>........................] - ETA: 1:05:47 - loss: 3.2177 - regression_loss: 2.3465 - classification_lo 2170/10000 [=====>........................] - ETA: 1:05:47 - loss: 3.2183 - regression_loss: 2.3470 - classification_lo 2171/10000 [=====>........................] - ETA: 1:05:46 - loss: 3.2180 - regression_loss: 2.3468 - classification_lo 2172/10000 [=====>........................] - ETA: 1:05:45 - loss: 3.2180 - regression_loss: 2.3468 - classification_lo 2173/10000 [=====>........................] - ETA: 1:05:44 - loss: 3.2179 - regression_loss: 2.3467 - classification_lo 2174/10000 [=====>........................] - ETA: 1:05:44 - loss: 3.2177 - regression_loss: 2.3466 - classification_lo 2175/10000 [=====>........................] - ETA: 1:05:43 - loss: 3.2176 - regression_loss: 2.3467 - classification_lo 2176/10000 [=====>........................] - ETA: 1:05:42 - loss: 3.2175 - regression_loss: 2.3467 - classification_lo 2177/10000 [=====>........................] - ETA: 1:05:42 - loss: 3.2174 - regression_loss: 2.3467 - classification_lo 2178/10000 [=====>........................] - ETA: 1:05:41 - loss: 3.2172 - regression_loss: 2.3466 - classification_lo 2179/10000 [=====>........................] - ETA: 1:05:40 - loss: 3.2169 - regression_loss: 2.3464 - classification_lo 2180/10000 [=====>........................] - ETA: 1:05:40 - loss: 3.2163 - regression_loss: 2.3457 - classification_lo 2181/10000 [=====>........................] - ETA: 1:05:39 - loss: 3.2161 - regression_loss: 2.3457 - classification_lo 2182/10000 [=====>........................] - ETA: 1:05:39 - loss: 3.2160 - regression_loss: 2.3458 - classification_lo 2183/10000 [=====>........................] - ETA: 1:05:38 - loss: 3.2159 - regression_loss: 2.3457 - classification_lo 2184/10000 [=====>........................] - ETA: 1:05:37 - loss: 3.2156 - regression_loss: 2.3454 - classification_lo 2185/10000 [=====>........................] - ETA: 1:05:36 - loss: 3.2152 - regression_loss: 2.3452 - classification_lo 2186/10000 [=====>........................] - ETA: 1:05:36 - loss: 3.2150 - regression_loss: 2.3451 - classification_lo 2187/10000 [=====>........................] - ETA: 1:05:35 - loss: 3.2145 - regression_loss: 2.3447 - classification_lo 2188/10000 [=====>........................] - ETA: 1:05:35 - loss: 3.2147 - regression_loss: 2.3450 - classification_lo 2189/10000 [=====>........................] - ETA: 1:05:34 - loss: 3.2145 - regression_loss: 2.3449 - classification_lo 2190/10000 [=====>........................] - ETA: 1:05:34 - loss: 3.2144 - regression_loss: 2.3449 - classification_lo 2191/10000 [=====>........................] - ETA: 1:05:33 - loss: 3.2141 - regression_loss: 2.3447 - classification_lo 2192/10000 [=====>........................] - ETA: 1:05:32 - loss: 3.2140 - regression_loss: 2.3446 - classification_lo 2193/10000 [=====>........................] - ETA: 1:05:32 - loss: 3.2138 - regression_loss: 2.3444 - classification_lo 2194/10000 [=====>........................] - ETA: 1:05:31 - loss: 3.2137 - regression_loss: 2.3442 - classification_lo 2195/10000 [=====>........................] - ETA: 1:05:30 - loss: 3.2135 - regression_loss: 2.3441 - classification_lo 2196/10000 [=====>........................] - ETA: 1:05:30 - loss: 3.2134 - regression_loss: 2.3442 - classification_lo 2197/10000 [=====>........................] - ETA: 1:05:29 - loss: 3.2133 - regression_loss: 2.3441 - classification_lo 2198/10000 [=====>........................] - ETA: 1:05:28 - loss: 3.2132 - regression_loss: 2.3441 - classification_lo 2199/10000 [=====>........................] - ETA: 1:05:28 - loss: 3.2129 - regression_loss: 2.3438 - classification_lo 2200/10000 [=====>........................] - ETA: 1:05:27 - loss: 3.2127 - regression_loss: 2.3437 - classification_lo 2201/10000 [=====>........................] - ETA: 1:05:26 - loss: 3.2126 - regression_loss: 2.3436 - classification_lo 2202/10000 [=====>........................] - ETA: 1:05:26 - loss: 3.2125 - regression_loss: 2.3436 - classification_lo 2203/10000 [=====>........................] - ETA: 1:05:25 - loss: 3.2124 - regression_loss: 2.3435 - classification_lo 2204/10000 [=====>........................] - ETA: 1:05:25 - loss: 3.2121 - regression_loss: 2.3433 - classification_lo 2205/10000 [=====>........................] - ETA: 1:05:24 - loss: 3.2120 - regression_loss: 2.3432 - classification_lo 2206/10000 [=====>........................] - ETA: 1:05:23 - loss: 3.2114 - regression_loss: 2.3426 - classification_lo 2207/10000 [=====>........................] - ETA: 1:05:22 - loss: 3.2113 - regression_loss: 2.3426 - classification_lo 2208/10000 [=====>........................] - ETA: 1:05:22 - loss: 3.2111 - regression_loss: 2.3425 - classification_lo 2209/10000 [=====>........................] - ETA: 1:05:21 - loss: 3.2109 - regression_loss: 2.3424 - classification_lo 2210/10000 [=====>........................] - ETA: 1:05:21 - loss: 3.2103 - regression_loss: 2.3419 - classification_lo 2211/10000 [=====>........................] - ETA: 1:05:20 - loss: 3.2096 - regression_loss: 2.3414 - classification_lo 2212/10000 [=====>........................] - ETA: 1:05:19 - loss: 3.2092 - regression_loss: 2.3411 - classification_lo 2213/10000 [=====>........................] - ETA: 1:05:18 - loss: 3.2087 - regression_loss: 2.3407 - classification_lo 2214/10000 [=====>........................] - ETA: 1:05:18 - loss: 3.2086 - regression_loss: 2.3407 - classification_lo 2215/10000 [=====>........................] - ETA: 1:05:17 - loss: 3.2085 - regression_loss: 2.3407 - classification_lo 2216/10000 [=====>........................] - ETA: 1:05:16 - loss: 3.2083 - regression_loss: 2.3406 - classification_lo 2217/10000 [=====>........................] - ETA: 1:05:18 - loss: 3.2080 - regression_loss: 2.3404 - classification_lo 2218/10000 [=====>........................] - ETA: 1:05:17 - loss: 3.2076 - regression_loss: 2.3402 - classification_lo 2219/10000 [=====>........................] - ETA: 1:05:16 - loss: 3.2074 - regression_loss: 2.3400 - classification_lo 2220/10000 [=====>........................] - ETA: 1:05:16 - loss: 3.2069 - regression_loss: 2.3396 - classification_lo 2221/10000 [=====>........................] - ETA: 1:05:15 - loss: 3.2061 - regression_loss: 2.3389 - classification_lo 2222/10000 [=====>........................] - ETA: 1:05:15 - loss: 3.2057 - regression_loss: 2.3386 - classification_lo 2223/10000 [=====>........................] - ETA: 1:05:14 - loss: 3.2051 - regression_loss: 2.3382 - classification_lo 2224/10000 [=====>........................] - ETA: 1:05:13 - loss: 3.2050 - regression_loss: 2.3381 - classification_lo 2225/10000 [=====>........................] - ETA: 1:05:13 - loss: 3.2048 - regression_loss: 2.3381 - classification_lo 2226/10000 [=====>........................] - ETA: 1:05:12 - loss: 3.2044 - regression_loss: 2.3378 - classification_lo 2227/10000 [=====>........................] - ETA: 1:05:12 - loss: 3.2042 - regression_loss: 2.3377 - classification_lo 2228/10000 [=====>........................] - ETA: 1:05:11 - loss: 3.2037 - regression_loss: 2.3373 - classification_lo 2229/10000 [=====>........................] - ETA: 1:05:10 - loss: 3.2039 - regression_loss: 2.3375 - classification_lo 2230/10000 [=====>........................] - ETA: 1:05:09 - loss: 3.2037 - regression_loss: 2.3373 - classification_lo 2231/10000 [=====>........................] - ETA: 1:05:09 - loss: 3.2037 - regression_loss: 2.3373 - classification_lo 2232/10000 [=====>........................] - ETA: 1:05:08 - loss: 3.2035 - regression_loss: 2.3372 - classification_lo 2233/10000 [=====>........................] - ETA: 1:05:07 - loss: 3.2033 - regression_loss: 2.3371 - classification_lo 2234/10000 [=====>........................] - ETA: 1:05:07 - loss: 3.2029 - regression_loss: 2.3368 - classification_lo 2235/10000 [=====>........................] - ETA: 1:05:06 - loss: 3.2025 - regression_loss: 2.3366 - classification_lo 2236/10000 [=====>........................] - ETA: 1:05:05 - loss: 3.2023 - regression_loss: 2.3364 - classification_lo 2237/10000 [=====>........................] - ETA: 1:05:04 - loss: 3.2019 - regression_loss: 2.3362 - classification_lo 2238/10000 [=====>........................] - ETA: 1:05:04 - loss: 3.2013 - regression_loss: 2.3357 - classification_lo 2239/10000 [=====>........................] - ETA: 1:05:03 - loss: 3.2014 - regression_loss: 2.3358 - classification_lo 2240/10000 [=====>........................] - ETA: 1:05:03 - loss: 3.2012 - regression_loss: 2.3356 - classification_lo 2241/10000 [=====>........................] - ETA: 1:05:02 - loss: 3.2012 - regression_loss: 2.3356 - classification_lo 2242/10000 [=====>........................] - ETA: 1:05:01 - loss: 3.2011 - regression_loss: 2.3355 - classification_lo 2243/10000 [=====>........................] - ETA: 1:05:01 - loss: 3.2008 - regression_loss: 2.3353 - classification_lo 2244/10000 [=====>........................] - ETA: 1:05:00 - loss: 3.2005 - regression_loss: 2.3351 - classification_lo 2245/10000 [=====>........................] - ETA: 1:05:00 - loss: 3.2002 - regression_loss: 2.3348 - classification_lo 2246/10000 [=====>........................] - ETA: 1:04:59 - loss: 3.2000 - regression_loss: 2.3347 - classification_lo 2247/10000 [=====>........................] - ETA: 1:04:58 - loss: 3.1996 - regression_loss: 2.3344 - classification_lo 2248/10000 [=====>........................] - ETA: 1:04:58 - loss: 3.1991 - regression_loss: 2.3340 - classification_lo 2249/10000 [=====>........................] - ETA: 1:04:57 - loss: 3.1986 - regression_loss: 2.3337 - classification_lo 2250/10000 [=====>........................] - ETA: 1:04:56 - loss: 3.1988 - regression_loss: 2.3337 - classification_lo 2251/10000 [=====>........................] - ETA: 1:04:56 - loss: 3.1987 - regression_loss: 2.3336 - classification_lo 2252/10000 [=====>........................] - ETA: 1:04:55 - loss: 3.1981 - regression_loss: 2.3332 - classification_lo 2253/10000 [=====>........................] - ETA: 1:04:54 - loss: 3.1981 - regression_loss: 2.3332 - classification_lo 2254/10000 [=====>........................] - ETA: 1:04:54 - loss: 3.1977 - regression_loss: 2.3329 - classification_lo 2255/10000 [=====>........................] - ETA: 1:04:53 - loss: 3.1975 - regression_loss: 2.3327 - classification_lo 2256/10000 [=====>........................] - ETA: 1:04:52 - loss: 3.1969 - regression_loss: 2.3323 - classification_lo 2257/10000 [=====>........................] - ETA: 1:04:52 - loss: 3.1965 - regression_loss: 2.3320 - classification_lo 2258/10000 [=====>........................] - ETA: 1:04:51 - loss: 3.1962 - regression_loss: 2.3318 - classification_lo 2259/10000 [=====>........................] - ETA: 1:04:50 - loss: 3.1954 - regression_loss: 2.3312 - classification_lo 2260/10000 [=====>........................] - ETA: 1:04:50 - loss: 3.1953 - regression_loss: 2.3311 - classification_lo 2261/10000 [=====>........................] - ETA: 1:04:49 - loss: 3.1950 - regression_loss: 2.3310 - classification_lo 2262/10000 [=====>........................] - ETA: 1:04:48 - loss: 3.1948 - regression_loss: 2.3309 - classification_lo 2263/10000 [=====>........................] - ETA: 1:04:47 - loss: 3.1948 - regression_loss: 2.3310 - classification_lo 2264/10000 [=====>........................] - ETA: 1:04:47 - loss: 3.1945 - regression_loss: 2.3307 - classification_lo 2265/10000 [=====>........................] - ETA: 1:04:47 - loss: 3.1942 - regression_loss: 2.3305 - classification_lo 2266/10000 [=====>........................] - ETA: 1:04:46 - loss: 3.1947 - regression_loss: 2.3295 - classification_lo 2267/10000 [=====>........................] - ETA: 1:04:45 - loss: 3.1945 - regression_loss: 2.3293 - classification_lo 2268/10000 [=====>........................] - ETA: 1:04:45 - loss: 3.1940 - regression_loss: 2.3288 - classification_lo 2269/10000 [=====>........................] - ETA: 1:04:44 - loss: 3.1933 - regression_loss: 2.3283 - classification_lo 2270/10000 [=====>........................] - ETA: 1:04:43 - loss: 3.1929 - regression_loss: 2.3279 - classification_lo 2271/10000 [=====>........................] - ETA: 1:04:43 - loss: 3.1929 - regression_loss: 2.3279 - classification_lo 2272/10000 [=====>........................] - ETA: 1:04:44 - loss: 3.1924 - regression_loss: 2.3275 - classification_lo 2273/10000 [=====>........................] - ETA: 1:04:43 - loss: 3.1923 - regression_loss: 2.3275 - classification_lo 2274/10000 [=====>........................] - ETA: 1:04:43 - loss: 3.1919 - regression_loss: 2.3273 - classification_lo 2275/10000 [=====>........................] - ETA: 1:04:42 - loss: 3.1915 - regression_loss: 2.3271 - classification_lo 2276/10000 [=====>........................] - ETA: 1:04:41 - loss: 3.1913 - regression_loss: 2.3269 - classification_lo 2277/10000 [=====>........................] - ETA: 1:04:41 - loss: 3.1911 - regression_loss: 2.3267 - classification_lo 2278/10000 [=====>........................] - ETA: 1:04:40 - loss: 3.1903 - regression_loss: 2.3261 - classification_lo 2279/10000 [=====>........................] - ETA: 1:04:39 - loss: 3.1903 - regression_loss: 2.3260 - classification_lo 2280/10000 [=====>........................] - ETA: 1:04:39 - loss: 3.1901 - regression_loss: 2.3260 - classification_lo 2281/10000 [=====>........................] - ETA: 1:04:38 - loss: 3.1902 - regression_loss: 2.3261 - classification_lo 2282/10000 [=====>........................] - ETA: 1:04:37 - loss: 3.1898 - regression_loss: 2.3257 - classification_lo 2283/10000 [=====>........................] - ETA: 1:04:37 - loss: 3.1896 - regression_loss: 2.3256 - classification_lo 2284/10000 [=====>........................] - ETA: 1:04:36 - loss: 3.1893 - regression_loss: 2.3256 - classification_lo 2285/10000 [=====>........................] - ETA: 1:04:36 - loss: 3.1887 - regression_loss: 2.3251 - classification_lo 2286/10000 [=====>........................] - ETA: 1:04:35 - loss: 3.1886 - regression_loss: 2.3249 - classification_lo 2287/10000 [=====>........................] - ETA: 1:04:34 - loss: 3.1886 - regression_loss: 2.3250 - classification_lo 2288/10000 [=====>........................] - ETA: 1:04:34 - loss: 3.1886 - regression_loss: 2.3251 - classification_lo 2289/10000 [=====>........................] - ETA: 1:04:33 - loss: 3.1886 - regression_loss: 2.3250 - classification_lo 2290/10000 [=====>........................] - ETA: 1:04:32 - loss: 3.1892 - regression_loss: 2.3254 - classification_lo 2291/10000 [=====>........................] - ETA: 1:04:32 - loss: 3.1893 - regression_loss: 2.3255 - classification_lo 2292/10000 [=====>........................] - ETA: 1:04:31 - loss: 3.1891 - regression_loss: 2.3254 - classification_lo 2293/10000 [=====>........................] - ETA: 1:04:31 - loss: 3.1888 - regression_loss: 2.3252 - classification_lo 2294/10000 [=====>........................] - ETA: 1:04:30 - loss: 3.1882 - regression_loss: 2.3248 - classification_lo 2295/10000 [=====>........................] - ETA: 1:04:29 - loss: 3.1878 - regression_loss: 2.3245 - classification_lo 2296/10000 [=====>........................] - ETA: 1:04:29 - loss: 3.1872 - regression_loss: 2.3240 - classification_lo 2297/10000 [=====>........................] - ETA: 1:04:28 - loss: 3.1870 - regression_loss: 2.3239 - classification_lo 2298/10000 [=====>........................] - ETA: 1:04:27 - loss: 3.1870 - regression_loss: 2.3241 - classification_lo 2299/10000 [=====>........................] - ETA: 1:04:27 - loss: 3.1868 - regression_loss: 2.3240 - classification_lo 2300/10000 [=====>........................] - ETA: 1:04:26 - loss: 3.1867 - regression_loss: 2.3240 - classification_lo 2301/10000 [=====>........................] - ETA: 1:04:25 - loss: 3.1866 - regression_loss: 2.3240 - classification_lo 2302/10000 [=====>........................] - ETA: 1:04:25 - loss: 3.1864 - regression_loss: 2.3238 - classification_lo 2303/10000 [=====>........................] - ETA: 1:04:24 - loss: 3.1862 - regression_loss: 2.3237 - classification_lo 2304/10000 [=====>........................] - ETA: 1:04:24 - loss: 3.1860 - regression_loss: 2.3235 - classification_lo 2305/10000 [=====>........................] - ETA: 1:04:23 - loss: 3.1860 - regression_loss: 2.3236 - classification_lo 2306/10000 [=====>........................] - ETA: 1:04:22 - loss: 3.1853 - regression_loss: 2.3230 - classification_lo 2307/10000 [=====>........................] - ETA: 1:04:22 - loss: 3.1850 - regression_loss: 2.3229 - classification_lo 2308/10000 [=====>........................] - ETA: 1:04:21 - loss: 3.1845 - regression_loss: 2.3226 - classification_lo 2309/10000 [=====>........................] - ETA: 1:04:21 - loss: 3.1841 - regression_loss: 2.3223 - classification_lo 2310/10000 [=====>........................] - ETA: 1:04:20 - loss: 3.1836 - regression_loss: 2.3219 - classification_lo 2311/10000 [=====>........................] - ETA: 1:04:19 - loss: 3.1834 - regression_loss: 2.3217 - classification_lo 2312/10000 [=====>........................] - ETA: 1:04:19 - loss: 3.1833 - regression_loss: 2.3217 - classification_lo 2313/10000 [=====>........................] - ETA: 1:04:18 - loss: 3.1830 - regression_loss: 2.3214 - classification_lo 2314/10000 [=====>........................] - ETA: 1:04:17 - loss: 3.1827 - regression_loss: 2.3213 - classification_lo 2315/10000 [=====>........................] - ETA: 1:04:17 - loss: 3.1828 - regression_loss: 2.3214 - classification_lo 2316/10000 [=====>........................] - ETA: 1:04:16 - loss: 3.1824 - regression_loss: 2.3212 - classification_lo 2317/10000 [=====>........................] - ETA: 1:04:15 - loss: 3.1832 - regression_loss: 2.3216 - classification_lo 2318/10000 [=====>........................] - ETA: 1:04:15 - loss: 3.1831 - regression_loss: 2.3217 - classification_lo 2319/10000 [=====>........................] - ETA: 1:04:14 - loss: 3.1830 - regression_loss: 2.3217 - classification_lo 2320/10000 [=====>........................] - ETA: 1:04:13 - loss: 3.1831 - regression_loss: 2.3220 - classification_lo 2321/10000 [=====>........................] - ETA: 1:04:13 - loss: 3.1827 - regression_loss: 2.3217 - classification_lo 2322/10000 [=====>........................] - ETA: 1:04:12 - loss: 3.1828 - regression_loss: 2.3218 - classification_lo 2323/10000 [=====>........................] - ETA: 1:04:12 - loss: 3.1826 - regression_loss: 2.3216 - classification_lo 2324/10000 [=====>........................] - ETA: 1:04:11 - loss: 3.1820 - regression_loss: 2.3212 - classification_lo 2325/10000 [=====>........................] - ETA: 1:04:10 - loss: 3.1816 - regression_loss: 2.3208 - classification_lo 2326/10000 [=====>........................] - ETA: 1:04:10 - loss: 3.1813 - regression_loss: 2.3205 - classification_lo 2327/10000 [=====>........................] - ETA: 1:04:09 - loss: 3.1810 - regression_loss: 2.3203 - classification_lo 2328/10000 [=====>........................] - ETA: 1:04:08 - loss: 3.1809 - regression_loss: 2.3203 - classification_lo 2329/10000 [=====>........................] - ETA: 1:04:08 - loss: 3.1807 - regression_loss: 2.3201 - classification_lo 2330/10000 [=====>........................] - ETA: 1:04:07 - loss: 3.1803 - regression_loss: 2.3199 - classification_lo 2331/10000 [=====>........................] - ETA: 1:04:07 - loss: 3.1802 - regression_loss: 2.3198 - classification_lo 2332/10000 [=====>........................] - ETA: 1:04:06 - loss: 3.1800 - regression_loss: 2.3197 - classification_lo 2333/10000 [=====>........................] - ETA: 1:04:05 - loss: 3.1800 - regression_loss: 2.3198 - classification_lo 2334/10000 [======>.......................] - ETA: 1:04:04 - loss: 3.1801 - regression_loss: 2.3198 - classification_lo 2335/10000 [======>.......................] - ETA: 1:04:05 - loss: 3.1798 - regression_loss: 2.3196 - classification_lo 2336/10000 [======>.......................] - ETA: 1:04:05 - loss: 3.1802 - regression_loss: 2.3199 - classification_lo 2337/10000 [======>.......................] - ETA: 1:04:04 - loss: 3.1799 - regression_loss: 2.3195 - classification_lo 2338/10000 [======>.......................] - ETA: 1:04:04 - loss: 3.1798 - regression_loss: 2.3195 - classification_lo 2339/10000 [======>.......................] - ETA: 1:04:03 - loss: 3.1799 - regression_loss: 2.3197 - classification_lo 2340/10000 [======>.......................] - ETA: 1:04:03 - loss: 3.1799 - regression_loss: 2.3197 - classification_lo 2341/10000 [======>.......................] - ETA: 1:04:02 - loss: 3.1792 - regression_loss: 2.3192 - classification_lo 2342/10000 [======>.......................] - ETA: 1:04:02 - loss: 3.1789 - regression_loss: 2.3190 - classification_lo 2343/10000 [======>.......................] - ETA: 1:04:01 - loss: 3.1784 - regression_loss: 2.3186 - classification_lo 2344/10000 [======>.......................] - ETA: 1:04:00 - loss: 3.1780 - regression_loss: 2.3184 - classification_lo 2345/10000 [======>.......................] - ETA: 1:04:00 - loss: 3.1780 - regression_loss: 2.3183 - classification_lo 2346/10000 [======>.......................] - ETA: 1:03:59 - loss: 3.1778 - regression_loss: 2.3183 - classification_lo 2347/10000 [======>.......................] - ETA: 1:03:59 - loss: 3.1780 - regression_loss: 2.3184 - classification_lo 2348/10000 [======>.......................] - ETA: 1:03:58 - loss: 3.1776 - regression_loss: 2.3180 - classification_lo 2349/10000 [======>.......................] - ETA: 1:03:58 - loss: 3.1772 - regression_loss: 2.3176 - classification_lo 2350/10000 [======>.......................] - ETA: 1:03:57 - loss: 3.1769 - regression_loss: 2.3173 - classification_lo 2351/10000 [======>.......................] - ETA: 1:03:56 - loss: 3.1769 - regression_loss: 2.3173 - classification_lo 2352/10000 [======>.......................] - ETA: 1:03:56 - loss: 3.1770 - regression_loss: 2.3174 - classification_lo 2353/10000 [======>.......................] - ETA: 1:03:55 - loss: 3.1769 - regression_loss: 2.3173 - classification_lo 2354/10000 [======>.......................] - ETA: 1:03:55 - loss: 3.1767 - regression_loss: 2.3171 - classification_lo 2355/10000 [======>.......................] - ETA: 1:03:54 - loss: 3.1767 - regression_loss: 2.3171 - classification_lo 2356/10000 [======>.......................] - ETA: 1:03:53 - loss: 3.1765 - regression_loss: 2.3169 - classification_lo 2357/10000 [======>.......................] - ETA: 1:03:53 - loss: 3.1760 - regression_loss: 2.3166 - classification_lo 2358/10000 [======>.......................] - ETA: 1:03:52 - loss: 3.1755 - regression_loss: 2.3163 - classification_lo 2359/10000 [======>.......................] - ETA: 1:03:51 - loss: 3.1752 - regression_loss: 2.3161 - classification_lo 2360/10000 [======>.......................] - ETA: 1:03:51 - loss: 3.1752 - regression_loss: 2.3161 - classification_lo 2361/10000 [======>.......................] - ETA: 1:03:50 - loss: 3.1750 - regression_loss: 2.3159 - classification_lo 2362/10000 [======>.......................] - ETA: 1:03:49 - loss: 3.1750 - regression_loss: 2.3160 - classification_lo 2363/10000 [======>.......................] - ETA: 1:03:49 - loss: 3.1744 - regression_loss: 2.3155 - classification_lo 2364/10000 [======>.......................] - ETA: 1:03:48 - loss: 3.1738 - regression_loss: 2.3151 - classification_lo 2365/10000 [======>.......................] - ETA: 1:03:48 - loss: 3.1736 - regression_loss: 2.3149 - classification_lo 2366/10000 [======>.......................] - ETA: 1:03:47 - loss: 3.1732 - regression_loss: 2.3146 - classification_lo 2367/10000 [======>.......................] - ETA: 1:03:46 - loss: 3.1731 - regression_loss: 2.3146 - classification_lo 2368/10000 [======>.......................] - ETA: 1:03:46 - loss: 3.1725 - regression_loss: 2.3141 - classification_lo 2369/10000 [======>.......................] - ETA: 1:03:45 - loss: 3.1723 - regression_loss: 2.3140 - classification_lo 2370/10000 [======>.......................] - ETA: 1:03:45 - loss: 3.1719 - regression_loss: 2.3136 - classification_lo 2371/10000 [======>.......................] - ETA: 1:03:44 - loss: 3.1714 - regression_loss: 2.3133 - classification_lo 2372/10000 [======>.......................] - ETA: 1:03:43 - loss: 3.1715 - regression_loss: 2.3133 - classification_lo 2373/10000 [======>.......................] - ETA: 1:03:43 - loss: 3.1713 - regression_loss: 2.3132 - classification_lo 2374/10000 [======>.......................] - ETA: 1:03:42 - loss: 3.1712 - regression_loss: 2.3131 - classification_lo 2375/10000 [======>.......................] - ETA: 1:03:41 - loss: 3.1710 - regression_loss: 2.3129 - classification_lo 2376/10000 [======>.......................] - ETA: 1:03:41 - loss: 3.1704 - regression_loss: 2.3124 - classification_lo 2377/10000 [======>.......................] - ETA: 1:03:40 - loss: 3.1696 - regression_loss: 2.3118 - classification_lo 2378/10000 [======>.......................] - ETA: 1:03:40 - loss: 3.1690 - regression_loss: 2.3113 - classification_lo 2379/10000 [======>.......................] - ETA: 1:03:39 - loss: 3.1686 - regression_loss: 2.3110 - classification_lo 2380/10000 [======>.......................] - ETA: 1:03:38 - loss: 3.1683 - regression_loss: 2.3108 - classification_lo 2381/10000 [======>.......................] - ETA: 1:03:38 - loss: 3.1683 - regression_loss: 2.3108 - classification_lo 2382/10000 [======>.......................] - ETA: 1:03:37 - loss: 3.1681 - regression_loss: 2.3107 - classification_lo 2383/10000 [======>.......................] - ETA: 1:03:36 - loss: 3.1677 - regression_loss: 2.3104 - classification_lo 2384/10000 [======>.......................] - ETA: 1:03:36 - loss: 3.1675 - regression_loss: 2.3103 - classification_lo 2385/10000 [======>.......................] - ETA: 1:03:35 - loss: 3.1673 - regression_loss: 2.3101 - classification_lo 2386/10000 [======>.......................] - ETA: 1:03:34 - loss: 3.1670 - regression_loss: 2.3101 - classification_lo 2387/10000 [======>.......................] - ETA: 1:03:34 - loss: 3.1668 - regression_loss: 2.3100 - classification_lo 2388/10000 [======>.......................] - ETA: 1:03:33 - loss: 3.1666 - regression_loss: 2.3100 - classification_lo 2389/10000 [======>.......................] - ETA: 1:03:34 - loss: 3.1661 - regression_loss: 2.3095 - classification_lo 2390/10000 [======>.......................] - ETA: 1:03:34 - loss: 3.1658 - regression_loss: 2.3092 - classification_lo 2391/10000 [======>.......................] - ETA: 1:03:33 - loss: 3.1656 - regression_loss: 2.3091 - classification_lo 2392/10000 [======>.......................] - ETA: 1:03:33 - loss: 3.1654 - regression_loss: 2.3091 - classification_lo 2393/10000 [======>.......................] - ETA: 1:03:32 - loss: 3.1650 - regression_loss: 2.3088 - classification_lo 2394/10000 [======>.......................] - ETA: 1:03:31 - loss: 3.1645 - regression_loss: 2.3083 - classification_lo 2395/10000 [======>.......................] - ETA: 1:03:31 - loss: 3.1644 - regression_loss: 2.3082 - classification_lo 2396/10000 [======>.......................] - ETA: 1:03:30 - loss: 3.1641 - regression_loss: 2.3081 - classification_lo 2397/10000 [======>.......................] - ETA: 1:03:29 - loss: 3.1641 - regression_loss: 2.3081 - classification_lo 2398/10000 [======>.......................] - ETA: 1:03:29 - loss: 3.1641 - regression_loss: 2.3081 - classification_lo 2399/10000 [======>.......................] - ETA: 1:03:28 - loss: 3.1637 - regression_loss: 2.3079 - classification_lo 2400/10000 [======>.......................] - ETA: 1:03:27 - loss: 3.1631 - regression_loss: 2.3075 - classification_lo 2401/10000 [======>.......................] - ETA: 1:03:27 - loss: 3.1628 - regression_loss: 2.3073 - classification_lo 2402/10000 [======>.......................] - ETA: 1:03:26 - loss: 3.1625 - regression_loss: 2.3071 - classification_lo 2403/10000 [======>.......................] - ETA: 1:03:26 - loss: 3.1622 - regression_loss: 2.3068 - classification_lo 2404/10000 [======>.......................] - ETA: 1:03:25 - loss: 3.1619 - regression_loss: 2.3066 - classification_lo 2405/10000 [======>.......................] - ETA: 1:03:24 - loss: 3.1618 - regression_loss: 2.3065 - classification_lo 2406/10000 [======>.......................] - ETA: 1:03:24 - loss: 3.1616 - regression_loss: 2.3064 - classification_lo 2407/10000 [======>.......................] - ETA: 1:03:23 - loss: 3.1614 - regression_loss: 2.3064 - classification_lo 2408/10000 [======>.......................] - ETA: 1:03:23 - loss: 3.1609 - regression_loss: 2.3060 - classification_lo 2409/10000 [======>.......................] - ETA: 1:03:22 - loss: 3.1608 - regression_loss: 2.3060 - classification_lo 2410/10000 [======>.......................] - ETA: 1:03:21 - loss: 3.1605 - regression_loss: 2.3058 - classification_lo 2411/10000 [======>.......................] - ETA: 1:03:21 - loss: 3.1603 - regression_loss: 2.3058 - classification_lo 2412/10000 [======>.......................] - ETA: 1:03:20 - loss: 3.1601 - regression_loss: 2.3057 - classification_lo 2413/10000 [======>.......................] - ETA: 1:03:19 - loss: 3.1596 - regression_loss: 2.3052 - classification_lo 2414/10000 [======>.......................] - ETA: 1:03:18 - loss: 3.1595 - regression_loss: 2.3052 - classification_lo 2415/10000 [======>.......................] - ETA: 1:03:18 - loss: 3.1590 - regression_loss: 2.3048 - classification_lo 2416/10000 [======>.......................] - ETA: 1:03:18 - loss: 3.1590 - regression_loss: 2.3049 - classification_lo 2417/10000 [======>.......................] - ETA: 1:03:17 - loss: 3.1586 - regression_loss: 2.3046 - classification_lo 2418/10000 [======>.......................] - ETA: 1:03:16 - loss: 3.1585 - regression_loss: 2.3044 - classification_lo 2419/10000 [======>.......................] - ETA: 1:03:17 - loss: 3.1582 - regression_loss: 2.3042 - classification_lo 2420/10000 [======>.......................] - ETA: 1:03:17 - loss: 3.1581 - regression_loss: 2.3043 - classification_lo 2421/10000 [======>.......................] - ETA: 1:03:16 - loss: 3.1578 - regression_loss: 2.3041 - classification_lo 2422/10000 [======>.......................] - ETA: 1:03:15 - loss: 3.1576 - regression_loss: 2.3040 - classification_lo 2423/10000 [======>.......................] - ETA: 1:03:15 - loss: 3.1573 - regression_loss: 2.3038 - classification_lo 2424/10000 [======>.......................] - ETA: 1:03:14 - loss: 3.1568 - regression_loss: 2.3034 - classification_lo 2425/10000 [======>.......................] - ETA: 1:03:14 - loss: 3.1568 - regression_loss: 2.3035 - classification_lo 2426/10000 [======>.......................] - ETA: 1:03:13 - loss: 3.1568 - regression_loss: 2.3035 - classification_lo 2427/10000 [======>.......................] - ETA: 1:03:13 - loss: 3.1562 - regression_loss: 2.3031 - classification_lo 2428/10000 [======>.......................] - ETA: 1:03:12 - loss: 3.1561 - regression_loss: 2.3030 - classification_lo 2429/10000 [======>.......................] - ETA: 1:03:11 - loss: 3.1562 - regression_loss: 2.3031 - classification_lo 2430/10000 [======>.......................] - ETA: 1:03:11 - loss: 3.1561 - regression_loss: 2.3031 - classification_lo 2431/10000 [======>.......................] - ETA: 1:03:10 - loss: 3.1561 - regression_loss: 2.3031 - classification_lo 2432/10000 [======>.......................] - ETA: 1:03:09 - loss: 3.1559 - regression_loss: 2.3031 - classification_lo 2433/10000 [======>.......................] - ETA: 1:03:09 - loss: 3.1557 - regression_loss: 2.3028 - classification_lo 2434/10000 [======>.......................] - ETA: 1:03:08 - loss: 3.1553 - regression_loss: 2.3024 - classification_lo 2435/10000 [======>.......................] - ETA: 1:03:07 - loss: 3.1551 - regression_loss: 2.3022 - classification_lo 2436/10000 [======>.......................] - ETA: 1:03:07 - loss: 3.1547 - regression_loss: 2.3020 - classification_lo 2437/10000 [======>.......................] - ETA: 1:03:06 - loss: 3.1544 - regression_loss: 2.3018 - classification_lo 2438/10000 [======>.......................] - ETA: 1:03:05 - loss: 3.1547 - regression_loss: 2.3020 - classification_lo 2439/10000 [======>.......................] - ETA: 1:03:05 - loss: 3.1544 - regression_loss: 2.3017 - classification_lo 2440/10000 [======>.......................] - ETA: 1:03:04 - loss: 3.1540 - regression_loss: 2.3015 - classification_lo 2441/10000 [======>.......................] - ETA: 1:03:04 - loss: 3.1537 - regression_loss: 2.3013 - classification_lo 2442/10000 [======>.......................] - ETA: 1:03:03 - loss: 3.1539 - regression_loss: 2.3013 - classification_lo 2443/10000 [======>.......................] - ETA: 1:03:03 - loss: 3.1539 - regression_loss: 2.3014 - classification_lo 2444/10000 [======>.......................] - ETA: 1:03:02 - loss: 3.1536 - regression_loss: 2.3013 - classification_lo 2445/10000 [======>.......................] - ETA: 1:03:02 - loss: 3.1537 - regression_loss: 2.3014 - classification_lo 2446/10000 [======>.......................] - ETA: 1:03:01 - loss: 3.1534 - regression_loss: 2.3013 - classification_lo 2447/10000 [======>.......................] - ETA: 1:03:00 - loss: 3.1533 - regression_loss: 2.3012 - classification_lo 2448/10000 [======>.......................] - ETA: 1:03:00 - loss: 3.1531 - regression_loss: 2.3011 - classification_lo 2449/10000 [======>.......................] - ETA: 1:02:59 - loss: 3.1527 - regression_loss: 2.3007 - classification_lo 2450/10000 [======>.......................] - ETA: 1:02:58 - loss: 3.1529 - regression_loss: 2.3008 - classification_lo 2451/10000 [======>.......................] - ETA: 1:02:58 - loss: 3.1525 - regression_loss: 2.3006 - classification_lo 2452/10000 [======>.......................] - ETA: 1:02:57 - loss: 3.1522 - regression_loss: 2.3004 - classification_lo 2453/10000 [======>.......................] - ETA: 1:02:56 - loss: 3.1518 - regression_loss: 2.3001 - classification_lo 2454/10000 [======>.......................] - ETA: 1:02:56 - loss: 3.1516 - regression_loss: 2.3000 - classification_lo 2455/10000 [======>.......................] - ETA: 1:02:55 - loss: 3.1515 - regression_loss: 2.3000 - classification_lo 2456/10000 [======>.......................] - ETA: 1:02:54 - loss: 3.1508 - regression_loss: 2.2995 - classification_lo 2457/10000 [======>.......................] - ETA: 1:02:54 - loss: 3.1504 - regression_loss: 2.2992 - classification_lo 2458/10000 [======>.......................] - ETA: 1:02:53 - loss: 3.1501 - regression_loss: 2.2990 - classification_lo 2459/10000 [======>.......................] - ETA: 1:02:53 - loss: 3.1502 - regression_loss: 2.2990 - classification_lo 2460/10000 [======>.......................] - ETA: 1:02:52 - loss: 3.1499 - regression_loss: 2.2988 - classification_lo 2461/10000 [======>.......................] - ETA: 1:02:51 - loss: 3.1497 - regression_loss: 2.2987 - classification_lo 2462/10000 [======>.......................] - ETA: 1:02:51 - loss: 3.1495 - regression_loss: 2.2986 - classification_lo 2463/10000 [======>.......................] - ETA: 1:02:50 - loss: 3.1495 - regression_loss: 2.2986 - classification_lo 2464/10000 [======>.......................] - ETA: 1:02:50 - loss: 3.1493 - regression_loss: 2.2985 - classification_lo 2465/10000 [======>.......................] - ETA: 1:02:49 - loss: 3.1488 - regression_loss: 2.2980 - classification_lo 2466/10000 [======>.......................] - ETA: 1:02:49 - loss: 3.1487 - regression_loss: 2.2981 - classification_lo 2467/10000 [======>.......................] - ETA: 1:02:48 - loss: 3.1487 - regression_loss: 2.2981 - classification_lo 2468/10000 [======>.......................] - ETA: 1:02:47 - loss: 3.1482 - regression_loss: 2.2977 - classification_lo 2469/10000 [======>.......................] - ETA: 1:02:47 - loss: 3.1477 - regression_loss: 2.2974 - classification_lo 2470/10000 [======>.......................] - ETA: 1:02:46 - loss: 3.1478 - regression_loss: 2.2975 - classification_lo 2471/10000 [======>.......................] - ETA: 1:02:45 - loss: 3.1475 - regression_loss: 2.2972 - classification_lo 2472/10000 [======>.......................] - ETA: 1:02:45 - loss: 3.1473 - regression_loss: 2.2972 - classification_lo 2473/10000 [======>.......................] - ETA: 1:02:44 - loss: 3.1473 - regression_loss: 2.2971 - classification_lo 2474/10000 [======>.......................] - ETA: 1:02:43 - loss: 3.1472 - regression_loss: 2.2969 - classification_lo 2475/10000 [======>.......................] - ETA: 1:02:43 - loss: 3.1467 - regression_loss: 2.2965 - classification_lo 2476/10000 [======>.......................] - ETA: 1:02:42 - loss: 3.1465 - regression_loss: 2.2964 - classification_lo 2477/10000 [======>.......................] - ETA: 1:02:42 - loss: 3.1464 - regression_loss: 2.2963 - classification_lo 2478/10000 [======>.......................] - ETA: 1:02:41 - loss: 3.1460 - regression_loss: 2.2960 - classification_lo 2479/10000 [======>.......................] - ETA: 1:02:40 - loss: 3.1456 - regression_loss: 2.2957 - classification_lo 2480/10000 [======>.......................] - ETA: 1:02:40 - loss: 3.1454 - regression_loss: 2.2956 - classification_lo 2481/10000 [======>.......................] - ETA: 1:02:39 - loss: 3.1453 - regression_loss: 2.2956 - classification_lo 2482/10000 [======>.......................] - ETA: 1:02:38 - loss: 3.1451 - regression_loss: 2.2952 - classification_lo 2483/10000 [======>.......................] - ETA: 1:02:38 - loss: 3.1450 - regression_loss: 2.2952 - classification_lo 2484/10000 [======>.......................] - ETA: 1:02:37 - loss: 3.1451 - regression_loss: 2.2951 - classification_lo 2485/10000 [======>.......................] - ETA: 1:02:36 - loss: 3.1448 - regression_loss: 2.2948 - classification_lo 2486/10000 [======>.......................] - ETA: 1:02:36 - loss: 3.1444 - regression_loss: 2.2946 - classification_lo 2487/10000 [======>.......................] - ETA: 1:02:35 - loss: 3.1445 - regression_loss: 2.2946 - classification_lo 2488/10000 [======>.......................] - ETA: 1:02:35 - loss: 3.1442 - regression_loss: 2.2944 - classification_lo 2489/10000 [======>.......................] - ETA: 1:02:34 - loss: 3.1440 - regression_loss: 2.2943 - classification_lo 2490/10000 [======>.......................] - ETA: 1:02:33 - loss: 3.1439 - regression_loss: 2.2943 - classification_lo 2491/10000 [======>.......................] - ETA: 1:02:33 - loss: 3.1439 - regression_loss: 2.2943 - classification_lo 2492/10000 [======>.......................] - ETA: 1:02:32 - loss: 3.1435 - regression_loss: 2.2941 - classification_lo 2493/10000 [======>.......................] - ETA: 1:02:32 - loss: 3.1433 - regression_loss: 2.2940 - classification_lo 2494/10000 [======>.......................] - ETA: 1:02:31 - loss: 3.1429 - regression_loss: 2.2937 - classification_lo 2495/10000 [======>.......................] - ETA: 1:02:30 - loss: 3.1426 - regression_loss: 2.2936 - classification_lo 2496/10000 [======>.......................] - ETA: 1:02:30 - loss: 3.1427 - regression_loss: 2.2936 - classification_lo 2497/10000 [======>.......................] - ETA: 1:02:29 - loss: 3.1429 - regression_loss: 2.2938 - classification_lo 2498/10000 [======>.......................] - ETA: 1:02:29 - loss: 3.1423 - regression_loss: 2.2933 - classification_lo 2499/10000 [======>.......................] - ETA: 1:02:28 - loss: 3.1422 - regression_loss: 2.2932 - classification_lo 2500/10000 [======>.......................] - ETA: 1:02:28 - loss: 3.1421 - regression_loss: 2.2932 - classification_lo 2501/10000 [======>.......................] - ETA: 1:02:27 - loss: 3.1419 - regression_loss: 2.2930 - classification_lo 2502/10000 [======>.......................] - ETA: 1:02:27 - loss: 3.1420 - regression_loss: 2.2931 - classification_lo 2503/10000 [======>.......................] - ETA: 1:02:26 - loss: 3.1420 - regression_loss: 2.2931 - classification_lo 2504/10000 [======>.......................] - ETA: 1:02:25 - loss: 3.1417 - regression_loss: 2.2930 - classification_lo 2505/10000 [======>.......................] - ETA: 1:02:25 - loss: 3.1412 - regression_loss: 2.2926 - classification_lo 2506/10000 [======>.......................] - ETA: 1:02:24 - loss: 3.1411 - regression_loss: 2.2925 - classification_lo 2507/10000 [======>.......................] - ETA: 1:02:23 - loss: 3.1410 - regression_loss: 2.2925 - classification_lo 2508/10000 [======>.......................] - ETA: 1:02:23 - loss: 3.1408 - regression_loss: 2.2924 - classification_lo 2509/10000 [======>.......................] - ETA: 1:02:22 - loss: 3.1408 - regression_loss: 2.2925 - classification_lo 2510/10000 [======>.......................] - ETA: 1:02:21 - loss: 3.1402 - regression_loss: 2.2920 - classification_lo 2511/10000 [======>.......................] - ETA: 1:02:21 - loss: 3.1403 - regression_loss: 2.2921 - classification_lo 2512/10000 [======>.......................] - ETA: 1:02:20 - loss: 3.1400 - regression_loss: 2.2919 - classification_lo 2513/10000 [======>.......................] - ETA: 1:02:19 - loss: 3.1399 - regression_loss: 2.2919 - classification_lo 2514/10000 [======>.......................] - ETA: 1:02:19 - loss: 3.1400 - regression_loss: 2.2920 - classification_lo 2515/10000 [======>.......................] - ETA: 1:02:18 - loss: 3.1400 - regression_loss: 2.2921 - classification_lo 2516/10000 [======>.......................] - ETA: 1:02:17 - loss: 3.1402 - regression_loss: 2.2922 - classification_lo 2517/10000 [======>.......................] - ETA: 1:02:17 - loss: 3.1399 - regression_loss: 2.2919 - classification_lo 2518/10000 [======>.......................] - ETA: 1:02:16 - loss: 3.1396 - regression_loss: 2.2917 - classification_lo 2519/10000 [======>.......................] - ETA: 1:02:16 - loss: 3.1396 - regression_loss: 2.2916 - classification_lo 2520/10000 [======>.......................] - ETA: 1:02:15 - loss: 3.1397 - regression_loss: 2.2917 - classification_lo 2521/10000 [======>.......................] - ETA: 1:02:14 - loss: 3.1401 - regression_loss: 2.2921 - classification_lo 2522/10000 [======>.......................] - ETA: 1:02:14 - loss: 3.1399 - regression_loss: 2.2919 - classification_lo 2523/10000 [======>.......................] - ETA: 1:02:13 - loss: 3.1396 - regression_loss: 2.2919 - classification_lo 2524/10000 [======>.......................] - ETA: 1:02:13 - loss: 3.1395 - regression_loss: 2.2918 - classification_lo 2525/10000 [======>.......................] - ETA: 1:02:13 - loss: 3.1394 - regression_loss: 2.2917 - classification_lo 2526/10000 [======>.......................] - ETA: 1:02:12 - loss: 3.1392 - regression_loss: 2.2915 - classification_lo 2527/10000 [======>.......................] - ETA: 1:02:11 - loss: 3.1390 - regression_loss: 2.2913 - classification_lo 2528/10000 [======>.......................] - ETA: 1:02:11 - loss: 3.1385 - regression_loss: 2.2909 - classification_lo 2529/10000 [======>.......................] - ETA: 1:02:10 - loss: 3.1384 - regression_loss: 2.2908 - classification_lo 2530/10000 [======>.......................] - ETA: 1:02:10 - loss: 3.1386 - regression_loss: 2.2910 - classification_lo 2531/10000 [======>.......................] - ETA: 1:02:09 - loss: 3.1383 - regression_loss: 2.2908 - classification_lo 2532/10000 [======>.......................] - ETA: 1:02:08 - loss: 3.1377 - regression_loss: 2.2903 - classification_lo 2533/10000 [======>.......................] - ETA: 1:02:08 - loss: 3.1375 - regression_loss: 2.2902 - classification_lo 2534/10000 [======>.......................] - ETA: 1:02:07 - loss: 3.1378 - regression_loss: 2.2904 - classification_lo 2535/10000 [======>.......................] - ETA: 1:02:06 - loss: 3.1378 - regression_loss: 2.2904 - classification_lo 2536/10000 [======>.......................] - ETA: 1:02:06 - loss: 3.1376 - regression_loss: 2.2903 - classification_lo 2537/10000 [======>.......................] - ETA: 1:02:06 - loss: 3.1374 - regression_loss: 2.2901 - classification_lo 2538/10000 [======>.......................] - ETA: 1:02:05 - loss: 3.1373 - regression_loss: 2.2901 - classification_lo 2539/10000 [======>.......................] - ETA: 1:02:04 - loss: 3.1376 - regression_loss: 2.2903 - classification_lo 2540/10000 [======>.......................] - ETA: 1:02:04 - loss: 3.1373 - regression_loss: 2.2901 - classification_lo 2541/10000 [======>.......................] - ETA: 1:02:03 - loss: 3.1369 - regression_loss: 2.2897 - classification_lo 2542/10000 [======>.......................] - ETA: 1:02:03 - loss: 3.1365 - regression_loss: 2.2894 - classification_lo 2543/10000 [======>.......................] - ETA: 1:02:02 - loss: 3.1363 - regression_loss: 2.2894 - classification_lo 2544/10000 [======>.......................] - ETA: 1:02:01 - loss: 3.1362 - regression_loss: 2.2892 - classification_lo 2545/10000 [======>.......................] - ETA: 1:02:01 - loss: 3.1364 - regression_loss: 2.2894 - classification_lo 2546/10000 [======>.......................] - ETA: 1:02:00 - loss: 3.1362 - regression_loss: 2.2892 - classification_lo 2547/10000 [======>.......................] - ETA: 1:02:00 - loss: 3.1356 - regression_loss: 2.2888 - classification_lo 2548/10000 [======>.......................] - ETA: 1:01:59 - loss: 3.1356 - regression_loss: 2.2888 - classification_lo 2549/10000 [======>.......................] - ETA: 1:01:59 - loss: 3.1353 - regression_loss: 2.2886 - classification_lo 2550/10000 [======>.......................] - ETA: 1:01:58 - loss: 3.1350 - regression_loss: 2.2883 - classification_lo 2551/10000 [======>.......................] - ETA: 1:01:57 - loss: 3.1346 - regression_loss: 2.2880 - classification_lo 2552/10000 [======>.......................] - ETA: 1:01:57 - loss: 3.1344 - regression_loss: 2.2879 - classification_lo 2553/10000 [======>.......................] - ETA: 1:01:56 - loss: 3.1346 - regression_loss: 2.2880 - classification_lo 2554/10000 [======>.......................] - ETA: 1:01:56 - loss: 3.1344 - regression_loss: 2.2880 - classification_lo 2555/10000 [======>.......................] - ETA: 1:01:55 - loss: 3.1340 - regression_loss: 2.2877 - classification_lo 2556/10000 [======>.......................] - ETA: 1:01:55 - loss: 3.1338 - regression_loss: 2.2875 - classification_lo 2557/10000 [======>.......................] - ETA: 1:01:54 - loss: 3.1333 - regression_loss: 2.2871 - classification_lo 2558/10000 [======>.......................] - ETA: 1:01:53 - loss: 3.1328 - regression_loss: 2.2868 - classification_lo 2559/10000 [======>.......................] - ETA: 1:01:53 - loss: 3.1325 - regression_loss: 2.2863 - classification_lo 2560/10000 [======>.......................] - ETA: 1:01:53 - loss: 3.1324 - regression_loss: 2.2863 - classification_lo 2561/10000 [======>.......................] - ETA: 1:01:52 - loss: 3.1320 - regression_loss: 2.2859 - classification_lo 2562/10000 [======>.......................] - ETA: 1:01:51 - loss: 3.1318 - regression_loss: 2.2858 - classification_lo 2563/10000 [======>.......................] - ETA: 1:01:51 - loss: 3.1316 - regression_loss: 2.2857 - classification_lo 2564/10000 [======>.......................] - ETA: 1:01:50 - loss: 3.1312 - regression_loss: 2.2855 - classification_lo 2565/10000 [======>.......................] - ETA: 1:01:49 - loss: 3.1309 - regression_loss: 2.2852 - classification_lo 2566/10000 [======>.......................] - ETA: 1:01:49 - loss: 3.1307 - regression_loss: 2.2852 - classification_lo 2567/10000 [======>.......................] - ETA: 1:01:48 - loss: 3.1305 - regression_loss: 2.2850 - classification_lo 2568/10000 [======>.......................] - ETA: 1:01:48 - loss: 3.1305 - regression_loss: 2.2851 - classification_lo 2569/10000 [======>.......................] - ETA: 1:01:47 - loss: 3.1303 - regression_loss: 2.2851 - classification_lo 2570/10000 [======>.......................] - ETA: 1:01:46 - loss: 3.1303 - regression_loss: 2.2850 - classification_lo 2571/10000 [======>.......................] - ETA: 1:01:46 - loss: 3.1301 - regression_loss: 2.2848 - classification_lo 2572/10000 [======>.......................] - ETA: 1:01:45 - loss: 3.1300 - regression_loss: 2.2849 - classification_lo 2573/10000 [======>.......................] - ETA: 1:01:45 - loss: 3.1297 - regression_loss: 2.2845 - classification_lo 2574/10000 [======>.......................] - ETA: 1:01:44 - loss: 3.1296 - regression_loss: 2.2845 - classification_lo 2575/10000 [======>.......................] - ETA: 1:01:44 - loss: 3.1293 - regression_loss: 2.2844 - classification_lo 2576/10000 [======>.......................] - ETA: 1:01:43 - loss: 3.1294 - regression_loss: 2.2845 - classification_lo 2577/10000 [======>.......................] - ETA: 1:01:42 - loss: 3.1292 - regression_loss: 2.2843 - classification_lo 2578/10000 [======>.......................] - ETA: 1:01:42 - loss: 3.1291 - regression_loss: 2.2843 - classification_lo 2579/10000 [======>.......................] - ETA: 1:01:41 - loss: 3.1290 - regression_loss: 2.2842 - classification_lo 2580/10000 [======>.......................] - ETA: 1:01:41 - loss: 3.1286 - regression_loss: 2.2839 - classification_lo 2581/10000 [======>.......................] - ETA: 1:01:40 - loss: 3.1281 - regression_loss: 2.2835 - classification_lo 2582/10000 [======>.......................] - ETA: 1:01:39 - loss: 3.1281 - regression_loss: 2.2836 - classification_lo 2583/10000 [======>.......................] - ETA: 1:01:39 - loss: 3.1280 - regression_loss: 2.2836 - classification_lo 2584/10000 [======>.......................] - ETA: 1:01:38 - loss: 3.1280 - regression_loss: 2.2834 - classification_lo 2585/10000 [======>.......................] - ETA: 1:01:37 - loss: 3.1278 - regression_loss: 2.2833 - classification_lo 2586/10000 [======>.......................] - ETA: 1:01:37 - loss: 3.1277 - regression_loss: 2.2833 - classification_lo 2587/10000 [======>.......................] - ETA: 1:01:36 - loss: 3.1276 - regression_loss: 2.2833 - classification_lo 2588/10000 [======>.......................] - ETA: 1:01:36 - loss: 3.1275 - regression_loss: 2.2833 - classification_lo 2589/10000 [======>.......................] - ETA: 1:01:35 - loss: 3.1273 - regression_loss: 2.2832 - classification_lo 2590/10000 [======>.......................] - ETA: 1:01:34 - loss: 3.1271 - regression_loss: 2.2830 - classification_lo 2591/10000 [======>.......................] - ETA: 1:01:34 - loss: 3.1271 - regression_loss: 2.2830 - classification_lo 2592/10000 [======>.......................] - ETA: 1:01:33 - loss: 3.1270 - regression_loss: 2.2830 - classification_lo 2593/10000 [======>.......................] - ETA: 1:01:33 - loss: 3.1268 - regression_loss: 2.2829 - classification_lo 2594/10000 [======>.......................] - ETA: 1:01:32 - loss: 3.1267 - regression_loss: 2.2829 - classification_lo 2595/10000 [======>.......................] - ETA: 1:01:32 - loss: 3.1264 - regression_loss: 2.2828 - classification_lo 2596/10000 [======>.......................] - ETA: 1:01:31 - loss: 3.1262 - regression_loss: 2.2826 - classification_lo 2597/10000 [======>.......................] - ETA: 1:01:30 - loss: 3.1257 - regression_loss: 2.2821 - classification_lo 2598/10000 [======>.......................] - ETA: 1:01:30 - loss: 3.1254 - regression_loss: 2.2818 - classification_lo 2599/10000 [======>.......................] - ETA: 1:01:29 - loss: 3.1255 - regression_loss: 2.2820 - classification_lo 2600/10000 [======>.......................] - ETA: 1:01:29 - loss: 3.1250 - regression_loss: 2.2816 - classification_lo 2601/10000 [======>.......................] - ETA: 1:01:28 - loss: 3.1250 - regression_loss: 2.2817 - classification_lo 2602/10000 [======>.......................] - ETA: 1:01:27 - loss: 3.1247 - regression_loss: 2.2814 - classification_lo 2603/10000 [======>.......................] - ETA: 1:01:27 - loss: 3.1248 - regression_loss: 2.2815 - classification_lo 2604/10000 [======>.......................] - ETA: 1:01:26 - loss: 3.1249 - regression_loss: 2.2816 - classification_lo 2605/10000 [======>.......................] - ETA: 1:01:25 - loss: 3.1247 - regression_loss: 2.2814 - classification_lo 2606/10000 [======>.......................] - ETA: 1:01:25 - loss: 3.1243 - regression_loss: 2.2810 - classification_lo 2607/10000 [======>.......................] - ETA: 1:01:24 - loss: 3.1242 - regression_loss: 2.2810 - classification_lo 2608/10000 [======>.......................] - ETA: 1:01:23 - loss: 3.1240 - regression_loss: 2.2809 - classification_lo 2609/10000 [======>.......................] - ETA: 1:01:23 - loss: 3.1237 - regression_loss: 2.2806 - classification_lo 2610/10000 [======>.......................] - ETA: 1:01:22 - loss: 3.1235 - regression_loss: 2.2805 - classification_lo 2611/10000 [======>.......................] - ETA: 1:01:22 - loss: 3.1230 - regression_loss: 2.2801 - classification_lo 2612/10000 [======>.......................] - ETA: 1:01:21 - loss: 3.1227 - regression_loss: 2.2798 - classification_lo 2613/10000 [======>.......................] - ETA: 1:01:20 - loss: 3.1223 - regression_loss: 2.2795 - classification_lo 2614/10000 [======>.......................] - ETA: 1:01:20 - loss: 3.1225 - regression_loss: 2.2797 - classification_lo 2615/10000 [======>.......................] - ETA: 1:01:19 - loss: 3.1227 - regression_loss: 2.2800 - classification_lo 2616/10000 [======>.......................] - ETA: 1:01:19 - loss: 3.1224 - regression_loss: 2.2798 - classification_lo 2617/10000 [======>.......................] - ETA: 1:01:18 - loss: 3.1222 - regression_loss: 2.2795 - classification_lo 2618/10000 [======>.......................] - ETA: 1:01:17 - loss: 3.1221 - regression_loss: 2.2795 - classification_lo 2619/10000 [======>.......................] - ETA: 1:01:17 - loss: 3.1221 - regression_loss: 2.2795 - classification_lo 2620/10000 [======>.......................] - ETA: 1:01:16 - loss: 3.1216 - regression_loss: 2.2792 - classification_lo 2621/10000 [======>.......................] - ETA: 1:01:15 - loss: 3.1214 - regression_loss: 2.2791 - classification_lo 2622/10000 [======>.......................] - ETA: 1:01:15 - loss: 3.1214 - regression_loss: 2.2793 - classification_lo 2623/10000 [======>.......................] - ETA: 1:01:14 - loss: 3.1209 - regression_loss: 2.2788 - classification_lo 2624/10000 [======>.......................] - ETA: 1:01:14 - loss: 3.1207 - regression_loss: 2.2788 - classification_lo 2625/10000 [======>.......................] - ETA: 1:01:13 - loss: 3.1209 - regression_loss: 2.2789 - classification_lo 2626/10000 [======>.......................] - ETA: 1:01:13 - loss: 3.1205 - regression_loss: 2.2786 - classification_lo 2627/10000 [======>.......................] - ETA: 1:01:12 - loss: 3.1200 - regression_loss: 2.2782 - classification_lo 2628/10000 [======>.......................] - ETA: 1:01:11 - loss: 3.1199 - regression_loss: 2.2781 - classification_lo 2629/10000 [======>.......................] - ETA: 1:01:11 - loss: 3.1197 - regression_loss: 2.2780 - classification_lo 2630/10000 [======>.......................] - ETA: 1:01:10 - loss: 3.1196 - regression_loss: 2.2779 - classification_lo 2631/10000 [======>.......................] - ETA: 1:01:09 - loss: 3.1194 - regression_loss: 2.2778 - classification_lo 2632/10000 [======>.......................] - ETA: 1:01:09 - loss: 3.1191 - regression_loss: 2.2776 - classification_lo 2633/10000 [======>.......................] - ETA: 1:01:08 - loss: 3.1188 - regression_loss: 2.2774 - classification_lo 2634/10000 [======>.......................] - ETA: 1:01:08 - loss: 3.1187 - regression_loss: 2.2773 - classification_lo 2635/10000 [======>.......................] - ETA: 1:01:07 - loss: 3.1187 - regression_loss: 2.2774 - classification_lo 2636/10000 [======>.......................] - ETA: 1:01:06 - loss: 3.1185 - regression_loss: 2.2774 - classification_lo 2637/10000 [======>.......................] - ETA: 1:01:06 - loss: 3.1180 - regression_loss: 2.2769 - classification_lo 2638/10000 [======>.......................] - ETA: 1:01:05 - loss: 3.1175 - regression_loss: 2.2765 - classification_lo 2639/10000 [======>.......................] - ETA: 1:01:04 - loss: 3.1173 - regression_loss: 2.2765 - classification_lo 2640/10000 [======>.......................] - ETA: 1:01:04 - loss: 3.1172 - regression_loss: 2.2764 - classification_lo 2641/10000 [======>.......................] - ETA: 1:01:03 - loss: 3.1167 - regression_loss: 2.2759 - classification_lo 2642/10000 [======>.......................] - ETA: 1:01:03 - loss: 3.1162 - regression_loss: 2.2755 - classification_lo 2643/10000 [======>.......................] - ETA: 1:01:02 - loss: 3.1160 - regression_loss: 2.2754 - classification_lo 2644/10000 [======>.......................] - ETA: 1:01:02 - loss: 3.1159 - regression_loss: 2.2754 - classification_lo 2645/10000 [======>.......................] - ETA: 1:01:01 - loss: 3.1155 - regression_loss: 2.2751 - classification_lo 2646/10000 [======>.......................] - ETA: 1:01:01 - loss: 3.1155 - regression_loss: 2.2751 - classification_lo 2647/10000 [======>.......................] - ETA: 1:01:00 - loss: 3.1149 - regression_loss: 2.2746 - classification_lo 2648/10000 [======>.......................] - ETA: 1:00:59 - loss: 3.1149 - regression_loss: 2.2745 - classification_lo 2649/10000 [======>.......................] - ETA: 1:00:59 - loss: 3.1143 - regression_loss: 2.2741 - classification_lo 2650/10000 [======>.......................] - ETA: 1:00:58 - loss: 3.1140 - regression_loss: 2.2739 - classification_lo 2651/10000 [======>.......................] - ETA: 1:00:58 - loss: 3.1136 - regression_loss: 2.2736 - classification_lo 2652/10000 [======>.......................] - ETA: 1:00:57 - loss: 3.1133 - regression_loss: 2.2734 - classification_lo 2653/10000 [======>.......................] - ETA: 1:00:56 - loss: 3.1130 - regression_loss: 2.2732 - classification_lo 2654/10000 [======>.......................] - ETA: 1:00:56 - loss: 3.1130 - regression_loss: 2.2732 - classification_lo 2655/10000 [======>.......................] - ETA: 1:00:55 - loss: 3.1129 - regression_loss: 2.2730 - classification_lo 2656/10000 [======>.......................] - ETA: 1:00:54 - loss: 3.1129 - regression_loss: 2.2731 - classification_lo 2657/10000 [======>.......................] - ETA: 1:00:54 - loss: 3.1128 - regression_loss: 2.2730 - classification_lo 2658/10000 [======>.......................] - ETA: 1:00:53 - loss: 3.1124 - regression_loss: 2.2727 - classification_lo 2659/10000 [======>.......................] - ETA: 1:00:53 - loss: 3.1122 - regression_loss: 2.2725 - classification_lo 2660/10000 [======>.......................] - ETA: 1:00:52 - loss: 3.1119 - regression_loss: 2.2723 - classification_lo 2661/10000 [======>.......................] - ETA: 1:00:51 - loss: 3.1115 - regression_loss: 2.2721 - classification_lo 2662/10000 [======>.......................] - ETA: 1:00:51 - loss: 3.1113 - regression_loss: 2.2719 - classification_lo 2663/10000 [======>.......................] - ETA: 1:00:50 - loss: 3.1111 - regression_loss: 2.2718 - classification_lo 2664/10000 [======>.......................] - ETA: 1:00:50 - loss: 3.1110 - regression_loss: 2.2718 - classification_lo 2665/10000 [======>.......................] - ETA: 1:00:49 - loss: 3.1106 - regression_loss: 2.2715 - classification_lo 2666/10000 [======>.......................] - ETA: 1:00:48 - loss: 3.1101 - regression_loss: 2.2711 - classification_lo 2667/10000 [=======>......................] - ETA: 1:00:48 - loss: 3.1099 - regression_loss: 2.2711 - classification_lo 2668/10000 [=======>......................] - ETA: 1:00:47 - loss: 3.1104 - regression_loss: 2.2711 - classification_lo 2669/10000 [=======>......................] - ETA: 1:00:47 - loss: 3.1102 - regression_loss: 2.2708 - classification_lo 2670/10000 [=======>......................] - ETA: 1:00:46 - loss: 3.1102 - regression_loss: 2.2708 - classification_lo 2671/10000 [=======>......................] - ETA: 1:00:45 - loss: 3.1096 - regression_loss: 2.2703 - classification_lo 2672/10000 [=======>......................] - ETA: 1:00:45 - loss: 3.1093 - regression_loss: 2.2699 - classification_lo 2673/10000 [=======>......................] - ETA: 1:00:44 - loss: 3.1090 - regression_loss: 2.2698 - classification_lo 2674/10000 [=======>......................] - ETA: 1:00:44 - loss: 3.1089 - regression_loss: 2.2697 - classification_lo 2675/10000 [=======>......................] - ETA: 1:00:43 - loss: 3.1088 - regression_loss: 2.2696 - classification_lo 2676/10000 [=======>......................] - ETA: 1:00:44 - loss: 3.1088 - regression_loss: 2.2696 - classification_lo 2677/10000 [=======>......................] - ETA: 1:00:44 - loss: 3.1091 - regression_loss: 2.2698 - classification_lo 2678/10000 [=======>......................] - ETA: 1:00:43 - loss: 3.1089 - regression_loss: 2.2697 - classification_lo 2679/10000 [=======>......................] - ETA: 1:00:43 - loss: 3.1086 - regression_loss: 2.2695 - classification_lo 2680/10000 [=======>......................] - ETA: 1:00:42 - loss: 3.1085 - regression_loss: 2.2695 - classification_lo 2681/10000 [=======>......................] - ETA: 1:00:41 - loss: 3.1085 - regression_loss: 2.2695 - classification_lo 2682/10000 [=======>......................] - ETA: 1:00:41 - loss: 3.1083 - regression_loss: 2.2695 - classification_lo 2683/10000 [=======>......................] - ETA: 1:00:40 - loss: 3.1081 - regression_loss: 2.2694 - classification_lo 2684/10000 [=======>......................] - ETA: 1:00:40 - loss: 3.1077 - regression_loss: 2.2691 - classification_lo 2685/10000 [=======>......................] - ETA: 1:00:39 - loss: 3.1074 - regression_loss: 2.2690 - classification_lo 2686/10000 [=======>......................] - ETA: 1:00:38 - loss: 3.1072 - regression_loss: 2.2688 - classification_lo 2687/10000 [=======>......................] - ETA: 1:00:39 - loss: 3.1069 - regression_loss: 2.2687 - classification_lo 2688/10000 [=======>......................] - ETA: 1:00:39 - loss: 3.1069 - regression_loss: 2.2687 - classification_lo 2689/10000 [=======>......................] - ETA: 1:00:38 - loss: 3.1070 - regression_loss: 2.2688 - classification_lo 2690/10000 [=======>......................] - ETA: 1:00:37 - loss: 3.1070 - regression_loss: 2.2688 - classification_lo 2691/10000 [=======>......................] - ETA: 1:00:37 - loss: 3.1067 - regression_loss: 2.2687 - classification_lo 2692/10000 [=======>......................] - ETA: 1:00:36 - loss: 3.1063 - regression_loss: 2.2684 - classification_lo 2693/10000 [=======>......................] - ETA: 1:00:36 - loss: 3.1060 - regression_loss: 2.2682 - classification_lo 2694/10000 [=======>......................] - ETA: 1:00:35 - loss: 3.1058 - regression_loss: 2.2680 - classification_lo 2695/10000 [=======>......................] - ETA: 1:00:34 - loss: 3.1055 - regression_loss: 2.2677 - classification_lo 2696/10000 [=======>......................] - ETA: 1:00:34 - loss: 3.1052 - regression_loss: 2.2675 - classification_lo 2697/10000 [=======>......................] - ETA: 1:00:33 - loss: 3.1051 - regression_loss: 2.2674 - classification_lo 2698/10000 [=======>......................] - ETA: 1:00:33 - loss: 3.1049 - regression_loss: 2.2675 - classification_lo 2699/10000 [=======>......................] - ETA: 1:00:32 - loss: 3.1049 - regression_loss: 2.2674 - classification_lo 2700/10000 [=======>......................] - ETA: 1:00:32 - loss: 3.1049 - regression_loss: 2.2674 - classification_lo 2701/10000 [=======>......................] - ETA: 1:00:31 - loss: 3.1046 - regression_loss: 2.2672 - classification_lo 2702/10000 [=======>......................] - ETA: 1:00:30 - loss: 3.1044 - regression_loss: 2.2671 - classification_lo 2703/10000 [=======>......................] - ETA: 1:00:30 - loss: 3.1041 - regression_loss: 2.2670 - classification_lo 2704/10000 [=======>......................] - ETA: 1:00:29 - loss: 3.1043 - regression_loss: 2.2671 - classification_lo 2705/10000 [=======>......................] - ETA: 1:00:29 - loss: 3.1040 - regression_loss: 2.2669 - classification_lo 2706/10000 [=======>......................] - ETA: 1:00:28 - loss: 3.1038 - regression_loss: 2.2668 - classification_lo 2707/10000 [=======>......................] - ETA: 1:00:28 - loss: 3.1037 - regression_loss: 2.2667 - classification_lo 2708/10000 [=======>......................] - ETA: 1:00:27 - loss: 3.1035 - regression_loss: 2.2665 - classification_lo 2709/10000 [=======>......................] - ETA: 1:00:27 - loss: 3.1033 - regression_loss: 2.2663 - classification_lo 2710/10000 [=======>......................] - ETA: 1:00:26 - loss: 3.1030 - regression_loss: 2.2661 - classification_lo 2711/10000 [=======>......................] - ETA: 1:00:25 - loss: 3.1027 - regression_loss: 2.2659 - classification_lo 2712/10000 [=======>......................] - ETA: 1:00:25 - loss: 3.1024 - regression_loss: 2.2658 - classification_lo 2713/10000 [=======>......................] - ETA: 1:00:24 - loss: 3.1020 - regression_loss: 2.2654 - classification_lo 2714/10000 [=======>......................] - ETA: 1:00:24 - loss: 3.1019 - regression_loss: 2.2655 - classification_lo 2715/10000 [=======>......................] - ETA: 1:00:23 - loss: 3.1018 - regression_loss: 2.2654 - classification_lo 2716/10000 [=======>......................] - ETA: 1:00:22 - loss: 3.1014 - regression_loss: 2.2651 - classification_lo 2717/10000 [=======>......................] - ETA: 1:00:22 - loss: 3.1014 - regression_loss: 2.2651 - classification_lo 2718/10000 [=======>......................] - ETA: 1:00:21 - loss: 3.1013 - regression_loss: 2.2651 - classification_lo 2719/10000 [=======>......................] - ETA: 1:00:21 - loss: 3.1013 - regression_loss: 2.2651 - classification_lo 2720/10000 [=======>......................] - ETA: 1:00:20 - loss: 3.1011 - regression_loss: 2.2650 - classification_lo 2721/10000 [=======>......................] - ETA: 1:00:20 - loss: 3.1012 - regression_loss: 2.2649 - classification_lo 2722/10000 [=======>......................] - ETA: 1:00:19 - loss: 3.1009 - regression_loss: 2.2647 - classification_lo 2723/10000 [=======>......................] - ETA: 1:00:19 - loss: 3.1011 - regression_loss: 2.2648 - classification_lo 2724/10000 [=======>......................] - ETA: 1:00:18 - loss: 3.1008 - regression_loss: 2.2646 - classification_lo 2725/10000 [=======>......................] - ETA: 1:00:17 - loss: 3.1006 - regression_loss: 2.2645 - classification_lo 2726/10000 [=======>......................] - ETA: 1:00:17 - loss: 3.1005 - regression_loss: 2.2644 - classification_lo 2727/10000 [=======>......................] - ETA: 1:00:16 - loss: 3.1004 - regression_loss: 2.2644 - classification_lo 2728/10000 [=======>......................] - ETA: 1:00:15 - loss: 3.1003 - regression_loss: 2.2643 - classification_lo 2729/10000 [=======>......................] - ETA: 1:00:15 - loss: 3.1003 - regression_loss: 2.2644 - classification_lo 2730/10000 [=======>......................] - ETA: 1:00:14 - loss: 3.0999 - regression_loss: 2.2640 - classification_lo 2731/10000 [=======>......................] - ETA: 1:00:14 - loss: 3.0996 - regression_loss: 2.2638 - classification_lo 2732/10000 [=======>......................] - ETA: 1:00:13 - loss: 3.0990 - regression_loss: 2.2632 - classification_lo 2733/10000 [=======>......................] - ETA: 1:00:13 - loss: 3.0985 - regression_loss: 2.2628 - classification_lo 2734/10000 [=======>......................] - ETA: 1:00:12 - loss: 3.0983 - regression_loss: 2.2627 - classification_lo 2735/10000 [=======>......................] - ETA: 1:00:12 - loss: 3.0982 - regression_loss: 2.2626 - classification_lo 2736/10000 [=======>......................] - ETA: 1:00:11 - loss: 3.0982 - regression_loss: 2.2626 - classification_lo 2737/10000 [=======>......................] - ETA: 1:00:11 - loss: 3.0977 - regression_loss: 2.2621 - classification_lo 2738/10000 [=======>......................] - ETA: 1:00:10 - loss: 3.0978 - regression_loss: 2.2622 - classification_lo 2739/10000 [=======>......................] - ETA: 1:00:09 - loss: 3.0975 - regression_loss: 2.2621 - classification_lo 2740/10000 [=======>......................] - ETA: 1:00:09 - loss: 3.0971 - regression_loss: 2.2617 - classification_lo 2741/10000 [=======>......................] - ETA: 1:00:08 - loss: 3.0971 - regression_loss: 2.2617 - classification_lo 2742/10000 [=======>......................] - ETA: 1:00:08 - loss: 3.0971 - regression_loss: 2.2617 - classification_lo 2743/10000 [=======>......................] - ETA: 1:00:07 - loss: 3.0968 - regression_loss: 2.2615 - classification_lo 2744/10000 [=======>......................] - ETA: 1:00:06 - loss: 3.0967 - regression_loss: 2.2614 - classification_lo 2745/10000 [=======>......................] - ETA: 1:00:06 - loss: 3.0966 - regression_loss: 2.2613 - classification_lo 2746/10000 [=======>......................] - ETA: 1:00:05 - loss: 3.0963 - regression_loss: 2.2611 - classification_lo 2747/10000 [=======>......................] - ETA: 1:00:05 - loss: 3.0965 - regression_loss: 2.2613 - classification_lo 2748/10000 [=======>......................] - ETA: 1:00:04 - loss: 3.0960 - regression_loss: 2.2609 - classification_lo 2749/10000 [=======>......................] - ETA: 1:00:04 - loss: 3.0959 - regression_loss: 2.2609 - classification_lo 2750/10000 [=======>......................] - ETA: 1:00:03 - loss: 3.0957 - regression_loss: 2.2608 - classification_lo 2751/10000 [=======>......................] - ETA: 1:00:03 - loss: 3.0954 - regression_loss: 2.2604 - classification_lo 2752/10000 [=======>......................] - ETA: 1:00:02 - loss: 3.0952 - regression_loss: 2.2603 - classification_lo 2753/10000 [=======>......................] - ETA: 1:00:01 - loss: 3.0952 - regression_loss: 2.2603 - classification_lo 2754/10000 [=======>......................] - ETA: 1:00:01 - loss: 3.0951 - regression_loss: 2.2602 - classification_lo 2755/10000 [=======>......................] - ETA: 1:00:00 - loss: 3.0949 - regression_loss: 2.2601 - classification_lo 2756/10000 [=======>......................] - ETA: 1:00:00 - loss: 3.0946 - regression_loss: 2.2598 - classification_lo 2757/10000 [=======>......................] - ETA: 59:59 - loss: 3.0943 - regression_loss: 2.2596 - classification_loss 2758/10000 [=======>......................] - ETA: 59:59 - loss: 3.0944 - regression_loss: 2.2588 - classification_loss 2759/10000 [=======>......................] - ETA: 59:58 - loss: 3.0945 - regression_loss: 2.2589 - classification_loss 2760/10000 [=======>......................] - ETA: 59:58 - loss: 3.0940 - regression_loss: 2.2585 - classification_loss 2761/10000 [=======>......................] - ETA: 59:57 - loss: 3.0940 - regression_loss: 2.2584 - classification_loss 2762/10000 [=======>......................] - ETA: 59:57 - loss: 3.0937 - regression_loss: 2.2583 - classification_loss 2763/10000 [=======>......................] - ETA: 59:56 - loss: 3.0934 - regression_loss: 2.2580 - classification_loss 2764/10000 [=======>......................] - ETA: 59:55 - loss: 3.0931 - regression_loss: 2.2578 - classification_loss 2765/10000 [=======>......................] - ETA: 59:55 - loss: 3.0938 - regression_loss: 2.2583 - classification_loss 2766/10000 [=======>......................] - ETA: 59:54 - loss: 3.0937 - regression_loss: 2.2583 - classification_loss 2767/10000 [=======>......................] - ETA: 59:53 - loss: 3.0934 - regression_loss: 2.2581 - classification_loss 2768/10000 [=======>......................] - ETA: 59:53 - loss: 3.0934 - regression_loss: 2.2581 - classification_loss 2769/10000 [=======>......................] - ETA: 59:52 - loss: 3.0934 - regression_loss: 2.2581 - classification_loss 2770/10000 [=======>......................] - ETA: 59:52 - loss: 3.0933 - regression_loss: 2.2581 - classification_loss 2771/10000 [=======>......................] - ETA: 59:51 - loss: 3.0932 - regression_loss: 2.2580 - classification_loss 2772/10000 [=======>......................] - ETA: 59:51 - loss: 3.0933 - regression_loss: 2.2581 - classification_loss 2773/10000 [=======>......................] - ETA: 59:50 - loss: 3.0929 - regression_loss: 2.2578 - classification_loss 2774/10000 [=======>......................] - ETA: 59:49 - loss: 3.0928 - regression_loss: 2.2578 - classification_loss 2775/10000 [=======>......................] - ETA: 59:49 - loss: 3.0926 - regression_loss: 2.2576 - classification_loss 2776/10000 [=======>......................] - ETA: 59:48 - loss: 3.0925 - regression_loss: 2.2575 - classification_loss 2777/10000 [=======>......................] - ETA: 59:47 - loss: 3.0921 - regression_loss: 2.2571 - classification_loss 2778/10000 [=======>......................] - ETA: 59:47 - loss: 3.0920 - regression_loss: 2.2571 - classification_loss 2779/10000 [=======>......................] - ETA: 59:46 - loss: 3.0919 - regression_loss: 2.2570 - classification_loss 2780/10000 [=======>......................] - ETA: 59:46 - loss: 3.0918 - regression_loss: 2.2570 - classification_loss 2781/10000 [=======>......................] - ETA: 59:45 - loss: 3.0913 - regression_loss: 2.2566 - classification_loss 2782/10000 [=======>......................] - ETA: 59:44 - loss: 3.0912 - regression_loss: 2.2567 - classification_loss 2783/10000 [=======>......................] - ETA: 59:44 - loss: 3.0906 - regression_loss: 2.2562 - classification_loss 2784/10000 [=======>......................] - ETA: 59:43 - loss: 3.0906 - regression_loss: 2.2562 - classification_loss 2785/10000 [=======>......................] - ETA: 59:42 - loss: 3.0906 - regression_loss: 2.2562 - classification_loss 2786/10000 [=======>......................] - ETA: 59:42 - loss: 3.0903 - regression_loss: 2.2559 - classification_loss 2787/10000 [=======>......................] - ETA: 59:41 - loss: 3.0899 - regression_loss: 2.2557 - classification_loss 2788/10000 [=======>......................] - ETA: 59:41 - loss: 3.0899 - regression_loss: 2.2556 - classification_loss 2789/10000 [=======>......................] - ETA: 59:40 - loss: 3.0898 - regression_loss: 2.2555 - classification_loss 2790/10000 [=======>......................] - ETA: 59:39 - loss: 3.0897 - regression_loss: 2.2555 - classification_loss 2791/10000 [=======>......................] - ETA: 59:39 - loss: 3.0891 - regression_loss: 2.2550 - classification_loss 2792/10000 [=======>......................] - ETA: 59:38 - loss: 3.0890 - regression_loss: 2.2551 - classification_loss 2793/10000 [=======>......................] - ETA: 59:38 - loss: 3.0890 - regression_loss: 2.2551 - classification_loss 2794/10000 [=======>......................] - ETA: 59:37 - loss: 3.0887 - regression_loss: 2.2548 - classification_loss 2795/10000 [=======>......................] - ETA: 59:36 - loss: 3.0886 - regression_loss: 2.2548 - classification_loss 2796/10000 [=======>......................] - ETA: 59:36 - loss: 3.0884 - regression_loss: 2.2546 - classification_loss 2797/10000 [=======>......................] - ETA: 59:35 - loss: 3.0879 - regression_loss: 2.2543 - classification_loss 2798/10000 [=======>......................] - ETA: 59:35 - loss: 3.0877 - regression_loss: 2.2542 - classification_loss 2799/10000 [=======>......................] - ETA: 59:34 - loss: 3.0875 - regression_loss: 2.2540 - classification_loss 2800/10000 [=======>......................] - ETA: 59:33 - loss: 3.0872 - regression_loss: 2.2538 - classification_loss 2801/10000 [=======>......................] - ETA: 59:33 - loss: 3.0870 - regression_loss: 2.2537 - classification_loss 2802/10000 [=======>......................] - ETA: 59:32 - loss: 3.0870 - regression_loss: 2.2537 - classification_loss 2803/10000 [=======>......................] - ETA: 59:32 - loss: 3.0869 - regression_loss: 2.2537 - classification_loss 2804/10000 [=======>......................] - ETA: 59:31 - loss: 3.0870 - regression_loss: 2.2538 - classification_loss 2805/10000 [=======>......................] - ETA: 59:31 - loss: 3.0868 - regression_loss: 2.2535 - classification_loss 2806/10000 [=======>......................] - ETA: 59:30 - loss: 3.0866 - regression_loss: 2.2534 - classification_loss 2807/10000 [=======>......................] - ETA: 59:30 - loss: 3.0869 - regression_loss: 2.2537 - classification_loss 2808/10000 [=======>......................] - ETA: 59:29 - loss: 3.0866 - regression_loss: 2.2536 - classification_loss 2809/10000 [=======>......................] - ETA: 59:29 - loss: 3.0863 - regression_loss: 2.2534 - classification_loss 2810/10000 [=======>......................] - ETA: 59:28 - loss: 3.0863 - regression_loss: 2.2534 - classification_loss 2811/10000 [=======>......................] - ETA: 59:27 - loss: 3.0860 - regression_loss: 2.2531 - classification_loss 2812/10000 [=======>......................] - ETA: 59:27 - loss: 3.0859 - regression_loss: 2.2532 - classification_loss 2813/10000 [=======>......................] - ETA: 59:26 - loss: 3.0855 - regression_loss: 2.2530 - classification_loss 2814/10000 [=======>......................] - ETA: 59:26 - loss: 3.0855 - regression_loss: 2.2529 - classification_loss 2815/10000 [=======>......................] - ETA: 59:25 - loss: 3.0855 - regression_loss: 2.2530 - classification_loss 2816/10000 [=======>......................] - ETA: 59:25 - loss: 3.0854 - regression_loss: 2.2529 - classification_loss 2817/10000 [=======>......................] - ETA: 59:24 - loss: 3.0853 - regression_loss: 2.2529 - classification_loss 2818/10000 [=======>......................] - ETA: 59:24 - loss: 3.0851 - regression_loss: 2.2529 - classification_loss 2819/10000 [=======>......................] - ETA: 59:23 - loss: 3.0852 - regression_loss: 2.2528 - classification_loss 2820/10000 [=======>......................] - ETA: 59:23 - loss: 3.0850 - regression_loss: 2.2527 - classification_loss 2821/10000 [=======>......................] - ETA: 59:22 - loss: 3.0851 - regression_loss: 2.2527 - classification_loss 2822/10000 [=======>......................] - ETA: 59:21 - loss: 3.0849 - regression_loss: 2.2526 - classification_loss 2823/10000 [=======>......................] - ETA: 59:21 - loss: 3.0845 - regression_loss: 2.2524 - classification_loss 2824/10000 [=======>......................] - ETA: 59:20 - loss: 3.0844 - regression_loss: 2.2523 - classification_loss 2825/10000 [=======>......................] - ETA: 59:20 - loss: 3.0843 - regression_loss: 2.2522 - classification_loss 2826/10000 [=======>......................] - ETA: 59:19 - loss: 3.0843 - regression_loss: 2.2522 - classification_loss 2827/10000 [=======>......................] - ETA: 59:18 - loss: 3.0840 - regression_loss: 2.2520 - classification_loss 2828/10000 [=======>......................] - ETA: 59:18 - loss: 3.0839 - regression_loss: 2.2520 - classification_loss 2829/10000 [=======>......................] - ETA: 59:17 - loss: 3.0838 - regression_loss: 2.2519 - classification_loss 2830/10000 [=======>......................] - ETA: 59:17 - loss: 3.0836 - regression_loss: 2.2518 - classification_loss 2831/10000 [=======>......................] - ETA: 59:16 - loss: 3.0833 - regression_loss: 2.2516 - classification_loss 2832/10000 [=======>......................] - ETA: 59:15 - loss: 3.0832 - regression_loss: 2.2516 - classification_loss 2833/10000 [=======>......................] - ETA: 59:15 - loss: 3.0830 - regression_loss: 2.2515 - classification_loss 2834/10000 [=======>......................] - ETA: 59:14 - loss: 3.0825 - regression_loss: 2.2511 - classification_loss 2835/10000 [=======>......................] - ETA: 59:14 - loss: 3.0825 - regression_loss: 2.2511 - classification_loss 2836/10000 [=======>......................] - ETA: 59:13 - loss: 3.0821 - regression_loss: 2.2508 - classification_loss 2837/10000 [=======>......................] - ETA: 59:13 - loss: 3.0819 - regression_loss: 2.2507 - classification_loss 2838/10000 [=======>......................] - ETA: 59:12 - loss: 3.0818 - regression_loss: 2.2506 - classification_loss 2839/10000 [=======>......................] - ETA: 59:12 - loss: 3.0815 - regression_loss: 2.2504 - classification_loss 2840/10000 [=======>......................] - ETA: 59:11 - loss: 3.0810 - regression_loss: 2.2500 - classification_loss 2841/10000 [=======>......................] - ETA: 59:10 - loss: 3.0811 - regression_loss: 2.2499 - classification_loss 2842/10000 [=======>......................] - ETA: 59:10 - loss: 3.0809 - regression_loss: 2.2498 - classification_loss 2843/10000 [=======>......................] - ETA: 59:09 - loss: 3.0805 - regression_loss: 2.2496 - classification_loss 2844/10000 [=======>......................] - ETA: 59:09 - loss: 3.0802 - regression_loss: 2.2493 - classification_loss 2845/10000 [=======>......................] - ETA: 59:08 - loss: 3.0801 - regression_loss: 2.2493 - classification_loss 2846/10000 [=======>......................] - ETA: 59:08 - loss: 3.0801 - regression_loss: 2.2493 - classification_loss 2847/10000 [=======>......................] - ETA: 59:07 - loss: 3.0797 - regression_loss: 2.2491 - classification_loss 2848/10000 [=======>......................] - ETA: 59:07 - loss: 3.0792 - regression_loss: 2.2487 - classification_loss 2849/10000 [=======>......................] - ETA: 59:06 - loss: 3.0794 - regression_loss: 2.2489 - classification_loss 2850/10000 [=======>......................] - ETA: 59:05 - loss: 3.0792 - regression_loss: 2.2488 - classification_loss 2851/10000 [=======>......................] - ETA: 59:05 - loss: 3.0789 - regression_loss: 2.2487 - classification_loss 2852/10000 [=======>......................] - ETA: 59:04 - loss: 3.0792 - regression_loss: 2.2488 - classification_loss 2853/10000 [=======>......................] - ETA: 59:04 - loss: 3.0789 - regression_loss: 2.2486 - classification_loss 2854/10000 [=======>......................] - ETA: 59:03 - loss: 3.0786 - regression_loss: 2.2485 - classification_loss 2855/10000 [=======>......................] - ETA: 59:02 - loss: 3.0785 - regression_loss: 2.2484 - classification_loss 2856/10000 [=======>......................] - ETA: 59:02 - loss: 3.0783 - regression_loss: 2.2483 - classification_loss 2857/10000 [=======>......................] - ETA: 59:01 - loss: 3.0782 - regression_loss: 2.2482 - classification_loss 2858/10000 [=======>......................] - ETA: 59:01 - loss: 3.0779 - regression_loss: 2.2480 - classification_loss 2859/10000 [=======>......................] - ETA: 59:00 - loss: 3.0779 - regression_loss: 2.2480 - classification_loss 2860/10000 [=======>......................] - ETA: 59:00 - loss: 3.0777 - regression_loss: 2.2478 - classification_loss 2861/10000 [=======>......................] - ETA: 58:59 - loss: 3.0772 - regression_loss: 2.2473 - classification_loss 2862/10000 [=======>......................] - ETA: 58:59 - loss: 3.0768 - regression_loss: 2.2471 - classification_loss 2863/10000 [=======>......................] - ETA: 58:58 - loss: 3.0763 - regression_loss: 2.2467 - classification_loss 2864/10000 [=======>......................] - ETA: 58:57 - loss: 3.0762 - regression_loss: 2.2467 - classification_loss 2865/10000 [=======>......................] - ETA: 58:57 - loss: 3.0761 - regression_loss: 2.2466 - classification_loss 2866/10000 [=======>......................] - ETA: 58:56 - loss: 3.0761 - regression_loss: 2.2467 - classification_loss 2867/10000 [=======>......................] - ETA: 58:56 - loss: 3.0759 - regression_loss: 2.2466 - classification_loss 2868/10000 [=======>......................] - ETA: 58:55 - loss: 3.0757 - regression_loss: 2.2465 - classification_loss 2869/10000 [=======>......................] - ETA: 58:55 - loss: 3.0757 - regression_loss: 2.2464 - classification_loss 2870/10000 [=======>......................] - ETA: 58:54 - loss: 3.0754 - regression_loss: 2.2462 - classification_loss 2871/10000 [=======>......................] - ETA: 58:54 - loss: 3.0752 - regression_loss: 2.2460 - classification_loss 2872/10000 [=======>......................] - ETA: 58:53 - loss: 3.0749 - regression_loss: 2.2458 - classification_loss 2873/10000 [=======>......................] - ETA: 58:53 - loss: 3.0749 - regression_loss: 2.2458 - classification_loss 2874/10000 [=======>......................] - ETA: 58:52 - loss: 3.0752 - regression_loss: 2.2459 - classification_loss 2875/10000 [=======>......................] - ETA: 58:51 - loss: 3.0750 - regression_loss: 2.2458 - classification_loss 2876/10000 [=======>......................] - ETA: 58:51 - loss: 3.0748 - regression_loss: 2.2458 - classification_loss 2877/10000 [=======>......................] - ETA: 58:50 - loss: 3.0749 - regression_loss: 2.2459 - classification_loss 2878/10000 [=======>......................] - ETA: 58:50 - loss: 3.0751 - regression_loss: 2.2461 - classification_loss 2879/10000 [=======>......................] - ETA: 58:49 - loss: 3.0749 - regression_loss: 2.2459 - classification_loss 2880/10000 [=======>......................] - ETA: 58:48 - loss: 3.0747 - regression_loss: 2.2458 - classification_loss 2881/10000 [=======>......................] - ETA: 58:48 - loss: 3.0744 - regression_loss: 2.2456 - classification_loss 2882/10000 [=======>......................] - ETA: 58:47 - loss: 3.0741 - regression_loss: 2.2454 - classification_loss 2883/10000 [=======>......................] - ETA: 58:47 - loss: 3.0740 - regression_loss: 2.2453 - classification_loss 2884/10000 [=======>......................] - ETA: 58:46 - loss: 3.0738 - regression_loss: 2.2451 - classification_loss 2885/10000 [=======>......................] - ETA: 58:46 - loss: 3.0737 - regression_loss: 2.2450 - classification_loss 2886/10000 [=======>......................] - ETA: 58:45 - loss: 3.0735 - regression_loss: 2.2449 - classification_loss 2887/10000 [=======>......................] - ETA: 58:45 - loss: 3.0731 - regression_loss: 2.2447 - classification_loss 2888/10000 [=======>......................] - ETA: 58:44 - loss: 3.0730 - regression_loss: 2.2447 - classification_loss 2889/10000 [=======>......................] - ETA: 58:43 - loss: 3.0729 - regression_loss: 2.2446 - classification_loss 2890/10000 [=======>......................] - ETA: 58:43 - loss: 3.0728 - regression_loss: 2.2446 - classification_loss 2891/10000 [=======>......................] - ETA: 58:42 - loss: 3.0728 - regression_loss: 2.2446 - classification_loss 2892/10000 [=======>......................] - ETA: 58:42 - loss: 3.0726 - regression_loss: 2.2446 - classification_loss 2893/10000 [=======>......................] - ETA: 58:41 - loss: 3.0724 - regression_loss: 2.2444 - classification_loss 2894/10000 [=======>......................] - ETA: 58:41 - loss: 3.0722 - regression_loss: 2.2443 - classification_loss 2895/10000 [=======>......................] - ETA: 58:40 - loss: 3.0723 - regression_loss: 2.2444 - classification_loss 2896/10000 [=======>......................] - ETA: 58:39 - loss: 3.0721 - regression_loss: 2.2442 - classification_loss 2897/10000 [=======>......................] - ETA: 58:39 - loss: 3.0719 - regression_loss: 2.2441 - classification_loss 2898/10000 [=======>......................] - ETA: 58:38 - loss: 3.0719 - regression_loss: 2.2442 - classification_loss 2899/10000 [=======>......................] - ETA: 58:38 - loss: 3.0718 - regression_loss: 2.2442 - classification_loss 2900/10000 [=======>......................] - ETA: 58:37 - loss: 3.0714 - regression_loss: 2.2439 - classification_loss 2901/10000 [=======>......................] - ETA: 58:36 - loss: 3.0712 - regression_loss: 2.2437 - classification_loss 2902/10000 [=======>......................] - ETA: 58:36 - loss: 3.0711 - regression_loss: 2.2437 - classification_loss 2903/10000 [=======>......................] - ETA: 58:35 - loss: 3.0709 - regression_loss: 2.2435 - classification_loss 2904/10000 [=======>......................] - ETA: 58:35 - loss: 3.0707 - regression_loss: 2.2434 - classification_loss 2905/10000 [=======>......................] - ETA: 58:34 - loss: 3.0703 - regression_loss: 2.2432 - classification_loss 2906/10000 [=======>......................] - ETA: 58:33 - loss: 3.0700 - regression_loss: 2.2429 - classification_loss 2907/10000 [=======>......................] - ETA: 58:33 - loss: 3.0698 - regression_loss: 2.2429 - classification_loss 2908/10000 [=======>......................] - ETA: 58:32 - loss: 3.0698 - regression_loss: 2.2429 - classification_loss 2909/10000 [=======>......................] - ETA: 58:32 - loss: 3.0698 - regression_loss: 2.2430 - classification_loss 2910/10000 [=======>......................] - ETA: 58:31 - loss: 3.0694 - regression_loss: 2.2427 - classification_loss 2911/10000 [=======>......................] - ETA: 58:30 - loss: 3.0691 - regression_loss: 2.2426 - classification_loss 2912/10000 [=======>......................] - ETA: 58:30 - loss: 3.0690 - regression_loss: 2.2425 - classification_loss 2913/10000 [=======>......................] - ETA: 58:29 - loss: 3.0690 - regression_loss: 2.2425 - classification_loss 2914/10000 [=======>......................] - ETA: 58:29 - loss: 3.0687 - regression_loss: 2.2422 - classification_loss 2915/10000 [=======>......................] - ETA: 58:28 - loss: 3.0686 - regression_loss: 2.2421 - classification_loss 2916/10000 [=======>......................] - ETA: 58:27 - loss: 3.0685 - regression_loss: 2.2421 - classification_loss 2917/10000 [=======>......................] - ETA: 58:27 - loss: 3.0684 - regression_loss: 2.2421 - classification_loss 2918/10000 [=======>......................] - ETA: 58:26 - loss: 3.0681 - regression_loss: 2.2419 - classification_loss 2919/10000 [=======>......................] - ETA: 58:26 - loss: 3.0680 - regression_loss: 2.2419 - classification_loss 2920/10000 [=======>......................] - ETA: 58:25 - loss: 3.0680 - regression_loss: 2.2420 - classification_loss 2921/10000 [=======>......................] - ETA: 58:25 - loss: 3.0680 - regression_loss: 2.2421 - classification_loss 2922/10000 [=======>......................] - ETA: 58:24 - loss: 3.0681 - regression_loss: 2.2420 - classification_loss 2923/10000 [=======>......................] - ETA: 58:24 - loss: 3.0679 - regression_loss: 2.2419 - classification_loss 2924/10000 [=======>......................] - ETA: 58:23 - loss: 3.0678 - regression_loss: 2.2418 - classification_loss 2925/10000 [=======>......................] - ETA: 58:22 - loss: 3.0676 - regression_loss: 2.2417 - classification_loss 2926/10000 [=======>......................] - ETA: 58:22 - loss: 3.0676 - regression_loss: 2.2418 - classification_loss 2927/10000 [=======>......................] - ETA: 58:21 - loss: 3.0673 - regression_loss: 2.2415 - classification_loss 2928/10000 [=======>......................] - ETA: 58:21 - loss: 3.0670 - regression_loss: 2.2413 - classification_loss 2929/10000 [=======>......................] - ETA: 58:20 - loss: 3.0671 - regression_loss: 2.2413 - classification_loss 2930/10000 [=======>......................] - ETA: 58:20 - loss: 3.0671 - regression_loss: 2.2415 - classification_loss 2931/10000 [=======>......................] - ETA: 58:19 - loss: 3.0668 - regression_loss: 2.2413 - classification_loss 2932/10000 [=======>......................] - ETA: 58:18 - loss: 3.0666 - regression_loss: 2.2413 - classification_loss 2933/10000 [=======>......................] - ETA: 58:18 - loss: 3.0665 - regression_loss: 2.2411 - classification_loss 2934/10000 [=======>......................] - ETA: 58:17 - loss: 3.0664 - regression_loss: 2.2412 - classification_loss 2935/10000 [=======>......................] - ETA: 58:16 - loss: 3.0662 - regression_loss: 2.2410 - classification_loss 2936/10000 [=======>......................] - ETA: 58:16 - loss: 3.0662 - regression_loss: 2.2411 - classification_loss 2937/10000 [=======>......................] - ETA: 58:15 - loss: 3.0660 - regression_loss: 2.2410 - classification_loss 2938/10000 [=======>......................] - ETA: 58:15 - loss: 3.0659 - regression_loss: 2.2409 - classification_loss 2939/10000 [=======>......................] - ETA: 58:14 - loss: 3.0657 - regression_loss: 2.2409 - classification_loss 2940/10000 [=======>......................] - ETA: 58:13 - loss: 3.0653 - regression_loss: 2.2405 - classification_loss 2941/10000 [=======>......................] - ETA: 58:13 - loss: 3.0653 - regression_loss: 2.2405 - classification_loss 2942/10000 [=======>......................] - ETA: 58:12 - loss: 3.0650 - regression_loss: 2.2403 - classification_loss 2943/10000 [=======>......................] - ETA: 58:11 - loss: 3.0650 - regression_loss: 2.2404 - classification_loss 2944/10000 [=======>......................] - ETA: 58:11 - loss: 3.0649 - regression_loss: 2.2403 - classification_loss 2945/10000 [=======>......................] - ETA: 58:10 - loss: 3.0647 - regression_loss: 2.2402 - classification_loss 2946/10000 [=======>......................] - ETA: 58:10 - loss: 3.0646 - regression_loss: 2.2403 - classification_loss 2947/10000 [=======>......................] - ETA: 58:09 - loss: 3.0646 - regression_loss: 2.2403 - classification_loss 2948/10000 [=======>......................] - ETA: 58:09 - loss: 3.0645 - regression_loss: 2.2403 - classification_loss 2949/10000 [=======>......................] - ETA: 58:08 - loss: 3.0645 - regression_loss: 2.2401 - classification_loss 2950/10000 [=======>......................] - ETA: 58:07 - loss: 3.0642 - regression_loss: 2.2399 - classification_loss 2951/10000 [=======>......................] - ETA: 58:07 - loss: 3.0643 - regression_loss: 2.2399 - classification_loss 2952/10000 [=======>......................] - ETA: 58:06 - loss: 3.0641 - regression_loss: 2.2399 - classification_loss 2953/10000 [=======>......................] - ETA: 58:06 - loss: 3.0643 - regression_loss: 2.2401 - classification_loss 2954/10000 [=======>......................] - ETA: 58:05 - loss: 3.0645 - regression_loss: 2.2403 - classification_loss 2955/10000 [=======>......................] - ETA: 58:05 - loss: 3.0642 - regression_loss: 2.2400 - classification_loss 2956/10000 [=======>......................] - ETA: 58:04 - loss: 3.0640 - regression_loss: 2.2400 - classification_loss 2957/10000 [=======>......................] - ETA: 58:04 - loss: 3.0638 - regression_loss: 2.2397 - classification_loss 2958/10000 [=======>......................] - ETA: 58:03 - loss: 3.0636 - regression_loss: 2.2396 - classification_loss 2959/10000 [=======>......................] - ETA: 58:03 - loss: 3.0631 - regression_loss: 2.2392 - classification_loss 2960/10000 [=======>......................] - ETA: 58:02 - loss: 3.0628 - regression_loss: 2.2389 - classification_loss 2961/10000 [=======>......................] - ETA: 58:02 - loss: 3.0627 - regression_loss: 2.2390 - classification_loss 2962/10000 [=======>......................] - ETA: 58:01 - loss: 3.0626 - regression_loss: 2.2390 - classification_loss 2963/10000 [=======>......................] - ETA: 58:01 - loss: 3.0624 - regression_loss: 2.2389 - classification_loss 2964/10000 [=======>......................] - ETA: 58:00 - loss: 3.0624 - regression_loss: 2.2389 - classification_loss 2965/10000 [=======>......................] - ETA: 57:59 - loss: 3.0625 - regression_loss: 2.2390 - classification_loss 2966/10000 [=======>......................] - ETA: 57:59 - loss: 3.0621 - regression_loss: 2.2387 - classification_loss 2967/10000 [=======>......................] - ETA: 57:58 - loss: 3.0618 - regression_loss: 2.2386 - classification_loss 2968/10000 [=======>......................] - ETA: 57:58 - loss: 3.0616 - regression_loss: 2.2385 - classification_loss 2969/10000 [=======>......................] - ETA: 57:57 - loss: 3.0615 - regression_loss: 2.2384 - classification_loss 2970/10000 [=======>......................] - ETA: 57:56 - loss: 3.0613 - regression_loss: 2.2383 - classification_loss 2971/10000 [=======>......................] - ETA: 57:56 - loss: 3.0611 - regression_loss: 2.2382 - classification_loss 2972/10000 [=======>......................] - ETA: 57:55 - loss: 3.0609 - regression_loss: 2.2381 - classification_loss 2973/10000 [=======>......................] - ETA: 57:55 - loss: 3.0606 - regression_loss: 2.2378 - classification_loss 2974/10000 [=======>......................] - ETA: 57:54 - loss: 3.0605 - regression_loss: 2.2377 - classification_loss 2975/10000 [=======>......................] - ETA: 57:53 - loss: 3.0602 - regression_loss: 2.2376 - classification_loss 2976/10000 [=======>......................] - ETA: 57:53 - loss: 3.0598 - regression_loss: 2.2373 - classification_loss 2977/10000 [=======>......................] - ETA: 57:52 - loss: 3.0594 - regression_loss: 2.2370 - classification_loss 2978/10000 [=======>......................] - ETA: 57:52 - loss: 3.0592 - regression_loss: 2.2368 - classification_loss 2979/10000 [=======>......................] - ETA: 57:51 - loss: 3.0590 - regression_loss: 2.2367 - classification_loss 2980/10000 [=======>......................] - ETA: 57:50 - loss: 3.0589 - regression_loss: 2.2366 - classification_loss 2981/10000 [=======>......................] - ETA: 57:50 - loss: 3.0588 - regression_loss: 2.2366 - classification_loss 2982/10000 [=======>......................] - ETA: 57:49 - loss: 3.0586 - regression_loss: 2.2364 - classification_loss 2983/10000 [=======>......................] - ETA: 57:49 - loss: 3.0585 - regression_loss: 2.2364 - classification_loss 2984/10000 [=======>......................] - ETA: 57:48 - loss: 3.0583 - regression_loss: 2.2362 - classification_loss 2985/10000 [=======>......................] - ETA: 57:48 - loss: 3.0581 - regression_loss: 2.2360 - classification_loss 2986/10000 [=======>......................] - ETA: 57:47 - loss: 3.0577 - regression_loss: 2.2357 - classification_loss 2987/10000 [=======>......................] - ETA: 57:47 - loss: 3.0573 - regression_loss: 2.2353 - classification_loss 2988/10000 [=======>......................] - ETA: 57:46 - loss: 3.0571 - regression_loss: 2.2352 - classification_loss 2989/10000 [=======>......................] - ETA: 57:45 - loss: 3.0567 - regression_loss: 2.2349 - classification_loss 2990/10000 [=======>......................] - ETA: 57:45 - loss: 3.0568 - regression_loss: 2.2349 - classification_loss 2991/10000 [=======>......................] - ETA: 57:44 - loss: 3.0565 - regression_loss: 2.2348 - classification_loss 2992/10000 [=======>......................] - ETA: 57:44 - loss: 3.0563 - regression_loss: 2.2347 - classification_loss 2993/10000 [=======>......................] - ETA: 57:43 - loss: 3.0560 - regression_loss: 2.2344 - classification_loss 2994/10000 [=======>......................] - ETA: 57:43 - loss: 3.0559 - regression_loss: 2.2344 - classification_loss 2995/10000 [=======>......................] - ETA: 57:42 - loss: 3.0554 - regression_loss: 2.2340 - classification_loss 2996/10000 [=======>......................] - ETA: 57:41 - loss: 3.0554 - regression_loss: 2.2340 - classification_loss 2997/10000 [=======>......................] - ETA: 57:41 - loss: 3.0551 - regression_loss: 2.2338 - classification_loss 2998/10000 [=======>......................] - ETA: 57:40 - loss: 3.0551 - regression_loss: 2.2338 - classification_loss 2999/10000 [=======>......................] - ETA: 57:40 - loss: 3.0550 - regression_loss: 2.2339 - classification_loss 3000/10000 [========>.....................] - ETA: 57:39 - loss: 3.0551 - regression_loss: 2.2340 - classification_loss 3001/10000 [========>.....................] - ETA: 57:39 - loss: 3.0551 - regression_loss: 2.2340 - classification_loss 3002/10000 [========>.....................] - ETA: 57:38 - loss: 3.0552 - regression_loss: 2.2340 - classification_loss 3003/10000 [========>.....................] - ETA: 57:37 - loss: 3.0551 - regression_loss: 2.2341 - classification_loss 3004/10000 [========>.....................] - ETA: 57:37 - loss: 3.0549 - regression_loss: 2.2339 - classification_loss 3005/10000 [========>.....................] - ETA: 57:36 - loss: 3.0549 - regression_loss: 2.2339 - classification_loss 3006/10000 [========>.....................] - ETA: 57:36 - loss: 3.0546 - regression_loss: 2.2337 - classification_loss 3007/10000 [========>.....................] - ETA: 57:35 - loss: 3.0545 - regression_loss: 2.2337 - classification_loss 3008/10000 [========>.....................] - ETA: 57:35 - loss: 3.0544 - regression_loss: 2.2336 - classification_loss 3009/10000 [========>.....................] - ETA: 57:34 - loss: 3.0541 - regression_loss: 2.2334 - classification_loss 3010/10000 [========>.....................] - ETA: 57:34 - loss: 3.0538 - regression_loss: 2.2330 - classification_loss 3011/10000 [========>.....................] - ETA: 57:33 - loss: 3.0536 - regression_loss: 2.2329 - classification_loss 3012/10000 [========>.....................] - ETA: 57:32 - loss: 3.0534 - regression_loss: 2.2328 - classification_loss 3013/10000 [========>.....................] - ETA: 57:32 - loss: 3.0533 - regression_loss: 2.2327 - classification_loss 3014/10000 [========>.....................] - ETA: 57:31 - loss: 3.0538 - regression_loss: 2.2330 - classification_loss 3015/10000 [========>.....................] - ETA: 57:31 - loss: 3.0537 - regression_loss: 2.2329 - classification_loss 3016/10000 [========>.....................] - ETA: 57:30 - loss: 3.0531 - regression_loss: 2.2325 - classification_loss 3017/10000 [========>.....................] - ETA: 57:30 - loss: 3.0529 - regression_loss: 2.2324 - classification_loss 3018/10000 [========>.....................] - ETA: 57:29 - loss: 3.0527 - regression_loss: 2.2323 - classification_loss 3019/10000 [========>.....................] - ETA: 57:28 - loss: 3.0525 - regression_loss: 2.2322 - classification_loss 3020/10000 [========>.....................] - ETA: 57:28 - loss: 3.0520 - regression_loss: 2.2317 - classification_loss 3021/10000 [========>.....................] - ETA: 57:27 - loss: 3.0518 - regression_loss: 2.2316 - classification_loss 3022/10000 [========>.....................] - ETA: 57:27 - loss: 3.0517 - regression_loss: 2.2315 - classification_loss 3023/10000 [========>.....................] - ETA: 57:26 - loss: 3.0514 - regression_loss: 2.2313 - classification_loss 3024/10000 [========>.....................] - ETA: 57:26 - loss: 3.0511 - regression_loss: 2.2310 - classification_loss 3025/10000 [========>.....................] - ETA: 57:26 - loss: 3.0513 - regression_loss: 2.2310 - classification_loss 3026/10000 [========>.....................] - ETA: 57:26 - loss: 3.0510 - regression_loss: 2.2307 - classification_loss 3027/10000 [========>.....................] - ETA: 57:25 - loss: 3.0507 - regression_loss: 2.2305 - classification_loss 3028/10000 [========>.....................] - ETA: 57:24 - loss: 3.0504 - regression_loss: 2.2303 - classification_loss 3029/10000 [========>.....................] - ETA: 57:24 - loss: 3.0504 - regression_loss: 2.2303 - classification_loss 3030/10000 [========>.....................] - ETA: 57:23 - loss: 3.0503 - regression_loss: 2.2303 - classification_loss 3031/10000 [========>.....................] - ETA: 57:23 - loss: 3.0503 - regression_loss: 2.2303 - classification_loss 3032/10000 [========>.....................] - ETA: 57:22 - loss: 3.0502 - regression_loss: 2.2303 - classification_loss 3033/10000 [========>.....................] - ETA: 57:22 - loss: 3.0498 - regression_loss: 2.2300 - classification_loss 3034/10000 [========>.....................] - ETA: 57:21 - loss: 3.0496 - regression_loss: 2.2299 - classification_loss 3035/10000 [========>.....................] - ETA: 57:20 - loss: 3.0496 - regression_loss: 2.2299 - classification_loss 3036/10000 [========>.....................] - ETA: 57:20 - loss: 3.0495 - regression_loss: 2.2298 - classification_loss 3037/10000 [========>.....................] - ETA: 57:19 - loss: 3.0494 - regression_loss: 2.2297 - classification_loss 3038/10000 [========>.....................] - ETA: 57:18 - loss: 3.0488 - regression_loss: 2.2293 - classification_loss 3039/10000 [========>.....................] - ETA: 57:18 - loss: 3.0489 - regression_loss: 2.2293 - classification_loss 3040/10000 [========>.....................] - ETA: 57:17 - loss: 3.0486 - regression_loss: 2.2290 - classification_loss 3041/10000 [========>.....................] - ETA: 57:17 - loss: 3.0484 - regression_loss: 2.2289 - classification_loss 3042/10000 [========>.....................] - ETA: 57:16 - loss: 3.0483 - regression_loss: 2.2288 - classification_loss 3043/10000 [========>.....................] - ETA: 57:15 - loss: 3.0479 - regression_loss: 2.2284 - classification_loss 3044/10000 [========>.....................] - ETA: 57:15 - loss: 3.0478 - regression_loss: 2.2282 - classification_loss 3045/10000 [========>.....................] - ETA: 57:15 - loss: 3.0475 - regression_loss: 2.2280 - classification_loss 3046/10000 [========>.....................] - ETA: 57:14 - loss: 3.0471 - regression_loss: 2.2277 - classification_loss 3047/10000 [========>.....................] - ETA: 57:14 - loss: 3.0466 - regression_loss: 2.2273 - classification_loss 3048/10000 [========>.....................] - ETA: 57:13 - loss: 3.0465 - regression_loss: 2.2273 - classification_loss 3049/10000 [========>.....................] - ETA: 57:12 - loss: 3.0465 - regression_loss: 2.2273 - classification_loss 3050/10000 [========>.....................] - ETA: 57:12 - loss: 3.0461 - regression_loss: 2.2270 - classification_loss 3051/10000 [========>.....................] - ETA: 57:11 - loss: 3.0461 - regression_loss: 2.2271 - classification_loss 3052/10000 [========>.....................] - ETA: 57:11 - loss: 3.0459 - regression_loss: 2.2270 - classification_loss 3053/10000 [========>.....................] - ETA: 57:10 - loss: 3.0454 - regression_loss: 2.2265 - classification_loss 3054/10000 [========>.....................] - ETA: 57:09 - loss: 3.0453 - regression_loss: 2.2264 - classification_loss 3055/10000 [========>.....................] - ETA: 57:09 - loss: 3.0448 - regression_loss: 2.2261 - classification_loss 3056/10000 [========>.....................] - ETA: 57:08 - loss: 3.0443 - regression_loss: 2.2257 - classification_loss 3057/10000 [========>.....................] - ETA: 57:08 - loss: 3.0442 - regression_loss: 2.2257 - classification_loss 3058/10000 [========>.....................] - ETA: 57:07 - loss: 3.0438 - regression_loss: 2.2253 - classification_loss 3059/10000 [========>.....................] - ETA: 57:07 - loss: 3.0437 - regression_loss: 2.2253 - classification_loss 3060/10000 [========>.....................] - ETA: 57:06 - loss: 3.0435 - regression_loss: 2.2252 - classification_loss 3061/10000 [========>.....................] - ETA: 57:05 - loss: 3.0435 - regression_loss: 2.2253 - classification_loss 3062/10000 [========>.....................] - ETA: 57:05 - loss: 3.0433 - regression_loss: 2.2252 - classification_loss 3063/10000 [========>.....................] - ETA: 57:04 - loss: 3.0433 - regression_loss: 2.2252 - classification_loss 3064/10000 [========>.....................] - ETA: 57:04 - loss: 3.0436 - regression_loss: 2.2255 - classification_loss 3065/10000 [========>.....................] - ETA: 57:03 - loss: 3.0436 - regression_loss: 2.2255 - classification_loss 3066/10000 [========>.....................] - ETA: 57:03 - loss: 3.0436 - regression_loss: 2.2256 - classification_loss 3067/10000 [========>.....................] - ETA: 57:02 - loss: 3.0435 - regression_loss: 2.2256 - classification_loss 3068/10000 [========>.....................] - ETA: 57:01 - loss: 3.0434 - regression_loss: 2.2253 - classification_loss 3069/10000 [========>.....................] - ETA: 57:01 - loss: 3.0431 - regression_loss: 2.2252 - classification_loss 3070/10000 [========>.....................] - ETA: 57:00 - loss: 3.0430 - regression_loss: 2.2250 - classification_loss 3071/10000 [========>.....................] - ETA: 57:00 - loss: 3.0429 - regression_loss: 2.2250 - classification_loss 3072/10000 [========>.....................] - ETA: 56:59 - loss: 3.0426 - regression_loss: 2.2248 - classification_loss 3073/10000 [========>.....................] - ETA: 56:59 - loss: 3.0423 - regression_loss: 2.2246 - classification_loss 3074/10000 [========>.....................] - ETA: 56:58 - loss: 3.0420 - regression_loss: 2.2244 - classification_loss 3075/10000 [========>.....................] - ETA: 56:57 - loss: 3.0418 - regression_loss: 2.2242 - classification_loss 3076/10000 [========>.....................] - ETA: 56:57 - loss: 3.0416 - regression_loss: 2.2241 - classification_loss 3077/10000 [========>.....................] - ETA: 56:56 - loss: 3.0411 - regression_loss: 2.2237 - classification_loss 3078/10000 [========>.....................] - ETA: 56:56 - loss: 3.0410 - regression_loss: 2.2236 - classification_loss 3079/10000 [========>.....................] - ETA: 56:55 - loss: 3.0407 - regression_loss: 2.2234 - classification_loss 3080/10000 [========>.....................] - ETA: 56:55 - loss: 3.0408 - regression_loss: 2.2235 - classification_loss 3081/10000 [========>.....................] - ETA: 56:54 - loss: 3.0404 - regression_loss: 2.2232 - classification_loss 3082/10000 [========>.....................] - ETA: 56:54 - loss: 3.0400 - regression_loss: 2.2229 - classification_loss 3083/10000 [========>.....................] - ETA: 56:53 - loss: 3.0403 - regression_loss: 2.2232 - classification_loss 3084/10000 [========>.....................] - ETA: 56:53 - loss: 3.0402 - regression_loss: 2.2231 - classification_loss 3085/10000 [========>.....................] - ETA: 56:52 - loss: 3.0399 - regression_loss: 2.2228 - classification_loss 3086/10000 [========>.....................] - ETA: 56:51 - loss: 3.0396 - regression_loss: 2.2227 - classification_loss 3087/10000 [========>.....................] - ETA: 56:51 - loss: 3.0394 - regression_loss: 2.2225 - classification_loss 3088/10000 [========>.....................] - ETA: 56:50 - loss: 3.0392 - regression_loss: 2.2225 - classification_loss 3089/10000 [========>.....................] - ETA: 56:50 - loss: 3.0390 - regression_loss: 2.2224 - classification_loss 3090/10000 [========>.....................] - ETA: 56:49 - loss: 3.0389 - regression_loss: 2.2223 - classification_loss 3091/10000 [========>.....................] - ETA: 56:49 - loss: 3.0389 - regression_loss: 2.2223 - classification_loss 3092/10000 [========>.....................] - ETA: 56:48 - loss: 3.0389 - regression_loss: 2.2225 - classification_loss 3093/10000 [========>.....................] - ETA: 56:47 - loss: 3.0384 - regression_loss: 2.2221 - classification_loss 3094/10000 [========>.....................] - ETA: 56:47 - loss: 3.0385 - regression_loss: 2.2222 - classification_loss 3095/10000 [========>.....................] - ETA: 56:46 - loss: 3.0385 - regression_loss: 2.2222 - classification_loss 3096/10000 [========>.....................] - ETA: 56:46 - loss: 3.0383 - regression_loss: 2.2221 - classification_loss 3097/10000 [========>.....................] - ETA: 56:45 - loss: 3.0381 - regression_loss: 2.2220 - classification_loss 3098/10000 [========>.....................] - ETA: 56:45 - loss: 3.0379 - regression_loss: 2.2218 - classification_loss 3099/10000 [========>.....................] - ETA: 56:44 - loss: 3.0377 - regression_loss: 2.2218 - classification_loss 3100/10000 [========>.....................] - ETA: 56:43 - loss: 3.0379 - regression_loss: 2.2219 - classification_loss 3101/10000 [========>.....................] - ETA: 56:43 - loss: 3.0377 - regression_loss: 2.2217 - classification_loss 3102/10000 [========>.....................] - ETA: 56:42 - loss: 3.0377 - regression_loss: 2.2217 - classification_loss 3103/10000 [========>.....................] - ETA: 56:42 - loss: 3.0375 - regression_loss: 2.2215 - classification_loss 3104/10000 [========>.....................] - ETA: 56:41 - loss: 3.0370 - regression_loss: 2.2212 - classification_loss 3105/10000 [========>.....................] - ETA: 56:41 - loss: 3.0366 - regression_loss: 2.2208 - classification_loss 3106/10000 [========>.....................] - ETA: 56:40 - loss: 3.0366 - regression_loss: 2.2208 - classification_loss 3107/10000 [========>.....................] - ETA: 56:40 - loss: 3.0363 - regression_loss: 2.2206 - classification_loss 3108/10000 [========>.....................] - ETA: 56:39 - loss: 3.0364 - regression_loss: 2.2207 - classification_loss 3109/10000 [========>.....................] - ETA: 56:39 - loss: 3.0366 - regression_loss: 2.2209 - classification_loss 3110/10000 [========>.....................] - ETA: 56:38 - loss: 3.0364 - regression_loss: 2.2208 - classification_loss 3111/10000 [========>.....................] - ETA: 56:37 - loss: 3.0361 - regression_loss: 2.2206 - classification_loss 3112/10000 [========>.....................] - ETA: 56:37 - loss: 3.0360 - regression_loss: 2.2206 - classification_loss 3113/10000 [========>.....................] - ETA: 56:36 - loss: 3.0356 - regression_loss: 2.2204 - classification_loss 3114/10000 [========>.....................] - ETA: 56:36 - loss: 3.0353 - regression_loss: 2.2200 - classification_loss 3115/10000 [========>.....................] - ETA: 56:35 - loss: 3.0351 - regression_loss: 2.2199 - classification_loss 3116/10000 [========>.....................] - ETA: 56:34 - loss: 3.0347 - regression_loss: 2.2195 - classification_loss 3117/10000 [========>.....................] - ETA: 56:34 - loss: 3.0344 - regression_loss: 2.2193 - classification_loss 3118/10000 [========>.....................] - ETA: 56:33 - loss: 3.0344 - regression_loss: 2.2193 - classification_loss 3119/10000 [========>.....................] - ETA: 56:33 - loss: 3.0342 - regression_loss: 2.2192 - classification_loss 3120/10000 [========>.....................] - ETA: 56:32 - loss: 3.0342 - regression_loss: 2.2185 - classification_loss 3121/10000 [========>.....................] - ETA: 56:32 - loss: 3.0340 - regression_loss: 2.2183 - classification_loss 3122/10000 [========>.....................] - ETA: 56:31 - loss: 3.0337 - regression_loss: 2.2181 - classification_loss 3123/10000 [========>.....................] - ETA: 56:31 - loss: 3.0335 - regression_loss: 2.2180 - classification_loss 3124/10000 [========>.....................] - ETA: 56:30 - loss: 3.0333 - regression_loss: 2.2178 - classification_loss 3125/10000 [========>.....................] - ETA: 56:29 - loss: 3.0331 - regression_loss: 2.2177 - classification_loss 3126/10000 [========>.....................] - ETA: 56:29 - loss: 3.0330 - regression_loss: 2.2176 - classification_loss 3127/10000 [========>.....................] - ETA: 56:28 - loss: 3.0326 - regression_loss: 2.2173 - classification_loss 3128/10000 [========>.....................] - ETA: 56:28 - loss: 3.0326 - regression_loss: 2.2173 - classification_loss 3129/10000 [========>.....................] - ETA: 56:27 - loss: 3.0322 - regression_loss: 2.2169 - classification_loss 3130/10000 [========>.....................] - ETA: 56:26 - loss: 3.0318 - regression_loss: 2.2166 - classification_loss 3131/10000 [========>.....................] - ETA: 56:26 - loss: 3.0317 - regression_loss: 2.2165 - classification_loss 3132/10000 [========>.....................] - ETA: 56:25 - loss: 3.0316 - regression_loss: 2.2164 - classification_loss 3133/10000 [========>.....................] - ETA: 56:25 - loss: 3.0315 - regression_loss: 2.2165 - classification_loss 3134/10000 [========>.....................] - ETA: 56:25 - loss: 3.0314 - regression_loss: 2.2165 - classification_loss 3135/10000 [========>.....................] - ETA: 56:24 - loss: 3.0312 - regression_loss: 2.2163 - classification_loss 3136/10000 [========>.....................] - ETA: 56:23 - loss: 3.0311 - regression_loss: 2.2162 - classification_loss 3137/10000 [========>.....................] - ETA: 56:23 - loss: 3.0307 - regression_loss: 2.2159 - classification_loss 3138/10000 [========>.....................] - ETA: 56:22 - loss: 3.0304 - regression_loss: 2.2157 - classification_loss 3139/10000 [========>.....................] - ETA: 56:22 - loss: 3.0300 - regression_loss: 2.2154 - classification_loss 3140/10000 [========>.....................] - ETA: 56:21 - loss: 3.0300 - regression_loss: 2.2154 - classification_loss 3141/10000 [========>.....................] - ETA: 56:20 - loss: 3.0299 - regression_loss: 2.2154 - classification_loss 3142/10000 [========>.....................] - ETA: 56:20 - loss: 3.0298 - regression_loss: 2.2154 - classification_loss 3143/10000 [========>.....................] - ETA: 56:20 - loss: 3.0294 - regression_loss: 2.2149 - classification_loss 3144/10000 [========>.....................] - ETA: 56:19 - loss: 3.0291 - regression_loss: 2.2147 - classification_loss 3145/10000 [========>.....................] - ETA: 56:18 - loss: 3.0288 - regression_loss: 2.2145 - classification_loss 3146/10000 [========>.....................] - ETA: 56:18 - loss: 3.0286 - regression_loss: 2.2145 - classification_loss 3147/10000 [========>.....................] - ETA: 56:17 - loss: 3.0285 - regression_loss: 2.2143 - classification_loss 3148/10000 [========>.....................] - ETA: 56:17 - loss: 3.0285 - regression_loss: 2.2144 - classification_loss 3149/10000 [========>.....................] - ETA: 56:16 - loss: 3.0284 - regression_loss: 2.2143 - classification_loss 3150/10000 [========>.....................] - ETA: 56:16 - loss: 3.0278 - regression_loss: 2.2138 - classification_loss 3151/10000 [========>.....................] - ETA: 56:15 - loss: 3.0277 - regression_loss: 2.2138 - classification_loss 3152/10000 [========>.....................] - ETA: 56:14 - loss: 3.0276 - regression_loss: 2.2137 - classification_loss 3153/10000 [========>.....................] - ETA: 56:14 - loss: 3.0274 - regression_loss: 2.2137 - classification_loss 3154/10000 [========>.....................] - ETA: 56:13 - loss: 3.0275 - regression_loss: 2.2137 - classification_loss 3155/10000 [========>.....................] - ETA: 56:13 - loss: 3.0273 - regression_loss: 2.2137 - classification_loss 3156/10000 [========>.....................] - ETA: 56:12 - loss: 3.0273 - regression_loss: 2.2137 - classification_loss 3157/10000 [========>.....................] - ETA: 56:12 - loss: 3.0272 - regression_loss: 2.2136 - classification_loss 3158/10000 [========>.....................] - ETA: 56:11 - loss: 3.0268 - regression_loss: 2.2133 - classification_loss 3159/10000 [========>.....................] - ETA: 56:10 - loss: 3.0267 - regression_loss: 2.2132 - classification_loss 3160/10000 [========>.....................] - ETA: 56:10 - loss: 3.0265 - regression_loss: 2.2132 - classification_loss 3161/10000 [========>.....................] - ETA: 56:09 - loss: 3.0262 - regression_loss: 2.2129 - classification_loss 3162/10000 [========>.....................] - ETA: 56:09 - loss: 3.0262 - regression_loss: 2.2128 - classification_loss 3163/10000 [========>.....................] - ETA: 56:08 - loss: 3.0262 - regression_loss: 2.2129 - classification_loss 3164/10000 [========>.....................] - ETA: 56:08 - loss: 3.0263 - regression_loss: 2.2130 - classification_loss 3165/10000 [========>.....................] - ETA: 56:07 - loss: 3.0262 - regression_loss: 2.2129 - classification_loss 3166/10000 [========>.....................] - ETA: 56:06 - loss: 3.0261 - regression_loss: 2.2129 - classification_loss 3167/10000 [========>.....................] - ETA: 56:06 - loss: 3.0256 - regression_loss: 2.2125 - classification_loss 3168/10000 [========>.....................] - ETA: 56:05 - loss: 3.0254 - regression_loss: 2.2124 - classification_loss 3169/10000 [========>.....................] - ETA: 56:05 - loss: 3.0256 - regression_loss: 2.2126 - classification_loss 3170/10000 [========>.....................] - ETA: 56:04 - loss: 3.0253 - regression_loss: 2.2123 - classification_loss 3171/10000 [========>.....................] - ETA: 56:04 - loss: 3.0253 - regression_loss: 2.2123 - classification_loss 3172/10000 [========>.....................] - ETA: 56:03 - loss: 3.0253 - regression_loss: 2.2123 - classification_loss 3173/10000 [========>.....................] - ETA: 56:02 - loss: 3.0251 - regression_loss: 2.2122 - classification_loss 3174/10000 [========>.....................] - ETA: 56:02 - loss: 3.0251 - regression_loss: 2.2122 - classification_loss 3175/10000 [========>.....................] - ETA: 56:01 - loss: 3.0249 - regression_loss: 2.2120 - classification_loss 3176/10000 [========>.....................] - ETA: 56:01 - loss: 3.0248 - regression_loss: 2.2120 - classification_loss 3177/10000 [========>.....................] - ETA: 56:00 - loss: 3.0245 - regression_loss: 2.2118 - classification_loss 3178/10000 [========>.....................] - ETA: 55:59 - loss: 3.0242 - regression_loss: 2.2116 - classification_loss 3179/10000 [========>.....................] - ETA: 55:59 - loss: 3.0241 - regression_loss: 2.2116 - classification_loss 3180/10000 [========>.....................] - ETA: 55:58 - loss: 3.0241 - regression_loss: 2.2116 - classification_loss 3181/10000 [========>.....................] - ETA: 55:58 - loss: 3.0240 - regression_loss: 2.2116 - classification_loss 3182/10000 [========>.....................] - ETA: 55:57 - loss: 3.0239 - regression_loss: 2.2115 - classification_loss 3183/10000 [========>.....................] - ETA: 55:57 - loss: 3.0238 - regression_loss: 2.2115 - classification_loss 3184/10000 [========>.....................] - ETA: 55:56 - loss: 3.0238 - regression_loss: 2.2116 - classification_loss 3185/10000 [========>.....................] - ETA: 55:55 - loss: 3.0236 - regression_loss: 2.2115 - classification_loss 3186/10000 [========>.....................] - ETA: 55:56 - loss: 3.0234 - regression_loss: 2.2114 - classification_loss 3187/10000 [========>.....................] - ETA: 55:55 - loss: 3.0230 - regression_loss: 2.2110 - classification_loss 3188/10000 [========>.....................] - ETA: 55:54 - loss: 3.0230 - regression_loss: 2.2111 - classification_loss 3189/10000 [========>.....................] - ETA: 55:54 - loss: 3.0230 - regression_loss: 2.2111 - classification_loss 3190/10000 [========>.....................] - ETA: 55:53 - loss: 3.0229 - regression_loss: 2.2112 - classification_loss 3191/10000 [========>.....................] - ETA: 55:53 - loss: 3.0227 - regression_loss: 2.2110 - classification_loss 3192/10000 [========>.....................] - ETA: 55:52 - loss: 3.0230 - regression_loss: 2.2112 - classification_loss 3193/10000 [========>.....................] - ETA: 55:52 - loss: 3.0230 - regression_loss: 2.2112 - classification_loss 3194/10000 [========>.....................] - ETA: 55:51 - loss: 3.0228 - regression_loss: 2.2111 - classification_loss 3195/10000 [========>.....................] - ETA: 55:51 - loss: 3.0225 - regression_loss: 2.2109 - classification_loss 3196/10000 [========>.....................] - ETA: 55:50 - loss: 3.0225 - regression_loss: 2.2109 - classification_loss 3197/10000 [========>.....................] - ETA: 55:50 - loss: 3.0225 - regression_loss: 2.2109 - classification_loss 3198/10000 [========>.....................] - ETA: 55:49 - loss: 3.0222 - regression_loss: 2.2107 - classification_loss 3199/10000 [========>.....................] - ETA: 55:49 - loss: 3.0222 - regression_loss: 2.2107 - classification_loss 3200/10000 [========>.....................] - ETA: 55:49 - loss: 3.0220 - regression_loss: 2.2105 - classification_loss 3201/10000 [========>.....................] - ETA: 55:49 - loss: 3.0218 - regression_loss: 2.2104 - classification_loss 3202/10000 [========>.....................] - ETA: 55:48 - loss: 3.0219 - regression_loss: 2.2104 - classification_loss 3203/10000 [========>.....................] - ETA: 55:47 - loss: 3.0219 - regression_loss: 2.2105 - classification_loss 3204/10000 [========>.....................] - ETA: 55:47 - loss: 3.0217 - regression_loss: 2.2103 - classification_loss 3205/10000 [========>.....................] - ETA: 55:46 - loss: 3.0216 - regression_loss: 2.2102 - classification_loss 3206/10000 [========>.....................] - ETA: 55:46 - loss: 3.0215 - regression_loss: 2.2102 - classification_loss 3207/10000 [========>.....................] - ETA: 55:45 - loss: 3.0211 - regression_loss: 2.2100 - classification_loss 3208/10000 [========>.....................] - ETA: 55:45 - loss: 3.0209 - regression_loss: 2.2098 - classification_loss 3209/10000 [========>.....................] - ETA: 55:44 - loss: 3.0206 - regression_loss: 2.2096 - classification_loss 3210/10000 [========>.....................] - ETA: 55:43 - loss: 3.0205 - regression_loss: 2.2095 - classification_loss 3211/10000 [========>.....................] - ETA: 55:43 - loss: 3.0204 - regression_loss: 2.2095 - classification_loss 3212/10000 [========>.....................] - ETA: 55:42 - loss: 3.0200 - regression_loss: 2.2092 - classification_loss 3213/10000 [========>.....................] - ETA: 55:42 - loss: 3.0197 - regression_loss: 2.2089 - classification_loss 3214/10000 [========>.....................] - ETA: 55:41 - loss: 3.0195 - regression_loss: 2.2087 - classification_loss 3215/10000 [========>.....................] - ETA: 55:41 - loss: 3.0193 - regression_loss: 2.2086 - classification_loss 3216/10000 [========>.....................] - ETA: 55:40 - loss: 3.0188 - regression_loss: 2.2083 - classification_loss 3217/10000 [========>.....................] - ETA: 55:39 - loss: 3.0184 - regression_loss: 2.2080 - classification_loss 3218/10000 [========>.....................] - ETA: 55:39 - loss: 3.0182 - regression_loss: 2.2079 - classification_loss 3219/10000 [========>.....................] - ETA: 55:38 - loss: 3.0182 - regression_loss: 2.2079 - classification_loss 3220/10000 [========>.....................] - ETA: 55:38 - loss: 3.0181 - regression_loss: 2.2079 - classification_loss 3221/10000 [========>.....................] - ETA: 55:37 - loss: 3.0178 - regression_loss: 2.2076 - classification_loss 3222/10000 [========>.....................] - ETA: 55:37 - loss: 3.0178 - regression_loss: 2.2076 - classification_loss 3223/10000 [========>.....................] - ETA: 55:36 - loss: 3.0173 - regression_loss: 2.2072 - classification_loss 3224/10000 [========>.....................] - ETA: 55:35 - loss: 3.0171 - regression_loss: 2.2071 - classification_loss 3225/10000 [========>.....................] - ETA: 55:35 - loss: 3.0167 - regression_loss: 2.2067 - classification_loss 3226/10000 [========>.....................] - ETA: 55:34 - loss: 3.0164 - regression_loss: 2.2064 - classification_loss 3227/10000 [========>.....................] - ETA: 55:34 - loss: 3.0162 - regression_loss: 2.2062 - classification_loss 3228/10000 [========>.....................] - ETA: 55:33 - loss: 3.0161 - regression_loss: 2.2063 - classification_loss 3229/10000 [========>.....................] - ETA: 55:33 - loss: 3.0156 - regression_loss: 2.2059 - classification_loss 3230/10000 [========>.....................] - ETA: 55:32 - loss: 3.0155 - regression_loss: 2.2058 - classification_loss 3231/10000 [========>.....................] - ETA: 55:32 - loss: 3.0153 - regression_loss: 2.2055 - classification_loss 3232/10000 [========>.....................] - ETA: 55:31 - loss: 3.0150 - regression_loss: 2.2054 - classification_loss 3233/10000 [========>.....................] - ETA: 55:31 - loss: 3.0149 - regression_loss: 2.2052 - classification_loss 3234/10000 [========>.....................] - ETA: 55:30 - loss: 3.0148 - regression_loss: 2.2052 - classification_loss 3235/10000 [========>.....................] - ETA: 55:30 - loss: 3.0149 - regression_loss: 2.2054 - classification_loss 3236/10000 [========>.....................] - ETA: 55:29 - loss: 3.0148 - regression_loss: 2.2053 - classification_loss 3237/10000 [========>.....................] - ETA: 55:29 - loss: 3.0148 - regression_loss: 2.2054 - classification_loss 3238/10000 [========>.....................] - ETA: 55:28 - loss: 3.0149 - regression_loss: 2.2055 - classification_loss 3239/10000 [========>.....................] - ETA: 55:28 - loss: 3.0146 - regression_loss: 2.2052 - classification_loss 3240/10000 [========>.....................] - ETA: 55:27 - loss: 3.0141 - regression_loss: 2.2048 - classification_loss 3241/10000 [========>.....................] - ETA: 55:26 - loss: 3.0142 - regression_loss: 2.2049 - classification_loss 3242/10000 [========>.....................] - ETA: 55:26 - loss: 3.0141 - regression_loss: 2.2048 - classification_loss 3243/10000 [========>.....................] - ETA: 55:25 - loss: 3.0140 - regression_loss: 2.2048 - classification_loss 3244/10000 [========>.....................] - ETA: 55:25 - loss: 3.0138 - regression_loss: 2.2046 - classification_loss 3245/10000 [========>.....................] - ETA: 55:24 - loss: 3.0137 - regression_loss: 2.2046 - classification_loss 3246/10000 [========>.....................] - ETA: 55:24 - loss: 3.0134 - regression_loss: 2.2044 - classification_loss 3247/10000 [========>.....................] - ETA: 55:23 - loss: 3.0131 - regression_loss: 2.2041 - classification_loss 3248/10000 [========>.....................] - ETA: 55:22 - loss: 3.0131 - regression_loss: 2.2041 - classification_loss 3249/10000 [========>.....................] - ETA: 55:22 - loss: 3.0129 - regression_loss: 2.2040 - classification_loss 3250/10000 [========>.....................] - ETA: 55:21 - loss: 3.0126 - regression_loss: 2.2039 - classification_loss 3251/10000 [========>.....................] - ETA: 55:21 - loss: 3.0124 - regression_loss: 2.2037 - classification_loss 3252/10000 [========>.....................] - ETA: 55:20 - loss: 3.0123 - regression_loss: 2.2037 - classification_loss 3253/10000 [========>.....................] - ETA: 55:20 - loss: 3.0121 - regression_loss: 2.2035 - classification_loss 3254/10000 [========>.....................] - ETA: 55:19 - loss: 3.0120 - regression_loss: 2.2035 - classification_loss 3255/10000 [========>.....................] - ETA: 55:19 - loss: 3.0118 - regression_loss: 2.2034 - classification_loss 3256/10000 [========>.....................] - ETA: 55:18 - loss: 3.0117 - regression_loss: 2.2034 - classification_loss 3257/10000 [========>.....................] - ETA: 55:18 - loss: 3.0115 - regression_loss: 2.2031 - classification_loss 3258/10000 [========>.....................] - ETA: 55:17 - loss: 3.0112 - regression_loss: 2.2030 - classification_loss 3259/10000 [========>.....................] - ETA: 55:17 - loss: 3.0110 - regression_loss: 2.2028 - classification_loss 3260/10000 [========>.....................] - ETA: 55:16 - loss: 3.0109 - regression_loss: 2.2028 - classification_loss 3261/10000 [========>.....................] - ETA: 55:15 - loss: 3.0108 - regression_loss: 2.2028 - classification_loss 3262/10000 [========>.....................] - ETA: 55:15 - loss: 3.0106 - regression_loss: 2.2026 - classification_loss 3263/10000 [========>.....................] - ETA: 55:14 - loss: 3.0107 - regression_loss: 2.2027 - classification_loss 3264/10000 [========>.....................] - ETA: 55:14 - loss: 3.0105 - regression_loss: 2.2025 - classification_loss 3265/10000 [========>.....................] - ETA: 55:13 - loss: 3.0105 - regression_loss: 2.2026 - classification_loss 3266/10000 [========>.....................] - ETA: 55:13 - loss: 3.0105 - regression_loss: 2.2026 - classification_loss 3267/10000 [========>.....................] - ETA: 55:12 - loss: 3.0102 - regression_loss: 2.2025 - classification_loss 3268/10000 [========>.....................] - ETA: 55:11 - loss: 3.0100 - regression_loss: 2.2023 - classification_loss 3269/10000 [========>.....................] - ETA: 55:11 - loss: 3.0098 - regression_loss: 2.2022 - classification_loss 3270/10000 [========>.....................] - ETA: 55:10 - loss: 3.0100 - regression_loss: 2.2023 - classification_loss 3271/10000 [========>.....................] - ETA: 55:10 - loss: 3.0100 - regression_loss: 2.2024 - classification_loss 3272/10000 [========>.....................] - ETA: 55:09 - loss: 3.0099 - regression_loss: 2.2023 - classification_loss 3273/10000 [========>.....................] - ETA: 55:09 - loss: 3.0095 - regression_loss: 2.2019 - classification_loss 3274/10000 [========>.....................] - ETA: 55:08 - loss: 3.0094 - regression_loss: 2.2019 - classification_loss 3275/10000 [========>.....................] - ETA: 55:08 - loss: 3.0094 - regression_loss: 2.2020 - classification_loss 3276/10000 [========>.....................] - ETA: 55:07 - loss: 3.0094 - regression_loss: 2.2019 - classification_loss 3277/10000 [========>.....................] - ETA: 55:06 - loss: 3.0093 - regression_loss: 2.2019 - classification_loss 3278/10000 [========>.....................] - ETA: 55:06 - loss: 3.0093 - regression_loss: 2.2019 - classification_loss 3279/10000 [========>.....................] - ETA: 55:05 - loss: 3.0093 - regression_loss: 2.2019 - classification_loss 3280/10000 [========>.....................] - ETA: 55:05 - loss: 3.0091 - regression_loss: 2.2017 - classification_loss 3281/10000 [========>.....................] - ETA: 55:04 - loss: 3.0088 - regression_loss: 2.2016 - classification_loss 3282/10000 [========>.....................] - ETA: 55:04 - loss: 3.0091 - regression_loss: 2.2018 - classification_loss 3283/10000 [========>.....................] - ETA: 55:03 - loss: 3.0090 - regression_loss: 2.2018 - classification_loss 3284/10000 [========>.....................] - ETA: 55:03 - loss: 3.0091 - regression_loss: 2.2019 - classification_loss 3285/10000 [========>.....................] - ETA: 55:02 - loss: 3.0091 - regression_loss: 2.2019 - classification_loss 3286/10000 [========>.....................] - ETA: 55:01 - loss: 3.0093 - regression_loss: 2.2021 - classification_loss 3287/10000 [========>.....................] - ETA: 55:01 - loss: 3.0093 - regression_loss: 2.2021 - classification_loss 3288/10000 [========>.....................] - ETA: 55:00 - loss: 3.0094 - regression_loss: 2.2022 - classification_loss 3289/10000 [========>.....................] - ETA: 55:00 - loss: 3.0092 - regression_loss: 2.2021 - classification_loss 3290/10000 [========>.....................] - ETA: 54:59 - loss: 3.0092 - regression_loss: 2.2022 - classification_loss 3291/10000 [========>.....................] - ETA: 54:59 - loss: 3.0092 - regression_loss: 2.2021 - classification_loss 3292/10000 [========>.....................] - ETA: 54:58 - loss: 3.0091 - regression_loss: 2.2021 - classification_loss 3293/10000 [========>.....................] - ETA: 54:57 - loss: 3.0087 - regression_loss: 2.2018 - classification_loss 3294/10000 [========>.....................] - ETA: 54:57 - loss: 3.0087 - regression_loss: 2.2019 - classification_loss 3295/10000 [========>.....................] - ETA: 54:56 - loss: 3.0084 - regression_loss: 2.2015 - classification_loss 3296/10000 [========>.....................] - ETA: 54:56 - loss: 3.0081 - regression_loss: 2.2013 - classification_loss 3297/10000 [========>.....................] - ETA: 54:55 - loss: 3.0080 - regression_loss: 2.2012 - classification_loss 3298/10000 [========>.....................] - ETA: 54:55 - loss: 3.0078 - regression_loss: 2.2011 - classification_loss 3299/10000 [========>.....................] - ETA: 54:54 - loss: 3.0077 - regression_loss: 2.2011 - classification_loss 3300/10000 [========>.....................] - ETA: 54:53 - loss: 3.0078 - regression_loss: 2.2011 - classification_loss 3301/10000 [========>.....................] - ETA: 54:53 - loss: 3.0076 - regression_loss: 2.2010 - classification_loss 3302/10000 [========>.....................] - ETA: 54:52 - loss: 3.0074 - regression_loss: 2.2009 - classification_loss 3303/10000 [========>.....................] - ETA: 54:52 - loss: 3.0072 - regression_loss: 2.2007 - classification_loss 3304/10000 [========>.....................] - ETA: 54:51 - loss: 3.0069 - regression_loss: 2.2005 - classification_loss 3305/10000 [========>.....................] - ETA: 54:51 - loss: 3.0065 - regression_loss: 2.2002 - classification_loss 3306/10000 [========>.....................] - ETA: 54:50 - loss: 3.0063 - regression_loss: 2.2000 - classification_loss 3307/10000 [========>.....................] - ETA: 54:50 - loss: 3.0062 - regression_loss: 2.1999 - classification_loss 3308/10000 [========>.....................] - ETA: 54:49 - loss: 3.0058 - regression_loss: 2.1996 - classification_loss 3309/10000 [========>.....................] - ETA: 54:49 - loss: 3.0059 - regression_loss: 2.1996 - classification_loss 3310/10000 [========>.....................] - ETA: 54:48 - loss: 3.0058 - regression_loss: 2.1996 - classification_loss 3311/10000 [========>.....................] - ETA: 54:48 - loss: 3.0056 - regression_loss: 2.1994 - classification_loss 3312/10000 [========>.....................] - ETA: 54:47 - loss: 3.0056 - regression_loss: 2.1994 - classification_loss 3313/10000 [========>.....................] - ETA: 54:46 - loss: 3.0054 - regression_loss: 2.1993 - classification_loss 3314/10000 [========>.....................] - ETA: 54:46 - loss: 3.0053 - regression_loss: 2.1993 - classification_loss 3315/10000 [========>.....................] - ETA: 54:45 - loss: 3.0052 - regression_loss: 2.1992 - classification_loss 3316/10000 [========>.....................] - ETA: 54:45 - loss: 3.0049 - regression_loss: 2.1990 - classification_loss 3317/10000 [========>.....................] - ETA: 54:44 - loss: 3.0045 - regression_loss: 2.1986 - classification_loss 3318/10000 [========>.....................] - ETA: 54:44 - loss: 3.0045 - regression_loss: 2.1987 - classification_loss 3319/10000 [========>.....................] - ETA: 54:43 - loss: 3.0039 - regression_loss: 2.1983 - classification_loss 3320/10000 [========>.....................] - ETA: 54:42 - loss: 3.0040 - regression_loss: 2.1984 - classification_loss 3321/10000 [========>.....................] - ETA: 54:42 - loss: 3.0037 - regression_loss: 2.1981 - classification_loss 3322/10000 [========>.....................] - ETA: 54:41 - loss: 3.0033 - regression_loss: 2.1978 - classification_loss 3323/10000 [========>.....................] - ETA: 54:41 - loss: 3.0032 - regression_loss: 2.1978 - classification_loss 3324/10000 [========>.....................] - ETA: 54:40 - loss: 3.0030 - regression_loss: 2.1976 - classification_loss 3325/10000 [========>.....................] - ETA: 54:40 - loss: 3.0027 - regression_loss: 2.1974 - classification_loss 3326/10000 [========>.....................] - ETA: 54:39 - loss: 3.0028 - regression_loss: 2.1976 - classification_loss 3327/10000 [========>.....................] - ETA: 54:38 - loss: 3.0024 - regression_loss: 2.1973 - classification_loss 3328/10000 [========>.....................] - ETA: 54:38 - loss: 3.0022 - regression_loss: 2.1972 - classification_loss 3329/10000 [========>.....................] - ETA: 54:37 - loss: 3.0023 - regression_loss: 2.1973 - classification_loss 3330/10000 [========>.....................] - ETA: 54:37 - loss: 3.0023 - regression_loss: 2.1974 - classification_loss 3331/10000 [========>.....................] - ETA: 54:36 - loss: 3.0019 - regression_loss: 2.1971 - classification_loss 3332/10000 [========>.....................] - ETA: 54:36 - loss: 3.0018 - regression_loss: 2.1971 - classification_loss 3333/10000 [========>.....................] - ETA: 54:35 - loss: 3.0017 - regression_loss: 2.1970 - classification_loss 3334/10000 [=========>....................] - ETA: 54:35 - loss: 3.0015 - regression_loss: 2.1969 - classification_loss 3335/10000 [=========>....................] - ETA: 54:34 - loss: 3.0014 - regression_loss: 2.1969 - classification_loss 3336/10000 [=========>....................] - ETA: 54:35 - loss: 3.0011 - regression_loss: 2.1966 - classification_loss 3337/10000 [=========>....................] - ETA: 54:34 - loss: 3.0011 - regression_loss: 2.1966 - classification_loss 3338/10000 [=========>....................] - ETA: 54:34 - loss: 3.0010 - regression_loss: 2.1965 - classification_loss 3339/10000 [=========>....................] - ETA: 54:33 - loss: 3.0008 - regression_loss: 2.1964 - classification_loss 3340/10000 [=========>....................] - ETA: 54:33 - loss: 3.0007 - regression_loss: 2.1964 - classification_loss 3341/10000 [=========>....................] - ETA: 54:32 - loss: 3.0008 - regression_loss: 2.1964 - classification_loss 3342/10000 [=========>....................] - ETA: 54:32 - loss: 3.0005 - regression_loss: 2.1962 - classification_loss 3343/10000 [=========>....................] - ETA: 54:31 - loss: 3.0001 - regression_loss: 2.1959 - classification_loss 3344/10000 [=========>....................] - ETA: 54:30 - loss: 2.9996 - regression_loss: 2.1955 - classification_loss 3345/10000 [=========>....................] - ETA: 54:30 - loss: 2.9993 - regression_loss: 2.1953 - classification_loss 3346/10000 [=========>....................] - ETA: 54:29 - loss: 2.9993 - regression_loss: 2.1953 - classification_loss 3347/10000 [=========>....................] - ETA: 54:29 - loss: 2.9992 - regression_loss: 2.1952 - classification_loss 3348/10000 [=========>....................] - ETA: 54:28 - loss: 2.9989 - regression_loss: 2.1949 - classification_loss 3349/10000 [=========>....................] - ETA: 54:28 - loss: 2.9986 - regression_loss: 2.1948 - classification_loss 3350/10000 [=========>....................] - ETA: 54:28 - loss: 2.9985 - regression_loss: 2.1948 - classification_loss 3351/10000 [=========>....................] - ETA: 54:27 - loss: 2.9985 - regression_loss: 2.1948 - classification_loss 3352/10000 [=========>....................] - ETA: 54:26 - loss: 2.9983 - regression_loss: 2.1947 - classification_loss 3353/10000 [=========>....................] - ETA: 54:26 - loss: 2.9981 - regression_loss: 2.1945 - classification_loss 3354/10000 [=========>....................] - ETA: 54:25 - loss: 2.9976 - regression_loss: 2.1941 - classification_loss 3355/10000 [=========>....................] - ETA: 54:25 - loss: 2.9974 - regression_loss: 2.1939 - classification_loss 3356/10000 [=========>....................] - ETA: 54:24 - loss: 2.9970 - regression_loss: 2.1936 - classification_loss 3357/10000 [=========>....................] - ETA: 54:24 - loss: 2.9970 - regression_loss: 2.1937 - classification_loss 3358/10000 [=========>....................] - ETA: 54:23 - loss: 2.9968 - regression_loss: 2.1935 - classification_loss 3359/10000 [=========>....................] - ETA: 54:22 - loss: 2.9968 - regression_loss: 2.1935 - classification_loss 3360/10000 [=========>....................] - ETA: 54:22 - loss: 2.9965 - regression_loss: 2.1933 - classification_loss 3361/10000 [=========>....................] - ETA: 54:21 - loss: 2.9963 - regression_loss: 2.1933 - classification_loss 3362/10000 [=========>....................] - ETA: 54:21 - loss: 2.9961 - regression_loss: 2.1932 - classification_loss 3363/10000 [=========>....................] - ETA: 54:20 - loss: 2.9959 - regression_loss: 2.1930 - classification_loss 3364/10000 [=========>....................] - ETA: 54:20 - loss: 2.9959 - regression_loss: 2.1930 - classification_loss 3365/10000 [=========>....................] - ETA: 54:19 - loss: 2.9959 - regression_loss: 2.1929 - classification_loss 3366/10000 [=========>....................] - ETA: 54:19 - loss: 2.9956 - regression_loss: 2.1928 - classification_loss 3367/10000 [=========>....................] - ETA: 54:18 - loss: 2.9956 - regression_loss: 2.1929 - classification_loss 3368/10000 [=========>....................] - ETA: 54:17 - loss: 2.9956 - regression_loss: 2.1930 - classification_loss 3369/10000 [=========>....................] - ETA: 54:17 - loss: 2.9954 - regression_loss: 2.1929 - classification_loss 3370/10000 [=========>....................] - ETA: 54:16 - loss: 2.9960 - regression_loss: 2.1934 - classification_loss 3371/10000 [=========>....................] - ETA: 54:16 - loss: 2.9956 - regression_loss: 2.1931 - classification_loss 3372/10000 [=========>....................] - ETA: 54:15 - loss: 2.9952 - regression_loss: 2.1928 - classification_loss 3373/10000 [=========>....................] - ETA: 54:15 - loss: 2.9950 - regression_loss: 2.1926 - classification_loss 3374/10000 [=========>....................] - ETA: 54:14 - loss: 2.9949 - regression_loss: 2.1926 - classification_loss 3375/10000 [=========>....................] - ETA: 54:14 - loss: 2.9950 - regression_loss: 2.1926 - classification_loss 3376/10000 [=========>....................] - ETA: 54:13 - loss: 2.9948 - regression_loss: 2.1925 - classification_loss 3377/10000 [=========>....................] - ETA: 54:12 - loss: 2.9947 - regression_loss: 2.1925 - classification_loss 3378/10000 [=========>....................] - ETA: 54:12 - loss: 2.9949 - regression_loss: 2.1926 - classification_loss 3379/10000 [=========>....................] - ETA: 54:11 - loss: 2.9947 - regression_loss: 2.1924 - classification_loss 3380/10000 [=========>....................] - ETA: 54:10 - loss: 2.9943 - regression_loss: 2.1921 - classification_loss 3381/10000 [=========>....................] - ETA: 54:10 - loss: 2.9943 - regression_loss: 2.1921 - classification_loss 3382/10000 [=========>....................] - ETA: 54:09 - loss: 2.9941 - regression_loss: 2.1921 - classification_loss 3383/10000 [=========>....................] - ETA: 54:09 - loss: 2.9939 - regression_loss: 2.1920 - classification_loss 3384/10000 [=========>....................] - ETA: 54:08 - loss: 2.9937 - regression_loss: 2.1918 - classification_loss 3385/10000 [=========>....................] - ETA: 54:08 - loss: 2.9936 - regression_loss: 2.1917 - classification_loss 3386/10000 [=========>....................] - ETA: 54:07 - loss: 2.9933 - regression_loss: 2.1914 - classification_loss 3387/10000 [=========>....................] - ETA: 54:07 - loss: 2.9931 - regression_loss: 2.1913 - classification_loss 3388/10000 [=========>....................] - ETA: 54:06 - loss: 2.9928 - regression_loss: 2.1910 - classification_loss 3389/10000 [=========>....................] - ETA: 54:06 - loss: 2.9927 - regression_loss: 2.1911 - classification_loss 3390/10000 [=========>....................] - ETA: 54:05 - loss: 2.9925 - regression_loss: 2.1909 - classification_loss 3391/10000 [=========>....................] - ETA: 54:04 - loss: 2.9923 - regression_loss: 2.1907 - classification_loss 3392/10000 [=========>....................] - ETA: 54:04 - loss: 2.9922 - regression_loss: 2.1907 - classification_loss 3393/10000 [=========>....................] - ETA: 54:03 - loss: 2.9918 - regression_loss: 2.1903 - classification_loss 3394/10000 [=========>....................] - ETA: 54:03 - loss: 2.9916 - regression_loss: 2.1902 - classification_loss 3395/10000 [=========>....................] - ETA: 54:02 - loss: 2.9911 - regression_loss: 2.1899 - classification_loss 3396/10000 [=========>....................] - ETA: 54:02 - loss: 2.9910 - regression_loss: 2.1898 - classification_loss 3397/10000 [=========>....................] - ETA: 54:01 - loss: 2.9910 - regression_loss: 2.1899 - classification_loss 3398/10000 [=========>....................] - ETA: 54:01 - loss: 2.9908 - regression_loss: 2.1897 - classification_loss 3399/10000 [=========>....................] - ETA: 54:00 - loss: 2.9914 - regression_loss: 2.1902 - classification_loss 3400/10000 [=========>....................] - ETA: 54:00 - loss: 2.9914 - regression_loss: 2.1901 - classification_loss 3401/10000 [=========>....................] - ETA: 53:59 - loss: 2.9911 - regression_loss: 2.1899 - classification_loss 3402/10000 [=========>....................] - ETA: 53:59 - loss: 2.9909 - regression_loss: 2.1897 - classification_loss 3403/10000 [=========>....................] - ETA: 53:58 - loss: 2.9906 - regression_loss: 2.1895 - classification_loss 3404/10000 [=========>....................] - ETA: 53:59 - loss: 2.9906 - regression_loss: 2.1895 - classification_loss 3405/10000 [=========>....................] - ETA: 53:58 - loss: 2.9904 - regression_loss: 2.1894 - classification_loss 3406/10000 [=========>....................] - ETA: 53:58 - loss: 2.9905 - regression_loss: 2.1895 - classification_loss 3407/10000 [=========>....................] - ETA: 53:57 - loss: 2.9903 - regression_loss: 2.1894 - classification_loss 3408/10000 [=========>....................] - ETA: 53:57 - loss: 2.9900 - regression_loss: 2.1892 - classification_loss 3409/10000 [=========>....................] - ETA: 53:56 - loss: 2.9899 - regression_loss: 2.1892 - classification_loss 3410/10000 [=========>....................] - ETA: 53:55 - loss: 2.9898 - regression_loss: 2.1891 - classification_loss 3411/10000 [=========>....................] - ETA: 53:55 - loss: 2.9896 - regression_loss: 2.1890 - classification_loss 3412/10000 [=========>....................] - ETA: 53:54 - loss: 2.9894 - regression_loss: 2.1888 - classification_loss 3413/10000 [=========>....................] - ETA: 53:54 - loss: 2.9890 - regression_loss: 2.1884 - classification_loss 3414/10000 [=========>....................] - ETA: 53:53 - loss: 2.9890 - regression_loss: 2.1885 - classification_loss 3415/10000 [=========>....................] - ETA: 53:53 - loss: 2.9888 - regression_loss: 2.1883 - classification_loss 3416/10000 [=========>....................] - ETA: 53:52 - loss: 2.9885 - regression_loss: 2.1881 - classification_loss 3417/10000 [=========>....................] - ETA: 53:51 - loss: 2.9884 - regression_loss: 2.1881 - classification_loss 3418/10000 [=========>....................] - ETA: 53:51 - loss: 2.9884 - regression_loss: 2.1881 - classification_loss 3419/10000 [=========>....................] - ETA: 53:50 - loss: 2.9882 - regression_loss: 2.1881 - classification_loss 3420/10000 [=========>....................] - ETA: 53:50 - loss: 2.9882 - regression_loss: 2.1881 - classification_loss 3421/10000 [=========>....................] - ETA: 53:49 - loss: 2.9878 - regression_loss: 2.1877 - classification_loss 3422/10000 [=========>....................] - ETA: 53:49 - loss: 2.9880 - regression_loss: 2.1880 - classification_loss 3423/10000 [=========>....................] - ETA: 53:48 - loss: 2.9877 - regression_loss: 2.1877 - classification_loss 3424/10000 [=========>....................] - ETA: 53:48 - loss: 2.9875 - regression_loss: 2.1875 - classification_loss 3425/10000 [=========>....................] - ETA: 53:47 - loss: 2.9872 - regression_loss: 2.1873 - classification_loss 3426/10000 [=========>....................] - ETA: 53:47 - loss: 2.9870 - regression_loss: 2.1872 - classification_loss 3427/10000 [=========>....................] - ETA: 53:46 - loss: 2.9870 - regression_loss: 2.1872 - classification_loss 3428/10000 [=========>....................] - ETA: 53:46 - loss: 2.9867 - regression_loss: 2.1870 - classification_loss 3429/10000 [=========>....................] - ETA: 53:45 - loss: 2.9866 - regression_loss: 2.1869 - classification_loss 3430/10000 [=========>....................] - ETA: 53:44 - loss: 2.9865 - regression_loss: 2.1868 - classification_loss 3431/10000 [=========>....................] - ETA: 53:44 - loss: 2.9864 - regression_loss: 2.1868 - classification_loss 3432/10000 [=========>....................] - ETA: 53:43 - loss: 2.9862 - regression_loss: 2.1868 - classification_loss 3433/10000 [=========>....................] - ETA: 53:43 - loss: 2.9861 - regression_loss: 2.1868 - classification_loss 3434/10000 [=========>....................] - ETA: 53:42 - loss: 2.9859 - regression_loss: 2.1866 - classification_loss 3435/10000 [=========>....................] - ETA: 53:42 - loss: 2.9856 - regression_loss: 2.1864 - classification_loss 3436/10000 [=========>....................] - ETA: 53:41 - loss: 2.9857 - regression_loss: 2.1865 - classification_loss 3437/10000 [=========>....................] - ETA: 53:40 - loss: 2.9854 - regression_loss: 2.1864 - classification_loss 3438/10000 [=========>....................] - ETA: 53:41 - loss: 2.9856 - regression_loss: 2.1865 - classification_loss 3439/10000 [=========>....................] - ETA: 53:40 - loss: 2.9857 - regression_loss: 2.1865 - classification_loss 3440/10000 [=========>....................] - ETA: 53:39 - loss: 2.9856 - regression_loss: 2.1865 - classification_loss 3441/10000 [=========>....................] - ETA: 53:39 - loss: 2.9855 - regression_loss: 2.1864 - classification_loss 3442/10000 [=========>....................] - ETA: 53:38 - loss: 2.9853 - regression_loss: 2.1863 - classification_loss 3443/10000 [=========>....................] - ETA: 53:38 - loss: 2.9853 - regression_loss: 2.1862 - classification_loss 3444/10000 [=========>....................] - ETA: 53:37 - loss: 2.9852 - regression_loss: 2.1862 - classification_loss 3445/10000 [=========>....................] - ETA: 53:37 - loss: 2.9848 - regression_loss: 2.1859 - classification_loss 3446/10000 [=========>....................] - ETA: 53:36 - loss: 2.9848 - regression_loss: 2.1860 - classification_loss 3447/10000 [=========>....................] - ETA: 53:36 - loss: 2.9846 - regression_loss: 2.1858 - classification_loss 3448/10000 [=========>....................] - ETA: 53:35 - loss: 2.9845 - regression_loss: 2.1858 - classification_loss 3449/10000 [=========>....................] - ETA: 53:35 - loss: 2.9847 - regression_loss: 2.1859 - classification_loss 3450/10000 [=========>....................] - ETA: 53:34 - loss: 2.9846 - regression_loss: 2.1858 - classification_loss 3451/10000 [=========>....................] - ETA: 53:34 - loss: 2.9847 - regression_loss: 2.1859 - classification_loss 3452/10000 [=========>....................] - ETA: 53:33 - loss: 2.9844 - regression_loss: 2.1857 - classification_loss 3453/10000 [=========>....................] - ETA: 53:32 - loss: 2.9842 - regression_loss: 2.1856 - classification_loss 3454/10000 [=========>....................] - ETA: 53:32 - loss: 2.9841 - regression_loss: 2.1856 - classification_loss 3455/10000 [=========>....................] - ETA: 53:31 - loss: 2.9838 - regression_loss: 2.1854 - classification_loss 3456/10000 [=========>....................] - ETA: 53:31 - loss: 2.9837 - regression_loss: 2.1853 - classification_loss 3457/10000 [=========>....................] - ETA: 53:30 - loss: 2.9835 - regression_loss: 2.1852 - classification_loss 3458/10000 [=========>....................] - ETA: 53:30 - loss: 2.9837 - regression_loss: 2.1853 - classification_loss 3459/10000 [=========>....................] - ETA: 53:29 - loss: 2.9836 - regression_loss: 2.1853 - classification_loss 3460/10000 [=========>....................] - ETA: 53:29 - loss: 2.9836 - regression_loss: 2.1853 - classification_loss 3461/10000 [=========>....................] - ETA: 53:28 - loss: 2.9835 - regression_loss: 2.1853 - classification_loss 3462/10000 [=========>....................] - ETA: 53:28 - loss: 2.9834 - regression_loss: 2.1853 - classification_loss 3463/10000 [=========>....................] - ETA: 53:27 - loss: 2.9832 - regression_loss: 2.1853 - classification_loss 3464/10000 [=========>....................] - ETA: 53:27 - loss: 2.9828 - regression_loss: 2.1849 - classification_loss 3465/10000 [=========>....................] - ETA: 53:26 - loss: 2.9827 - regression_loss: 2.1848 - classification_loss 3466/10000 [=========>....................] - ETA: 53:25 - loss: 2.9827 - regression_loss: 2.1848 - classification_loss 3467/10000 [=========>....................] - ETA: 53:25 - loss: 2.9825 - regression_loss: 2.1847 - classification_loss 3468/10000 [=========>....................] - ETA: 53:24 - loss: 2.9823 - regression_loss: 2.1846 - classification_loss 3469/10000 [=========>....................] - ETA: 53:24 - loss: 2.9821 - regression_loss: 2.1845 - classification_loss 3470/10000 [=========>....................] - ETA: 53:23 - loss: 2.9820 - regression_loss: 2.1845 - classification_loss 3471/10000 [=========>....................] - ETA: 53:23 - loss: 2.9819 - regression_loss: 2.1844 - classification_loss 3472/10000 [=========>....................] - ETA: 53:22 - loss: 2.9818 - regression_loss: 2.1844 - classification_loss 3473/10000 [=========>....................] - ETA: 53:22 - loss: 2.9815 - regression_loss: 2.1842 - classification_loss 3474/10000 [=========>....................] - ETA: 53:21 - loss: 2.9814 - regression_loss: 2.1842 - classification_loss 3475/10000 [=========>....................] - ETA: 53:21 - loss: 2.9814 - regression_loss: 2.1842 - classification_loss 3476/10000 [=========>....................] - ETA: 53:20 - loss: 2.9812 - regression_loss: 2.1841 - classification_loss 3477/10000 [=========>....................] - ETA: 53:20 - loss: 2.9809 - regression_loss: 2.1839 - classification_loss 3478/10000 [=========>....................] - ETA: 53:19 - loss: 2.9812 - regression_loss: 2.1840 - classification_loss 3479/10000 [=========>....................] - ETA: 53:19 - loss: 2.9811 - regression_loss: 2.1840 - classification_loss 3480/10000 [=========>....................] - ETA: 53:18 - loss: 2.9808 - regression_loss: 2.1837 - classification_loss 3481/10000 [=========>....................] - ETA: 53:18 - loss: 2.9807 - regression_loss: 2.1837 - classification_loss 3482/10000 [=========>....................] - ETA: 53:17 - loss: 2.9804 - regression_loss: 2.1833 - classification_loss 3483/10000 [=========>....................] - ETA: 53:17 - loss: 2.9803 - regression_loss: 2.1834 - classification_loss 3484/10000 [=========>....................] - ETA: 53:16 - loss: 2.9801 - regression_loss: 2.1832 - classification_loss 3485/10000 [=========>....................] - ETA: 53:15 - loss: 2.9799 - regression_loss: 2.1830 - classification_loss 3486/10000 [=========>....................] - ETA: 53:15 - loss: 2.9801 - regression_loss: 2.1832 - classification_loss 3487/10000 [=========>....................] - ETA: 53:14 - loss: 2.9800 - regression_loss: 2.1831 - classification_loss 3488/10000 [=========>....................] - ETA: 53:14 - loss: 2.9798 - regression_loss: 2.1830 - classification_loss 3489/10000 [=========>....................] - ETA: 53:13 - loss: 2.9797 - regression_loss: 2.1830 - classification_loss 3490/10000 [=========>....................] - ETA: 53:13 - loss: 2.9796 - regression_loss: 2.1829 - classification_loss 3491/10000 [=========>....................] - ETA: 53:12 - loss: 2.9794 - regression_loss: 2.1828 - classification_loss 3492/10000 [=========>....................] - ETA: 53:12 - loss: 2.9793 - regression_loss: 2.1828 - classification_loss 3493/10000 [=========>....................] - ETA: 53:11 - loss: 2.9791 - regression_loss: 2.1826 - classification_loss 3494/10000 [=========>....................] - ETA: 53:10 - loss: 2.9789 - regression_loss: 2.1825 - classification_loss 3495/10000 [=========>....................] - ETA: 53:10 - loss: 2.9789 - regression_loss: 2.1825 - classification_loss 3496/10000 [=========>....................] - ETA: 53:09 - loss: 2.9784 - regression_loss: 2.1821 - classification_loss 3497/10000 [=========>....................] - ETA: 53:09 - loss: 2.9783 - regression_loss: 2.1821 - classification_loss 3498/10000 [=========>....................] - ETA: 53:08 - loss: 2.9783 - regression_loss: 2.1821 - classification_loss 3499/10000 [=========>....................] - ETA: 53:08 - loss: 2.9780 - regression_loss: 2.1819 - classification_loss 3500/10000 [=========>....................] - ETA: 53:07 - loss: 2.9779 - regression_loss: 2.1819 - classification_loss 3501/10000 [=========>....................] - ETA: 53:07 - loss: 2.9777 - regression_loss: 2.1817 - classification_loss 3502/10000 [=========>....................] - ETA: 53:06 - loss: 2.9777 - regression_loss: 2.1818 - classification_loss 3503/10000 [=========>....................] - ETA: 53:06 - loss: 2.9773 - regression_loss: 2.1816 - classification_loss 3504/10000 [=========>....................] - ETA: 53:05 - loss: 2.9773 - regression_loss: 2.1816 - classification_loss 3505/10000 [=========>....................] - ETA: 53:05 - loss: 2.9769 - regression_loss: 2.1813 - classification_loss 3506/10000 [=========>....................] - ETA: 53:04 - loss: 2.9766 - regression_loss: 2.1810 - classification_loss 3507/10000 [=========>....................] - ETA: 53:03 - loss: 2.9765 - regression_loss: 2.1810 - classification_loss 3508/10000 [=========>....................] - ETA: 53:03 - loss: 2.9764 - regression_loss: 2.1809 - classification_loss 3509/10000 [=========>....................] - ETA: 53:02 - loss: 2.9763 - regression_loss: 2.1809 - classification_loss 3510/10000 [=========>....................] - ETA: 53:02 - loss: 2.9763 - regression_loss: 2.1809 - classification_loss 3511/10000 [=========>....................] - ETA: 53:01 - loss: 2.9761 - regression_loss: 2.1808 - classification_loss 3512/10000 [=========>....................] - ETA: 53:01 - loss: 2.9760 - regression_loss: 2.1807 - classification_loss 3513/10000 [=========>....................] - ETA: 53:00 - loss: 2.9758 - regression_loss: 2.1807 - classification_loss 3514/10000 [=========>....................] - ETA: 53:00 - loss: 2.9756 - regression_loss: 2.1806 - classification_loss 3515/10000 [=========>....................] - ETA: 52:59 - loss: 2.9755 - regression_loss: 2.1805 - classification_loss 3516/10000 [=========>....................] - ETA: 52:59 - loss: 2.9754 - regression_loss: 2.1804 - classification_loss 3517/10000 [=========>....................] - ETA: 52:58 - loss: 2.9753 - regression_loss: 2.1804 - classification_loss 3518/10000 [=========>....................] - ETA: 52:58 - loss: 2.9751 - regression_loss: 2.1802 - classification_loss 3519/10000 [=========>....................] - ETA: 52:57 - loss: 2.9753 - regression_loss: 2.1803 - classification_loss 3520/10000 [=========>....................] - ETA: 52:56 - loss: 2.9751 - regression_loss: 2.1802 - classification_loss 3521/10000 [=========>....................] - ETA: 52:56 - loss: 2.9750 - regression_loss: 2.1802 - classification_loss 3522/10000 [=========>....................] - ETA: 52:55 - loss: 2.9750 - regression_loss: 2.1802 - classification_loss 3523/10000 [=========>....................] - ETA: 52:55 - loss: 2.9750 - regression_loss: 2.1802 - classification_loss 3524/10000 [=========>....................] - ETA: 52:54 - loss: 2.9751 - regression_loss: 2.1803 - classification_loss 3525/10000 [=========>....................] - ETA: 52:54 - loss: 2.9751 - regression_loss: 2.1802 - classification_loss 3526/10000 [=========>....................] - ETA: 52:53 - loss: 2.9749 - regression_loss: 2.1800 - classification_loss 3527/10000 [=========>....................] - ETA: 52:52 - loss: 2.9749 - regression_loss: 2.1801 - classification_loss 3528/10000 [=========>....................] - ETA: 52:52 - loss: 2.9746 - regression_loss: 2.1799 - classification_loss 3529/10000 [=========>....................] - ETA: 52:51 - loss: 2.9745 - regression_loss: 2.1798 - classification_loss 3530/10000 [=========>....................] - ETA: 52:51 - loss: 2.9743 - regression_loss: 2.1797 - classification_loss 3531/10000 [=========>....................] - ETA: 52:50 - loss: 2.9747 - regression_loss: 2.1801 - classification_loss 3532/10000 [=========>....................] - ETA: 52:50 - loss: 2.9742 - regression_loss: 2.1797 - classification_loss 3533/10000 [=========>....................] - ETA: 52:49 - loss: 2.9739 - regression_loss: 2.1795 - classification_loss 3534/10000 [=========>....................] - ETA: 52:49 - loss: 2.9737 - regression_loss: 2.1794 - classification_loss 3535/10000 [=========>....................] - ETA: 52:48 - loss: 2.9736 - regression_loss: 2.1793 - classification_loss 3536/10000 [=========>....................] - ETA: 52:48 - loss: 2.9735 - regression_loss: 2.1793 - classification_loss 3537/10000 [=========>....................] - ETA: 52:47 - loss: 2.9734 - regression_loss: 2.1793 - classification_loss 3538/10000 [=========>....................] - ETA: 52:46 - loss: 2.9730 - regression_loss: 2.1789 - classification_loss 3539/10000 [=========>....................] - ETA: 52:46 - loss: 2.9729 - regression_loss: 2.1789 - classification_loss 3540/10000 [=========>....................] - ETA: 52:45 - loss: 2.9727 - regression_loss: 2.1788 - classification_loss 3541/10000 [=========>....................] - ETA: 52:45 - loss: 2.9724 - regression_loss: 2.1786 - classification_loss 3542/10000 [=========>....................] - ETA: 52:44 - loss: 2.9723 - regression_loss: 2.1786 - classification_loss 3543/10000 [=========>....................] - ETA: 52:44 - loss: 2.9722 - regression_loss: 2.1786 - classification_loss 3544/10000 [=========>....................] - ETA: 52:43 - loss: 2.9721 - regression_loss: 2.1785 - classification_loss 3545/10000 [=========>....................] - ETA: 52:43 - loss: 2.9720 - regression_loss: 2.1784 - classification_loss 3546/10000 [=========>....................] - ETA: 52:42 - loss: 2.9716 - regression_loss: 2.1781 - classification_loss 3547/10000 [=========>....................] - ETA: 52:42 - loss: 2.9714 - regression_loss: 2.1779 - classification_loss 3548/10000 [=========>....................] - ETA: 52:41 - loss: 2.9715 - regression_loss: 2.1780 - classification_loss 3549/10000 [=========>....................] - ETA: 52:41 - loss: 2.9712 - regression_loss: 2.1778 - classification_loss 3550/10000 [=========>....................] - ETA: 52:40 - loss: 2.9710 - regression_loss: 2.1777 - classification_loss 3551/10000 [=========>....................] - ETA: 52:40 - loss: 2.9708 - regression_loss: 2.1776 - classification_loss 3552/10000 [=========>....................] - ETA: 52:39 - loss: 2.9705 - regression_loss: 2.1773 - classification_loss 3553/10000 [=========>....................] - ETA: 52:39 - loss: 2.9701 - regression_loss: 2.1770 - classification_loss 3554/10000 [=========>....................] - ETA: 52:38 - loss: 2.9698 - regression_loss: 2.1768 - classification_loss 3555/10000 [=========>....................] - ETA: 52:38 - loss: 2.9698 - regression_loss: 2.1768 - classification_loss 3556/10000 [=========>....................] - ETA: 52:37 - loss: 2.9696 - regression_loss: 2.1767 - classification_loss 3557/10000 [=========>....................] - ETA: 52:37 - loss: 2.9693 - regression_loss: 2.1765 - classification_loss 3558/10000 [=========>....................] - ETA: 52:36 - loss: 2.9693 - regression_loss: 2.1766 - classification_loss 3559/10000 [=========>....................] - ETA: 52:35 - loss: 2.9695 - regression_loss: 2.1767 - classification_loss 3560/10000 [=========>....................] - ETA: 52:35 - loss: 2.9690 - regression_loss: 2.1763 - classification_loss 3561/10000 [=========>....................] - ETA: 52:34 - loss: 2.9687 - regression_loss: 2.1761 - classification_loss 3562/10000 [=========>....................] - ETA: 52:34 - loss: 2.9689 - regression_loss: 2.1761 - classification_loss 3563/10000 [=========>....................] - ETA: 52:33 - loss: 2.9686 - regression_loss: 2.1759 - classification_loss 3564/10000 [=========>....................] - ETA: 52:33 - loss: 2.9685 - regression_loss: 2.1759 - classification_loss 3565/10000 [=========>....................] - ETA: 52:32 - loss: 2.9684 - regression_loss: 2.1758 - classification_loss 3566/10000 [=========>....................] - ETA: 52:31 - loss: 2.9682 - regression_loss: 2.1757 - classification_loss 3567/10000 [=========>....................] - ETA: 52:31 - loss: 2.9681 - regression_loss: 2.1756 - classification_loss 3568/10000 [=========>....................] - ETA: 52:30 - loss: 2.9679 - regression_loss: 2.1755 - classification_loss 3569/10000 [=========>....................] - ETA: 52:30 - loss: 2.9681 - regression_loss: 2.1756 - classification_loss 3570/10000 [=========>....................] - ETA: 52:29 - loss: 2.9678 - regression_loss: 2.1754 - classification_loss 3571/10000 [=========>....................] - ETA: 52:29 - loss: 2.9678 - regression_loss: 2.1753 - classification_loss 3572/10000 [=========>....................] - ETA: 52:28 - loss: 2.9676 - regression_loss: 2.1752 - classification_loss 3573/10000 [=========>....................] - ETA: 52:28 - loss: 2.9675 - regression_loss: 2.1751 - classification_loss 3574/10000 [=========>....................] - ETA: 52:27 - loss: 2.9673 - regression_loss: 2.1750 - classification_loss 3575/10000 [=========>....................] - ETA: 52:27 - loss: 2.9668 - regression_loss: 2.1746 - classification_loss 3576/10000 [=========>....................] - ETA: 52:26 - loss: 2.9666 - regression_loss: 2.1745 - classification_loss 3577/10000 [=========>....................] - ETA: 52:26 - loss: 2.9663 - regression_loss: 2.1742 - classification_loss 3578/10000 [=========>....................] - ETA: 52:25 - loss: 2.9661 - regression_loss: 2.1741 - classification_loss 3579/10000 [=========>....................] - ETA: 52:25 - loss: 2.9659 - regression_loss: 2.1739 - classification_loss 3580/10000 [=========>....................] - ETA: 52:24 - loss: 2.9658 - regression_loss: 2.1738 - classification_loss 3581/10000 [=========>....................] - ETA: 52:23 - loss: 2.9655 - regression_loss: 2.1737 - classification_loss 3582/10000 [=========>....................] - ETA: 52:23 - loss: 2.9656 - regression_loss: 2.1737 - classification_loss 3583/10000 [=========>....................] - ETA: 52:22 - loss: 2.9652 - regression_loss: 2.1733 - classification_loss 3584/10000 [=========>....................] - ETA: 52:22 - loss: 2.9650 - regression_loss: 2.1732 - classification_loss 3585/10000 [=========>....................] - ETA: 52:21 - loss: 2.9650 - regression_loss: 2.1733 - classification_loss 3586/10000 [=========>....................] - ETA: 52:21 - loss: 2.9649 - regression_loss: 2.1732 - classification_loss 3587/10000 [=========>....................] - ETA: 52:20 - loss: 2.9647 - regression_loss: 2.1731 - classification_loss 3588/10000 [=========>....................] - ETA: 52:20 - loss: 2.9646 - regression_loss: 2.1730 - classification_loss 3589/10000 [=========>....................] - ETA: 52:19 - loss: 2.9644 - regression_loss: 2.1730 - classification_loss 3590/10000 [=========>....................] - ETA: 52:19 - loss: 2.9643 - regression_loss: 2.1729 - classification_loss 3591/10000 [=========>....................] - ETA: 52:18 - loss: 2.9641 - regression_loss: 2.1727 - classification_loss 3592/10000 [=========>....................] - ETA: 52:18 - loss: 2.9639 - regression_loss: 2.1726 - classification_loss 3593/10000 [=========>....................] - ETA: 52:17 - loss: 2.9635 - regression_loss: 2.1723 - classification_loss 3594/10000 [=========>....................] - ETA: 52:17 - loss: 2.9632 - regression_loss: 2.1720 - classification_loss 3595/10000 [=========>....................] - ETA: 52:16 - loss: 2.9629 - regression_loss: 2.1717 - classification_loss 3596/10000 [=========>....................] - ETA: 52:15 - loss: 2.9627 - regression_loss: 2.1715 - classification_loss 3597/10000 [=========>....................] - ETA: 52:15 - loss: 2.9625 - regression_loss: 2.1714 - classification_loss 3598/10000 [=========>....................] - ETA: 52:14 - loss: 2.9621 - regression_loss: 2.1711 - classification_loss 3599/10000 [=========>....................] - ETA: 52:14 - loss: 2.9620 - regression_loss: 2.1710 - classification_loss 3600/10000 [=========>....................] - ETA: 52:13 - loss: 2.9616 - regression_loss: 2.1708 - classification_loss 3601/10000 [=========>....................] - ETA: 52:13 - loss: 2.9616 - regression_loss: 2.1708 - classification_loss 3602/10000 [=========>....................] - ETA: 52:12 - loss: 2.9616 - regression_loss: 2.1708 - classification_loss 3603/10000 [=========>....................] - ETA: 52:12 - loss: 2.9616 - regression_loss: 2.1709 - classification_loss 3604/10000 [=========>....................] - ETA: 52:11 - loss: 2.9615 - regression_loss: 2.1709 - classification_loss 3605/10000 [=========>....................] - ETA: 52:11 - loss: 2.9617 - regression_loss: 2.1710 - classification_loss 3606/10000 [=========>....................] - ETA: 52:10 - loss: 2.9613 - regression_loss: 2.1707 - classification_loss 3607/10000 [=========>....................] - ETA: 52:10 - loss: 2.9615 - regression_loss: 2.1708 - classification_loss 3608/10000 [=========>....................] - ETA: 52:09 - loss: 2.9612 - regression_loss: 2.1706 - classification_loss 3609/10000 [=========>....................] - ETA: 52:09 - loss: 2.9608 - regression_loss: 2.1702 - classification_loss 3610/10000 [=========>....................] - ETA: 52:08 - loss: 2.9607 - regression_loss: 2.1702 - classification_loss 3611/10000 [=========>....................] - ETA: 52:08 - loss: 2.9607 - regression_loss: 2.1701 - classification_loss 3612/10000 [=========>....................] - ETA: 52:07 - loss: 2.9607 - regression_loss: 2.1701 - classification_loss 3613/10000 [=========>....................] - ETA: 52:06 - loss: 2.9604 - regression_loss: 2.1699 - classification_loss 3614/10000 [=========>....................] - ETA: 52:06 - loss: 2.9604 - regression_loss: 2.1699 - classification_loss 3615/10000 [=========>....................] - ETA: 52:06 - loss: 2.9602 - regression_loss: 2.1699 - classification_loss 3616/10000 [=========>....................] - ETA: 52:05 - loss: 2.9600 - regression_loss: 2.1697 - classification_loss 3617/10000 [=========>....................] - ETA: 52:04 - loss: 2.9597 - regression_loss: 2.1695 - classification_loss 3618/10000 [=========>....................] - ETA: 52:04 - loss: 2.9596 - regression_loss: 2.1694 - classification_loss 3619/10000 [=========>....................] - ETA: 52:03 - loss: 2.9594 - regression_loss: 2.1692 - classification_loss 3620/10000 [=========>....................] - ETA: 52:03 - loss: 2.9591 - regression_loss: 2.1689 - classification_loss 3621/10000 [=========>....................] - ETA: 52:02 - loss: 2.9587 - regression_loss: 2.1687 - classification_loss 3622/10000 [=========>....................] - ETA: 52:02 - loss: 2.9585 - regression_loss: 2.1684 - classification_loss 3623/10000 [=========>....................] - ETA: 52:01 - loss: 2.9586 - regression_loss: 2.1685 - classification_loss 3624/10000 [=========>....................] - ETA: 52:01 - loss: 2.9585 - regression_loss: 2.1685 - classification_loss 3625/10000 [=========>....................] - ETA: 52:00 - loss: 2.9585 - regression_loss: 2.1685 - classification_loss 3626/10000 [=========>....................] - ETA: 51:59 - loss: 2.9583 - regression_loss: 2.1685 - classification_loss 3627/10000 [=========>....................] - ETA: 51:59 - loss: 2.9581 - regression_loss: 2.1683 - classification_loss 3628/10000 [=========>....................] - ETA: 51:58 - loss: 2.9580 - regression_loss: 2.1683 - classification_loss 3629/10000 [=========>....................] - ETA: 51:58 - loss: 2.9579 - regression_loss: 2.1682 - classification_loss 3630/10000 [=========>....................] - ETA: 51:57 - loss: 2.9578 - regression_loss: 2.1682 - classification_loss 3631/10000 [=========>....................] - ETA: 51:57 - loss: 2.9573 - regression_loss: 2.1679 - classification_loss 3632/10000 [=========>....................] - ETA: 51:56 - loss: 2.9571 - regression_loss: 2.1677 - classification_loss 3633/10000 [=========>....................] - ETA: 51:56 - loss: 2.9568 - regression_loss: 2.1675 - classification_loss 3634/10000 [=========>....................] - ETA: 51:55 - loss: 2.9567 - regression_loss: 2.1675 - classification_loss 3635/10000 [=========>....................] - ETA: 51:55 - loss: 2.9566 - regression_loss: 2.1674 - classification_loss 3636/10000 [=========>....................] - ETA: 51:54 - loss: 2.9567 - regression_loss: 2.1674 - classification_loss 3637/10000 [=========>....................] - ETA: 51:53 - loss: 2.9566 - regression_loss: 2.1674 - classification_loss 3638/10000 [=========>....................] - ETA: 51:53 - loss: 2.9565 - regression_loss: 2.1674 - classification_loss 3639/10000 [=========>....................] - ETA: 51:52 - loss: 2.9563 - regression_loss: 2.1673 - classification_loss 3640/10000 [=========>....................] - ETA: 51:52 - loss: 2.9561 - regression_loss: 2.1670 - classification_loss 3641/10000 [=========>....................] - ETA: 51:51 - loss: 2.9562 - regression_loss: 2.1671 - classification_loss 3642/10000 [=========>....................] - ETA: 51:51 - loss: 2.9561 - regression_loss: 2.1672 - classification_loss 3643/10000 [=========>....................] - ETA: 51:50 - loss: 2.9559 - regression_loss: 2.1669 - classification_loss 3644/10000 [=========>....................] - ETA: 51:50 - loss: 2.9556 - regression_loss: 2.1667 - classification_loss 3645/10000 [=========>....................] - ETA: 51:49 - loss: 2.9553 - regression_loss: 2.1665 - classification_loss 3646/10000 [=========>....................] - ETA: 51:49 - loss: 2.9549 - regression_loss: 2.1662 - classification_loss 3647/10000 [=========>....................] - ETA: 51:48 - loss: 2.9546 - regression_loss: 2.1660 - classification_loss 3648/10000 [=========>....................] - ETA: 51:47 - loss: 2.9545 - regression_loss: 2.1660 - classification_loss 3649/10000 [=========>....................] - ETA: 51:47 - loss: 2.9542 - regression_loss: 2.1658 - classification_loss 3650/10000 [=========>....................] - ETA: 51:46 - loss: 2.9541 - regression_loss: 2.1658 - classification_loss 3651/10000 [=========>....................] - ETA: 51:46 - loss: 2.9537 - regression_loss: 2.1654 - classification_loss 3652/10000 [=========>....................] - ETA: 51:45 - loss: 2.9534 - regression_loss: 2.1652 - classification_loss 3653/10000 [=========>....................] - ETA: 51:45 - loss: 2.9532 - regression_loss: 2.1651 - classification_loss 3654/10000 [=========>....................] - ETA: 51:44 - loss: 2.9530 - regression_loss: 2.1649 - classification_loss 3655/10000 [=========>....................] - ETA: 51:44 - loss: 2.9531 - regression_loss: 2.1651 - classification_loss 3656/10000 [=========>....................] - ETA: 51:43 - loss: 2.9531 - regression_loss: 2.1651 - classification_loss 3657/10000 [=========>....................] - ETA: 51:43 - loss: 2.9526 - regression_loss: 2.1648 - classification_loss 3658/10000 [=========>....................] - ETA: 51:42 - loss: 2.9525 - regression_loss: 2.1647 - classification_loss 3659/10000 [=========>....................] - ETA: 51:42 - loss: 2.9522 - regression_loss: 2.1645 - classification_loss 3660/10000 [=========>....................] - ETA: 51:41 - loss: 2.9521 - regression_loss: 2.1644 - classification_loss 3661/10000 [=========>....................] - ETA: 51:41 - loss: 2.9519 - regression_loss: 2.1642 - classification_loss 3662/10000 [=========>....................] - ETA: 51:40 - loss: 2.9517 - regression_loss: 2.1641 - classification_loss 3663/10000 [=========>....................] - ETA: 51:39 - loss: 2.9515 - regression_loss: 2.1640 - classification_loss 3664/10000 [=========>....................] - ETA: 51:39 - loss: 2.9513 - regression_loss: 2.1639 - classification_loss 3665/10000 [=========>....................] - ETA: 51:38 - loss: 2.9513 - regression_loss: 2.1639 - classification_loss 3666/10000 [=========>....................] - ETA: 51:38 - loss: 2.9512 - regression_loss: 2.1639 - classification_loss 3667/10000 [==========>...................] - ETA: 51:37 - loss: 2.9511 - regression_loss: 2.1639 - classification_loss 3668/10000 [==========>...................] - ETA: 51:37 - loss: 2.9508 - regression_loss: 2.1637 - classification_loss 3669/10000 [==========>...................] - ETA: 51:36 - loss: 2.9508 - regression_loss: 2.1637 - classification_loss 3670/10000 [==========>...................] - ETA: 51:36 - loss: 2.9513 - regression_loss: 2.1641 - classification_loss 3671/10000 [==========>...................] - ETA: 51:35 - loss: 2.9511 - regression_loss: 2.1640 - classification_loss 3672/10000 [==========>...................] - ETA: 51:35 - loss: 2.9509 - regression_loss: 2.1639 - classification_loss 3673/10000 [==========>...................] - ETA: 51:34 - loss: 2.9509 - regression_loss: 2.1638 - classification_loss 3674/10000 [==========>...................] - ETA: 51:34 - loss: 2.9509 - regression_loss: 2.1639 - classification_loss 3675/10000 [==========>...................] - ETA: 51:33 - loss: 2.9507 - regression_loss: 2.1638 - classification_loss 3676/10000 [==========>...................] - ETA: 51:33 - loss: 2.9507 - regression_loss: 2.1637 - classification_loss 3677/10000 [==========>...................] - ETA: 51:32 - loss: 2.9505 - regression_loss: 2.1637 - classification_loss 3678/10000 [==========>...................] - ETA: 51:31 - loss: 2.9504 - regression_loss: 2.1637 - classification_loss 3679/10000 [==========>...................] - ETA: 51:31 - loss: 2.9503 - regression_loss: 2.1637 - classification_loss 3680/10000 [==========>...................] - ETA: 51:30 - loss: 2.9502 - regression_loss: 2.1636 - classification_loss 3681/10000 [==========>...................] - ETA: 51:30 - loss: 2.9499 - regression_loss: 2.1633 - classification_loss 3682/10000 [==========>...................] - ETA: 51:29 - loss: 2.9496 - regression_loss: 2.1631 - classification_loss 3683/10000 [==========>...................] - ETA: 51:29 - loss: 2.9494 - regression_loss: 2.1630 - classification_loss 3684/10000 [==========>...................] - ETA: 51:28 - loss: 2.9493 - regression_loss: 2.1629 - classification_loss 3685/10000 [==========>...................] - ETA: 51:28 - loss: 2.9494 - regression_loss: 2.1630 - classification_loss 3686/10000 [==========>...................] - ETA: 51:27 - loss: 2.9490 - regression_loss: 2.1627 - classification_loss 3687/10000 [==========>...................] - ETA: 51:27 - loss: 2.9487 - regression_loss: 2.1625 - classification_loss 3688/10000 [==========>...................] - ETA: 51:26 - loss: 2.9487 - regression_loss: 2.1624 - classification_loss 3689/10000 [==========>...................] - ETA: 51:26 - loss: 2.9486 - regression_loss: 2.1624 - classification_loss 3690/10000 [==========>...................] - ETA: 51:25 - loss: 2.9484 - regression_loss: 2.1622 - classification_loss 3691/10000 [==========>...................] - ETA: 51:25 - loss: 2.9483 - regression_loss: 2.1622 - classification_loss 3692/10000 [==========>...................] - ETA: 51:24 - loss: 2.9483 - regression_loss: 2.1621 - classification_loss 3693/10000 [==========>...................] - ETA: 51:24 - loss: 2.9479 - regression_loss: 2.1619 - classification_loss 3694/10000 [==========>...................] - ETA: 51:23 - loss: 2.9477 - regression_loss: 2.1617 - classification_loss 3695/10000 [==========>...................] - ETA: 51:23 - loss: 2.9475 - regression_loss: 2.1616 - classification_loss 3696/10000 [==========>...................] - ETA: 51:22 - loss: 2.9474 - regression_loss: 2.1616 - classification_loss 3697/10000 [==========>...................] - ETA: 51:21 - loss: 2.9471 - regression_loss: 2.1613 - classification_loss 3698/10000 [==========>...................] - ETA: 51:21 - loss: 2.9468 - regression_loss: 2.1611 - classification_loss 3699/10000 [==========>...................] - ETA: 51:20 - loss: 2.9466 - regression_loss: 2.1609 - classification_loss 3700/10000 [==========>...................] - ETA: 51:20 - loss: 2.9464 - regression_loss: 2.1608 - classification_loss 3701/10000 [==========>...................] - ETA: 51:19 - loss: 2.9461 - regression_loss: 2.1605 - classification_loss 3702/10000 [==========>...................] - ETA: 51:19 - loss: 2.9460 - regression_loss: 2.1605 - classification_loss 3703/10000 [==========>...................] - ETA: 51:18 - loss: 2.9457 - regression_loss: 2.1603 - classification_loss 3704/10000 [==========>...................] - ETA: 51:18 - loss: 2.9454 - regression_loss: 2.1600 - classification_loss 3705/10000 [==========>...................] - ETA: 51:17 - loss: 2.9452 - regression_loss: 2.1599 - classification_loss 3706/10000 [==========>...................] - ETA: 51:17 - loss: 2.9450 - regression_loss: 2.1598 - classification_loss 3707/10000 [==========>...................] - ETA: 51:16 - loss: 2.9449 - regression_loss: 2.1597 - classification_loss 3708/10000 [==========>...................] - ETA: 51:16 - loss: 2.9445 - regression_loss: 2.1594 - classification_loss 3709/10000 [==========>...................] - ETA: 51:15 - loss: 2.9445 - regression_loss: 2.1594 - classification_loss 3710/10000 [==========>...................] - ETA: 51:15 - loss: 2.9441 - regression_loss: 2.1591 - classification_loss 3711/10000 [==========>...................] - ETA: 51:14 - loss: 2.9440 - regression_loss: 2.1590 - classification_loss 3712/10000 [==========>...................] - ETA: 51:14 - loss: 2.9440 - regression_loss: 2.1590 - classification_loss 3713/10000 [==========>...................] - ETA: 51:13 - loss: 2.9439 - regression_loss: 2.1588 - classification_loss 3714/10000 [==========>...................] - ETA: 51:12 - loss: 2.9438 - regression_loss: 2.1588 - classification_loss 3715/10000 [==========>...................] - ETA: 51:12 - loss: 2.9436 - regression_loss: 2.1587 - classification_loss 3716/10000 [==========>...................] - ETA: 51:11 - loss: 2.9436 - regression_loss: 2.1587 - classification_loss 3717/10000 [==========>...................] - ETA: 51:11 - loss: 2.9434 - regression_loss: 2.1585 - classification_loss 3718/10000 [==========>...................] - ETA: 51:10 - loss: 2.9436 - regression_loss: 2.1586 - classification_loss 3719/10000 [==========>...................] - ETA: 51:10 - loss: 2.9432 - regression_loss: 2.1584 - classification_loss 3720/10000 [==========>...................] - ETA: 51:09 - loss: 2.9430 - regression_loss: 2.1583 - classification_loss 3721/10000 [==========>...................] - ETA: 51:09 - loss: 2.9429 - regression_loss: 2.1583 - classification_loss 3722/10000 [==========>...................] - ETA: 51:08 - loss: 2.9425 - regression_loss: 2.1580 - classification_loss 3723/10000 [==========>...................] - ETA: 51:07 - loss: 2.9421 - regression_loss: 2.1577 - classification_loss 3724/10000 [==========>...................] - ETA: 51:07 - loss: 2.9418 - regression_loss: 2.1575 - classification_loss 3725/10000 [==========>...................] - ETA: 51:06 - loss: 2.9415 - regression_loss: 2.1573 - classification_loss 3726/10000 [==========>...................] - ETA: 51:06 - loss: 2.9412 - regression_loss: 2.1571 - classification_loss 3727/10000 [==========>...................] - ETA: 51:05 - loss: 2.9411 - regression_loss: 2.1571 - classification_loss 3728/10000 [==========>...................] - ETA: 51:05 - loss: 2.9409 - regression_loss: 2.1569 - classification_loss 3729/10000 [==========>...................] - ETA: 51:04 - loss: 2.9407 - regression_loss: 2.1568 - classification_loss 3730/10000 [==========>...................] - ETA: 51:04 - loss: 2.9407 - regression_loss: 2.1568 - classification_loss 3731/10000 [==========>...................] - ETA: 51:03 - loss: 2.9406 - regression_loss: 2.1567 - classification_loss 3732/10000 [==========>...................] - ETA: 51:03 - loss: 2.9404 - regression_loss: 2.1565 - classification_loss 3733/10000 [==========>...................] - ETA: 51:02 - loss: 2.9403 - regression_loss: 2.1564 - classification_loss 3734/10000 [==========>...................] - ETA: 51:02 - loss: 2.9401 - regression_loss: 2.1563 - classification_loss 3735/10000 [==========>...................] - ETA: 51:01 - loss: 2.9401 - regression_loss: 2.1563 - classification_loss 3736/10000 [==========>...................] - ETA: 51:00 - loss: 2.9400 - regression_loss: 2.1563 - classification_loss 3737/10000 [==========>...................] - ETA: 51:00 - loss: 2.9399 - regression_loss: 2.1563 - classification_loss 3738/10000 [==========>...................] - ETA: 50:59 - loss: 2.9397 - regression_loss: 2.1561 - classification_loss 3739/10000 [==========>...................] - ETA: 50:59 - loss: 2.9395 - regression_loss: 2.1559 - classification_loss 3740/10000 [==========>...................] - ETA: 50:58 - loss: 2.9396 - regression_loss: 2.1559 - classification_loss 3741/10000 [==========>...................] - ETA: 50:58 - loss: 2.9394 - regression_loss: 2.1559 - classification_loss 3742/10000 [==========>...................] - ETA: 50:57 - loss: 2.9396 - regression_loss: 2.1560 - classification_loss 3743/10000 [==========>...................] - ETA: 50:57 - loss: 2.9395 - regression_loss: 2.1561 - classification_loss 3744/10000 [==========>...................] - ETA: 50:56 - loss: 2.9394 - regression_loss: 2.1560 - classification_loss 3745/10000 [==========>...................] - ETA: 50:56 - loss: 2.9395 - regression_loss: 2.1561 - classification_loss 3746/10000 [==========>...................] - ETA: 50:55 - loss: 2.9393 - regression_loss: 2.1560 - classification_loss 3747/10000 [==========>...................] - ETA: 50:55 - loss: 2.9394 - regression_loss: 2.1560 - classification_loss 3748/10000 [==========>...................] - ETA: 50:54 - loss: 2.9391 - regression_loss: 2.1558 - classification_loss 3749/10000 [==========>...................] - ETA: 50:54 - loss: 2.9388 - regression_loss: 2.1556 - classification_loss 3750/10000 [==========>...................] - ETA: 50:53 - loss: 2.9389 - regression_loss: 2.1556 - classification_loss 3751/10000 [==========>...................] - ETA: 50:53 - loss: 2.9387 - regression_loss: 2.1556 - classification_loss 3752/10000 [==========>...................] - ETA: 50:52 - loss: 2.9388 - regression_loss: 2.1556 - classification_loss 3753/10000 [==========>...................] - ETA: 50:52 - loss: 2.9388 - regression_loss: 2.1556 - classification_loss 3754/10000 [==========>...................] - ETA: 50:51 - loss: 2.9385 - regression_loss: 2.1554 - classification_loss 3755/10000 [==========>...................] - ETA: 50:51 - loss: 2.9385 - regression_loss: 2.1555 - classification_loss 3756/10000 [==========>...................] - ETA: 50:50 - loss: 2.9381 - regression_loss: 2.1551 - classification_loss 3757/10000 [==========>...................] - ETA: 50:50 - loss: 2.9377 - regression_loss: 2.1549 - classification_loss 3758/10000 [==========>...................] - ETA: 50:49 - loss: 2.9376 - regression_loss: 2.1548 - classification_loss 3759/10000 [==========>...................] - ETA: 50:48 - loss: 2.9375 - regression_loss: 2.1547 - classification_loss 3760/10000 [==========>...................] - ETA: 50:48 - loss: 2.9374 - regression_loss: 2.1547 - classification_loss 3761/10000 [==========>...................] - ETA: 50:47 - loss: 2.9373 - regression_loss: 2.1546 - classification_loss 3762/10000 [==========>...................] - ETA: 50:47 - loss: 2.9370 - regression_loss: 2.1544 - classification_loss 3763/10000 [==========>...................] - ETA: 50:46 - loss: 2.9367 - regression_loss: 2.1541 - classification_loss 3764/10000 [==========>...................] - ETA: 50:46 - loss: 2.9367 - regression_loss: 2.1541 - classification_loss 3765/10000 [==========>...................] - ETA: 50:45 - loss: 2.9368 - regression_loss: 2.1542 - classification_loss 3766/10000 [==========>...................] - ETA: 50:45 - loss: 2.9365 - regression_loss: 2.1541 - classification_loss 3767/10000 [==========>...................] - ETA: 50:44 - loss: 2.9364 - regression_loss: 2.1540 - classification_loss 3768/10000 [==========>...................] - ETA: 50:44 - loss: 2.9361 - regression_loss: 2.1537 - classification_loss 3769/10000 [==========>...................] - ETA: 50:43 - loss: 2.9359 - regression_loss: 2.1536 - classification_loss 3770/10000 [==========>...................] - ETA: 50:43 - loss: 2.9356 - regression_loss: 2.1534 - classification_loss 3771/10000 [==========>...................] - ETA: 50:42 - loss: 2.9351 - regression_loss: 2.1530 - classification_loss 3772/10000 [==========>...................] - ETA: 50:42 - loss: 2.9350 - regression_loss: 2.1530 - classification_loss 3773/10000 [==========>...................] - ETA: 50:41 - loss: 2.9348 - regression_loss: 2.1528 - classification_loss 3774/10000 [==========>...................] - ETA: 50:40 - loss: 2.9348 - regression_loss: 2.1528 - classification_loss 3775/10000 [==========>...................] - ETA: 50:40 - loss: 2.9348 - regression_loss: 2.1528 - classification_loss 3776/10000 [==========>...................] - ETA: 50:40 - loss: 2.9346 - regression_loss: 2.1526 - classification_loss 3777/10000 [==========>...................] - ETA: 50:39 - loss: 2.9344 - regression_loss: 2.1525 - classification_loss 3778/10000 [==========>...................] - ETA: 50:38 - loss: 2.9344 - regression_loss: 2.1525 - classification_loss 3779/10000 [==========>...................] - ETA: 50:38 - loss: 2.9342 - regression_loss: 2.1525 - classification_loss 3780/10000 [==========>...................] - ETA: 50:37 - loss: 2.9340 - regression_loss: 2.1524 - classification_loss 3781/10000 [==========>...................] - ETA: 50:37 - loss: 2.9341 - regression_loss: 2.1524 - classification_loss 3782/10000 [==========>...................] - ETA: 50:36 - loss: 2.9339 - regression_loss: 2.1523 - classification_loss 3783/10000 [==========>...................] - ETA: 50:36 - loss: 2.9339 - regression_loss: 2.1523 - classification_loss 3784/10000 [==========>...................] - ETA: 50:35 - loss: 2.9337 - regression_loss: 2.1520 - classification_loss 3785/10000 [==========>...................] - ETA: 50:35 - loss: 2.9334 - regression_loss: 2.1518 - classification_loss 3786/10000 [==========>...................] - ETA: 50:34 - loss: 2.9331 - regression_loss: 2.1516 - classification_loss 3787/10000 [==========>...................] - ETA: 50:33 - loss: 2.9329 - regression_loss: 2.1515 - classification_loss 3788/10000 [==========>...................] - ETA: 50:33 - loss: 2.9328 - regression_loss: 2.1514 - classification_loss 3789/10000 [==========>...................] - ETA: 50:32 - loss: 2.9328 - regression_loss: 2.1514 - classification_loss 3790/10000 [==========>...................] - ETA: 50:32 - loss: 2.9327 - regression_loss: 2.1514 - classification_loss 3791/10000 [==========>...................] - ETA: 50:31 - loss: 2.9326 - regression_loss: 2.1514 - classification_loss 3792/10000 [==========>...................] - ETA: 50:31 - loss: 2.9325 - regression_loss: 2.1513 - classification_loss 3793/10000 [==========>...................] - ETA: 50:30 - loss: 2.9323 - regression_loss: 2.1511 - classification_loss 3794/10000 [==========>...................] - ETA: 50:30 - loss: 2.9321 - regression_loss: 2.1510 - classification_loss 3795/10000 [==========>...................] - ETA: 50:29 - loss: 2.9321 - regression_loss: 2.1510 - classification_loss 3796/10000 [==========>...................] - ETA: 50:29 - loss: 2.9322 - regression_loss: 2.1511 - classification_loss 3797/10000 [==========>...................] - ETA: 50:29 - loss: 2.9319 - regression_loss: 2.1509 - classification_loss 3798/10000 [==========>...................] - ETA: 50:28 - loss: 2.9317 - regression_loss: 2.1507 - classification_loss 3799/10000 [==========>...................] - ETA: 50:28 - loss: 2.9318 - regression_loss: 2.1508 - classification_loss 3800/10000 [==========>...................] - ETA: 50:27 - loss: 2.9318 - regression_loss: 2.1508 - classification_loss 3801/10000 [==========>...................] - ETA: 50:27 - loss: 2.9314 - regression_loss: 2.1506 - classification_loss 3802/10000 [==========>...................] - ETA: 50:26 - loss: 2.9312 - regression_loss: 2.1505 - classification_loss 3803/10000 [==========>...................] - ETA: 50:26 - loss: 2.9312 - regression_loss: 2.1505 - classification_loss 3804/10000 [==========>...................] - ETA: 50:25 - loss: 2.9311 - regression_loss: 2.1504 - classification_loss 3805/10000 [==========>...................] - ETA: 50:25 - loss: 2.9307 - regression_loss: 2.1502 - classification_loss 3806/10000 [==========>...................] - ETA: 50:24 - loss: 2.9308 - regression_loss: 2.1503 - classification_loss 3807/10000 [==========>...................] - ETA: 50:24 - loss: 2.9306 - regression_loss: 2.1502 - classification_loss 3808/10000 [==========>...................] - ETA: 50:23 - loss: 2.9305 - regression_loss: 2.1501 - classification_loss 3809/10000 [==========>...................] - ETA: 50:23 - loss: 2.9303 - regression_loss: 2.1499 - classification_loss 3810/10000 [==========>...................] - ETA: 50:22 - loss: 2.9300 - regression_loss: 2.1497 - classification_loss 3811/10000 [==========>...................] - ETA: 50:22 - loss: 2.9300 - regression_loss: 2.1498 - classification_loss 3812/10000 [==========>...................] - ETA: 50:21 - loss: 2.9300 - regression_loss: 2.1498 - classification_loss 3813/10000 [==========>...................] - ETA: 50:21 - loss: 2.9296 - regression_loss: 2.1495 - classification_loss 3814/10000 [==========>...................] - ETA: 50:20 - loss: 2.9296 - regression_loss: 2.1495 - classification_loss 3815/10000 [==========>...................] - ETA: 50:19 - loss: 2.9294 - regression_loss: 2.1493 - classification_loss 3816/10000 [==========>...................] - ETA: 50:19 - loss: 2.9291 - regression_loss: 2.1491 - classification_loss 3817/10000 [==========>...................] - ETA: 50:18 - loss: 2.9291 - regression_loss: 2.1491 - classification_loss 3818/10000 [==========>...................] - ETA: 50:18 - loss: 2.9289 - regression_loss: 2.1489 - classification_loss 3819/10000 [==========>...................] - ETA: 50:18 - loss: 2.9288 - regression_loss: 2.1488 - classification_loss 3820/10000 [==========>...................] - ETA: 50:17 - loss: 2.9287 - regression_loss: 2.1487 - classification_loss 3821/10000 [==========>...................] - ETA: 50:17 - loss: 2.9284 - regression_loss: 2.1485 - classification_loss 3822/10000 [==========>...................] - ETA: 50:16 - loss: 2.9281 - regression_loss: 2.1484 - classification_loss 3823/10000 [==========>...................] - ETA: 50:16 - loss: 2.9278 - regression_loss: 2.1481 - classification_loss 3824/10000 [==========>...................] - ETA: 50:16 - loss: 2.9277 - regression_loss: 2.1481 - classification_loss 3825/10000 [==========>...................] - ETA: 50:15 - loss: 2.9277 - regression_loss: 2.1480 - classification_loss 3826/10000 [==========>...................] - ETA: 50:15 - loss: 2.9274 - regression_loss: 2.1479 - classification_loss 3827/10000 [==========>...................] - ETA: 50:14 - loss: 2.9272 - regression_loss: 2.1478 - classification_loss 3828/10000 [==========>...................] - ETA: 50:14 - loss: 2.9273 - regression_loss: 2.1479 - classification_loss 3829/10000 [==========>...................] - ETA: 50:13 - loss: 2.9271 - regression_loss: 2.1477 - classification_loss 3830/10000 [==========>...................] - ETA: 50:13 - loss: 2.9270 - regression_loss: 2.1477 - classification_loss 3831/10000 [==========>...................] - ETA: 50:12 - loss: 2.9268 - regression_loss: 2.1475 - classification_loss 3832/10000 [==========>...................] - ETA: 50:11 - loss: 2.9267 - regression_loss: 2.1475 - classification_loss 3833/10000 [==========>...................] - ETA: 50:11 - loss: 2.9266 - regression_loss: 2.1475 - classification_loss 3834/10000 [==========>...................] - ETA: 50:10 - loss: 2.9264 - regression_loss: 2.1474 - classification_loss 3835/10000 [==========>...................] - ETA: 50:10 - loss: 2.9261 - regression_loss: 2.1472 - classification_loss 3836/10000 [==========>...................] - ETA: 50:09 - loss: 2.9261 - regression_loss: 2.1472 - classification_loss 3837/10000 [==========>...................] - ETA: 50:09 - loss: 2.9259 - regression_loss: 2.1471 - classification_loss 3838/10000 [==========>...................] - ETA: 50:08 - loss: 2.9258 - regression_loss: 2.1470 - classification_loss 3839/10000 [==========>...................] - ETA: 50:08 - loss: 2.9257 - regression_loss: 2.1469 - classification_loss 3840/10000 [==========>...................] - ETA: 50:07 - loss: 2.9255 - regression_loss: 2.1467 - classification_loss 3841/10000 [==========>...................] - ETA: 50:07 - loss: 2.9252 - regression_loss: 2.1465 - classification_loss 3842/10000 [==========>...................] - ETA: 50:06 - loss: 2.9252 - regression_loss: 2.1466 - classification_loss 3843/10000 [==========>...................] - ETA: 50:05 - loss: 2.9252 - regression_loss: 2.1467 - classification_loss 3844/10000 [==========>...................] - ETA: 50:05 - loss: 2.9252 - regression_loss: 2.1467 - classification_loss 3845/10000 [==========>...................] - ETA: 50:04 - loss: 2.9252 - regression_loss: 2.1467 - classification_loss 3846/10000 [==========>...................] - ETA: 50:04 - loss: 2.9252 - regression_loss: 2.1467 - classification_loss 3847/10000 [==========>...................] - ETA: 50:03 - loss: 2.9248 - regression_loss: 2.1464 - classification_loss 3848/10000 [==========>...................] - ETA: 50:03 - loss: 2.9247 - regression_loss: 2.1463 - classification_loss 3849/10000 [==========>...................] - ETA: 50:02 - loss: 2.9243 - regression_loss: 2.1460 - classification_loss 3850/10000 [==========>...................] - ETA: 50:02 - loss: 2.9242 - regression_loss: 2.1459 - classification_loss 3851/10000 [==========>...................] - ETA: 50:01 - loss: 2.9241 - regression_loss: 2.1459 - classification_loss 3852/10000 [==========>...................] - ETA: 50:01 - loss: 2.9239 - regression_loss: 2.1457 - classification_loss 3853/10000 [==========>...................] - ETA: 50:00 - loss: 2.9238 - regression_loss: 2.1456 - classification_loss 3854/10000 [==========>...................] - ETA: 50:00 - loss: 2.9235 - regression_loss: 2.1454 - classification_loss 3855/10000 [==========>...................] - ETA: 49:59 - loss: 2.9236 - regression_loss: 2.1455 - classification_loss 3856/10000 [==========>...................] - ETA: 49:59 - loss: 2.9236 - regression_loss: 2.1455 - classification_loss 3857/10000 [==========>...................] - ETA: 49:58 - loss: 2.9235 - regression_loss: 2.1455 - classification_loss 3858/10000 [==========>...................] - ETA: 49:58 - loss: 2.9233 - regression_loss: 2.1453 - classification_loss 3859/10000 [==========>...................] - ETA: 49:57 - loss: 2.9232 - regression_loss: 2.1453 - classification_loss 3860/10000 [==========>...................] - ETA: 49:56 - loss: 2.9230 - regression_loss: 2.1452 - classification_loss 3861/10000 [==========>...................] - ETA: 49:56 - loss: 2.9230 - regression_loss: 2.1452 - classification_loss 3862/10000 [==========>...................] - ETA: 49:55 - loss: 2.9227 - regression_loss: 2.1451 - classification_loss 3863/10000 [==========>...................] - ETA: 49:55 - loss: 2.9227 - regression_loss: 2.1450 - classification_loss 3864/10000 [==========>...................] - ETA: 49:54 - loss: 2.9225 - regression_loss: 2.1449 - classification_loss 3865/10000 [==========>...................] - ETA: 49:54 - loss: 2.9225 - regression_loss: 2.1449 - classification_loss 3866/10000 [==========>...................] - ETA: 49:53 - loss: 2.9221 - regression_loss: 2.1446 - classification_loss 3867/10000 [==========>...................] - ETA: 49:53 - loss: 2.9218 - regression_loss: 2.1443 - classification_loss 3868/10000 [==========>...................] - ETA: 49:52 - loss: 2.9218 - regression_loss: 2.1444 - classification_loss 3869/10000 [==========>...................] - ETA: 49:52 - loss: 2.9215 - regression_loss: 2.1442 - classification_loss 3870/10000 [==========>...................] - ETA: 49:51 - loss: 2.9215 - regression_loss: 2.1441 - classification_loss 3871/10000 [==========>...................] - ETA: 49:51 - loss: 2.9218 - regression_loss: 2.1444 - classification_loss 3872/10000 [==========>...................] - ETA: 49:50 - loss: 2.9215 - regression_loss: 2.1442 - classification_loss 3873/10000 [==========>...................] - ETA: 49:50 - loss: 2.9213 - regression_loss: 2.1440 - classification_loss 3874/10000 [==========>...................] - ETA: 49:49 - loss: 2.9210 - regression_loss: 2.1438 - classification_loss 3875/10000 [==========>...................] - ETA: 49:49 - loss: 2.9208 - regression_loss: 2.1437 - classification_loss 3876/10000 [==========>...................] - ETA: 49:48 - loss: 2.9204 - regression_loss: 2.1434 - classification_loss 3877/10000 [==========>...................] - ETA: 49:48 - loss: 2.9203 - regression_loss: 2.1433 - classification_loss 3878/10000 [==========>...................] - ETA: 49:47 - loss: 2.9203 - regression_loss: 2.1434 - classification_loss 3879/10000 [==========>...................] - ETA: 49:47 - loss: 2.9202 - regression_loss: 2.1434 - classification_loss 3880/10000 [==========>...................] - ETA: 49:46 - loss: 2.9205 - regression_loss: 2.1435 - classification_loss 3881/10000 [==========>...................] - ETA: 49:46 - loss: 2.9204 - regression_loss: 2.1434 - classification_loss 3882/10000 [==========>...................] - ETA: 49:46 - loss: 2.9202 - regression_loss: 2.1433 - classification_loss 3883/10000 [==========>...................] - ETA: 49:45 - loss: 2.9199 - regression_loss: 2.1431 - classification_loss 3884/10000 [==========>...................] - ETA: 49:45 - loss: 2.9197 - regression_loss: 2.1430 - classification_loss 3885/10000 [==========>...................] - ETA: 49:44 - loss: 2.9195 - regression_loss: 2.1428 - classification_loss 3886/10000 [==========>...................] - ETA: 49:44 - loss: 2.9194 - regression_loss: 2.1428 - classification_loss 3887/10000 [==========>...................] - ETA: 49:43 - loss: 2.9194 - regression_loss: 2.1428 - classification_loss 3888/10000 [==========>...................] - ETA: 49:42 - loss: 2.9192 - regression_loss: 2.1427 - classification_loss 3889/10000 [==========>...................] - ETA: 49:42 - loss: 2.9192 - regression_loss: 2.1427 - classification_loss 3890/10000 [==========>...................] - ETA: 49:41 - loss: 2.9191 - regression_loss: 2.1426 - classification_loss 3891/10000 [==========>...................] - ETA: 49:41 - loss: 2.9188 - regression_loss: 2.1424 - classification_loss 3892/10000 [==========>...................] - ETA: 49:40 - loss: 2.9186 - regression_loss: 2.1423 - classification_loss 3893/10000 [==========>...................] - ETA: 49:40 - loss: 2.9184 - regression_loss: 2.1421 - classification_loss 3894/10000 [==========>...................] - ETA: 49:39 - loss: 2.9181 - regression_loss: 2.1419 - classification_loss 3895/10000 [==========>...................] - ETA: 49:39 - loss: 2.9181 - regression_loss: 2.1420 - classification_loss 3896/10000 [==========>...................] - ETA: 49:38 - loss: 2.9178 - regression_loss: 2.1417 - classification_loss 3897/10000 [==========>...................] - ETA: 49:38 - loss: 2.9176 - regression_loss: 2.1414 - classification_loss 3898/10000 [==========>...................] - ETA: 49:37 - loss: 2.9175 - regression_loss: 2.1414 - classification_loss 3899/10000 [==========>...................] - ETA: 49:37 - loss: 2.9173 - regression_loss: 2.1412 - classification_loss 3900/10000 [==========>...................] - ETA: 49:36 - loss: 2.9173 - regression_loss: 2.1412 - classification_loss 3901/10000 [==========>...................] - ETA: 49:35 - loss: 2.9171 - regression_loss: 2.1410 - classification_loss 3902/10000 [==========>...................] - ETA: 49:35 - loss: 2.9168 - regression_loss: 2.1408 - classification_loss 3903/10000 [==========>...................] - ETA: 49:34 - loss: 2.9167 - regression_loss: 2.1408 - classification_loss 3904/10000 [==========>...................] - ETA: 49:34 - loss: 2.9167 - regression_loss: 2.1408 - classification_loss 3905/10000 [==========>...................] - ETA: 49:33 - loss: 2.9166 - regression_loss: 2.1408 - classification_loss 3906/10000 [==========>...................] - ETA: 49:33 - loss: 2.9167 - regression_loss: 2.1409 - classification_loss 3907/10000 [==========>...................] - ETA: 49:32 - loss: 2.9167 - regression_loss: 2.1409 - classification_loss 3908/10000 [==========>...................] - ETA: 49:32 - loss: 2.9167 - regression_loss: 2.1410 - classification_loss 3909/10000 [==========>...................] - ETA: 49:31 - loss: 2.9170 - regression_loss: 2.1413 - classification_loss 3910/10000 [==========>...................] - ETA: 49:31 - loss: 2.9171 - regression_loss: 2.1414 - classification_loss 3911/10000 [==========>...................] - ETA: 49:30 - loss: 2.9167 - regression_loss: 2.1411 - classification_loss 3912/10000 [==========>...................] - ETA: 49:29 - loss: 2.9166 - regression_loss: 2.1411 - classification_loss 3913/10000 [==========>...................] - ETA: 49:29 - loss: 2.9164 - regression_loss: 2.1410 - classification_loss 3914/10000 [==========>...................] - ETA: 49:28 - loss: 2.9162 - regression_loss: 2.1409 - classification_loss 3915/10000 [==========>...................] - ETA: 49:28 - loss: 2.9159 - regression_loss: 2.1407 - classification_loss 3916/10000 [==========>...................] - ETA: 49:27 - loss: 2.9160 - regression_loss: 2.1408 - classification_loss 3917/10000 [==========>...................] - ETA: 49:27 - loss: 2.9159 - regression_loss: 2.1407 - classification_loss 3918/10000 [==========>...................] - ETA: 49:26 - loss: 2.9158 - regression_loss: 2.1407 - classification_loss 3919/10000 [==========>...................] - ETA: 49:26 - loss: 2.9157 - regression_loss: 2.1406 - classification_loss 3920/10000 [==========>...................] - ETA: 49:25 - loss: 2.9158 - regression_loss: 2.1408 - classification_loss 3921/10000 [==========>...................] - ETA: 49:25 - loss: 2.9155 - regression_loss: 2.1406 - classification_loss 3922/10000 [==========>...................] - ETA: 49:24 - loss: 2.9159 - regression_loss: 2.1409 - classification_loss 3923/10000 [==========>...................] - ETA: 49:24 - loss: 2.9158 - regression_loss: 2.1409 - classification_loss 3924/10000 [==========>...................] - ETA: 49:23 - loss: 2.9159 - regression_loss: 2.1409 - classification_loss 3925/10000 [==========>...................] - ETA: 49:23 - loss: 2.9159 - regression_loss: 2.1410 - classification_loss 3926/10000 [==========>...................] - ETA: 49:22 - loss: 2.9156 - regression_loss: 2.1408 - classification_loss 3927/10000 [==========>...................] - ETA: 49:22 - loss: 2.9156 - regression_loss: 2.1408 - classification_loss 3928/10000 [==========>...................] - ETA: 49:21 - loss: 2.9157 - regression_loss: 2.1410 - classification_loss 3929/10000 [==========>...................] - ETA: 49:21 - loss: 2.9157 - regression_loss: 2.1409 - classification_loss 3930/10000 [==========>...................] - ETA: 49:20 - loss: 2.9156 - regression_loss: 2.1408 - classification_loss 3931/10000 [==========>...................] - ETA: 49:20 - loss: 2.9153 - regression_loss: 2.1406 - classification_loss 3932/10000 [==========>...................] - ETA: 49:19 - loss: 2.9152 - regression_loss: 2.1405 - classification_loss 3933/10000 [==========>...................] - ETA: 49:19 - loss: 2.9152 - regression_loss: 2.1405 - classification_loss 3934/10000 [==========>...................] - ETA: 49:18 - loss: 2.9153 - regression_loss: 2.1406 - classification_loss 3935/10000 [==========>...................] - ETA: 49:18 - loss: 2.9151 - regression_loss: 2.1405 - classification_loss 3936/10000 [==========>...................] - ETA: 49:17 - loss: 2.9149 - regression_loss: 2.1403 - classification_loss 3937/10000 [==========>...................] - ETA: 49:17 - loss: 2.9147 - regression_loss: 2.1402 - classification_loss 3938/10000 [==========>...................] - ETA: 49:16 - loss: 2.9148 - regression_loss: 2.1403 - classification_loss 3939/10000 [==========>...................] - ETA: 49:16 - loss: 2.9146 - regression_loss: 2.1402 - classification_loss 3940/10000 [==========>...................] - ETA: 49:15 - loss: 2.9144 - regression_loss: 2.1401 - classification_loss 3941/10000 [==========>...................] - ETA: 49:15 - loss: 2.9142 - regression_loss: 2.1399 - classification_loss 3942/10000 [==========>...................] - ETA: 49:14 - loss: 2.9140 - regression_loss: 2.1398 - classification_loss 3943/10000 [==========>...................] - ETA: 49:14 - loss: 2.9138 - regression_loss: 2.1397 - classification_loss 3944/10000 [==========>...................] - ETA: 49:13 - loss: 2.9137 - regression_loss: 2.1396 - classification_loss 3945/10000 [==========>...................] - ETA: 49:12 - loss: 2.9137 - regression_loss: 2.1396 - classification_loss 3946/10000 [==========>...................] - ETA: 49:12 - loss: 2.9135 - regression_loss: 2.1395 - classification_loss 3947/10000 [==========>...................] - ETA: 49:11 - loss: 2.9134 - regression_loss: 2.1394 - classification_loss 3948/10000 [==========>...................] - ETA: 49:11 - loss: 2.9130 - regression_loss: 2.1391 - classification_loss 3949/10000 [==========>...................] - ETA: 49:10 - loss: 2.9131 - regression_loss: 2.1392 - classification_loss 3950/10000 [==========>...................] - ETA: 49:10 - loss: 2.9131 - regression_loss: 2.1392 - classification_loss 3951/10000 [==========>...................] - ETA: 49:09 - loss: 2.9131 - regression_loss: 2.1392 - classification_loss 3952/10000 [==========>...................] - ETA: 49:09 - loss: 2.9128 - regression_loss: 2.1390 - classification_loss 3953/10000 [==========>...................] - ETA: 49:08 - loss: 2.9127 - regression_loss: 2.1390 - classification_loss 3954/10000 [==========>...................] - ETA: 49:08 - loss: 2.9124 - regression_loss: 2.1387 - classification_loss 3955/10000 [==========>...................] - ETA: 49:07 - loss: 2.9121 - regression_loss: 2.1382 - classification_loss 3956/10000 [==========>...................] - ETA: 49:07 - loss: 2.9118 - regression_loss: 2.1379 - classification_loss 3957/10000 [==========>...................] - ETA: 49:06 - loss: 2.9116 - regression_loss: 2.1378 - classification_loss 3958/10000 [==========>...................] - ETA: 49:06 - loss: 2.9115 - regression_loss: 2.1378 - classification_loss 3959/10000 [==========>...................] - ETA: 49:05 - loss: 2.9117 - regression_loss: 2.1378 - classification_loss 3960/10000 [==========>...................] - ETA: 49:05 - loss: 2.9116 - regression_loss: 2.1378 - classification_loss 3961/10000 [==========>...................] - ETA: 49:04 - loss: 2.9115 - regression_loss: 2.1378 - classification_loss 3962/10000 [==========>...................] - ETA: 49:04 - loss: 2.9112 - regression_loss: 2.1375 - classification_loss 3963/10000 [==========>...................] - ETA: 49:03 - loss: 2.9111 - regression_loss: 2.1375 - classification_loss 3964/10000 [==========>...................] - ETA: 49:02 - loss: 2.9110 - regression_loss: 2.1373 - classification_loss 3965/10000 [==========>...................] - ETA: 49:02 - loss: 2.9108 - regression_loss: 2.1373 - classification_loss 3966/10000 [==========>...................] - ETA: 49:01 - loss: 2.9107 - regression_loss: 2.1372 - classification_loss 3967/10000 [==========>...................] - ETA: 49:01 - loss: 2.9105 - regression_loss: 2.1370 - classification_loss 3968/10000 [==========>...................] - ETA: 49:00 - loss: 2.9104 - regression_loss: 2.1369 - classification_loss 3969/10000 [==========>...................] - ETA: 49:00 - loss: 2.9101 - regression_loss: 2.1367 - classification_loss 3970/10000 [==========>...................] - ETA: 48:59 - loss: 2.9099 - regression_loss: 2.1365 - classification_loss 3971/10000 [==========>...................] - ETA: 48:59 - loss: 2.9097 - regression_loss: 2.1364 - classification_loss 3972/10000 [==========>...................] - ETA: 48:58 - loss: 2.9097 - regression_loss: 2.1364 - classification_loss 3973/10000 [==========>...................] - ETA: 48:58 - loss: 2.9095 - regression_loss: 2.1363 - classification_loss 3974/10000 [==========>...................] - ETA: 48:57 - loss: 2.9096 - regression_loss: 2.1364 - classification_loss 3975/10000 [==========>...................] - ETA: 48:57 - loss: 2.9094 - regression_loss: 2.1362 - classification_loss 3976/10000 [==========>...................] - ETA: 48:56 - loss: 2.9091 - regression_loss: 2.1359 - classification_loss 3977/10000 [==========>...................] - ETA: 48:56 - loss: 2.9090 - regression_loss: 2.1359 - classification_loss 3978/10000 [==========>...................] - ETA: 48:55 - loss: 2.9088 - regression_loss: 2.1357 - classification_loss 3979/10000 [==========>...................] - ETA: 48:55 - loss: 2.9085 - regression_loss: 2.1355 - classification_loss 3980/10000 [==========>...................] - ETA: 48:54 - loss: 2.9082 - regression_loss: 2.1352 - classification_loss 3981/10000 [==========>...................] - ETA: 48:54 - loss: 2.9081 - regression_loss: 2.1351 - classification_loss 3982/10000 [==========>...................] - ETA: 48:53 - loss: 2.9080 - regression_loss: 2.1350 - classification_loss 3983/10000 [==========>...................] - ETA: 48:52 - loss: 2.9079 - regression_loss: 2.1349 - classification_loss 3984/10000 [==========>...................] - ETA: 48:52 - loss: 2.9078 - regression_loss: 2.1349 - classification_loss 3985/10000 [==========>...................] - ETA: 48:52 - loss: 2.9079 - regression_loss: 2.1351 - classification_loss 3986/10000 [==========>...................] - ETA: 48:52 - loss: 2.9080 - regression_loss: 2.1352 - classification_loss 3987/10000 [==========>...................] - ETA: 48:51 - loss: 2.9077 - regression_loss: 2.1350 - classification_loss 3988/10000 [==========>...................] - ETA: 48:51 - loss: 2.9076 - regression_loss: 2.1349 - classification_loss 3989/10000 [==========>...................] - ETA: 48:50 - loss: 2.9074 - regression_loss: 2.1347 - classification_loss 3990/10000 [==========>...................] - ETA: 48:50 - loss: 2.9072 - regression_loss: 2.1345 - classification_loss 3991/10000 [==========>...................] - ETA: 48:49 - loss: 2.9072 - regression_loss: 2.1345 - classification_loss 3992/10000 [==========>...................] - ETA: 48:49 - loss: 2.9070 - regression_loss: 2.1344 - classification_loss 3993/10000 [==========>...................] - ETA: 48:48 - loss: 2.9071 - regression_loss: 2.1345 - classification_loss 3994/10000 [==========>...................] - ETA: 48:47 - loss: 2.9068 - regression_loss: 2.1342 - classification_loss 3995/10000 [==========>...................] - ETA: 48:47 - loss: 2.9066 - regression_loss: 2.1340 - classification_loss 3996/10000 [==========>...................] - ETA: 48:46 - loss: 2.9065 - regression_loss: 2.1340 - classification_loss 3997/10000 [==========>...................] - ETA: 48:46 - loss: 2.9063 - regression_loss: 2.1339 - classification_loss 3998/10000 [==========>...................] - ETA: 48:45 - loss: 2.9064 - regression_loss: 2.1340 - classification_loss 3999/10000 [==========>...................] - ETA: 48:45 - loss: 2.9061 - regression_loss: 2.1338 - classification_loss 4000/10000 [===========>..................] - ETA: 48:44 - loss: 2.9058 - regression_loss: 2.1336 - classification_loss 4001/10000 [===========>..................] - ETA: 48:44 - loss: 2.9057 - regression_loss: 2.1336 - classification_loss 4002/10000 [===========>..................] - ETA: 48:44 - loss: 2.9058 - regression_loss: 2.1337 - classification_loss 4003/10000 [===========>..................] - ETA: 48:43 - loss: 2.9059 - regression_loss: 2.1338 - classification_loss 4004/10000 [===========>..................] - ETA: 48:43 - loss: 2.9057 - regression_loss: 2.1336 - classification_loss 4005/10000 [===========>..................] - ETA: 48:42 - loss: 2.9059 - regression_loss: 2.1338 - classification_loss 4006/10000 [===========>..................] - ETA: 48:42 - loss: 2.9059 - regression_loss: 2.1338 - classification_loss 4007/10000 [===========>..................] - ETA: 48:41 - loss: 2.9060 - regression_loss: 2.1339 - classification_loss 4008/10000 [===========>..................] - ETA: 48:41 - loss: 2.9061 - regression_loss: 2.1340 - classification_loss 4009/10000 [===========>..................] - ETA: 48:40 - loss: 2.9061 - regression_loss: 2.1340 - classification_loss 4010/10000 [===========>..................] - ETA: 48:40 - loss: 2.9061 - regression_loss: 2.1340 - classification_loss 4011/10000 [===========>..................] - ETA: 48:39 - loss: 2.9060 - regression_loss: 2.1341 - classification_loss 4012/10000 [===========>..................] - ETA: 48:39 - loss: 2.9060 - regression_loss: 2.1341 - classification_loss 4013/10000 [===========>..................] - ETA: 48:38 - loss: 2.9059 - regression_loss: 2.1340 - classification_loss 4014/10000 [===========>..................] - ETA: 48:38 - loss: 2.9058 - regression_loss: 2.1340 - classification_loss 4015/10000 [===========>..................] - ETA: 48:37 - loss: 2.9057 - regression_loss: 2.1340 - classification_loss 4016/10000 [===========>..................] - ETA: 48:37 - loss: 2.9057 - regression_loss: 2.1339 - classification_loss 4017/10000 [===========>..................] - ETA: 48:36 - loss: 2.9055 - regression_loss: 2.1338 - classification_loss 4018/10000 [===========>..................] - ETA: 48:36 - loss: 2.9055 - regression_loss: 2.1337 - classification_loss 4019/10000 [===========>..................] - ETA: 48:35 - loss: 2.9053 - regression_loss: 2.1337 - classification_loss 4020/10000 [===========>..................] - ETA: 48:35 - loss: 2.9051 - regression_loss: 2.1335 - classification_loss 4021/10000 [===========>..................] - ETA: 48:34 - loss: 2.9052 - regression_loss: 2.1335 - classification_loss 4022/10000 [===========>..................] - ETA: 48:34 - loss: 2.9050 - regression_loss: 2.1334 - classification_loss 4023/10000 [===========>..................] - ETA: 48:33 - loss: 2.9049 - regression_loss: 2.1333 - classification_loss 4024/10000 [===========>..................] - ETA: 48:33 - loss: 2.9048 - regression_loss: 2.1332 - classification_loss 4025/10000 [===========>..................] - ETA: 48:32 - loss: 2.9048 - regression_loss: 2.1332 - classification_loss 4026/10000 [===========>..................] - ETA: 48:31 - loss: 2.9044 - regression_loss: 2.1330 - classification_loss 4027/10000 [===========>..................] - ETA: 48:31 - loss: 2.9042 - regression_loss: 2.1329 - classification_loss 4028/10000 [===========>..................] - ETA: 48:30 - loss: 2.9041 - regression_loss: 2.1328 - classification_loss 4029/10000 [===========>..................] - ETA: 48:30 - loss: 2.9040 - regression_loss: 2.1327 - classification_loss 4030/10000 [===========>..................] - ETA: 48:29 - loss: 2.9037 - regression_loss: 2.1324 - classification_loss 4031/10000 [===========>..................] - ETA: 48:29 - loss: 2.9037 - regression_loss: 2.1325 - classification_loss 4032/10000 [===========>..................] - ETA: 48:28 - loss: 2.9037 - regression_loss: 2.1326 - classification_loss 4033/10000 [===========>..................] - ETA: 48:28 - loss: 2.9035 - regression_loss: 2.1324 - classification_loss 4034/10000 [===========>..................] - ETA: 48:27 - loss: 2.9033 - regression_loss: 2.1323 - classification_loss 4035/10000 [===========>..................] - ETA: 48:27 - loss: 2.9032 - regression_loss: 2.1322 - classification_loss 4036/10000 [===========>..................] - ETA: 48:26 - loss: 2.9032 - regression_loss: 2.1322 - classification_loss 4037/10000 [===========>..................] - ETA: 48:26 - loss: 2.9032 - regression_loss: 2.1322 - classification_loss 4038/10000 [===========>..................] - ETA: 48:25 - loss: 2.9032 - regression_loss: 2.1322 - classification_loss 4039/10000 [===========>..................] - ETA: 48:25 - loss: 2.9032 - regression_loss: 2.1322 - classification_loss 4040/10000 [===========>..................] - ETA: 48:24 - loss: 2.9033 - regression_loss: 2.1323 - classification_loss 4041/10000 [===========>..................] - ETA: 48:23 - loss: 2.9033 - regression_loss: 2.1323 - classification_loss 4042/10000 [===========>..................] - ETA: 48:23 - loss: 2.9031 - regression_loss: 2.1322 - classification_loss 4043/10000 [===========>..................] - ETA: 48:22 - loss: 2.9032 - regression_loss: 2.1323 - classification_loss 4044/10000 [===========>..................] - ETA: 48:22 - loss: 2.9030 - regression_loss: 2.1321 - classification_loss 4045/10000 [===========>..................] - ETA: 48:21 - loss: 2.9029 - regression_loss: 2.1321 - classification_loss 4046/10000 [===========>..................] - ETA: 48:21 - loss: 2.9031 - regression_loss: 2.1322 - classification_loss 4047/10000 [===========>..................] - ETA: 48:20 - loss: 2.9028 - regression_loss: 2.1320 - classification_loss 4048/10000 [===========>..................] - ETA: 48:20 - loss: 2.9027 - regression_loss: 2.1319 - classification_loss 4049/10000 [===========>..................] - ETA: 48:19 - loss: 2.9026 - regression_loss: 2.1318 - classification_loss 4050/10000 [===========>..................] - ETA: 48:19 - loss: 2.9026 - regression_loss: 2.1317 - classification_loss 4051/10000 [===========>..................] - ETA: 48:18 - loss: 2.9025 - regression_loss: 2.1317 - classification_loss 4052/10000 [===========>..................] - ETA: 48:18 - loss: 2.9026 - regression_loss: 2.1317 - classification_loss 4053/10000 [===========>..................] - ETA: 48:17 - loss: 2.9027 - regression_loss: 2.1317 - classification_loss 4054/10000 [===========>..................] - ETA: 48:17 - loss: 2.9025 - regression_loss: 2.1316 - classification_loss 4055/10000 [===========>..................] - ETA: 48:16 - loss: 2.9024 - regression_loss: 2.1315 - classification_loss 4056/10000 [===========>..................] - ETA: 48:16 - loss: 2.9022 - regression_loss: 2.1314 - classification_loss 4057/10000 [===========>..................] - ETA: 48:15 - loss: 2.9022 - regression_loss: 2.1313 - classification_loss 4058/10000 [===========>..................] - ETA: 48:15 - loss: 2.9020 - regression_loss: 2.1312 - classification_loss 4059/10000 [===========>..................] - ETA: 48:14 - loss: 2.9022 - regression_loss: 2.1314 - classification_loss 4060/10000 [===========>..................] - ETA: 48:14 - loss: 2.9018 - regression_loss: 2.1311 - classification_loss 4061/10000 [===========>..................] - ETA: 48:13 - loss: 2.9018 - regression_loss: 2.1312 - classification_loss 4062/10000 [===========>..................] - ETA: 48:13 - loss: 2.9019 - regression_loss: 2.1312 - classification_loss 4063/10000 [===========>..................] - ETA: 48:12 - loss: 2.9018 - regression_loss: 2.1312 - classification_loss 4064/10000 [===========>..................] - ETA: 48:11 - loss: 2.9019 - regression_loss: 2.1314 - classification_loss 4065/10000 [===========>..................] - ETA: 48:11 - loss: 2.9017 - regression_loss: 2.1312 - classification_loss 4066/10000 [===========>..................] - ETA: 48:11 - loss: 2.9017 - regression_loss: 2.1313 - classification_loss 4067/10000 [===========>..................] - ETA: 48:10 - loss: 2.9016 - regression_loss: 2.1312 - classification_loss 4068/10000 [===========>..................] - ETA: 48:09 - loss: 2.9013 - regression_loss: 2.1310 - classification_loss 4069/10000 [===========>..................] - ETA: 48:09 - loss: 2.9013 - regression_loss: 2.1310 - classification_loss 4070/10000 [===========>..................] - ETA: 48:08 - loss: 2.9011 - regression_loss: 2.1309 - classification_loss 4071/10000 [===========>..................] - ETA: 48:08 - loss: 2.9011 - regression_loss: 2.1310 - classification_loss 4072/10000 [===========>..................] - ETA: 48:07 - loss: 2.9010 - regression_loss: 2.1309 - classification_loss 4073/10000 [===========>..................] - ETA: 48:07 - loss: 2.9007 - regression_loss: 2.1306 - classification_loss 4074/10000 [===========>..................] - ETA: 48:06 - loss: 2.9007 - regression_loss: 2.1306 - classification_loss 4075/10000 [===========>..................] - ETA: 48:06 - loss: 2.9005 - regression_loss: 2.1305 - classification_loss 4076/10000 [===========>..................] - ETA: 48:05 - loss: 2.9004 - regression_loss: 2.1303 - classification_loss 4077/10000 [===========>..................] - ETA: 48:05 - loss: 2.9002 - regression_loss: 2.1302 - classification_loss 4078/10000 [===========>..................] - ETA: 48:04 - loss: 2.9001 - regression_loss: 2.1302 - classification_loss 4079/10000 [===========>..................] - ETA: 48:04 - loss: 2.8999 - regression_loss: 2.1300 - classification_loss 4080/10000 [===========>..................] - ETA: 48:03 - loss: 2.8996 - regression_loss: 2.1298 - classification_loss 4081/10000 [===========>..................] - ETA: 48:03 - loss: 2.8992 - regression_loss: 2.1295 - classification_loss 4082/10000 [===========>..................] - ETA: 48:02 - loss: 2.8992 - regression_loss: 2.1295 - classification_loss 4083/10000 [===========>..................] - ETA: 48:02 - loss: 2.8989 - regression_loss: 2.1293 - classification_loss 4084/10000 [===========>..................] - ETA: 48:01 - loss: 2.8987 - regression_loss: 2.1291 - classification_loss 4085/10000 [===========>..................] - ETA: 48:01 - loss: 2.8986 - regression_loss: 2.1290 - classification_loss 4086/10000 [===========>..................] - ETA: 48:00 - loss: 2.8985 - regression_loss: 2.1289 - classification_loss 4087/10000 [===========>..................] - ETA: 48:00 - loss: 2.8984 - regression_loss: 2.1289 - classification_loss 4088/10000 [===========>..................] - ETA: 47:59 - loss: 2.8983 - regression_loss: 2.1288 - classification_loss 4089/10000 [===========>..................] - ETA: 47:59 - loss: 2.8983 - regression_loss: 2.1288 - classification_loss 4090/10000 [===========>..................] - ETA: 47:58 - loss: 2.8982 - regression_loss: 2.1288 - classification_loss 4091/10000 [===========>..................] - ETA: 47:58 - loss: 2.8982 - regression_loss: 2.1288 - classification_loss 4092/10000 [===========>..................] - ETA: 47:57 - loss: 2.8979 - regression_loss: 2.1286 - classification_loss 4093/10000 [===========>..................] - ETA: 47:57 - loss: 2.8976 - regression_loss: 2.1284 - classification_loss 4094/10000 [===========>..................] - ETA: 47:56 - loss: 2.8976 - regression_loss: 2.1284 - classification_loss 4095/10000 [===========>..................] - ETA: 47:55 - loss: 2.8975 - regression_loss: 2.1285 - classification_loss 4096/10000 [===========>..................] - ETA: 47:55 - loss: 2.8977 - regression_loss: 2.1285 - classification_loss 4097/10000 [===========>..................] - ETA: 47:54 - loss: 2.8975 - regression_loss: 2.1285 - classification_loss 4098/10000 [===========>..................] - ETA: 47:54 - loss: 2.8975 - regression_loss: 2.1285 - classification_loss 4099/10000 [===========>..................] - ETA: 47:53 - loss: 2.8973 - regression_loss: 2.1283 - classification_loss 4100/10000 [===========>..................] - ETA: 47:53 - loss: 2.8971 - regression_loss: 2.1281 - classification_loss 4101/10000 [===========>..................] - ETA: 47:52 - loss: 2.8972 - regression_loss: 2.1282 - classification_loss 4102/10000 [===========>..................] - ETA: 47:52 - loss: 2.8972 - regression_loss: 2.1283 - classification_loss 4103/10000 [===========>..................] - ETA: 47:51 - loss: 2.8972 - regression_loss: 2.1283 - classification_loss 4104/10000 [===========>..................] - ETA: 47:51 - loss: 2.8971 - regression_loss: 2.1282 - classification_loss 4105/10000 [===========>..................] - ETA: 47:50 - loss: 2.8971 - regression_loss: 2.1282 - classification_loss 4106/10000 [===========>..................] - ETA: 47:50 - loss: 2.8972 - regression_loss: 2.1282 - classification_loss 4107/10000 [===========>..................] - ETA: 47:49 - loss: 2.8971 - regression_loss: 2.1282 - classification_loss 4108/10000 [===========>..................] - ETA: 47:49 - loss: 2.8967 - regression_loss: 2.1279 - classification_loss 4109/10000 [===========>..................] - ETA: 47:48 - loss: 2.8966 - regression_loss: 2.1278 - classification_loss 4110/10000 [===========>..................] - ETA: 47:48 - loss: 2.8965 - regression_loss: 2.1277 - classification_loss 4111/10000 [===========>..................] - ETA: 47:47 - loss: 2.8965 - regression_loss: 2.1277 - classification_loss 4112/10000 [===========>..................] - ETA: 47:47 - loss: 2.8964 - regression_loss: 2.1277 - classification_loss 4113/10000 [===========>..................] - ETA: 47:46 - loss: 2.8963 - regression_loss: 2.1276 - classification_loss 4114/10000 [===========>..................] - ETA: 47:46 - loss: 2.8962 - regression_loss: 2.1275 - classification_loss 4115/10000 [===========>..................] - ETA: 47:45 - loss: 2.8962 - regression_loss: 2.1276 - classification_loss 4116/10000 [===========>..................] - ETA: 47:45 - loss: 2.8961 - regression_loss: 2.1275 - classification_loss 4117/10000 [===========>..................] - ETA: 47:44 - loss: 2.8961 - regression_loss: 2.1276 - classification_loss 4118/10000 [===========>..................] - ETA: 47:43 - loss: 2.8961 - regression_loss: 2.1276 - classification_loss 4119/10000 [===========>..................] - ETA: 47:43 - loss: 2.8962 - regression_loss: 2.1277 - classification_loss 4120/10000 [===========>..................] - ETA: 47:42 - loss: 2.8960 - regression_loss: 2.1276 - classification_loss 4121/10000 [===========>..................] - ETA: 47:42 - loss: 2.8957 - regression_loss: 2.1274 - classification_loss 4122/10000 [===========>..................] - ETA: 47:41 - loss: 2.8957 - regression_loss: 2.1274 - classification_loss 4123/10000 [===========>..................] - ETA: 47:41 - loss: 2.8954 - regression_loss: 2.1271 - classification_loss 4124/10000 [===========>..................] - ETA: 47:40 - loss: 2.8950 - regression_loss: 2.1268 - classification_loss 4125/10000 [===========>..................] - ETA: 47:40 - loss: 2.8950 - regression_loss: 2.1268 - classification_loss 4126/10000 [===========>..................] - ETA: 47:39 - loss: 2.8948 - regression_loss: 2.1266 - classification_loss 4127/10000 [===========>..................] - ETA: 47:39 - loss: 2.8947 - regression_loss: 2.1266 - classification_loss 4128/10000 [===========>..................] - ETA: 47:38 - loss: 2.8946 - regression_loss: 2.1265 - classification_loss 4129/10000 [===========>..................] - ETA: 47:38 - loss: 2.8944 - regression_loss: 2.1263 - classification_loss 4130/10000 [===========>..................] - ETA: 47:37 - loss: 2.8943 - regression_loss: 2.1263 - classification_loss 4131/10000 [===========>..................] - ETA: 47:37 - loss: 2.8941 - regression_loss: 2.1260 - classification_loss 4132/10000 [===========>..................] - ETA: 47:36 - loss: 2.8940 - regression_loss: 2.1260 - classification_loss 4133/10000 [===========>..................] - ETA: 47:36 - loss: 2.8937 - regression_loss: 2.1257 - classification_loss 4134/10000 [===========>..................] - ETA: 47:35 - loss: 2.8937 - regression_loss: 2.1257 - classification_loss 4135/10000 [===========>..................] - ETA: 47:35 - loss: 2.8938 - regression_loss: 2.1258 - classification_loss 4136/10000 [===========>..................] - ETA: 47:34 - loss: 2.8936 - regression_loss: 2.1256 - classification_loss 4137/10000 [===========>..................] - ETA: 47:34 - loss: 2.8932 - regression_loss: 2.1253 - classification_loss 4138/10000 [===========>..................] - ETA: 47:33 - loss: 2.8931 - regression_loss: 2.1253 - classification_loss 4139/10000 [===========>..................] - ETA: 47:32 - loss: 2.8930 - regression_loss: 2.1252 - classification_loss 4140/10000 [===========>..................] - ETA: 47:32 - loss: 2.8929 - regression_loss: 2.1252 - classification_loss 4141/10000 [===========>..................] - ETA: 47:31 - loss: 2.8928 - regression_loss: 2.1251 - classification_loss 4142/10000 [===========>..................] - ETA: 47:31 - loss: 2.8925 - regression_loss: 2.1249 - classification_loss 4143/10000 [===========>..................] - ETA: 47:30 - loss: 2.8924 - regression_loss: 2.1248 - classification_loss 4144/10000 [===========>..................] - ETA: 47:30 - loss: 2.8922 - regression_loss: 2.1247 - classification_loss 4145/10000 [===========>..................] - ETA: 47:29 - loss: 2.8918 - regression_loss: 2.1244 - classification_loss 4146/10000 [===========>..................] - ETA: 47:29 - loss: 2.8916 - regression_loss: 2.1243 - classification_loss 4147/10000 [===========>..................] - ETA: 47:28 - loss: 2.8916 - regression_loss: 2.1243 - classification_loss 4148/10000 [===========>..................] - ETA: 47:28 - loss: 2.8915 - regression_loss: 2.1242 - classification_loss 4149/10000 [===========>..................] - ETA: 47:27 - loss: 2.8913 - regression_loss: 2.1241 - classification_loss 4150/10000 [===========>..................] - ETA: 47:27 - loss: 2.8911 - regression_loss: 2.1239 - classification_loss 4151/10000 [===========>..................] - ETA: 47:26 - loss: 2.8909 - regression_loss: 2.1237 - classification_loss 4152/10000 [===========>..................] - ETA: 47:26 - loss: 2.8908 - regression_loss: 2.1237 - classification_loss 4153/10000 [===========>..................] - ETA: 47:25 - loss: 2.8907 - regression_loss: 2.1237 - classification_loss 4154/10000 [===========>..................] - ETA: 47:25 - loss: 2.8906 - regression_loss: 2.1236 - classification_loss 4155/10000 [===========>..................] - ETA: 47:24 - loss: 2.8905 - regression_loss: 2.1234 - classification_loss 4156/10000 [===========>..................] - ETA: 47:24 - loss: 2.8904 - regression_loss: 2.1234 - classification_loss 4157/10000 [===========>..................] - ETA: 47:23 - loss: 2.8903 - regression_loss: 2.1233 - classification_loss 4158/10000 [===========>..................] - ETA: 47:23 - loss: 2.8900 - regression_loss: 2.1230 - classification_loss 4159/10000 [===========>..................] - ETA: 47:22 - loss: 2.8901 - regression_loss: 2.1232 - classification_loss 4160/10000 [===========>..................] - ETA: 47:22 - loss: 2.8899 - regression_loss: 2.1230 - classification_loss 4161/10000 [===========>..................] - ETA: 47:21 - loss: 2.8897 - regression_loss: 2.1228 - classification_loss 4162/10000 [===========>..................] - ETA: 47:20 - loss: 2.8894 - regression_loss: 2.1226 - classification_loss 4163/10000 [===========>..................] - ETA: 47:20 - loss: 2.8893 - regression_loss: 2.1225 - classification_loss 4164/10000 [===========>..................] - ETA: 47:19 - loss: 2.8890 - regression_loss: 2.1223 - classification_loss 4165/10000 [===========>..................] - ETA: 47:19 - loss: 2.8888 - regression_loss: 2.1222 - classification_loss 4166/10000 [===========>..................] - ETA: 47:18 - loss: 2.8886 - regression_loss: 2.1221 - classification_loss 4167/10000 [===========>..................] - ETA: 47:18 - loss: 2.8884 - regression_loss: 2.1220 - classification_loss 4168/10000 [===========>..................] - ETA: 47:17 - loss: 2.8882 - regression_loss: 2.1218 - classification_loss 4169/10000 [===========>..................] - ETA: 47:17 - loss: 2.8880 - regression_loss: 2.1216 - classification_loss 4170/10000 [===========>..................] - ETA: 47:16 - loss: 2.8878 - regression_loss: 2.1215 - classification_loss 4171/10000 [===========>..................] - ETA: 47:16 - loss: 2.8878 - regression_loss: 2.1215 - classification_loss 4172/10000 [===========>..................] - ETA: 47:15 - loss: 2.8877 - regression_loss: 2.1215 - classification_loss 4173/10000 [===========>..................] - ETA: 47:15 - loss: 2.8875 - regression_loss: 2.1213 - classification_loss 4174/10000 [===========>..................] - ETA: 47:14 - loss: 2.8875 - regression_loss: 2.1213 - classification_loss 4175/10000 [===========>..................] - ETA: 47:14 - loss: 2.8875 - regression_loss: 2.1214 - classification_loss 4176/10000 [===========>..................] - ETA: 47:13 - loss: 2.8874 - regression_loss: 2.1213 - classification_loss 4177/10000 [===========>..................] - ETA: 47:13 - loss: 2.8875 - regression_loss: 2.1214 - classification_loss 4178/10000 [===========>..................] - ETA: 47:12 - loss: 2.8875 - regression_loss: 2.1214 - classification_loss 4179/10000 [===========>..................] - ETA: 47:11 - loss: 2.8873 - regression_loss: 2.1213 - classification_loss 4180/10000 [===========>..................] - ETA: 47:11 - loss: 2.8872 - regression_loss: 2.1213 - classification_loss 4181/10000 [===========>..................] - ETA: 47:10 - loss: 2.8871 - regression_loss: 2.1213 - classification_loss 4182/10000 [===========>..................] - ETA: 47:10 - loss: 2.8869 - regression_loss: 2.1211 - classification_loss 4183/10000 [===========>..................] - ETA: 47:09 - loss: 2.8867 - regression_loss: 2.1210 - classification_loss 4184/10000 [===========>..................] - ETA: 47:09 - loss: 2.8866 - regression_loss: 2.1209 - classification_loss 4185/10000 [===========>..................] - ETA: 47:08 - loss: 2.8863 - regression_loss: 2.1207 - classification_loss 4186/10000 [===========>..................] - ETA: 47:08 - loss: 2.8864 - regression_loss: 2.1208 - classification_loss 4187/10000 [===========>..................] - ETA: 47:07 - loss: 2.8860 - regression_loss: 2.1205 - classification_loss 4188/10000 [===========>..................] - ETA: 47:07 - loss: 2.8859 - regression_loss: 2.1205 - classification_loss 4189/10000 [===========>..................] - ETA: 47:06 - loss: 2.8859 - regression_loss: 2.1204 - classification_loss 4190/10000 [===========>..................] - ETA: 47:06 - loss: 2.8857 - regression_loss: 2.1203 - classification_loss 4191/10000 [===========>..................] - ETA: 47:05 - loss: 2.8857 - regression_loss: 2.1204 - classification_loss 4192/10000 [===========>..................] - ETA: 47:05 - loss: 2.8856 - regression_loss: 2.1204 - classification_loss 4193/10000 [===========>..................] - ETA: 47:04 - loss: 2.8856 - regression_loss: 2.1204 - classification_loss 4194/10000 [===========>..................] - ETA: 47:04 - loss: 2.8853 - regression_loss: 2.1202 - classification_loss 4195/10000 [===========>..................] - ETA: 47:03 - loss: 2.8851 - regression_loss: 2.1200 - classification_loss 4196/10000 [===========>..................] - ETA: 47:03 - loss: 2.8850 - regression_loss: 2.1199 - classification_loss 4197/10000 [===========>..................] - ETA: 47:02 - loss: 2.8848 - regression_loss: 2.1199 - classification_loss 4198/10000 [===========>..................] - ETA: 47:02 - loss: 2.8847 - regression_loss: 2.1197 - classification_loss 4199/10000 [===========>..................] - ETA: 47:01 - loss: 2.8844 - regression_loss: 2.1195 - classification_loss 4200/10000 [===========>..................] - ETA: 47:01 - loss: 2.8843 - regression_loss: 2.1194 - classification_loss 4201/10000 [===========>..................] - ETA: 47:00 - loss: 2.8843 - regression_loss: 2.1194 - classification_loss 4202/10000 [===========>..................] - ETA: 47:00 - loss: 2.8839 - regression_loss: 2.1191 - classification_loss 4203/10000 [===========>..................] - ETA: 46:59 - loss: 2.8838 - regression_loss: 2.1191 - classification_loss 4204/10000 [===========>..................] - ETA: 46:59 - loss: 2.8837 - regression_loss: 2.1190 - classification_loss 4205/10000 [===========>..................] - ETA: 46:58 - loss: 2.8837 - regression_loss: 2.1190 - classification_loss 4206/10000 [===========>..................] - ETA: 46:58 - loss: 2.8835 - regression_loss: 2.1189 - classification_loss 4207/10000 [===========>..................] - ETA: 46:57 - loss: 2.8836 - regression_loss: 2.1190 - classification_loss 4208/10000 [===========>..................] - ETA: 46:57 - loss: 2.8836 - regression_loss: 2.1190 - classification_loss 4209/10000 [===========>..................] - ETA: 46:56 - loss: 2.8836 - regression_loss: 2.1191 - classification_loss 4210/10000 [===========>..................] - ETA: 46:56 - loss: 2.8834 - regression_loss: 2.1189 - classification_loss 4211/10000 [===========>..................] - ETA: 46:55 - loss: 2.8832 - regression_loss: 2.1188 - classification_loss 4212/10000 [===========>..................] - ETA: 46:55 - loss: 2.8829 - regression_loss: 2.1185 - classification_loss 4213/10000 [===========>..................] - ETA: 46:54 - loss: 2.8826 - regression_loss: 2.1183 - classification_loss 4214/10000 [===========>..................] - ETA: 46:54 - loss: 2.8823 - regression_loss: 2.1181 - classification_loss 4215/10000 [===========>..................] - ETA: 46:53 - loss: 2.8823 - regression_loss: 2.1181 - classification_loss 4216/10000 [===========>..................] - ETA: 46:53 - loss: 2.8822 - regression_loss: 2.1180 - classification_loss 4217/10000 [===========>..................] - ETA: 46:52 - loss: 2.8821 - regression_loss: 2.1179 - classification_loss 4218/10000 [===========>..................] - ETA: 46:52 - loss: 2.8819 - regression_loss: 2.1178 - classification_loss 4219/10000 [===========>..................] - ETA: 46:51 - loss: 2.8818 - regression_loss: 2.1178 - classification_loss 4220/10000 [===========>..................] - ETA: 46:51 - loss: 2.8816 - regression_loss: 2.1177 - classification_loss 4221/10000 [===========>..................] - ETA: 46:50 - loss: 2.8816 - regression_loss: 2.1177 - classification_loss 4222/10000 [===========>..................] - ETA: 46:50 - loss: 2.8813 - regression_loss: 2.1174 - classification_loss 4223/10000 [===========>..................] - ETA: 46:49 - loss: 2.8813 - regression_loss: 2.1174 - classification_loss 4224/10000 [===========>..................] - ETA: 46:49 - loss: 2.8811 - regression_loss: 2.1172 - classification_loss 4225/10000 [===========>..................] - ETA: 46:48 - loss: 2.8809 - regression_loss: 2.1171 - classification_loss 4226/10000 [===========>..................] - ETA: 46:48 - loss: 2.8809 - regression_loss: 2.1171 - classification_loss 4227/10000 [===========>..................] - ETA: 46:47 - loss: 2.8807 - regression_loss: 2.1170 - classification_loss 4228/10000 [===========>..................] - ETA: 46:46 - loss: 2.8809 - regression_loss: 2.1171 - classification_loss 4229/10000 [===========>..................] - ETA: 46:46 - loss: 2.8809 - regression_loss: 2.1171 - classification_loss 4230/10000 [===========>..................] - ETA: 46:45 - loss: 2.8809 - regression_loss: 2.1171 - classification_loss 4231/10000 [===========>..................] - ETA: 46:45 - loss: 2.8808 - regression_loss: 2.1170 - classification_loss 4232/10000 [===========>..................] - ETA: 46:44 - loss: 2.8808 - regression_loss: 2.1170 - classification_loss 4233/10000 [===========>..................] - ETA: 46:44 - loss: 2.8807 - regression_loss: 2.1169 - classification_loss 4234/10000 [===========>..................] - ETA: 46:43 - loss: 2.8808 - regression_loss: 2.1170 - classification_loss 4235/10000 [===========>..................] - ETA: 46:43 - loss: 2.8806 - regression_loss: 2.1168 - classification_loss 4236/10000 [===========>..................] - ETA: 46:42 - loss: 2.8803 - regression_loss: 2.1166 - classification_loss 4237/10000 [===========>..................] - ETA: 46:42 - loss: 2.8804 - regression_loss: 2.1168 - classification_loss 4238/10000 [===========>..................] - ETA: 46:41 - loss: 2.8803 - regression_loss: 2.1167 - classification_loss 4239/10000 [===========>..................] - ETA: 46:41 - loss: 2.8801 - regression_loss: 2.1166 - classification_loss 4240/10000 [===========>..................] - ETA: 46:40 - loss: 2.8800 - regression_loss: 2.1166 - classification_loss 4241/10000 [===========>..................] - ETA: 46:40 - loss: 2.8799 - regression_loss: 2.1165 - classification_loss 4242/10000 [===========>..................] - ETA: 46:39 - loss: 2.8798 - regression_loss: 2.1165 - classification_loss 4243/10000 [===========>..................] - ETA: 46:38 - loss: 2.8797 - regression_loss: 2.1165 - classification_loss 4244/10000 [===========>..................] - ETA: 46:38 - loss: 2.8798 - regression_loss: 2.1165 - classification_loss 4245/10000 [===========>..................] - ETA: 46:37 - loss: 2.8797 - regression_loss: 2.1165 - classification_loss 4246/10000 [===========>..................] - ETA: 46:37 - loss: 2.8796 - regression_loss: 2.1164 - classification_loss 4247/10000 [===========>..................] - ETA: 46:36 - loss: 2.8796 - regression_loss: 2.1164 - classification_loss 4248/10000 [===========>..................] - ETA: 46:36 - loss: 2.8792 - regression_loss: 2.1161 - classification_loss 4249/10000 [===========>..................] - ETA: 46:35 - loss: 2.8792 - regression_loss: 2.1161 - classification_loss 4250/10000 [===========>..................] - ETA: 46:35 - loss: 2.8791 - regression_loss: 2.1160 - classification_loss 4251/10000 [===========>..................] - ETA: 46:34 - loss: 2.8790 - regression_loss: 2.1159 - classification_loss 4252/10000 [===========>..................] - ETA: 46:34 - loss: 2.8790 - regression_loss: 2.1159 - classification_loss 4253/10000 [===========>..................] - ETA: 46:33 - loss: 2.8787 - regression_loss: 2.1158 - classification_loss 4254/10000 [===========>..................] - ETA: 46:33 - loss: 2.8786 - regression_loss: 2.1158 - classification_loss 4255/10000 [===========>..................] - ETA: 46:32 - loss: 2.8783 - regression_loss: 2.1155 - classification_loss 4256/10000 [===========>..................] - ETA: 46:32 - loss: 2.8783 - regression_loss: 2.1155 - classification_loss 4257/10000 [===========>..................] - ETA: 46:31 - loss: 2.8780 - regression_loss: 2.1154 - classification_loss 4258/10000 [===========>..................] - ETA: 46:31 - loss: 2.8780 - regression_loss: 2.1153 - classification_loss 4259/10000 [===========>..................] - ETA: 46:30 - loss: 2.8778 - regression_loss: 2.1152 - classification_loss 4260/10000 [===========>..................] - ETA: 46:30 - loss: 2.8779 - regression_loss: 2.1153 - classification_loss 4261/10000 [===========>..................] - ETA: 46:29 - loss: 2.8778 - regression_loss: 2.1153 - classification_loss 4262/10000 [===========>..................] - ETA: 46:29 - loss: 2.8775 - regression_loss: 2.1150 - classification_loss 4263/10000 [===========>..................] - ETA: 46:28 - loss: 2.8776 - regression_loss: 2.1150 - classification_loss 4264/10000 [===========>..................] - ETA: 46:28 - loss: 2.8776 - regression_loss: 2.1151 - classification_loss 4265/10000 [===========>..................] - ETA: 46:27 - loss: 2.8774 - regression_loss: 2.1149 - classification_loss 4266/10000 [===========>..................] - ETA: 46:27 - loss: 2.8773 - regression_loss: 2.1149 - classification_loss 4267/10000 [===========>..................] - ETA: 46:26 - loss: 2.8773 - regression_loss: 2.1149 - classification_loss 4268/10000 [===========>..................] - ETA: 46:26 - loss: 2.8772 - regression_loss: 2.1149 - classification_loss 4269/10000 [===========>..................] - ETA: 46:25 - loss: 2.8770 - regression_loss: 2.1147 - classification_loss 4270/10000 [===========>..................] - ETA: 46:25 - loss: 2.8770 - regression_loss: 2.1147 - classification_loss 4271/10000 [===========>..................] - ETA: 46:24 - loss: 2.8767 - regression_loss: 2.1145 - classification_loss 4272/10000 [===========>..................] - ETA: 46:24 - loss: 2.8765 - regression_loss: 2.1144 - classification_loss 4273/10000 [===========>..................] - ETA: 46:23 - loss: 2.8767 - regression_loss: 2.1146 - classification_loss 4274/10000 [===========>..................] - ETA: 46:23 - loss: 2.8766 - regression_loss: 2.1145 - classification_loss 4275/10000 [===========>..................] - ETA: 46:22 - loss: 2.8765 - regression_loss: 2.1145 - classification_loss 4276/10000 [===========>..................] - ETA: 46:22 - loss: 2.8763 - regression_loss: 2.1143 - classification_loss 4277/10000 [===========>..................] - ETA: 46:21 - loss: 2.8761 - regression_loss: 2.1141 - classification_loss 4278/10000 [===========>..................] - ETA: 46:21 - loss: 2.8761 - regression_loss: 2.1142 - classification_loss 4279/10000 [===========>..................] - ETA: 46:20 - loss: 2.8760 - regression_loss: 2.1141 - classification_loss 4280/10000 [===========>..................] - ETA: 46:20 - loss: 2.8757 - regression_loss: 2.1139 - classification_loss 4281/10000 [===========>..................] - ETA: 46:19 - loss: 2.8758 - regression_loss: 2.1140 - classification_loss 4282/10000 [===========>..................] - ETA: 46:19 - loss: 2.8756 - regression_loss: 2.1139 - classification_loss 4283/10000 [===========>..................] - ETA: 46:18 - loss: 2.8756 - regression_loss: 2.1139 - classification_loss 4284/10000 [===========>..................] - ETA: 46:18 - loss: 2.8755 - regression_loss: 2.1138 - classification_loss 4285/10000 [===========>..................] - ETA: 46:17 - loss: 2.8752 - regression_loss: 2.1137 - classification_loss 4286/10000 [===========>..................] - ETA: 46:17 - loss: 2.8753 - regression_loss: 2.1137 - classification_loss 4287/10000 [===========>..................] - ETA: 46:16 - loss: 2.8750 - regression_loss: 2.1135 - classification_loss 4288/10000 [===========>..................] - ETA: 46:16 - loss: 2.8751 - regression_loss: 2.1136 - classification_loss 4289/10000 [===========>..................] - ETA: 46:15 - loss: 2.8749 - regression_loss: 2.1135 - classification_loss 4290/10000 [===========>..................] - ETA: 46:15 - loss: 2.8750 - regression_loss: 2.1135 - classification_loss 4291/10000 [===========>..................] - ETA: 46:14 - loss: 2.8748 - regression_loss: 2.1134 - classification_loss 4292/10000 [===========>..................] - ETA: 46:14 - loss: 2.8747 - regression_loss: 2.1133 - classification_loss 4293/10000 [===========>..................] - ETA: 46:13 - loss: 2.8745 - regression_loss: 2.1132 - classification_loss 4294/10000 [===========>..................] - ETA: 46:13 - loss: 2.8745 - regression_loss: 2.1132 - classification_loss 4295/10000 [===========>..................] - ETA: 46:12 - loss: 2.8744 - regression_loss: 2.1132 - classification_loss 4296/10000 [===========>..................] - ETA: 46:11 - loss: 2.8740 - regression_loss: 2.1128 - classification_loss 4297/10000 [===========>..................] - ETA: 46:11 - loss: 2.8738 - regression_loss: 2.1127 - classification_loss 4298/10000 [===========>..................] - ETA: 46:10 - loss: 2.8738 - regression_loss: 2.1128 - classification_loss 4299/10000 [===========>..................] - ETA: 46:10 - loss: 2.8736 - regression_loss: 2.1126 - classification_loss 4300/10000 [===========>..................] - ETA: 46:09 - loss: 2.8736 - regression_loss: 2.1126 - classification_loss 4301/10000 [===========>..................] - ETA: 46:09 - loss: 2.8735 - regression_loss: 2.1126 - classification_loss 4302/10000 [===========>..................] - ETA: 46:08 - loss: 2.8736 - regression_loss: 2.1126 - classification_loss 4303/10000 [===========>..................] - ETA: 46:08 - loss: 2.8735 - regression_loss: 2.1126 - classification_loss 4304/10000 [===========>..................] - ETA: 46:07 - loss: 2.8735 - regression_loss: 2.1125 - classification_loss 4305/10000 [===========>..................] - ETA: 46:07 - loss: 2.8733 - regression_loss: 2.1120 - classification_loss 4306/10000 [===========>..................] - ETA: 46:06 - loss: 2.8732 - regression_loss: 2.1120 - classification_loss 4307/10000 [===========>..................] - ETA: 46:06 - loss: 2.8732 - regression_loss: 2.1120 - classification_loss 4308/10000 [===========>..................] - ETA: 46:05 - loss: 2.8731 - regression_loss: 2.1120 - classification_loss 4309/10000 [===========>..................] - ETA: 46:04 - loss: 2.8728 - regression_loss: 2.1118 - classification_loss 4310/10000 [===========>..................] - ETA: 46:04 - loss: 2.8726 - regression_loss: 2.1116 - classification_loss 4311/10000 [===========>..................] - ETA: 46:04 - loss: 2.8724 - regression_loss: 2.1115 - classification_loss 4312/10000 [===========>..................] - ETA: 46:03 - loss: 2.8723 - regression_loss: 2.1114 - classification_loss 4313/10000 [===========>..................] - ETA: 46:02 - loss: 2.8719 - regression_loss: 2.1111 - classification_loss 4314/10000 [===========>..................] - ETA: 46:02 - loss: 2.8716 - regression_loss: 2.1109 - classification_loss 4315/10000 [===========>..................] - ETA: 46:02 - loss: 2.8715 - regression_loss: 2.1108 - classification_loss 4316/10000 [===========>..................] - ETA: 46:01 - loss: 2.8714 - regression_loss: 2.1107 - classification_loss 4317/10000 [===========>..................] - ETA: 46:01 - loss: 2.8713 - regression_loss: 2.1106 - classification_loss 4318/10000 [===========>..................] - ETA: 46:00 - loss: 2.8711 - regression_loss: 2.1105 - classification_loss 4319/10000 [===========>..................] - ETA: 45:59 - loss: 2.8711 - regression_loss: 2.1106 - classification_loss 4320/10000 [===========>..................] - ETA: 45:59 - loss: 2.8709 - regression_loss: 2.1103 - classification_loss 4321/10000 [===========>..................] - ETA: 45:58 - loss: 2.8707 - regression_loss: 2.1103 - classification_loss 4322/10000 [===========>..................] - ETA: 45:58 - loss: 2.8709 - regression_loss: 2.1104 - classification_loss 4323/10000 [===========>..................] - ETA: 45:58 - loss: 2.8707 - regression_loss: 2.1102 - classification_loss 4324/10000 [===========>..................] - ETA: 45:57 - loss: 2.8706 - regression_loss: 2.1101 - classification_loss 4325/10000 [===========>..................] - ETA: 45:57 - loss: 2.8705 - regression_loss: 2.1100 - classification_loss 4326/10000 [===========>..................] - ETA: 45:56 - loss: 2.8704 - regression_loss: 2.1099 - classification_loss 4327/10000 [===========>..................] - ETA: 45:56 - loss: 2.8705 - regression_loss: 2.1101 - classification_loss 4328/10000 [===========>..................] - ETA: 45:55 - loss: 2.8703 - regression_loss: 2.1099 - classification_loss 4329/10000 [===========>..................] - ETA: 45:54 - loss: 2.8701 - regression_loss: 2.1098 - classification_loss 4330/10000 [===========>..................] - ETA: 45:54 - loss: 2.8701 - regression_loss: 2.1098 - classification_loss 4331/10000 [===========>..................] - ETA: 45:53 - loss: 2.8698 - regression_loss: 2.1097 - classification_loss 4332/10000 [===========>..................] - ETA: 45:53 - loss: 2.8696 - regression_loss: 2.1095 - classification_loss 4333/10000 [===========>..................] - ETA: 45:52 - loss: 2.8696 - regression_loss: 2.1095 - classification_loss 4334/10000 [============>.................] - ETA: 45:52 - loss: 2.8694 - regression_loss: 2.1094 - classification_loss 4335/10000 [============>.................] - ETA: 45:51 - loss: 2.8692 - regression_loss: 2.1092 - classification_loss 4336/10000 [============>.................] - ETA: 45:51 - loss: 2.8690 - regression_loss: 2.1091 - classification_loss 4337/10000 [============>.................] - ETA: 45:50 - loss: 2.8687 - regression_loss: 2.1089 - classification_loss 4338/10000 [============>.................] - ETA: 45:50 - loss: 2.8685 - regression_loss: 2.1088 - classification_loss 4339/10000 [============>.................] - ETA: 45:49 - loss: 2.8683 - regression_loss: 2.1085 - classification_loss 4340/10000 [============>.................] - ETA: 45:49 - loss: 2.8681 - regression_loss: 2.1084 - classification_loss 4341/10000 [============>.................] - ETA: 45:48 - loss: 2.8679 - regression_loss: 2.1083 - classification_loss 4342/10000 [============>.................] - ETA: 45:48 - loss: 2.8680 - regression_loss: 2.1085 - classification_loss 4343/10000 [============>.................] - ETA: 45:47 - loss: 2.8678 - regression_loss: 2.1083 - classification_loss 4344/10000 [============>.................] - ETA: 45:47 - loss: 2.8675 - regression_loss: 2.1080 - classification_loss 4345/10000 [============>.................] - ETA: 45:46 - loss: 2.8674 - regression_loss: 2.1080 - classification_loss 4346/10000 [============>.................] - ETA: 45:46 - loss: 2.8675 - regression_loss: 2.1081 - classification_loss 4347/10000 [============>.................] - ETA: 45:45 - loss: 2.8673 - regression_loss: 2.1079 - classification_loss 4348/10000 [============>.................] - ETA: 45:45 - loss: 2.8671 - regression_loss: 2.1078 - classification_loss 4349/10000 [============>.................] - ETA: 45:44 - loss: 2.8671 - regression_loss: 2.1078 - classification_loss 4350/10000 [============>.................] - ETA: 45:44 - loss: 2.8668 - regression_loss: 2.1076 - classification_loss 4351/10000 [============>.................] - ETA: 45:43 - loss: 2.8668 - regression_loss: 2.1076 - classification_loss 4352/10000 [============>.................] - ETA: 45:43 - loss: 2.8665 - regression_loss: 2.1074 - classification_loss 4353/10000 [============>.................] - ETA: 45:42 - loss: 2.8661 - regression_loss: 2.1071 - classification_loss 4354/10000 [============>.................] - ETA: 45:42 - loss: 2.8659 - regression_loss: 2.1069 - classification_loss 4355/10000 [============>.................] - ETA: 45:41 - loss: 2.8658 - regression_loss: 2.1069 - classification_loss 4356/10000 [============>.................] - ETA: 45:41 - loss: 2.8659 - regression_loss: 2.1070 - classification_loss 4357/10000 [============>.................] - ETA: 45:40 - loss: 2.8658 - regression_loss: 2.1070 - classification_loss 4358/10000 [============>.................] - ETA: 45:40 - loss: 2.8657 - regression_loss: 2.1069 - classification_loss 4359/10000 [============>.................] - ETA: 45:39 - loss: 2.8656 - regression_loss: 2.1068 - classification_loss 4360/10000 [============>.................] - ETA: 45:39 - loss: 2.8653 - regression_loss: 2.1066 - classification_loss 4361/10000 [============>.................] - ETA: 45:38 - loss: 2.8650 - regression_loss: 2.1061 - classification_loss 4362/10000 [============>.................] - ETA: 45:38 - loss: 2.8650 - regression_loss: 2.1061 - classification_loss 4363/10000 [============>.................] - ETA: 45:37 - loss: 2.8647 - regression_loss: 2.1059 - classification_loss 4364/10000 [============>.................] - ETA: 45:37 - loss: 2.8647 - regression_loss: 2.1059 - classification_loss 4365/10000 [============>.................] - ETA: 45:36 - loss: 2.8644 - regression_loss: 2.1057 - classification_loss 4366/10000 [============>.................] - ETA: 45:36 - loss: 2.8641 - regression_loss: 2.1055 - classification_loss 4367/10000 [============>.................] - ETA: 45:35 - loss: 2.8640 - regression_loss: 2.1055 - classification_loss 4368/10000 [============>.................] - ETA: 45:35 - loss: 2.8640 - regression_loss: 2.1055 - classification_loss 4369/10000 [============>.................] - ETA: 45:34 - loss: 2.8641 - regression_loss: 2.1055 - classification_loss 4370/10000 [============>.................] - ETA: 45:33 - loss: 2.8638 - regression_loss: 2.1053 - classification_loss 4371/10000 [============>.................] - ETA: 45:33 - loss: 2.8637 - regression_loss: 2.1054 - classification_loss 4372/10000 [============>.................] - ETA: 45:32 - loss: 2.8637 - regression_loss: 2.1054 - classification_loss 4373/10000 [============>.................] - ETA: 45:32 - loss: 2.8636 - regression_loss: 2.1053 - classification_loss 4374/10000 [============>.................] - ETA: 45:31 - loss: 2.8635 - regression_loss: 2.1053 - classification_loss 4375/10000 [============>.................] - ETA: 45:31 - loss: 2.8633 - regression_loss: 2.1052 - classification_loss 4376/10000 [============>.................] - ETA: 45:30 - loss: 2.8631 - regression_loss: 2.1050 - classification_loss 4377/10000 [============>.................] - ETA: 45:30 - loss: 2.8630 - regression_loss: 2.1049 - classification_loss 4378/10000 [============>.................] - ETA: 45:29 - loss: 2.8627 - regression_loss: 2.1047 - classification_loss 4379/10000 [============>.................] - ETA: 45:29 - loss: 2.8627 - regression_loss: 2.1047 - classification_loss 4380/10000 [============>.................] - ETA: 45:28 - loss: 2.8624 - regression_loss: 2.1045 - classification_loss 4381/10000 [============>.................] - ETA: 45:28 - loss: 2.8623 - regression_loss: 2.1044 - classification_loss 4382/10000 [============>.................] - ETA: 45:27 - loss: 2.8625 - regression_loss: 2.1046 - classification_loss 4383/10000 [============>.................] - ETA: 45:27 - loss: 2.8623 - regression_loss: 2.1045 - classification_loss 4384/10000 [============>.................] - ETA: 45:26 - loss: 2.8621 - regression_loss: 2.1044 - classification_loss 4385/10000 [============>.................] - ETA: 45:26 - loss: 2.8620 - regression_loss: 2.1042 - classification_loss 4386/10000 [============>.................] - ETA: 45:25 - loss: 2.8619 - regression_loss: 2.1041 - classification_loss 4387/10000 [============>.................] - ETA: 45:25 - loss: 2.8617 - regression_loss: 2.1040 - classification_loss 4388/10000 [============>.................] - ETA: 45:24 - loss: 2.8618 - regression_loss: 2.1039 - classification_loss 4389/10000 [============>.................] - ETA: 45:24 - loss: 2.8617 - regression_loss: 2.1038 - classification_loss 4390/10000 [============>.................] - ETA: 45:23 - loss: 2.8614 - regression_loss: 2.1035 - classification_loss 4391/10000 [============>.................] - ETA: 45:23 - loss: 2.8612 - regression_loss: 2.1033 - classification_loss 4392/10000 [============>.................] - ETA: 45:22 - loss: 2.8608 - regression_loss: 2.1030 - classification_loss 4393/10000 [============>.................] - ETA: 45:21 - loss: 2.8607 - regression_loss: 2.1030 - classification_loss 4394/10000 [============>.................] - ETA: 45:21 - loss: 2.8607 - regression_loss: 2.1030 - classification_loss 4395/10000 [============>.................] - ETA: 45:20 - loss: 2.8604 - regression_loss: 2.1028 - classification_loss 4396/10000 [============>.................] - ETA: 45:20 - loss: 2.8602 - regression_loss: 2.1027 - classification_loss 4397/10000 [============>.................] - ETA: 45:19 - loss: 2.8601 - regression_loss: 2.1027 - classification_loss 4398/10000 [============>.................] - ETA: 45:19 - loss: 2.8602 - regression_loss: 2.1027 - classification_loss 4399/10000 [============>.................] - ETA: 45:18 - loss: 2.8600 - regression_loss: 2.1026 - classification_loss 4400/10000 [============>.................] - ETA: 45:18 - loss: 2.8600 - regression_loss: 2.1026 - classification_loss 4401/10000 [============>.................] - ETA: 45:17 - loss: 2.8600 - regression_loss: 2.1027 - classification_loss 4402/10000 [============>.................] - ETA: 45:17 - loss: 2.8599 - regression_loss: 2.1026 - classification_loss 4403/10000 [============>.................] - ETA: 45:16 - loss: 2.8596 - regression_loss: 2.1024 - classification_loss 4404/10000 [============>.................] - ETA: 45:16 - loss: 2.8594 - regression_loss: 2.1022 - classification_loss 4405/10000 [============>.................] - ETA: 45:15 - loss: 2.8592 - regression_loss: 2.1021 - classification_loss 4406/10000 [============>.................] - ETA: 45:15 - loss: 2.8591 - regression_loss: 2.1019 - classification_loss 4407/10000 [============>.................] - ETA: 45:14 - loss: 2.8590 - regression_loss: 2.1019 - classification_loss 4408/10000 [============>.................] - ETA: 45:14 - loss: 2.8590 - regression_loss: 2.1020 - classification_loss 4409/10000 [============>.................] - ETA: 45:13 - loss: 2.8588 - regression_loss: 2.1018 - classification_loss 4410/10000 [============>.................] - ETA: 45:13 - loss: 2.8587 - regression_loss: 2.1018 - classification_loss 4411/10000 [============>.................] - ETA: 45:12 - loss: 2.8584 - regression_loss: 2.1015 - classification_loss 4412/10000 [============>.................] - ETA: 45:12 - loss: 2.8584 - regression_loss: 2.1016 - classification_loss 4413/10000 [============>.................] - ETA: 45:11 - loss: 2.8584 - regression_loss: 2.1015 - classification_loss 4414/10000 [============>.................] - ETA: 45:11 - loss: 2.8585 - regression_loss: 2.1015 - classification_loss 4415/10000 [============>.................] - ETA: 45:10 - loss: 2.8582 - regression_loss: 2.1013 - classification_loss 4416/10000 [============>.................] - ETA: 45:10 - loss: 2.8581 - regression_loss: 2.1013 - classification_loss 4417/10000 [============>.................] - ETA: 45:09 - loss: 2.8581 - regression_loss: 2.1014 - classification_loss 4418/10000 [============>.................] - ETA: 45:09 - loss: 2.8580 - regression_loss: 2.1013 - classification_loss 4419/10000 [============>.................] - ETA: 45:08 - loss: 2.8578 - regression_loss: 2.1011 - classification_loss 4420/10000 [============>.................] - ETA: 45:08 - loss: 2.8578 - regression_loss: 2.1011 - classification_loss 4421/10000 [============>.................] - ETA: 45:07 - loss: 2.8577 - regression_loss: 2.1010 - classification_loss 4422/10000 [============>.................] - ETA: 45:07 - loss: 2.8576 - regression_loss: 2.1009 - classification_loss 4423/10000 [============>.................] - ETA: 45:06 - loss: 2.8577 - regression_loss: 2.1009 - classification_loss 4424/10000 [============>.................] - ETA: 45:06 - loss: 2.8575 - regression_loss: 2.1008 - classification_loss 4425/10000 [============>.................] - ETA: 45:05 - loss: 2.8571 - regression_loss: 2.1005 - classification_loss 4426/10000 [============>.................] - ETA: 45:05 - loss: 2.8570 - regression_loss: 2.1004 - classification_loss 4427/10000 [============>.................] - ETA: 45:04 - loss: 2.8568 - regression_loss: 2.1003 - classification_loss 4428/10000 [============>.................] - ETA: 45:04 - loss: 2.8569 - regression_loss: 2.1003 - classification_loss 4429/10000 [============>.................] - ETA: 45:03 - loss: 2.8568 - regression_loss: 2.1003 - classification_loss 4430/10000 [============>.................] - ETA: 45:03 - loss: 2.8565 - regression_loss: 2.1001 - classification_loss 4431/10000 [============>.................] - ETA: 45:02 - loss: 2.8565 - regression_loss: 2.1001 - classification_loss 4432/10000 [============>.................] - ETA: 45:02 - loss: 2.8562 - regression_loss: 2.0999 - classification_loss 4433/10000 [============>.................] - ETA: 45:01 - loss: 2.8562 - regression_loss: 2.0999 - classification_loss 4434/10000 [============>.................] - ETA: 45:01 - loss: 2.8563 - regression_loss: 2.1001 - classification_loss 4435/10000 [============>.................] - ETA: 45:00 - loss: 2.8562 - regression_loss: 2.1000 - classification_loss 4436/10000 [============>.................] - ETA: 45:00 - loss: 2.8561 - regression_loss: 2.0999 - classification_loss 4437/10000 [============>.................] - ETA: 44:59 - loss: 2.8561 - regression_loss: 2.1000 - classification_loss 4438/10000 [============>.................] - ETA: 44:59 - loss: 2.8563 - regression_loss: 2.1001 - classification_loss 4439/10000 [============>.................] - ETA: 44:58 - loss: 2.8561 - regression_loss: 2.0999 - classification_loss 4440/10000 [============>.................] - ETA: 44:58 - loss: 2.8560 - regression_loss: 2.0999 - classification_loss 4441/10000 [============>.................] - ETA: 44:57 - loss: 2.8561 - regression_loss: 2.0999 - classification_loss 4442/10000 [============>.................] - ETA: 44:57 - loss: 2.8560 - regression_loss: 2.0999 - classification_loss 4443/10000 [============>.................] - ETA: 44:56 - loss: 2.8559 - regression_loss: 2.0998 - classification_loss 4444/10000 [============>.................] - ETA: 44:56 - loss: 2.8558 - regression_loss: 2.0997 - classification_loss 4445/10000 [============>.................] - ETA: 44:55 - loss: 2.8556 - regression_loss: 2.0996 - classification_loss 4446/10000 [============>.................] - ETA: 44:55 - loss: 2.8555 - regression_loss: 2.0996 - classification_loss 4447/10000 [============>.................] - ETA: 44:54 - loss: 2.8556 - regression_loss: 2.0996 - classification_loss 4448/10000 [============>.................] - ETA: 44:54 - loss: 2.8554 - regression_loss: 2.0996 - classification_loss 4449/10000 [============>.................] - ETA: 44:53 - loss: 2.8553 - regression_loss: 2.0995 - classification_loss 4450/10000 [============>.................] - ETA: 44:53 - loss: 2.8553 - regression_loss: 2.0995 - classification_loss 4451/10000 [============>.................] - ETA: 44:52 - loss: 2.8551 - regression_loss: 2.0994 - classification_loss 4452/10000 [============>.................] - ETA: 44:52 - loss: 2.8549 - regression_loss: 2.0993 - classification_loss 4453/10000 [============>.................] - ETA: 44:51 - loss: 2.8548 - regression_loss: 2.0992 - classification_loss 4454/10000 [============>.................] - ETA: 44:51 - loss: 2.8546 - regression_loss: 2.0990 - classification_loss 4455/10000 [============>.................] - ETA: 44:50 - loss: 2.8544 - regression_loss: 2.0989 - classification_loss 4456/10000 [============>.................] - ETA: 44:50 - loss: 2.8541 - regression_loss: 2.0987 - classification_loss 4457/10000 [============>.................] - ETA: 44:49 - loss: 2.8538 - regression_loss: 2.0984 - classification_loss 4458/10000 [============>.................] - ETA: 44:49 - loss: 2.8538 - regression_loss: 2.0984 - classification_loss 4459/10000 [============>.................] - ETA: 44:48 - loss: 2.8537 - regression_loss: 2.0984 - classification_loss 4460/10000 [============>.................] - ETA: 44:47 - loss: 2.8536 - regression_loss: 2.0984 - classification_loss 4461/10000 [============>.................] - ETA: 44:47 - loss: 2.8535 - regression_loss: 2.0984 - classification_loss 4462/10000 [============>.................] - ETA: 44:46 - loss: 2.8534 - regression_loss: 2.0983 - classification_loss 4463/10000 [============>.................] - ETA: 44:46 - loss: 2.8534 - regression_loss: 2.0983 - classification_loss 4464/10000 [============>.................] - ETA: 44:45 - loss: 2.8533 - regression_loss: 2.0982 - classification_loss 4465/10000 [============>.................] - ETA: 44:45 - loss: 2.8533 - regression_loss: 2.0983 - classification_loss 4466/10000 [============>.................] - ETA: 44:44 - loss: 2.8533 - regression_loss: 2.0982 - classification_loss 4467/10000 [============>.................] - ETA: 44:44 - loss: 2.8530 - regression_loss: 2.0981 - classification_loss 4468/10000 [============>.................] - ETA: 44:43 - loss: 2.8529 - regression_loss: 2.0980 - classification_loss 4469/10000 [============>.................] - ETA: 44:43 - loss: 2.8528 - regression_loss: 2.0979 - classification_loss 4470/10000 [============>.................] - ETA: 44:42 - loss: 2.8529 - regression_loss: 2.0980 - classification_loss 4471/10000 [============>.................] - ETA: 44:42 - loss: 2.8526 - regression_loss: 2.0978 - classification_loss 4472/10000 [============>.................] - ETA: 44:41 - loss: 2.8526 - regression_loss: 2.0978 - classification_loss 4473/10000 [============>.................] - ETA: 44:41 - loss: 2.8524 - regression_loss: 2.0977 - classification_loss 4474/10000 [============>.................] - ETA: 44:40 - loss: 2.8523 - regression_loss: 2.0976 - classification_loss 4475/10000 [============>.................] - ETA: 44:40 - loss: 2.8523 - regression_loss: 2.0977 - classification_loss 4476/10000 [============>.................] - ETA: 44:39 - loss: 2.8523 - regression_loss: 2.0977 - classification_loss 4477/10000 [============>.................] - ETA: 44:39 - loss: 2.8522 - regression_loss: 2.0975 - classification_loss 4478/10000 [============>.................] - ETA: 44:38 - loss: 2.8519 - regression_loss: 2.0973 - classification_loss 4479/10000 [============>.................] - ETA: 44:38 - loss: 2.8518 - regression_loss: 2.0973 - classification_loss 4480/10000 [============>.................] - ETA: 44:37 - loss: 2.8521 - regression_loss: 2.0975 - classification_loss 4481/10000 [============>.................] - ETA: 44:37 - loss: 2.8519 - regression_loss: 2.0975 - classification_loss 4482/10000 [============>.................] - ETA: 44:36 - loss: 2.8517 - regression_loss: 2.0973 - classification_loss 4483/10000 [============>.................] - ETA: 44:36 - loss: 2.8515 - regression_loss: 2.0972 - classification_loss 4484/10000 [============>.................] - ETA: 44:35 - loss: 2.8514 - regression_loss: 2.0971 - classification_loss 4485/10000 [============>.................] - ETA: 44:35 - loss: 2.8514 - regression_loss: 2.0971 - classification_loss 4486/10000 [============>.................] - ETA: 44:34 - loss: 2.8517 - regression_loss: 2.0973 - classification_loss 4487/10000 [============>.................] - ETA: 44:34 - loss: 2.8514 - regression_loss: 2.0971 - classification_loss 4488/10000 [============>.................] - ETA: 44:33 - loss: 2.8514 - regression_loss: 2.0971 - classification_loss 4489/10000 [============>.................] - ETA: 44:33 - loss: 2.8513 - regression_loss: 2.0970 - classification_loss 4490/10000 [============>.................] - ETA: 44:32 - loss: 2.8512 - regression_loss: 2.0970 - classification_loss 4491/10000 [============>.................] - ETA: 44:32 - loss: 2.8510 - regression_loss: 2.0968 - classification_loss 4492/10000 [============>.................] - ETA: 44:31 - loss: 2.8508 - regression_loss: 2.0966 - classification_loss 4493/10000 [============>.................] - ETA: 44:31 - loss: 2.8509 - regression_loss: 2.0967 - classification_loss 4494/10000 [============>.................] - ETA: 44:30 - loss: 2.8506 - regression_loss: 2.0965 - classification_loss 4495/10000 [============>.................] - ETA: 44:30 - loss: 2.8505 - regression_loss: 2.0964 - classification_loss 4496/10000 [============>.................] - ETA: 44:29 - loss: 2.8503 - regression_loss: 2.0963 - classification_loss 4497/10000 [============>.................] - ETA: 44:29 - loss: 2.8500 - regression_loss: 2.0961 - classification_loss 4498/10000 [============>.................] - ETA: 44:28 - loss: 2.8499 - regression_loss: 2.0961 - classification_loss 4499/10000 [============>.................] - ETA: 44:28 - loss: 2.8496 - regression_loss: 2.0958 - classification_loss 4500/10000 [============>.................] - ETA: 44:27 - loss: 2.8494 - regression_loss: 2.0956 - classification_loss 4501/10000 [============>.................] - ETA: 44:27 - loss: 2.8494 - regression_loss: 2.0956 - classification_loss 4502/10000 [============>.................] - ETA: 44:26 - loss: 2.8494 - regression_loss: 2.0957 - classification_loss 4503/10000 [============>.................] - ETA: 44:25 - loss: 2.8490 - regression_loss: 2.0954 - classification_loss 4504/10000 [============>.................] - ETA: 44:25 - loss: 2.8486 - regression_loss: 2.0951 - classification_loss 4505/10000 [============>.................] - ETA: 44:25 - loss: 2.8484 - regression_loss: 2.0950 - classification_loss 4506/10000 [============>.................] - ETA: 44:24 - loss: 2.8481 - regression_loss: 2.0947 - classification_loss 4507/10000 [============>.................] - ETA: 44:24 - loss: 2.8478 - regression_loss: 2.0945 - classification_loss 4508/10000 [============>.................] - ETA: 44:23 - loss: 2.8474 - regression_loss: 2.0942 - classification_loss 4509/10000 [============>.................] - ETA: 44:23 - loss: 2.8474 - regression_loss: 2.0942 - classification_loss 4510/10000 [============>.................] - ETA: 44:22 - loss: 2.8473 - regression_loss: 2.0942 - classification_loss 4511/10000 [============>.................] - ETA: 44:21 - loss: 2.8471 - regression_loss: 2.0940 - classification_loss 4512/10000 [============>.................] - ETA: 44:21 - loss: 2.8469 - regression_loss: 2.0938 - classification_loss 4513/10000 [============>.................] - ETA: 44:20 - loss: 2.8470 - regression_loss: 2.0939 - classification_loss 4514/10000 [============>.................] - ETA: 44:20 - loss: 2.8469 - regression_loss: 2.0938 - classification_loss 4515/10000 [============>.................] - ETA: 44:19 - loss: 2.8468 - regression_loss: 2.0938 - classification_loss 4516/10000 [============>.................] - ETA: 44:19 - loss: 2.8466 - regression_loss: 2.0936 - classification_loss 4517/10000 [============>.................] - ETA: 44:18 - loss: 2.8462 - regression_loss: 2.0934 - classification_loss 4518/10000 [============>.................] - ETA: 44:18 - loss: 2.8463 - regression_loss: 2.0934 - classification_loss 4519/10000 [============>.................] - ETA: 44:17 - loss: 2.8462 - regression_loss: 2.0934 - classification_loss 4520/10000 [============>.................] - ETA: 44:17 - loss: 2.8460 - regression_loss: 2.0932 - classification_loss 4521/10000 [============>.................] - ETA: 44:17 - loss: 2.8460 - regression_loss: 2.0933 - classification_loss 4522/10000 [============>.................] - ETA: 44:16 - loss: 2.8460 - regression_loss: 2.0932 - classification_loss 4523/10000 [============>.................] - ETA: 44:16 - loss: 2.8460 - regression_loss: 2.0932 - classification_loss 4524/10000 [============>.................] - ETA: 44:15 - loss: 2.8457 - regression_loss: 2.0930 - classification_loss 4525/10000 [============>.................] - ETA: 44:15 - loss: 2.8454 - regression_loss: 2.0929 - classification_loss 4526/10000 [============>.................] - ETA: 44:14 - loss: 2.8454 - regression_loss: 2.0928 - classification_loss 4527/10000 [============>.................] - ETA: 44:14 - loss: 2.8453 - regression_loss: 2.0928 - classification_loss 4528/10000 [============>.................] - ETA: 44:13 - loss: 2.8456 - regression_loss: 2.0931 - classification_loss 4529/10000 [============>.................] - ETA: 44:13 - loss: 2.8455 - regression_loss: 2.0930 - classification_loss 4530/10000 [============>.................] - ETA: 44:12 - loss: 2.8457 - regression_loss: 2.0931 - classification_loss 4531/10000 [============>.................] - ETA: 44:12 - loss: 2.8456 - regression_loss: 2.0931 - classification_loss 4532/10000 [============>.................] - ETA: 44:11 - loss: 2.8454 - regression_loss: 2.0930 - classification_loss 4533/10000 [============>.................] - ETA: 44:10 - loss: 2.8454 - regression_loss: 2.0930 - classification_loss 4534/10000 [============>.................] - ETA: 44:10 - loss: 2.8451 - regression_loss: 2.0928 - classification_loss 4535/10000 [============>.................] - ETA: 44:09 - loss: 2.8450 - regression_loss: 2.0927 - classification_loss 4536/10000 [============>.................] - ETA: 44:09 - loss: 2.8450 - regression_loss: 2.0927 - classification_loss 4537/10000 [============>.................] - ETA: 44:08 - loss: 2.8450 - regression_loss: 2.0928 - classification_loss 4538/10000 [============>.................] - ETA: 44:08 - loss: 2.8450 - regression_loss: 2.0928 - classification_loss 4539/10000 [============>.................] - ETA: 44:07 - loss: 2.8447 - regression_loss: 2.0927 - classification_loss 4540/10000 [============>.................] - ETA: 44:07 - loss: 2.8445 - regression_loss: 2.0925 - classification_loss 4541/10000 [============>.................] - ETA: 44:06 - loss: 2.8444 - regression_loss: 2.0924 - classification_loss 4542/10000 [============>.................] - ETA: 44:06 - loss: 2.8444 - regression_loss: 2.0924 - classification_loss 4543/10000 [============>.................] - ETA: 44:05 - loss: 2.8444 - regression_loss: 2.0925 - classification_loss 4544/10000 [============>.................] - ETA: 44:05 - loss: 2.8443 - regression_loss: 2.0924 - classification_loss 4545/10000 [============>.................] - ETA: 44:04 - loss: 2.8441 - regression_loss: 2.0923 - classification_loss 4546/10000 [============>.................] - ETA: 44:04 - loss: 2.8441 - regression_loss: 2.0923 - classification_loss 4547/10000 [============>.................] - ETA: 44:03 - loss: 2.8440 - regression_loss: 2.0923 - classification_loss 4548/10000 [============>.................] - ETA: 44:03 - loss: 2.8439 - regression_loss: 2.0922 - classification_loss 4549/10000 [============>.................] - ETA: 44:02 - loss: 2.8438 - regression_loss: 2.0922 - classification_loss 4550/10000 [============>.................] - ETA: 44:02 - loss: 2.8435 - regression_loss: 2.0920 - classification_loss 4551/10000 [============>.................] - ETA: 44:01 - loss: 2.8435 - regression_loss: 2.0920 - classification_loss 4552/10000 [============>.................] - ETA: 44:01 - loss: 2.8435 - regression_loss: 2.0920 - classification_loss 4553/10000 [============>.................] - ETA: 44:00 - loss: 2.8434 - regression_loss: 2.0919 - classification_loss 4554/10000 [============>.................] - ETA: 44:00 - loss: 2.8434 - regression_loss: 2.0919 - classification_loss 4555/10000 [============>.................] - ETA: 43:59 - loss: 2.8434 - regression_loss: 2.0919 - classification_loss 4556/10000 [============>.................] - ETA: 43:59 - loss: 2.8432 - regression_loss: 2.0918 - classification_loss 4557/10000 [============>.................] - ETA: 43:58 - loss: 2.8432 - regression_loss: 2.0918 - classification_loss 4558/10000 [============>.................] - ETA: 43:57 - loss: 2.8432 - regression_loss: 2.0918 - classification_loss 4559/10000 [============>.................] - ETA: 43:57 - loss: 2.8431 - regression_loss: 2.0917 - classification_loss 4560/10000 [============>.................] - ETA: 43:56 - loss: 2.8431 - regression_loss: 2.0917 - classification_loss 4561/10000 [============>.................] - ETA: 43:56 - loss: 2.8430 - regression_loss: 2.0917 - classification_loss 4562/10000 [============>.................] - ETA: 43:55 - loss: 2.8430 - regression_loss: 2.0917 - classification_loss 4563/10000 [============>.................] - ETA: 43:55 - loss: 2.8428 - regression_loss: 2.0915 - classification_loss 4564/10000 [============>.................] - ETA: 43:54 - loss: 2.8427 - regression_loss: 2.0914 - classification_loss 4565/10000 [============>.................] - ETA: 43:54 - loss: 2.8425 - regression_loss: 2.0913 - classification_loss 4566/10000 [============>.................] - ETA: 43:53 - loss: 2.8427 - regression_loss: 2.0914 - classification_loss 4567/10000 [============>.................] - ETA: 43:53 - loss: 2.8425 - regression_loss: 2.0913 - classification_loss 4568/10000 [============>.................] - ETA: 43:52 - loss: 2.8423 - regression_loss: 2.0912 - classification_loss 4569/10000 [============>.................] - ETA: 43:52 - loss: 2.8422 - regression_loss: 2.0911 - classification_loss 4570/10000 [============>.................] - ETA: 43:51 - loss: 2.8422 - regression_loss: 2.0912 - classification_loss 4571/10000 [============>.................] - ETA: 43:51 - loss: 2.8422 - regression_loss: 2.0912 - classification_loss 4572/10000 [============>.................] - ETA: 43:50 - loss: 2.8420 - regression_loss: 2.0911 - classification_loss 4573/10000 [============>.................] - ETA: 43:50 - loss: 2.8419 - regression_loss: 2.0910 - classification_loss 4574/10000 [============>.................] - ETA: 43:49 - loss: 2.8418 - regression_loss: 2.0909 - classification_loss 4575/10000 [============>.................] - ETA: 43:49 - loss: 2.8417 - regression_loss: 2.0907 - classification_loss 4576/10000 [============>.................] - ETA: 43:49 - loss: 2.8417 - regression_loss: 2.0908 - classification_loss 4577/10000 [============>.................] - ETA: 43:48 - loss: 2.8413 - regression_loss: 2.0905 - classification_loss 4578/10000 [============>.................] - ETA: 43:49 - loss: 2.8414 - regression_loss: 2.0905 - classification_loss 4579/10000 [============>.................] - ETA: 43:48 - loss: 2.8412 - regression_loss: 2.0904 - classification_loss 4580/10000 [============>.................] - ETA: 43:48 - loss: 2.8411 - regression_loss: 2.0903 - classification_loss 4581/10000 [============>.................] - ETA: 43:47 - loss: 2.8411 - regression_loss: 2.0904 - classification_loss 4582/10000 [============>.................] - ETA: 43:47 - loss: 2.8412 - regression_loss: 2.0905 - classification_loss 4583/10000 [============>.................] - ETA: 43:46 - loss: 2.8410 - regression_loss: 2.0903 - classification_loss 4584/10000 [============>.................] - ETA: 43:46 - loss: 2.8410 - regression_loss: 2.0904 - classification_loss 4585/10000 [============>.................] - ETA: 43:45 - loss: 2.8409 - regression_loss: 2.0903 - classification_loss 4586/10000 [============>.................] - ETA: 43:45 - loss: 2.8407 - regression_loss: 2.0902 - classification_loss 4587/10000 [============>.................] - ETA: 43:44 - loss: 2.8405 - regression_loss: 2.0901 - classification_loss 4588/10000 [============>.................] - ETA: 43:44 - loss: 2.8404 - regression_loss: 2.0900 - classification_loss 4589/10000 [============>.................] - ETA: 43:43 - loss: 2.8404 - regression_loss: 2.0900 - classification_loss 4590/10000 [============>.................] - ETA: 43:43 - loss: 2.8403 - regression_loss: 2.0900 - classification_loss 4591/10000 [============>.................] - ETA: 43:42 - loss: 2.8402 - regression_loss: 2.0899 - classification_loss 4592/10000 [============>.................] - ETA: 43:42 - loss: 2.8399 - regression_loss: 2.0897 - classification_loss 4593/10000 [============>.................] - ETA: 43:41 - loss: 2.8397 - regression_loss: 2.0895 - classification_loss 4594/10000 [============>.................] - ETA: 43:40 - loss: 2.8395 - regression_loss: 2.0894 - classification_loss 4595/10000 [============>.................] - ETA: 43:40 - loss: 2.8395 - regression_loss: 2.0894 - classification_loss 4596/10000 [============>.................] - ETA: 43:39 - loss: 2.8393 - regression_loss: 2.0892 - classification_loss 4597/10000 [============>.................] - ETA: 43:39 - loss: 2.8393 - regression_loss: 2.0893 - classification_loss 4598/10000 [============>.................] - ETA: 43:38 - loss: 2.8391 - regression_loss: 2.0891 - classification_loss 4599/10000 [============>.................] - ETA: 43:38 - loss: 2.8391 - regression_loss: 2.0891 - classification_loss 4600/10000 [============>.................] - ETA: 43:37 - loss: 2.8388 - regression_loss: 2.0889 - classification_loss 4601/10000 [============>.................] - ETA: 43:37 - loss: 2.8388 - regression_loss: 2.0889 - classification_loss 4602/10000 [============>.................] - ETA: 43:36 - loss: 2.8386 - regression_loss: 2.0889 - classification_loss 4603/10000 [============>.................] - ETA: 43:36 - loss: 2.8387 - regression_loss: 2.0889 - classification_loss 4604/10000 [============>.................] - ETA: 43:35 - loss: 2.8387 - regression_loss: 2.0889 - classification_loss 4605/10000 [============>.................] - ETA: 43:35 - loss: 2.8386 - regression_loss: 2.0889 - classification_loss 4606/10000 [============>.................] - ETA: 43:34 - loss: 2.8386 - regression_loss: 2.0889 - classification_loss 4607/10000 [============>.................] - ETA: 43:34 - loss: 2.8385 - regression_loss: 2.0889 - classification_loss 4608/10000 [============>.................] - ETA: 43:33 - loss: 2.8383 - regression_loss: 2.0887 - classification_loss 4609/10000 [============>.................] - ETA: 43:33 - loss: 2.8382 - regression_loss: 2.0887 - classification_loss 4610/10000 [============>.................] - ETA: 43:32 - loss: 2.8380 - regression_loss: 2.0885 - classification_loss 4611/10000 [============>.................] - ETA: 43:32 - loss: 2.8378 - regression_loss: 2.0884 - classification_loss 4612/10000 [============>.................] - ETA: 43:31 - loss: 2.8378 - regression_loss: 2.0884 - classification_loss 4613/10000 [============>.................] - ETA: 43:31 - loss: 2.8377 - regression_loss: 2.0883 - classification_loss 4614/10000 [============>.................] - ETA: 43:30 - loss: 2.8375 - regression_loss: 2.0883 - classification_loss 4615/10000 [============>.................] - ETA: 43:30 - loss: 2.8374 - regression_loss: 2.0882 - classification_loss 4616/10000 [============>.................] - ETA: 43:29 - loss: 2.8376 - regression_loss: 2.0883 - classification_loss 4617/10000 [============>.................] - ETA: 43:29 - loss: 2.8374 - regression_loss: 2.0882 - classification_loss 4618/10000 [============>.................] - ETA: 43:28 - loss: 2.8374 - regression_loss: 2.0882 - classification_loss 4619/10000 [============>.................] - ETA: 43:28 - loss: 2.8374 - regression_loss: 2.0882 - classification_loss 4620/10000 [============>.................] - ETA: 43:27 - loss: 2.8374 - regression_loss: 2.0882 - classification_loss 4621/10000 [============>.................] - ETA: 43:27 - loss: 2.8372 - regression_loss: 2.0881 - classification_loss 4622/10000 [============>.................] - ETA: 43:26 - loss: 2.8371 - regression_loss: 2.0880 - classification_loss 4623/10000 [============>.................] - ETA: 43:25 - loss: 2.8370 - regression_loss: 2.0880 - classification_loss 4624/10000 [============>.................] - ETA: 43:25 - loss: 2.8370 - regression_loss: 2.0880 - classification_loss 4625/10000 [============>.................] - ETA: 43:24 - loss: 2.8371 - regression_loss: 2.0881 - classification_loss 4626/10000 [============>.................] - ETA: 43:24 - loss: 2.8370 - regression_loss: 2.0881 - classification_loss 4627/10000 [============>.................] - ETA: 43:23 - loss: 2.8371 - regression_loss: 2.0882 - classification_loss 4628/10000 [============>.................] - ETA: 43:23 - loss: 2.8370 - regression_loss: 2.0882 - classification_loss 4629/10000 [============>.................] - ETA: 43:22 - loss: 2.8369 - regression_loss: 2.0881 - classification_loss 4630/10000 [============>.................] - ETA: 43:22 - loss: 2.8366 - regression_loss: 2.0879 - classification_loss 4631/10000 [============>.................] - ETA: 43:21 - loss: 2.8365 - regression_loss: 2.0877 - classification_loss 4632/10000 [============>.................] - ETA: 43:21 - loss: 2.8365 - regression_loss: 2.0877 - classification_loss 4633/10000 [============>.................] - ETA: 43:20 - loss: 2.8364 - regression_loss: 2.0877 - classification_loss 4634/10000 [============>.................] - ETA: 43:20 - loss: 2.8363 - regression_loss: 2.0877 - classification_loss 4635/10000 [============>.................] - ETA: 43:19 - loss: 2.8362 - regression_loss: 2.0876 - classification_loss 4636/10000 [============>.................] - ETA: 43:19 - loss: 2.8360 - regression_loss: 2.0874 - classification_loss 4637/10000 [============>.................] - ETA: 43:18 - loss: 2.8359 - regression_loss: 2.0873 - classification_loss 4638/10000 [============>.................] - ETA: 43:18 - loss: 2.8359 - regression_loss: 2.0873 - classification_loss 4639/10000 [============>.................] - ETA: 43:17 - loss: 2.8356 - regression_loss: 2.0871 - classification_loss 4640/10000 [============>.................] - ETA: 43:17 - loss: 2.8354 - regression_loss: 2.0869 - classification_loss 4641/10000 [============>.................] - ETA: 43:16 - loss: 2.8352 - regression_loss: 2.0868 - classification_loss 4642/10000 [============>.................] - ETA: 43:16 - loss: 2.8349 - regression_loss: 2.0866 - classification_loss 4643/10000 [============>.................] - ETA: 43:15 - loss: 2.8348 - regression_loss: 2.0864 - classification_loss 4644/10000 [============>.................] - ETA: 43:15 - loss: 2.8348 - regression_loss: 2.0864 - classification_loss 4645/10000 [============>.................] - ETA: 43:14 - loss: 2.8347 - regression_loss: 2.0863 - classification_loss 4646/10000 [============>.................] - ETA: 43:14 - loss: 2.8347 - regression_loss: 2.0864 - classification_loss 4647/10000 [============>.................] - ETA: 43:13 - loss: 2.8347 - regression_loss: 2.0864 - classification_loss 4648/10000 [============>.................] - ETA: 43:13 - loss: 2.8345 - regression_loss: 2.0863 - classification_loss 4649/10000 [============>.................] - ETA: 43:12 - loss: 2.8342 - regression_loss: 2.0860 - classification_loss 4650/10000 [============>.................] - ETA: 43:11 - loss: 2.8340 - regression_loss: 2.0858 - classification_loss 4651/10000 [============>.................] - ETA: 43:11 - loss: 2.8339 - regression_loss: 2.0858 - classification_loss 4652/10000 [============>.................] - ETA: 43:10 - loss: 2.8339 - regression_loss: 2.0858 - classification_loss 4653/10000 [============>.................] - ETA: 43:10 - loss: 2.8337 - regression_loss: 2.0858 - classification_loss 4654/10000 [============>.................] - ETA: 43:10 - loss: 2.8335 - regression_loss: 2.0856 - classification_loss 4655/10000 [============>.................] - ETA: 43:09 - loss: 2.8335 - regression_loss: 2.0856 - classification_loss 4656/10000 [============>.................] - ETA: 43:08 - loss: 2.8334 - regression_loss: 2.0855 - classification_loss 4657/10000 [============>.................] - ETA: 43:08 - loss: 2.8333 - regression_loss: 2.0855 - classification_loss 4658/10000 [============>.................] - ETA: 43:08 - loss: 2.8332 - regression_loss: 2.0854 - classification_loss 4659/10000 [============>.................] - ETA: 43:07 - loss: 2.8331 - regression_loss: 2.0854 - classification_loss 4660/10000 [============>.................] - ETA: 43:06 - loss: 2.8331 - regression_loss: 2.0853 - classification_loss 4661/10000 [============>.................] - ETA: 43:06 - loss: 2.8332 - regression_loss: 2.0854 - classification_loss 4662/10000 [============>.................] - ETA: 43:05 - loss: 2.8328 - regression_loss: 2.0851 - classification_loss 4663/10000 [============>.................] - ETA: 43:05 - loss: 2.8328 - regression_loss: 2.0851 - classification_loss 4664/10000 [============>.................] - ETA: 43:04 - loss: 2.8328 - regression_loss: 2.0852 - classification_loss 4665/10000 [============>.................] - ETA: 43:04 - loss: 2.8325 - regression_loss: 2.0849 - classification_loss 4666/10000 [============>.................] - ETA: 43:03 - loss: 2.8323 - regression_loss: 2.0847 - classification_loss 4667/10000 [=============>................] - ETA: 43:03 - loss: 2.8322 - regression_loss: 2.0846 - classification_loss 4668/10000 [=============>................] - ETA: 43:02 - loss: 2.8322 - regression_loss: 2.0847 - classification_loss 4669/10000 [=============>................] - ETA: 43:02 - loss: 2.8322 - regression_loss: 2.0846 - classification_loss 4670/10000 [=============>................] - ETA: 43:02 - loss: 2.8322 - regression_loss: 2.0847 - classification_loss 4671/10000 [=============>................] - ETA: 43:01 - loss: 2.8321 - regression_loss: 2.0847 - classification_loss 4672/10000 [=============>................] - ETA: 43:01 - loss: 2.8320 - regression_loss: 2.0847 - classification_loss 4673/10000 [=============>................] - ETA: 43:00 - loss: 2.8321 - regression_loss: 2.0847 - classification_loss 4674/10000 [=============>................] - ETA: 43:00 - loss: 2.8319 - regression_loss: 2.0845 - classification_loss 4675/10000 [=============>................] - ETA: 42:59 - loss: 2.8320 - regression_loss: 2.0847 - classification_loss 4676/10000 [=============>................] - ETA: 42:58 - loss: 2.8319 - regression_loss: 2.0846 - classification_loss 4677/10000 [=============>................] - ETA: 42:58 - loss: 2.8317 - regression_loss: 2.0845 - classification_loss 4678/10000 [=============>................] - ETA: 42:58 - loss: 2.8315 - regression_loss: 2.0844 - classification_loss 4679/10000 [=============>................] - ETA: 42:57 - loss: 2.8314 - regression_loss: 2.0843 - classification_loss 4680/10000 [=============>................] - ETA: 42:57 - loss: 2.8312 - regression_loss: 2.0842 - classification_loss 4681/10000 [=============>................] - ETA: 42:56 - loss: 2.8310 - regression_loss: 2.0840 - classification_loss 4682/10000 [=============>................] - ETA: 42:55 - loss: 2.8310 - regression_loss: 2.0841 - classification_loss 4683/10000 [=============>................] - ETA: 42:55 - loss: 2.8311 - regression_loss: 2.0841 - classification_loss 4684/10000 [=============>................] - ETA: 42:55 - loss: 2.8310 - regression_loss: 2.0841 - classification_loss 4685/10000 [=============>................] - ETA: 42:54 - loss: 2.8309 - regression_loss: 2.0840 - classification_loss 4686/10000 [=============>................] - ETA: 42:54 - loss: 2.8307 - regression_loss: 2.0839 - classification_loss 4687/10000 [=============>................] - ETA: 42:53 - loss: 2.8305 - regression_loss: 2.0837 - classification_loss 4688/10000 [=============>................] - ETA: 42:53 - loss: 2.8304 - regression_loss: 2.0837 - classification_loss 4689/10000 [=============>................] - ETA: 42:52 - loss: 2.8303 - regression_loss: 2.0835 - classification_loss 4690/10000 [=============>................] - ETA: 42:51 - loss: 2.8300 - regression_loss: 2.0833 - classification_loss 4691/10000 [=============>................] - ETA: 42:51 - loss: 2.8298 - regression_loss: 2.0831 - classification_loss 4692/10000 [=============>................] - ETA: 42:50 - loss: 2.8298 - regression_loss: 2.0831 - classification_loss 4693/10000 [=============>................] - ETA: 42:50 - loss: 2.8298 - regression_loss: 2.0831 - classification_loss 4694/10000 [=============>................] - ETA: 42:49 - loss: 2.8297 - regression_loss: 2.0830 - classification_loss 4695/10000 [=============>................] - ETA: 42:49 - loss: 2.8296 - regression_loss: 2.0829 - classification_loss 4696/10000 [=============>................] - ETA: 42:48 - loss: 2.8295 - regression_loss: 2.0828 - classification_loss 4697/10000 [=============>................] - ETA: 42:48 - loss: 2.8294 - regression_loss: 2.0827 - classification_loss 4698/10000 [=============>................] - ETA: 42:47 - loss: 2.8293 - regression_loss: 2.0828 - classification_loss 4699/10000 [=============>................] - ETA: 42:47 - loss: 2.8289 - regression_loss: 2.0825 - classification_loss 4700/10000 [=============>................] - ETA: 42:46 - loss: 2.8288 - regression_loss: 2.0823 - classification_loss 4701/10000 [=============>................] - ETA: 42:46 - loss: 2.8287 - regression_loss: 2.0823 - classification_loss 4702/10000 [=============>................] - ETA: 42:45 - loss: 2.8285 - regression_loss: 2.0822 - classification_loss 4703/10000 [=============>................] - ETA: 42:45 - loss: 2.8285 - regression_loss: 2.0822 - classification_loss 4704/10000 [=============>................] - ETA: 42:44 - loss: 2.8283 - regression_loss: 2.0820 - classification_loss 4705/10000 [=============>................] - ETA: 42:44 - loss: 2.8282 - regression_loss: 2.0819 - classification_loss 4706/10000 [=============>................] - ETA: 42:43 - loss: 2.8280 - regression_loss: 2.0818 - classification_loss 4707/10000 [=============>................] - ETA: 42:43 - loss: 2.8278 - regression_loss: 2.0817 - classification_loss 4708/10000 [=============>................] - ETA: 42:42 - loss: 2.8277 - regression_loss: 2.0816 - classification_loss 4709/10000 [=============>................] - ETA: 42:42 - loss: 2.8277 - regression_loss: 2.0816 - classification_loss 4710/10000 [=============>................] - ETA: 42:41 - loss: 2.8276 - regression_loss: 2.0816 - classification_loss 4711/10000 [=============>................] - ETA: 42:41 - loss: 2.8276 - regression_loss: 2.0815 - classification_loss 4712/10000 [=============>................] - ETA: 42:40 - loss: 2.8274 - regression_loss: 2.0814 - classification_loss 4713/10000 [=============>................] - ETA: 42:40 - loss: 2.8274 - regression_loss: 2.0814 - classification_loss 4714/10000 [=============>................] - ETA: 42:39 - loss: 2.8272 - regression_loss: 2.0813 - classification_loss 4715/10000 [=============>................] - ETA: 42:39 - loss: 2.8271 - regression_loss: 2.0812 - classification_loss 4716/10000 [=============>................] - ETA: 42:38 - loss: 2.8269 - regression_loss: 2.0811 - classification_loss 4717/10000 [=============>................] - ETA: 42:38 - loss: 2.8270 - regression_loss: 2.0811 - classification_loss 4718/10000 [=============>................] - ETA: 42:37 - loss: 2.8268 - regression_loss: 2.0810 - classification_loss 4719/10000 [=============>................] - ETA: 42:37 - loss: 2.8267 - regression_loss: 2.0809 - classification_loss 4720/10000 [=============>................] - ETA: 42:36 - loss: 2.8266 - regression_loss: 2.0808 - classification_loss 4721/10000 [=============>................] - ETA: 42:36 - loss: 2.8265 - regression_loss: 2.0808 - classification_loss 4722/10000 [=============>................] - ETA: 42:35 - loss: 2.8264 - regression_loss: 2.0807 - classification_loss 4723/10000 [=============>................] - ETA: 42:35 - loss: 2.8263 - regression_loss: 2.0806 - classification_loss 4724/10000 [=============>................] - ETA: 42:34 - loss: 2.8260 - regression_loss: 2.0804 - classification_loss 4725/10000 [=============>................] - ETA: 42:34 - loss: 2.8259 - regression_loss: 2.0804 - classification_loss 4726/10000 [=============>................] - ETA: 42:33 - loss: 2.8261 - regression_loss: 2.0806 - classification_loss 4727/10000 [=============>................] - ETA: 42:33 - loss: 2.8258 - regression_loss: 2.0804 - classification_loss 4728/10000 [=============>................] - ETA: 42:32 - loss: 2.8257 - regression_loss: 2.0802 - classification_loss 4729/10000 [=============>................] - ETA: 42:32 - loss: 2.8257 - regression_loss: 2.0803 - classification_loss 4730/10000 [=============>................] - ETA: 42:31 - loss: 2.8255 - regression_loss: 2.0802 - classification_loss 4731/10000 [=============>................] - ETA: 42:31 - loss: 2.8255 - regression_loss: 2.0801 - classification_loss 4732/10000 [=============>................] - ETA: 42:30 - loss: 2.8253 - regression_loss: 2.0801 - classification_loss 4733/10000 [=============>................] - ETA: 42:30 - loss: 2.8252 - regression_loss: 2.0800 - classification_loss 4734/10000 [=============>................] - ETA: 42:29 - loss: 2.8254 - regression_loss: 2.0801 - classification_loss 4735/10000 [=============>................] - ETA: 42:29 - loss: 2.8251 - regression_loss: 2.0799 - classification_loss 4736/10000 [=============>................] - ETA: 42:28 - loss: 2.8249 - regression_loss: 2.0797 - classification_loss 4737/10000 [=============>................] - ETA: 42:28 - loss: 2.8250 - regression_loss: 2.0798 - classification_loss 4738/10000 [=============>................] - ETA: 42:27 - loss: 2.8248 - regression_loss: 2.0796 - classification_loss 4739/10000 [=============>................] - ETA: 42:27 - loss: 2.8246 - regression_loss: 2.0795 - classification_loss 4740/10000 [=============>................] - ETA: 42:26 - loss: 2.8245 - regression_loss: 2.0794 - classification_loss 4741/10000 [=============>................] - ETA: 42:25 - loss: 2.8246 - regression_loss: 2.0795 - classification_loss 4742/10000 [=============>................] - ETA: 42:25 - loss: 2.8244 - regression_loss: 2.0793 - classification_loss 4743/10000 [=============>................] - ETA: 42:24 - loss: 2.8242 - regression_loss: 2.0792 - classification_loss 4744/10000 [=============>................] - ETA: 42:24 - loss: 2.8240 - regression_loss: 2.0788 - classification_loss 4745/10000 [=============>................] - ETA: 42:23 - loss: 2.8241 - regression_loss: 2.0788 - classification_loss 4746/10000 [=============>................] - ETA: 42:23 - loss: 2.8240 - regression_loss: 2.0788 - classification_loss 4747/10000 [=============>................] - ETA: 42:22 - loss: 2.8240 - regression_loss: 2.0788 - classification_loss 4748/10000 [=============>................] - ETA: 42:22 - loss: 2.8239 - regression_loss: 2.0788 - classification_loss 4749/10000 [=============>................] - ETA: 42:22 - loss: 2.8237 - regression_loss: 2.0787 - classification_loss 4750/10000 [=============>................] - ETA: 42:21 - loss: 2.8237 - regression_loss: 2.0786 - classification_loss 4751/10000 [=============>................] - ETA: 42:20 - loss: 2.8237 - regression_loss: 2.0787 - classification_loss 4752/10000 [=============>................] - ETA: 42:20 - loss: 2.8235 - regression_loss: 2.0785 - classification_loss 4753/10000 [=============>................] - ETA: 42:19 - loss: 2.8233 - regression_loss: 2.0784 - classification_loss 4754/10000 [=============>................] - ETA: 42:19 - loss: 2.8233 - regression_loss: 2.0784 - classification_loss 4755/10000 [=============>................] - ETA: 42:18 - loss: 2.8232 - regression_loss: 2.0783 - classification_loss 4756/10000 [=============>................] - ETA: 42:18 - loss: 2.8229 - regression_loss: 2.0781 - classification_loss 4757/10000 [=============>................] - ETA: 42:17 - loss: 2.8229 - regression_loss: 2.0781 - classification_loss 4758/10000 [=============>................] - ETA: 42:17 - loss: 2.8227 - regression_loss: 2.0781 - classification_loss 4759/10000 [=============>................] - ETA: 42:16 - loss: 2.8226 - regression_loss: 2.0780 - classification_loss 4760/10000 [=============>................] - ETA: 42:16 - loss: 2.8225 - regression_loss: 2.0779 - classification_loss 4761/10000 [=============>................] - ETA: 42:15 - loss: 2.8226 - regression_loss: 2.0780 - classification_loss 4762/10000 [=============>................] - ETA: 42:15 - loss: 2.8227 - regression_loss: 2.0780 - classification_loss 4763/10000 [=============>................] - ETA: 42:14 - loss: 2.8224 - regression_loss: 2.0777 - classification_loss 4764/10000 [=============>................] - ETA: 42:14 - loss: 2.8222 - regression_loss: 2.0776 - classification_loss 4765/10000 [=============>................] - ETA: 42:13 - loss: 2.8219 - regression_loss: 2.0773 - classification_loss 4766/10000 [=============>................] - ETA: 42:13 - loss: 2.8218 - regression_loss: 2.0773 - classification_loss 4767/10000 [=============>................] - ETA: 42:12 - loss: 2.8216 - regression_loss: 2.0771 - classification_loss 4768/10000 [=============>................] - ETA: 42:12 - loss: 2.8215 - regression_loss: 2.0771 - classification_loss 4769/10000 [=============>................] - ETA: 42:11 - loss: 2.8214 - regression_loss: 2.0771 - classification_loss 4770/10000 [=============>................] - ETA: 42:11 - loss: 2.8214 - regression_loss: 2.0771 - classification_loss 4771/10000 [=============>................] - ETA: 42:10 - loss: 2.8212 - regression_loss: 2.0770 - classification_loss 4772/10000 [=============>................] - ETA: 42:10 - loss: 2.8213 - regression_loss: 2.0771 - classification_loss 4773/10000 [=============>................] - ETA: 42:09 - loss: 2.8210 - regression_loss: 2.0768 - classification_loss 4774/10000 [=============>................] - ETA: 42:09 - loss: 2.8209 - regression_loss: 2.0768 - classification_loss 4775/10000 [=============>................] - ETA: 42:08 - loss: 2.8207 - regression_loss: 2.0766 - classification_loss 4776/10000 [=============>................] - ETA: 42:08 - loss: 2.8207 - regression_loss: 2.0766 - classification_loss 4777/10000 [=============>................] - ETA: 42:07 - loss: 2.8205 - regression_loss: 2.0764 - classification_loss 4778/10000 [=============>................] - ETA: 42:07 - loss: 2.8203 - regression_loss: 2.0762 - classification_loss 4779/10000 [=============>................] - ETA: 42:06 - loss: 2.8200 - regression_loss: 2.0760 - classification_loss 4780/10000 [=============>................] - ETA: 42:06 - loss: 2.8199 - regression_loss: 2.0760 - classification_loss 4781/10000 [=============>................] - ETA: 42:05 - loss: 2.8197 - regression_loss: 2.0759 - classification_loss 4782/10000 [=============>................] - ETA: 42:05 - loss: 2.8196 - regression_loss: 2.0758 - classification_loss 4783/10000 [=============>................] - ETA: 42:04 - loss: 2.8194 - regression_loss: 2.0756 - classification_loss 4784/10000 [=============>................] - ETA: 42:04 - loss: 2.8192 - regression_loss: 2.0755 - classification_loss 4785/10000 [=============>................] - ETA: 42:03 - loss: 2.8194 - regression_loss: 2.0756 - classification_loss 4786/10000 [=============>................] - ETA: 42:03 - loss: 2.8194 - regression_loss: 2.0755 - classification_loss 4787/10000 [=============>................] - ETA: 42:02 - loss: 2.8193 - regression_loss: 2.0755 - classification_loss 4788/10000 [=============>................] - ETA: 42:02 - loss: 2.8191 - regression_loss: 2.0754 - classification_loss 4789/10000 [=============>................] - ETA: 42:01 - loss: 2.8189 - regression_loss: 2.0753 - classification_loss 4790/10000 [=============>................] - ETA: 42:01 - loss: 2.8186 - regression_loss: 2.0750 - classification_loss 4791/10000 [=============>................] - ETA: 42:00 - loss: 2.8184 - regression_loss: 2.0749 - classification_loss 4792/10000 [=============>................] - ETA: 42:00 - loss: 2.8182 - regression_loss: 2.0747 - classification_loss 4793/10000 [=============>................] - ETA: 41:59 - loss: 2.8181 - regression_loss: 2.0747 - classification_loss 4794/10000 [=============>................] - ETA: 41:59 - loss: 2.8180 - regression_loss: 2.0745 - classification_loss 4795/10000 [=============>................] - ETA: 41:58 - loss: 2.8180 - regression_loss: 2.0746 - classification_loss 4796/10000 [=============>................] - ETA: 41:58 - loss: 2.8180 - regression_loss: 2.0746 - classification_loss 4797/10000 [=============>................] - ETA: 41:57 - loss: 2.8180 - regression_loss: 2.0746 - classification_loss 4798/10000 [=============>................] - ETA: 41:57 - loss: 2.8178 - regression_loss: 2.0745 - classification_loss 4799/10000 [=============>................] - ETA: 41:56 - loss: 2.8177 - regression_loss: 2.0744 - classification_loss 4800/10000 [=============>................] - ETA: 41:56 - loss: 2.8176 - regression_loss: 2.0743 - classification_loss 4801/10000 [=============>................] - ETA: 41:55 - loss: 2.8175 - regression_loss: 2.0743 - classification_loss 4802/10000 [=============>................] - ETA: 41:55 - loss: 2.8174 - regression_loss: 2.0742 - classification_loss 4803/10000 [=============>................] - ETA: 41:54 - loss: 2.8171 - regression_loss: 2.0740 - classification_loss 4804/10000 [=============>................] - ETA: 41:54 - loss: 2.8169 - regression_loss: 2.0738 - classification_loss 4805/10000 [=============>................] - ETA: 41:53 - loss: 2.8168 - regression_loss: 2.0737 - classification_loss 4806/10000 [=============>................] - ETA: 41:52 - loss: 2.8166 - regression_loss: 2.0736 - classification_loss 4807/10000 [=============>................] - ETA: 41:52 - loss: 2.8165 - regression_loss: 2.0735 - classification_loss 4808/10000 [=============>................] - ETA: 41:52 - loss: 2.8163 - regression_loss: 2.0734 - classification_loss 4809/10000 [=============>................] - ETA: 41:51 - loss: 2.8163 - regression_loss: 2.0734 - classification_loss 4810/10000 [=============>................] - ETA: 41:51 - loss: 2.8161 - regression_loss: 2.0733 - classification_loss 4811/10000 [=============>................] - ETA: 41:50 - loss: 2.8162 - regression_loss: 2.0733 - classification_loss 4812/10000 [=============>................] - ETA: 41:50 - loss: 2.8160 - regression_loss: 2.0732 - classification_loss 4813/10000 [=============>................] - ETA: 41:49 - loss: 2.8158 - regression_loss: 2.0731 - classification_loss 4814/10000 [=============>................] - ETA: 41:48 - loss: 2.8156 - regression_loss: 2.0730 - classification_loss 4815/10000 [=============>................] - ETA: 41:48 - loss: 2.8157 - regression_loss: 2.0731 - classification_loss 4816/10000 [=============>................] - ETA: 41:47 - loss: 2.8155 - regression_loss: 2.0729 - classification_loss 4817/10000 [=============>................] - ETA: 41:47 - loss: 2.8153 - regression_loss: 2.0728 - classification_loss 4818/10000 [=============>................] - ETA: 41:46 - loss: 2.8152 - regression_loss: 2.0727 - classification_loss 4819/10000 [=============>................] - ETA: 41:46 - loss: 2.8149 - regression_loss: 2.0724 - classification_loss 4820/10000 [=============>................] - ETA: 41:45 - loss: 2.8148 - regression_loss: 2.0723 - classification_loss 4821/10000 [=============>................] - ETA: 41:45 - loss: 2.8148 - regression_loss: 2.0723 - classification_loss 4822/10000 [=============>................] - ETA: 41:44 - loss: 2.8147 - regression_loss: 2.0723 - classification_loss 4823/10000 [=============>................] - ETA: 41:44 - loss: 2.8146 - regression_loss: 2.0723 - classification_loss 4824/10000 [=============>................] - ETA: 41:43 - loss: 2.8144 - regression_loss: 2.0721 - classification_loss 4825/10000 [=============>................] - ETA: 41:43 - loss: 2.8143 - regression_loss: 2.0720 - classification_loss 4826/10000 [=============>................] - ETA: 41:42 - loss: 2.8141 - regression_loss: 2.0718 - classification_loss 4827/10000 [=============>................] - ETA: 41:42 - loss: 2.8140 - regression_loss: 2.0717 - classification_loss 4828/10000 [=============>................] - ETA: 41:41 - loss: 2.8137 - regression_loss: 2.0715 - classification_loss 4829/10000 [=============>................] - ETA: 41:41 - loss: 2.8134 - regression_loss: 2.0713 - classification_loss 4830/10000 [=============>................] - ETA: 41:40 - loss: 2.8135 - regression_loss: 2.0714 - classification_loss 4831/10000 [=============>................] - ETA: 41:40 - loss: 2.8135 - regression_loss: 2.0714 - classification_loss 4832/10000 [=============>................] - ETA: 41:39 - loss: 2.8133 - regression_loss: 2.0713 - classification_loss 4833/10000 [=============>................] - ETA: 41:39 - loss: 2.8130 - regression_loss: 2.0711 - classification_loss 4834/10000 [=============>................] - ETA: 41:38 - loss: 2.8132 - regression_loss: 2.0712 - classification_loss 4835/10000 [=============>................] - ETA: 41:38 - loss: 2.8129 - regression_loss: 2.0711 - classification_loss 4836/10000 [=============>................] - ETA: 41:37 - loss: 2.8128 - regression_loss: 2.0710 - classification_loss 4837/10000 [=============>................] - ETA: 41:37 - loss: 2.8128 - regression_loss: 2.0710 - classification_loss 4838/10000 [=============>................] - ETA: 41:36 - loss: 2.8129 - regression_loss: 2.0711 - classification_loss 4839/10000 [=============>................] - ETA: 41:36 - loss: 2.8127 - regression_loss: 2.0710 - classification_loss 4840/10000 [=============>................] - ETA: 41:35 - loss: 2.8125 - regression_loss: 2.0708 - classification_loss 4841/10000 [=============>................] - ETA: 41:35 - loss: 2.8124 - regression_loss: 2.0708 - classification_loss 4842/10000 [=============>................] - ETA: 41:34 - loss: 2.8121 - regression_loss: 2.0706 - classification_loss 4843/10000 [=============>................] - ETA: 41:34 - loss: 2.8121 - regression_loss: 2.0706 - classification_loss 4844/10000 [=============>................] - ETA: 41:33 - loss: 2.8120 - regression_loss: 2.0705 - classification_loss 4845/10000 [=============>................] - ETA: 41:33 - loss: 2.8120 - regression_loss: 2.0705 - classification_loss 4846/10000 [=============>................] - ETA: 41:32 - loss: 2.8119 - regression_loss: 2.0705 - classification_loss 4847/10000 [=============>................] - ETA: 41:32 - loss: 2.8119 - regression_loss: 2.0705 - classification_loss 4848/10000 [=============>................] - ETA: 41:31 - loss: 2.8117 - regression_loss: 2.0704 - classification_loss 4849/10000 [=============>................] - ETA: 41:31 - loss: 2.8116 - regression_loss: 2.0704 - classification_loss 4850/10000 [=============>................] - ETA: 41:30 - loss: 2.8114 - regression_loss: 2.0702 - classification_loss 4851/10000 [=============>................] - ETA: 41:30 - loss: 2.8112 - regression_loss: 2.0701 - classification_loss 4852/10000 [=============>................] - ETA: 41:29 - loss: 2.8109 - regression_loss: 2.0698 - classification_loss 4853/10000 [=============>................] - ETA: 41:29 - loss: 2.8106 - regression_loss: 2.0696 - classification_loss 4854/10000 [=============>................] - ETA: 41:28 - loss: 2.8103 - regression_loss: 2.0694 - classification_loss 4855/10000 [=============>................] - ETA: 41:28 - loss: 2.8103 - regression_loss: 2.0694 - classification_loss 4856/10000 [=============>................] - ETA: 41:27 - loss: 2.8101 - regression_loss: 2.0693 - classification_loss 4857/10000 [=============>................] - ETA: 41:27 - loss: 2.8100 - regression_loss: 2.0692 - classification_loss 4858/10000 [=============>................] - ETA: 41:26 - loss: 2.8099 - regression_loss: 2.0692 - classification_loss 4859/10000 [=============>................] - ETA: 41:26 - loss: 2.8098 - regression_loss: 2.0691 - classification_loss 4860/10000 [=============>................] - ETA: 41:25 - loss: 2.8098 - regression_loss: 2.0692 - classification_loss 4861/10000 [=============>................] - ETA: 41:25 - loss: 2.8096 - regression_loss: 2.0690 - classification_loss 4862/10000 [=============>................] - ETA: 41:24 - loss: 2.8096 - regression_loss: 2.0690 - classification_loss 4863/10000 [=============>................] - ETA: 41:24 - loss: 2.8095 - regression_loss: 2.0690 - classification_loss 4864/10000 [=============>................] - ETA: 41:23 - loss: 2.8094 - regression_loss: 2.0689 - classification_loss 4865/10000 [=============>................] - ETA: 41:23 - loss: 2.8093 - regression_loss: 2.0689 - classification_loss 4866/10000 [=============>................] - ETA: 41:22 - loss: 2.8093 - regression_loss: 2.0689 - classification_loss 4867/10000 [=============>................] - ETA: 41:22 - loss: 2.8093 - regression_loss: 2.0690 - classification_loss 4868/10000 [=============>................] - ETA: 41:21 - loss: 2.8090 - regression_loss: 2.0688 - classification_loss 4869/10000 [=============>................] - ETA: 41:21 - loss: 2.8089 - regression_loss: 2.0687 - classification_loss 4870/10000 [=============>................] - ETA: 41:20 - loss: 2.8088 - regression_loss: 2.0686 - classification_loss 4871/10000 [=============>................] - ETA: 41:20 - loss: 2.8086 - regression_loss: 2.0684 - classification_loss 4872/10000 [=============>................] - ETA: 41:19 - loss: 2.8088 - regression_loss: 2.0686 - classification_loss 4873/10000 [=============>................] - ETA: 41:19 - loss: 2.8088 - regression_loss: 2.0686 - classification_loss 4874/10000 [=============>................] - ETA: 41:18 - loss: 2.8089 - regression_loss: 2.0687 - classification_loss 4875/10000 [=============>................] - ETA: 41:18 - loss: 2.8089 - regression_loss: 2.0688 - classification_loss 4876/10000 [=============>................] - ETA: 41:17 - loss: 2.8088 - regression_loss: 2.0688 - classification_loss 4877/10000 [=============>................] - ETA: 41:17 - loss: 2.8085 - regression_loss: 2.0686 - classification_loss 4878/10000 [=============>................] - ETA: 41:16 - loss: 2.8083 - regression_loss: 2.0685 - classification_loss 4879/10000 [=============>................] - ETA: 41:16 - loss: 2.8082 - regression_loss: 2.0684 - classification_loss 4880/10000 [=============>................] - ETA: 41:15 - loss: 2.8080 - regression_loss: 2.0682 - classification_loss 4881/10000 [=============>................] - ETA: 41:15 - loss: 2.8078 - regression_loss: 2.0680 - classification_loss 4882/10000 [=============>................] - ETA: 41:14 - loss: 2.8076 - regression_loss: 2.0679 - classification_loss 4883/10000 [=============>................] - ETA: 41:14 - loss: 2.8076 - regression_loss: 2.0679 - classification_loss 4884/10000 [=============>................] - ETA: 41:13 - loss: 2.8074 - regression_loss: 2.0678 - classification_loss 4885/10000 [=============>................] - ETA: 41:13 - loss: 2.8075 - regression_loss: 2.0678 - classification_loss 4886/10000 [=============>................] - ETA: 41:12 - loss: 2.8074 - regression_loss: 2.0677 - classification_loss 4887/10000 [=============>................] - ETA: 41:12 - loss: 2.8072 - regression_loss: 2.0676 - classification_loss 4888/10000 [=============>................] - ETA: 41:11 - loss: 2.8070 - regression_loss: 2.0674 - classification_loss 4889/10000 [=============>................] - ETA: 41:11 - loss: 2.8070 - regression_loss: 2.0674 - classification_loss 4890/10000 [=============>................] - ETA: 41:10 - loss: 2.8067 - regression_loss: 2.0672 - classification_loss 4891/10000 [=============>................] - ETA: 41:10 - loss: 2.8066 - regression_loss: 2.0672 - classification_loss 4892/10000 [=============>................] - ETA: 41:09 - loss: 2.8066 - regression_loss: 2.0672 - classification_loss 4893/10000 [=============>................] - ETA: 41:09 - loss: 2.8066 - regression_loss: 2.0672 - classification_loss 4894/10000 [=============>................] - ETA: 41:08 - loss: 2.8064 - regression_loss: 2.0670 - classification_loss 4895/10000 [=============>................] - ETA: 41:08 - loss: 2.8063 - regression_loss: 2.0670 - classification_loss 4896/10000 [=============>................] - ETA: 41:08 - loss: 2.8061 - regression_loss: 2.0669 - classification_loss 4897/10000 [=============>................] - ETA: 41:07 - loss: 2.8060 - regression_loss: 2.0668 - classification_loss 4898/10000 [=============>................] - ETA: 41:07 - loss: 2.8058 - regression_loss: 2.0666 - classification_loss 4899/10000 [=============>................] - ETA: 41:06 - loss: 2.8056 - regression_loss: 2.0665 - classification_loss 4900/10000 [=============>................] - ETA: 41:06 - loss: 2.8055 - regression_loss: 2.0664 - classification_loss 4901/10000 [=============>................] - ETA: 41:05 - loss: 2.8053 - regression_loss: 2.0663 - classification_loss 4902/10000 [=============>................] - ETA: 41:05 - loss: 2.8052 - regression_loss: 2.0663 - classification_loss 4903/10000 [=============>................] - ETA: 41:04 - loss: 2.8051 - regression_loss: 2.0662 - classification_loss 4904/10000 [=============>................] - ETA: 41:04 - loss: 2.8051 - regression_loss: 2.0662 - classification_loss 4905/10000 [=============>................] - ETA: 41:03 - loss: 2.8048 - regression_loss: 2.0660 - classification_loss 4906/10000 [=============>................] - ETA: 41:03 - loss: 2.8047 - regression_loss: 2.0660 - classification_loss 4907/10000 [=============>................] - ETA: 41:02 - loss: 2.8045 - regression_loss: 2.0658 - classification_loss 4908/10000 [=============>................] - ETA: 41:02 - loss: 2.8044 - regression_loss: 2.0658 - classification_loss 4909/10000 [=============>................] - ETA: 41:01 - loss: 2.8042 - regression_loss: 2.0657 - classification_loss 4910/10000 [=============>................] - ETA: 41:01 - loss: 2.8042 - regression_loss: 2.0657 - classification_loss 4911/10000 [=============>................] - ETA: 41:00 - loss: 2.8042 - regression_loss: 2.0657 - classification_loss 4912/10000 [=============>................] - ETA: 41:00 - loss: 2.8041 - regression_loss: 2.0657 - classification_loss 4913/10000 [=============>................] - ETA: 40:59 - loss: 2.8039 - regression_loss: 2.0655 - classification_loss 4914/10000 [=============>................] - ETA: 40:59 - loss: 2.8038 - regression_loss: 2.0655 - classification_loss 4915/10000 [=============>................] - ETA: 40:58 - loss: 2.8037 - regression_loss: 2.0654 - classification_loss 4916/10000 [=============>................] - ETA: 40:58 - loss: 2.8035 - regression_loss: 2.0653 - classification_loss 4917/10000 [=============>................] - ETA: 40:57 - loss: 2.8035 - regression_loss: 2.0653 - classification_loss 4918/10000 [=============>................] - ETA: 40:57 - loss: 2.8036 - regression_loss: 2.0654 - classification_loss 4919/10000 [=============>................] - ETA: 40:56 - loss: 2.8033 - regression_loss: 2.0652 - classification_loss 4920/10000 [=============>................] - ETA: 40:56 - loss: 2.8032 - regression_loss: 2.0651 - classification_loss 4921/10000 [=============>................] - ETA: 40:55 - loss: 2.8031 - regression_loss: 2.0651 - classification_loss 4922/10000 [=============>................] - ETA: 40:55 - loss: 2.8028 - regression_loss: 2.0649 - classification_loss 4923/10000 [=============>................] - ETA: 40:54 - loss: 2.8027 - regression_loss: 2.0648 - classification_loss 4924/10000 [=============>................] - ETA: 40:54 - loss: 2.8028 - regression_loss: 2.0648 - classification_loss 4925/10000 [=============>................] - ETA: 40:53 - loss: 2.8026 - regression_loss: 2.0647 - classification_loss 4926/10000 [=============>................] - ETA: 40:53 - loss: 2.8024 - regression_loss: 2.0646 - classification_loss 4927/10000 [=============>................] - ETA: 40:52 - loss: 2.8024 - regression_loss: 2.0646 - classification_loss 4928/10000 [=============>................] - ETA: 40:51 - loss: 2.8025 - regression_loss: 2.0646 - classification_loss 4929/10000 [=============>................] - ETA: 40:51 - loss: 2.8025 - regression_loss: 2.0648 - classification_loss 4930/10000 [=============>................] - ETA: 40:50 - loss: 2.8025 - regression_loss: 2.0648 - classification_loss 4931/10000 [=============>................] - ETA: 40:50 - loss: 2.8025 - regression_loss: 2.0647 - classification_loss 4932/10000 [=============>................] - ETA: 40:49 - loss: 2.8024 - regression_loss: 2.0646 - classification_loss 4933/10000 [=============>................] - ETA: 40:49 - loss: 2.8025 - regression_loss: 2.0648 - classification_loss 4934/10000 [=============>................] - ETA: 40:48 - loss: 2.8024 - regression_loss: 2.0647 - classification_loss 4935/10000 [=============>................] - ETA: 40:48 - loss: 2.8026 - regression_loss: 2.0649 - classification_loss 4936/10000 [=============>................] - ETA: 40:47 - loss: 2.8024 - regression_loss: 2.0647 - classification_loss 4937/10000 [=============>................] - ETA: 40:47 - loss: 2.8024 - regression_loss: 2.0647 - classification_loss 4938/10000 [=============>................] - ETA: 40:46 - loss: 2.8024 - regression_loss: 2.0647 - classification_loss 4939/10000 [=============>................] - ETA: 40:46 - loss: 2.8023 - regression_loss: 2.0647 - classification_loss 4940/10000 [=============>................] - ETA: 40:45 - loss: 2.8022 - regression_loss: 2.0647 - classification_loss 4941/10000 [=============>................] - ETA: 40:45 - loss: 2.8020 - regression_loss: 2.0645 - classification_loss 4942/10000 [=============>................] - ETA: 40:44 - loss: 2.8018 - regression_loss: 2.0643 - classification_loss 4943/10000 [=============>................] - ETA: 40:44 - loss: 2.8019 - regression_loss: 2.0644 - classification_loss 4944/10000 [=============>................] - ETA: 40:43 - loss: 2.8017 - regression_loss: 2.0642 - classification_loss 4945/10000 [=============>................] - ETA: 40:43 - loss: 2.8016 - regression_loss: 2.0642 - classification_loss 4946/10000 [=============>................] - ETA: 40:42 - loss: 2.8016 - regression_loss: 2.0642 - classification_loss 4947/10000 [=============>................] - ETA: 40:42 - loss: 2.8016 - regression_loss: 2.0642 - classification_loss 4948/10000 [=============>................] - ETA: 40:41 - loss: 2.8013 - regression_loss: 2.0640 - classification_loss 4949/10000 [=============>................] - ETA: 40:41 - loss: 2.8013 - regression_loss: 2.0640 - classification_loss 4950/10000 [=============>................] - ETA: 40:40 - loss: 2.8013 - regression_loss: 2.0640 - classification_loss 4951/10000 [=============>................] - ETA: 40:40 - loss: 2.8013 - regression_loss: 2.0640 - classification_loss 4952/10000 [=============>................] - ETA: 40:39 - loss: 2.8013 - regression_loss: 2.0641 - classification_loss 4953/10000 [=============>................] - ETA: 40:39 - loss: 2.8013 - regression_loss: 2.0641 - classification_loss 4954/10000 [=============>................] - ETA: 40:38 - loss: 2.8012 - regression_loss: 2.0640 - classification_loss 4955/10000 [=============>................] - ETA: 40:38 - loss: 2.8009 - regression_loss: 2.0638 - classification_loss 4956/10000 [=============>................] - ETA: 40:37 - loss: 2.8008 - regression_loss: 2.0637 - classification_loss 4957/10000 [=============>................] - ETA: 40:37 - loss: 2.8008 - regression_loss: 2.0637 - classification_loss 4958/10000 [=============>................] - ETA: 40:36 - loss: 2.8007 - regression_loss: 2.0636 - classification_loss 4959/10000 [=============>................] - ETA: 40:36 - loss: 2.8005 - regression_loss: 2.0634 - classification_loss 4960/10000 [=============>................] - ETA: 40:35 - loss: 2.8004 - regression_loss: 2.0633 - classification_loss 4961/10000 [=============>................] - ETA: 40:35 - loss: 2.8002 - regression_loss: 2.0632 - classification_loss 4962/10000 [=============>................] - ETA: 40:34 - loss: 2.8000 - regression_loss: 2.0631 - classification_loss 4963/10000 [=============>................] - ETA: 40:34 - loss: 2.8000 - regression_loss: 2.0630 - classification_loss 4964/10000 [=============>................] - ETA: 40:33 - loss: 2.8000 - regression_loss: 2.0630 - classification_loss 4965/10000 [=============>................] - ETA: 40:33 - loss: 2.7999 - regression_loss: 2.0630 - classification_loss 4966/10000 [=============>................] - ETA: 40:32 - loss: 2.7997 - regression_loss: 2.0628 - classification_loss 4967/10000 [=============>................] - ETA: 40:32 - loss: 2.7995 - regression_loss: 2.0627 - classification_loss 4968/10000 [=============>................] - ETA: 40:31 - loss: 2.7994 - regression_loss: 2.0626 - classification_loss 4969/10000 [=============>................] - ETA: 40:31 - loss: 2.7994 - regression_loss: 2.0626 - classification_loss 4970/10000 [=============>................] - ETA: 40:30 - loss: 2.7994 - regression_loss: 2.0626 - classification_loss 4971/10000 [=============>................] - ETA: 40:30 - loss: 2.7994 - regression_loss: 2.0626 - classification_loss 4972/10000 [=============>................] - ETA: 40:29 - loss: 2.7992 - regression_loss: 2.0626 - classification_loss 4973/10000 [=============>................] - ETA: 40:29 - loss: 2.7991 - regression_loss: 2.0625 - classification_loss 4974/10000 [=============>................] - ETA: 40:28 - loss: 2.7990 - regression_loss: 2.0624 - classification_loss 4975/10000 [=============>................] - ETA: 40:28 - loss: 2.7989 - regression_loss: 2.0624 - classification_loss 4976/10000 [=============>................] - ETA: 40:27 - loss: 2.7988 - regression_loss: 2.0623 - classification_loss 4977/10000 [=============>................] - ETA: 40:27 - loss: 2.7985 - regression_loss: 2.0620 - classification_loss 4978/10000 [=============>................] - ETA: 40:26 - loss: 2.7985 - regression_loss: 2.0620 - classification_loss 4979/10000 [=============>................] - ETA: 40:26 - loss: 2.7984 - regression_loss: 2.0620 - classification_loss 4980/10000 [=============>................] - ETA: 40:25 - loss: 2.7985 - regression_loss: 2.0620 - classification_loss 4981/10000 [=============>................] - ETA: 40:25 - loss: 2.7982 - regression_loss: 2.0619 - classification_loss 4982/10000 [=============>................] - ETA: 40:24 - loss: 2.7980 - regression_loss: 2.0617 - classification_loss 4983/10000 [=============>................] - ETA: 40:24 - loss: 2.7980 - regression_loss: 2.0617 - classification_loss 4984/10000 [=============>................] - ETA: 40:23 - loss: 2.7979 - regression_loss: 2.0616 - classification_loss 4985/10000 [=============>................] - ETA: 40:23 - loss: 2.7977 - regression_loss: 2.0614 - classification_loss 4986/10000 [=============>................] - ETA: 40:22 - loss: 2.7977 - regression_loss: 2.0614 - classification_loss 4987/10000 [=============>................] - ETA: 40:22 - loss: 2.7976 - regression_loss: 2.0614 - classification_loss 4988/10000 [=============>................] - ETA: 40:21 - loss: 2.7974 - regression_loss: 2.0612 - classification_loss 4989/10000 [=============>................] - ETA: 40:21 - loss: 2.7973 - regression_loss: 2.0611 - classification_loss 4990/10000 [=============>................] - ETA: 40:20 - loss: 2.7972 - regression_loss: 2.0611 - classification_loss 4991/10000 [=============>................] - ETA: 40:20 - loss: 2.7972 - regression_loss: 2.0612 - classification_loss 4992/10000 [=============>................] - ETA: 40:19 - loss: 2.7970 - regression_loss: 2.0610 - classification_loss 4993/10000 [=============>................] - ETA: 40:19 - loss: 2.7968 - regression_loss: 2.0608 - classification_loss 4994/10000 [=============>................] - ETA: 40:18 - loss: 2.7968 - regression_loss: 2.0608 - classification_loss 4995/10000 [=============>................] - ETA: 40:18 - loss: 2.7968 - regression_loss: 2.0609 - classification_loss 4996/10000 [=============>................] - ETA: 40:17 - loss: 2.7967 - regression_loss: 2.0608 - classification_loss 4997/10000 [=============>................] - ETA: 40:17 - loss: 2.7965 - regression_loss: 2.0607 - classification_loss 4998/10000 [=============>................] - ETA: 40:16 - loss: 2.7963 - regression_loss: 2.0605 - classification_loss 4999/10000 [=============>................] - ETA: 40:16 - loss: 2.7963 - regression_loss: 2.0605 - classification_loss 5000/10000 [==============>...............] - ETA: 40:15 - loss: 2.7962 - regression_loss: 2.0605 - classification_loss 5001/10000 [==============>...............] - ETA: 40:15 - loss: 2.7960 - regression_loss: 2.0603 - classification_loss 5002/10000 [==============>...............] - ETA: 40:14 - loss: 2.7959 - regression_loss: 2.0602 - classification_loss 5003/10000 [==============>...............] - ETA: 40:14 - loss: 2.7957 - regression_loss: 2.0601 - classification_loss 5004/10000 [==============>...............] - ETA: 40:13 - loss: 2.7956 - regression_loss: 2.0600 - classification_loss 5005/10000 [==============>...............] - ETA: 40:13 - loss: 2.7953 - regression_loss: 2.0598 - classification_loss 5006/10000 [==============>...............] - ETA: 40:12 - loss: 2.7953 - regression_loss: 2.0598 - classification_loss 5007/10000 [==============>...............] - ETA: 40:12 - loss: 2.7951 - regression_loss: 2.0597 - classification_loss 5008/10000 [==============>...............] - ETA: 40:11 - loss: 2.7951 - regression_loss: 2.0597 - classification_loss 5009/10000 [==============>...............] - ETA: 40:11 - loss: 2.7948 - regression_loss: 2.0594 - classification_loss 5010/10000 [==============>...............] - ETA: 40:10 - loss: 2.7949 - regression_loss: 2.0595 - classification_loss 5011/10000 [==============>...............] - ETA: 40:10 - loss: 2.7947 - regression_loss: 2.0593 - classification_loss 5012/10000 [==============>...............] - ETA: 40:09 - loss: 2.7946 - regression_loss: 2.0592 - classification_loss 5013/10000 [==============>...............] - ETA: 40:09 - loss: 2.7943 - regression_loss: 2.0590 - classification_loss 5014/10000 [==============>...............] - ETA: 40:08 - loss: 2.7940 - regression_loss: 2.0587 - classification_loss 5015/10000 [==============>...............] - ETA: 40:08 - loss: 2.7937 - regression_loss: 2.0584 - classification_loss 5016/10000 [==============>...............] - ETA: 40:07 - loss: 2.7934 - regression_loss: 2.0583 - classification_loss 5017/10000 [==============>...............] - ETA: 40:07 - loss: 2.7934 - regression_loss: 2.0583 - classification_loss 5018/10000 [==============>...............] - ETA: 40:06 - loss: 2.7931 - regression_loss: 2.0581 - classification_loss 5019/10000 [==============>...............] - ETA: 40:06 - loss: 2.7930 - regression_loss: 2.0580 - classification_loss 5020/10000 [==============>...............] - ETA: 40:05 - loss: 2.7930 - regression_loss: 2.0581 - classification_loss 5021/10000 [==============>...............] - ETA: 40:05 - loss: 2.7928 - regression_loss: 2.0579 - classification_loss 5022/10000 [==============>...............] - ETA: 40:04 - loss: 2.7927 - regression_loss: 2.0577 - classification_loss 5023/10000 [==============>...............] - ETA: 40:04 - loss: 2.7924 - regression_loss: 2.0576 - classification_loss 5024/10000 [==============>...............] - ETA: 40:03 - loss: 2.7925 - regression_loss: 2.0576 - classification_loss 5025/10000 [==============>...............] - ETA: 40:03 - loss: 2.7923 - regression_loss: 2.0575 - classification_loss 5026/10000 [==============>...............] - ETA: 40:02 - loss: 2.7922 - regression_loss: 2.0574 - classification_loss 5027/10000 [==============>...............] - ETA: 40:02 - loss: 2.7920 - regression_loss: 2.0573 - classification_loss 5028/10000 [==============>...............] - ETA: 40:01 - loss: 2.7919 - regression_loss: 2.0572 - classification_loss 5029/10000 [==============>...............] - ETA: 40:01 - loss: 2.7916 - regression_loss: 2.0570 - classification_loss 5030/10000 [==============>...............] - ETA: 40:00 - loss: 2.7916 - regression_loss: 2.0570 - classification_loss 5031/10000 [==============>...............] - ETA: 40:00 - loss: 2.7914 - regression_loss: 2.0569 - classification_loss 5032/10000 [==============>...............] - ETA: 39:59 - loss: 2.7912 - regression_loss: 2.0567 - classification_loss 5033/10000 [==============>...............] - ETA: 39:59 - loss: 2.7911 - regression_loss: 2.0567 - classification_loss 5034/10000 [==============>...............] - ETA: 39:58 - loss: 2.7910 - regression_loss: 2.0566 - classification_loss 5035/10000 [==============>...............] - ETA: 39:58 - loss: 2.7909 - regression_loss: 2.0565 - classification_loss 5036/10000 [==============>...............] - ETA: 39:57 - loss: 2.7909 - regression_loss: 2.0566 - classification_loss 5037/10000 [==============>...............] - ETA: 39:57 - loss: 2.7909 - regression_loss: 2.0566 - classification_loss 5038/10000 [==============>...............] - ETA: 39:56 - loss: 2.7909 - regression_loss: 2.0566 - classification_loss 5039/10000 [==============>...............] - ETA: 39:56 - loss: 2.7908 - regression_loss: 2.0565 - classification_loss 5040/10000 [==============>...............] - ETA: 39:55 - loss: 2.7908 - regression_loss: 2.0565 - classification_loss 5041/10000 [==============>...............] - ETA: 39:55 - loss: 2.7905 - regression_loss: 2.0563 - classification_loss 5042/10000 [==============>...............] - ETA: 39:54 - loss: 2.7903 - regression_loss: 2.0561 - classification_loss 5043/10000 [==============>...............] - ETA: 39:54 - loss: 2.7902 - regression_loss: 2.0560 - classification_loss 5044/10000 [==============>...............] - ETA: 39:53 - loss: 2.7901 - regression_loss: 2.0559 - classification_loss 5045/10000 [==============>...............] - ETA: 39:53 - loss: 2.7898 - regression_loss: 2.0557 - classification_loss 5046/10000 [==============>...............] - ETA: 39:52 - loss: 2.7895 - regression_loss: 2.0554 - classification_loss 5047/10000 [==============>...............] - ETA: 39:51 - loss: 2.7892 - regression_loss: 2.0552 - classification_loss 5048/10000 [==============>...............] - ETA: 39:51 - loss: 2.7889 - regression_loss: 2.0550 - classification_loss 5049/10000 [==============>...............] - ETA: 39:50 - loss: 2.7889 - regression_loss: 2.0550 - classification_loss 5050/10000 [==============>...............] - ETA: 39:50 - loss: 2.7888 - regression_loss: 2.0550 - classification_loss 5051/10000 [==============>...............] - ETA: 39:49 - loss: 2.7889 - regression_loss: 2.0550 - classification_loss 5052/10000 [==============>...............] - ETA: 39:49 - loss: 2.7890 - regression_loss: 2.0550 - classification_loss 5053/10000 [==============>...............] - ETA: 39:48 - loss: 2.7888 - regression_loss: 2.0549 - classification_loss 5054/10000 [==============>...............] - ETA: 39:48 - loss: 2.7886 - regression_loss: 2.0548 - classification_loss 5055/10000 [==============>...............] - ETA: 39:47 - loss: 2.7887 - regression_loss: 2.0548 - classification_loss 5056/10000 [==============>...............] - ETA: 39:47 - loss: 2.7884 - regression_loss: 2.0546 - classification_loss 5057/10000 [==============>...............] - ETA: 39:46 - loss: 2.7881 - regression_loss: 2.0544 - classification_loss 5058/10000 [==============>...............] - ETA: 39:46 - loss: 2.7878 - regression_loss: 2.0542 - classification_loss 5059/10000 [==============>...............] - ETA: 39:45 - loss: 2.7878 - regression_loss: 2.0542 - classification_loss 5060/10000 [==============>...............] - ETA: 39:45 - loss: 2.7877 - regression_loss: 2.0542 - classification_loss 5061/10000 [==============>...............] - ETA: 39:44 - loss: 2.7876 - regression_loss: 2.0541 - classification_loss 5062/10000 [==============>...............] - ETA: 39:44 - loss: 2.7875 - regression_loss: 2.0541 - classification_loss 5063/10000 [==============>...............] - ETA: 39:43 - loss: 2.7873 - regression_loss: 2.0539 - classification_loss 5064/10000 [==============>...............] - ETA: 39:43 - loss: 2.7873 - regression_loss: 2.0539 - classification_loss 5065/10000 [==============>...............] - ETA: 39:42 - loss: 2.7873 - regression_loss: 2.0540 - classification_loss 5066/10000 [==============>...............] - ETA: 39:42 - loss: 2.7872 - regression_loss: 2.0539 - classification_loss 5067/10000 [==============>...............] - ETA: 39:41 - loss: 2.7871 - regression_loss: 2.0539 - classification_loss 5068/10000 [==============>...............] - ETA: 39:41 - loss: 2.7870 - regression_loss: 2.0538 - classification_loss 5069/10000 [==============>...............] - ETA: 39:40 - loss: 2.7870 - regression_loss: 2.0538 - classification_loss 5070/10000 [==============>...............] - ETA: 39:40 - loss: 2.7868 - regression_loss: 2.0536 - classification_loss 5071/10000 [==============>...............] - ETA: 39:39 - loss: 2.7867 - regression_loss: 2.0535 - classification_loss 5072/10000 [==============>...............] - ETA: 39:39 - loss: 2.7867 - regression_loss: 2.0535 - classification_loss 5073/10000 [==============>...............] - ETA: 39:38 - loss: 2.7868 - regression_loss: 2.0536 - classification_loss 5074/10000 [==============>...............] - ETA: 39:38 - loss: 2.7868 - regression_loss: 2.0536 - classification_loss 5075/10000 [==============>...............] - ETA: 39:37 - loss: 2.7866 - regression_loss: 2.0535 - classification_loss 5076/10000 [==============>...............] - ETA: 39:37 - loss: 2.7866 - regression_loss: 2.0535 - classification_loss 5077/10000 [==============>...............] - ETA: 39:36 - loss: 2.7864 - regression_loss: 2.0533 - classification_loss 5078/10000 [==============>...............] - ETA: 39:36 - loss: 2.7863 - regression_loss: 2.0533 - classification_loss 5079/10000 [==============>...............] - ETA: 39:35 - loss: 2.7864 - regression_loss: 2.0533 - classification_loss 5080/10000 [==============>...............] - ETA: 39:35 - loss: 2.7861 - regression_loss: 2.0531 - classification_loss 5081/10000 [==============>...............] - ETA: 39:34 - loss: 2.7862 - regression_loss: 2.0532 - classification_loss 5082/10000 [==============>...............] - ETA: 39:34 - loss: 2.7859 - regression_loss: 2.0529 - classification_loss 5083/10000 [==============>...............] - ETA: 39:33 - loss: 2.7857 - regression_loss: 2.0528 - classification_loss 5084/10000 [==============>...............] - ETA: 39:33 - loss: 2.7857 - regression_loss: 2.0527 - classification_loss 5085/10000 [==============>...............] - ETA: 39:32 - loss: 2.7856 - regression_loss: 2.0527 - classification_loss 5086/10000 [==============>...............] - ETA: 39:32 - loss: 2.7856 - regression_loss: 2.0527 - classification_loss 5087/10000 [==============>...............] - ETA: 39:31 - loss: 2.7855 - regression_loss: 2.0527 - classification_loss 5088/10000 [==============>...............] - ETA: 39:31 - loss: 2.7854 - regression_loss: 2.0526 - classification_loss 5089/10000 [==============>...............] - ETA: 39:30 - loss: 2.7851 - regression_loss: 2.0524 - classification_loss 5090/10000 [==============>...............] - ETA: 39:30 - loss: 2.7850 - regression_loss: 2.0523 - classification_loss 5091/10000 [==============>...............] - ETA: 39:29 - loss: 2.7850 - regression_loss: 2.0523 - classification_loss 5092/10000 [==============>...............] - ETA: 39:29 - loss: 2.7849 - regression_loss: 2.0522 - classification_loss 5093/10000 [==============>...............] - ETA: 39:28 - loss: 2.7848 - regression_loss: 2.0522 - classification_loss 5094/10000 [==============>...............] - ETA: 39:28 - loss: 2.7848 - regression_loss: 2.0522 - classification_loss 5095/10000 [==============>...............] - ETA: 39:27 - loss: 2.7845 - regression_loss: 2.0520 - classification_loss 5096/10000 [==============>...............] - ETA: 39:27 - loss: 2.7846 - regression_loss: 2.0520 - classification_loss 5097/10000 [==============>...............] - ETA: 39:26 - loss: 2.7844 - regression_loss: 2.0518 - classification_loss 5098/10000 [==============>...............] - ETA: 39:25 - loss: 2.7843 - regression_loss: 2.0518 - classification_loss 5099/10000 [==============>...............] - ETA: 39:25 - loss: 2.7843 - regression_loss: 2.0518 - classification_loss 5100/10000 [==============>...............] - ETA: 39:24 - loss: 2.7843 - regression_loss: 2.0518 - classification_loss 5101/10000 [==============>...............] - ETA: 39:24 - loss: 2.7841 - regression_loss: 2.0517 - classification_loss 5102/10000 [==============>...............] - ETA: 39:24 - loss: 2.7841 - regression_loss: 2.0517 - classification_loss 5103/10000 [==============>...............] - ETA: 39:23 - loss: 2.7840 - regression_loss: 2.0516 - classification_loss 5104/10000 [==============>...............] - ETA: 39:22 - loss: 2.7838 - regression_loss: 2.0515 - classification_loss 5105/10000 [==============>...............] - ETA: 39:22 - loss: 2.7837 - regression_loss: 2.0514 - classification_loss 5106/10000 [==============>...............] - ETA: 39:22 - loss: 2.7836 - regression_loss: 2.0513 - classification_loss 5107/10000 [==============>...............] - ETA: 39:21 - loss: 2.7835 - regression_loss: 2.0513 - classification_loss 5108/10000 [==============>...............] - ETA: 39:21 - loss: 2.7835 - regression_loss: 2.0513 - classification_loss 5109/10000 [==============>...............] - ETA: 39:20 - loss: 2.7833 - regression_loss: 2.0512 - classification_loss 5110/10000 [==============>...............] - ETA: 39:20 - loss: 2.7834 - regression_loss: 2.0512 - classification_loss 5111/10000 [==============>...............] - ETA: 39:19 - loss: 2.7834 - regression_loss: 2.0512 - classification_loss 5112/10000 [==============>...............] - ETA: 39:18 - loss: 2.7832 - regression_loss: 2.0511 - classification_loss 5113/10000 [==============>...............] - ETA: 39:18 - loss: 2.7830 - regression_loss: 2.0509 - classification_loss 5114/10000 [==============>...............] - ETA: 39:18 - loss: 2.7829 - regression_loss: 2.0509 - classification_loss 5115/10000 [==============>...............] - ETA: 39:17 - loss: 2.7830 - regression_loss: 2.0509 - classification_loss 5116/10000 [==============>...............] - ETA: 39:17 - loss: 2.7828 - regression_loss: 2.0509 - classification_loss 5117/10000 [==============>...............] - ETA: 39:16 - loss: 2.7826 - regression_loss: 2.0507 - classification_loss 5118/10000 [==============>...............] - ETA: 39:15 - loss: 2.7826 - regression_loss: 2.0506 - classification_loss 5119/10000 [==============>...............] - ETA: 39:15 - loss: 2.7823 - regression_loss: 2.0504 - classification_loss 5120/10000 [==============>...............] - ETA: 39:14 - loss: 2.7823 - regression_loss: 2.0504 - classification_loss 5121/10000 [==============>...............] - ETA: 39:14 - loss: 2.7820 - regression_loss: 2.0502 - classification_loss 5122/10000 [==============>...............] - ETA: 39:13 - loss: 2.7819 - regression_loss: 2.0501 - classification_loss 5123/10000 [==============>...............] - ETA: 39:13 - loss: 2.7817 - regression_loss: 2.0501 - classification_loss 5124/10000 [==============>...............] - ETA: 39:12 - loss: 2.7815 - regression_loss: 2.0499 - classification_loss 5125/10000 [==============>...............] - ETA: 39:12 - loss: 2.7816 - regression_loss: 2.0500 - classification_loss 5126/10000 [==============>...............] - ETA: 39:11 - loss: 2.7815 - regression_loss: 2.0499 - classification_loss 5127/10000 [==============>...............] - ETA: 39:11 - loss: 2.7812 - regression_loss: 2.0497 - classification_loss 5128/10000 [==============>...............] - ETA: 39:10 - loss: 2.7812 - regression_loss: 2.0497 - classification_loss 5129/10000 [==============>...............] - ETA: 39:10 - loss: 2.7811 - regression_loss: 2.0496 - classification_loss 5130/10000 [==============>...............] - ETA: 39:09 - loss: 2.7809 - regression_loss: 2.0495 - classification_loss 5131/10000 [==============>...............] - ETA: 39:09 - loss: 2.7808 - regression_loss: 2.0494 - classification_loss 5132/10000 [==============>...............] - ETA: 39:08 - loss: 2.7807 - regression_loss: 2.0493 - classification_loss 5133/10000 [==============>...............] - ETA: 39:08 - loss: 2.7807 - regression_loss: 2.0493 - classification_loss 5134/10000 [==============>...............] - ETA: 39:07 - loss: 2.7805 - regression_loss: 2.0491 - classification_loss 5135/10000 [==============>...............] - ETA: 39:07 - loss: 2.7802 - regression_loss: 2.0490 - classification_loss 5136/10000 [==============>...............] - ETA: 39:06 - loss: 2.7803 - regression_loss: 2.0491 - classification_loss 5137/10000 [==============>...............] - ETA: 39:06 - loss: 2.7802 - regression_loss: 2.0491 - classification_loss 5138/10000 [==============>...............] - ETA: 39:05 - loss: 2.7802 - regression_loss: 2.0490 - classification_loss 5139/10000 [==============>...............] - ETA: 39:05 - loss: 2.7800 - regression_loss: 2.0489 - classification_loss 5140/10000 [==============>...............] - ETA: 39:04 - loss: 2.7799 - regression_loss: 2.0489 - classification_loss 5141/10000 [==============>...............] - ETA: 39:04 - loss: 2.7795 - regression_loss: 2.0486 - classification_loss 5142/10000 [==============>...............] - ETA: 39:03 - loss: 2.7793 - regression_loss: 2.0484 - classification_loss 5143/10000 [==============>...............] - ETA: 39:03 - loss: 2.7793 - regression_loss: 2.0485 - classification_loss 5144/10000 [==============>...............] - ETA: 39:02 - loss: 2.7793 - regression_loss: 2.0486 - classification_loss 5145/10000 [==============>...............] - ETA: 39:02 - loss: 2.7792 - regression_loss: 2.0484 - classification_loss 5146/10000 [==============>...............] - ETA: 39:01 - loss: 2.7790 - regression_loss: 2.0483 - classification_loss 5147/10000 [==============>...............] - ETA: 39:01 - loss: 2.7790 - regression_loss: 2.0483 - classification_loss 5148/10000 [==============>...............] - ETA: 39:00 - loss: 2.7789 - regression_loss: 2.0482 - classification_loss 5149/10000 [==============>...............] - ETA: 39:00 - loss: 2.7787 - regression_loss: 2.0481 - classification_loss 5150/10000 [==============>...............] - ETA: 38:59 - loss: 2.7785 - regression_loss: 2.0480 - classification_loss 5151/10000 [==============>...............] - ETA: 38:59 - loss: 2.7783 - regression_loss: 2.0479 - classification_loss 5152/10000 [==============>...............] - ETA: 38:58 - loss: 2.7781 - regression_loss: 2.0477 - classification_loss 5153/10000 [==============>...............] - ETA: 38:58 - loss: 2.7778 - regression_loss: 2.0475 - classification_loss 5154/10000 [==============>...............] - ETA: 38:57 - loss: 2.7777 - regression_loss: 2.0473 - classification_loss 5155/10000 [==============>...............] - ETA: 38:57 - loss: 2.7775 - regression_loss: 2.0471 - classification_loss 5156/10000 [==============>...............] - ETA: 38:56 - loss: 2.7774 - regression_loss: 2.0471 - classification_loss 5157/10000 [==============>...............] - ETA: 38:56 - loss: 2.7772 - regression_loss: 2.0470 - classification_loss 5158/10000 [==============>...............] - ETA: 38:55 - loss: 2.7770 - regression_loss: 2.0468 - classification_loss 5159/10000 [==============>...............] - ETA: 38:55 - loss: 2.7770 - regression_loss: 2.0468 - classification_loss 5160/10000 [==============>...............] - ETA: 38:54 - loss: 2.7770 - regression_loss: 2.0468 - classification_loss 5161/10000 [==============>...............] - ETA: 38:54 - loss: 2.7769 - regression_loss: 2.0468 - classification_loss 5162/10000 [==============>...............] - ETA: 38:53 - loss: 2.7770 - regression_loss: 2.0469 - classification_loss 5163/10000 [==============>...............] - ETA: 38:53 - loss: 2.7769 - regression_loss: 2.0468 - classification_loss 5164/10000 [==============>...............] - ETA: 38:52 - loss: 2.7767 - regression_loss: 2.0466 - classification_loss 5165/10000 [==============>...............] - ETA: 38:52 - loss: 2.7765 - regression_loss: 2.0465 - classification_loss 5166/10000 [==============>...............] - ETA: 38:51 - loss: 2.7764 - regression_loss: 2.0464 - classification_loss 5167/10000 [==============>...............] - ETA: 38:51 - loss: 2.7762 - regression_loss: 2.0464 - classification_loss 5168/10000 [==============>...............] - ETA: 38:50 - loss: 2.7761 - regression_loss: 2.0463 - classification_loss 5169/10000 [==============>...............] - ETA: 38:50 - loss: 2.7760 - regression_loss: 2.0462 - classification_loss 5170/10000 [==============>...............] - ETA: 38:49 - loss: 2.7758 - regression_loss: 2.0461 - classification_loss 5171/10000 [==============>...............] - ETA: 38:49 - loss: 2.7759 - regression_loss: 2.0462 - classification_loss 5172/10000 [==============>...............] - ETA: 38:48 - loss: 2.7759 - regression_loss: 2.0462 - classification_loss 5173/10000 [==============>...............] - ETA: 38:48 - loss: 2.7757 - regression_loss: 2.0461 - classification_loss 5174/10000 [==============>...............] - ETA: 38:47 - loss: 2.7757 - regression_loss: 2.0461 - classification_loss 5175/10000 [==============>...............] - ETA: 38:47 - loss: 2.7755 - regression_loss: 2.0459 - classification_loss 5176/10000 [==============>...............] - ETA: 38:46 - loss: 2.7754 - regression_loss: 2.0459 - classification_loss 5177/10000 [==============>...............] - ETA: 38:46 - loss: 2.7752 - regression_loss: 2.0457 - classification_loss 5178/10000 [==============>...............] - ETA: 38:45 - loss: 2.7752 - regression_loss: 2.0457 - classification_loss 5179/10000 [==============>...............] - ETA: 38:45 - loss: 2.7749 - regression_loss: 2.0455 - classification_loss 5180/10000 [==============>...............] - ETA: 38:44 - loss: 2.7748 - regression_loss: 2.0454 - classification_loss 5181/10000 [==============>...............] - ETA: 38:44 - loss: 2.7748 - regression_loss: 2.0454 - classification_loss 5182/10000 [==============>...............] - ETA: 38:43 - loss: 2.7746 - regression_loss: 2.0453 - classification_loss 5183/10000 [==============>...............] - ETA: 38:43 - loss: 2.7745 - regression_loss: 2.0452 - classification_loss 5184/10000 [==============>...............] - ETA: 38:42 - loss: 2.7746 - regression_loss: 2.0453 - classification_loss 5185/10000 [==============>...............] - ETA: 38:42 - loss: 2.7746 - regression_loss: 2.0453 - classification_loss 5186/10000 [==============>...............] - ETA: 38:41 - loss: 2.7745 - regression_loss: 2.0452 - classification_loss 5187/10000 [==============>...............] - ETA: 38:41 - loss: 2.7743 - regression_loss: 2.0451 - classification_loss 5188/10000 [==============>...............] - ETA: 38:40 - loss: 2.7744 - regression_loss: 2.0452 - classification_loss 5189/10000 [==============>...............] - ETA: 38:40 - loss: 2.7743 - regression_loss: 2.0452 - classification_loss 5190/10000 [==============>...............] - ETA: 38:39 - loss: 2.7742 - regression_loss: 2.0450 - classification_loss 5191/10000 [==============>...............] - ETA: 38:39 - loss: 2.7740 - regression_loss: 2.0449 - classification_loss 5192/10000 [==============>...............] - ETA: 38:38 - loss: 2.7740 - regression_loss: 2.0449 - classification_loss 5193/10000 [==============>...............] - ETA: 38:38 - loss: 2.7739 - regression_loss: 2.0448 - classification_loss 5194/10000 [==============>...............] - ETA: 38:37 - loss: 2.7739 - regression_loss: 2.0449 - classification_loss 5195/10000 [==============>...............] - ETA: 38:37 - loss: 2.7739 - regression_loss: 2.0449 - classification_loss 5196/10000 [==============>...............] - ETA: 38:36 - loss: 2.7740 - regression_loss: 2.0449 - classification_loss 5197/10000 [==============>...............] - ETA: 38:36 - loss: 2.7739 - regression_loss: 2.0449 - classification_loss 5198/10000 [==============>...............] - ETA: 38:35 - loss: 2.7736 - regression_loss: 2.0446 - classification_loss 5199/10000 [==============>...............] - ETA: 38:35 - loss: 2.7735 - regression_loss: 2.0446 - classification_loss 5200/10000 [==============>...............] - ETA: 38:34 - loss: 2.7734 - regression_loss: 2.0445 - classification_loss 5201/10000 [==============>...............] - ETA: 38:34 - loss: 2.7735 - regression_loss: 2.0446 - classification_loss 5202/10000 [==============>...............] - ETA: 38:33 - loss: 2.7734 - regression_loss: 2.0445 - classification_loss 5203/10000 [==============>...............] - ETA: 38:33 - loss: 2.7734 - regression_loss: 2.0445 - classification_loss 5204/10000 [==============>...............] - ETA: 38:32 - loss: 2.7733 - regression_loss: 2.0444 - classification_loss 5205/10000 [==============>...............] - ETA: 38:32 - loss: 2.7730 - regression_loss: 2.0442 - classification_loss 5206/10000 [==============>...............] - ETA: 38:31 - loss: 2.7730 - regression_loss: 2.0442 - classification_loss 5207/10000 [==============>...............] - ETA: 38:31 - loss: 2.7728 - regression_loss: 2.0441 - classification_loss 5208/10000 [==============>...............] - ETA: 38:30 - loss: 2.7728 - regression_loss: 2.0441 - classification_loss 5209/10000 [==============>...............] - ETA: 38:30 - loss: 2.7727 - regression_loss: 2.0440 - classification_loss 5210/10000 [==============>...............] - ETA: 38:29 - loss: 2.7726 - regression_loss: 2.0440 - classification_loss 5211/10000 [==============>...............] - ETA: 38:29 - loss: 2.7726 - regression_loss: 2.0440 - classification_loss 5212/10000 [==============>...............] - ETA: 38:28 - loss: 2.7727 - regression_loss: 2.0441 - classification_loss 5213/10000 [==============>...............] - ETA: 38:28 - loss: 2.7728 - regression_loss: 2.0441 - classification_loss 5214/10000 [==============>...............] - ETA: 38:27 - loss: 2.7726 - regression_loss: 2.0440 - classification_loss 5215/10000 [==============>...............] - ETA: 38:27 - loss: 2.7724 - regression_loss: 2.0438 - classification_loss 5216/10000 [==============>...............] - ETA: 38:26 - loss: 2.7722 - regression_loss: 2.0437 - classification_loss 5217/10000 [==============>...............] - ETA: 38:26 - loss: 2.7722 - regression_loss: 2.0437 - classification_loss 5218/10000 [==============>...............] - ETA: 38:25 - loss: 2.7722 - regression_loss: 2.0437 - classification_loss 5219/10000 [==============>...............] - ETA: 38:25 - loss: 2.7721 - regression_loss: 2.0436 - classification_loss 5220/10000 [==============>...............] - ETA: 38:24 - loss: 2.7721 - regression_loss: 2.0437 - classification_loss 5221/10000 [==============>...............] - ETA: 38:24 - loss: 2.7719 - regression_loss: 2.0435 - classification_loss 5222/10000 [==============>...............] - ETA: 38:23 - loss: 2.7717 - regression_loss: 2.0434 - classification_loss 5223/10000 [==============>...............] - ETA: 38:23 - loss: 2.7717 - regression_loss: 2.0434 - classification_loss 5224/10000 [==============>...............] - ETA: 38:22 - loss: 2.7719 - regression_loss: 2.0435 - classification_loss 5225/10000 [==============>...............] - ETA: 38:22 - loss: 2.7716 - regression_loss: 2.0433 - classification_loss 5226/10000 [==============>...............] - ETA: 38:21 - loss: 2.7715 - regression_loss: 2.0431 - classification_loss 5227/10000 [==============>...............] - ETA: 38:21 - loss: 2.7714 - regression_loss: 2.0432 - classification_loss 5228/10000 [==============>...............] - ETA: 38:20 - loss: 2.7713 - regression_loss: 2.0431 - classification_loss 5229/10000 [==============>...............] - ETA: 38:20 - loss: 2.7712 - regression_loss: 2.0430 - classification_loss 5230/10000 [==============>...............] - ETA: 38:19 - loss: 2.7711 - regression_loss: 2.0430 - classification_loss 5231/10000 [==============>...............] - ETA: 38:19 - loss: 2.7711 - regression_loss: 2.0430 - classification_loss 5232/10000 [==============>...............] - ETA: 38:18 - loss: 2.7709 - regression_loss: 2.0428 - classification_loss 5233/10000 [==============>...............] - ETA: 38:18 - loss: 2.7707 - regression_loss: 2.0427 - classification_loss 5234/10000 [==============>...............] - ETA: 38:17 - loss: 2.7705 - regression_loss: 2.0425 - classification_loss 5235/10000 [==============>...............] - ETA: 38:17 - loss: 2.7705 - regression_loss: 2.0425 - classification_loss 5236/10000 [==============>...............] - ETA: 38:16 - loss: 2.7704 - regression_loss: 2.0425 - classification_loss 5237/10000 [==============>...............] - ETA: 38:16 - loss: 2.7704 - regression_loss: 2.0425 - classification_loss 5238/10000 [==============>...............] - ETA: 38:15 - loss: 2.7704 - regression_loss: 2.0425 - classification_loss 5239/10000 [==============>...............] - ETA: 38:15 - loss: 2.7703 - regression_loss: 2.0424 - classification_loss 5240/10000 [==============>...............] - ETA: 38:14 - loss: 2.7702 - regression_loss: 2.0424 - classification_loss 5241/10000 [==============>...............] - ETA: 38:14 - loss: 2.7701 - regression_loss: 2.0423 - classification_loss 5242/10000 [==============>...............] - ETA: 38:13 - loss: 2.7700 - regression_loss: 2.0422 - classification_loss 5243/10000 [==============>...............] - ETA: 38:13 - loss: 2.7700 - regression_loss: 2.0421 - classification_loss 5244/10000 [==============>...............] - ETA: 38:12 - loss: 2.7698 - regression_loss: 2.0420 - classification_loss 5245/10000 [==============>...............] - ETA: 38:12 - loss: 2.7696 - regression_loss: 2.0418 - classification_loss 5246/10000 [==============>...............] - ETA: 38:11 - loss: 2.7694 - regression_loss: 2.0418 - classification_loss 5247/10000 [==============>...............] - ETA: 38:11 - loss: 2.7693 - regression_loss: 2.0417 - classification_loss 5248/10000 [==============>...............] - ETA: 38:10 - loss: 2.7692 - regression_loss: 2.0416 - classification_loss 5249/10000 [==============>...............] - ETA: 38:10 - loss: 2.7689 - regression_loss: 2.0414 - classification_loss 5250/10000 [==============>...............] - ETA: 38:09 - loss: 2.7687 - regression_loss: 2.0413 - classification_loss 5251/10000 [==============>...............] - ETA: 38:09 - loss: 2.7688 - regression_loss: 2.0413 - classification_loss 5252/10000 [==============>...............] - ETA: 38:08 - loss: 2.7687 - regression_loss: 2.0412 - classification_loss 5253/10000 [==============>...............] - ETA: 38:08 - loss: 2.7684 - regression_loss: 2.0410 - classification_loss 5254/10000 [==============>...............] - ETA: 38:07 - loss: 2.7684 - regression_loss: 2.0410 - classification_loss 5255/10000 [==============>...............] - ETA: 38:07 - loss: 2.7684 - regression_loss: 2.0410 - classification_loss 5256/10000 [==============>...............] - ETA: 38:06 - loss: 2.7684 - regression_loss: 2.0410 - classification_loss 5257/10000 [==============>...............] - ETA: 38:06 - loss: 2.7682 - regression_loss: 2.0409 - classification_loss 5258/10000 [==============>...............] - ETA: 38:05 - loss: 2.7681 - regression_loss: 2.0408 - classification_loss 5259/10000 [==============>...............] - ETA: 38:05 - loss: 2.7680 - regression_loss: 2.0407 - classification_loss 5260/10000 [==============>...............] - ETA: 38:04 - loss: 2.7678 - regression_loss: 2.0407 - classification_loss 5261/10000 [==============>...............] - ETA: 38:04 - loss: 2.7678 - regression_loss: 2.0406 - classification_loss 5262/10000 [==============>...............] - ETA: 38:03 - loss: 2.7675 - regression_loss: 2.0404 - classification_loss 5263/10000 [==============>...............] - ETA: 38:03 - loss: 2.7676 - regression_loss: 2.0404 - classification_loss 5264/10000 [==============>...............] - ETA: 38:02 - loss: 2.7675 - regression_loss: 2.0404 - classification_loss 5265/10000 [==============>...............] - ETA: 38:02 - loss: 2.7673 - regression_loss: 2.0403 - classification_loss 5266/10000 [==============>...............] - ETA: 38:01 - loss: 2.7672 - regression_loss: 2.0403 - classification_loss 5267/10000 [==============>...............] - ETA: 38:01 - loss: 2.7671 - regression_loss: 2.0402 - classification_loss 5268/10000 [==============>...............] - ETA: 38:00 - loss: 2.7670 - regression_loss: 2.0400 - classification_loss 5269/10000 [==============>...............] - ETA: 38:00 - loss: 2.7671 - regression_loss: 2.0401 - classification_loss 5270/10000 [==============>...............] - ETA: 37:59 - loss: 2.7671 - regression_loss: 2.0401 - classification_loss 5271/10000 [==============>...............] - ETA: 37:59 - loss: 2.7668 - regression_loss: 2.0399 - classification_loss 5272/10000 [==============>...............] - ETA: 37:58 - loss: 2.7668 - regression_loss: 2.0399 - classification_loss 5273/10000 [==============>...............] - ETA: 37:58 - loss: 2.7668 - regression_loss: 2.0399 - classification_loss 5274/10000 [==============>...............] - ETA: 37:57 - loss: 2.7666 - regression_loss: 2.0397 - classification_loss 5275/10000 [==============>...............] - ETA: 37:57 - loss: 2.7666 - regression_loss: 2.0397 - classification_loss 5276/10000 [==============>...............] - ETA: 37:56 - loss: 2.7665 - regression_loss: 2.0397 - classification_loss 5277/10000 [==============>...............] - ETA: 37:56 - loss: 2.7664 - regression_loss: 2.0397 - classification_loss 5278/10000 [==============>...............] - ETA: 37:55 - loss: 2.7664 - regression_loss: 2.0396 - classification_loss 5279/10000 [==============>...............] - ETA: 37:55 - loss: 2.7662 - regression_loss: 2.0395 - classification_loss 5280/10000 [==============>...............] - ETA: 37:54 - loss: 2.7661 - regression_loss: 2.0395 - classification_loss 5281/10000 [==============>...............] - ETA: 37:54 - loss: 2.7658 - regression_loss: 2.0393 - classification_loss 5282/10000 [==============>...............] - ETA: 37:53 - loss: 2.7660 - regression_loss: 2.0394 - classification_loss 5283/10000 [==============>...............] - ETA: 37:53 - loss: 2.7659 - regression_loss: 2.0394 - classification_loss 5284/10000 [==============>...............] - ETA: 37:52 - loss: 2.7659 - regression_loss: 2.0393 - classification_loss 5285/10000 [==============>...............] - ETA: 37:52 - loss: 2.7659 - regression_loss: 2.0393 - classification_loss 5286/10000 [==============>...............] - ETA: 37:51 - loss: 2.7658 - regression_loss: 2.0393 - classification_loss 5287/10000 [==============>...............] - ETA: 37:51 - loss: 2.7657 - regression_loss: 2.0392 - classification_loss 5288/10000 [==============>...............] - ETA: 37:50 - loss: 2.7657 - regression_loss: 2.0392 - classification_loss 5289/10000 [==============>...............] - ETA: 37:50 - loss: 2.7656 - regression_loss: 2.0391 - classification_loss 5290/10000 [==============>...............] - ETA: 37:49 - loss: 2.7657 - regression_loss: 2.0392 - classification_loss 5291/10000 [==============>...............] - ETA: 37:49 - loss: 2.7656 - regression_loss: 2.0392 - classification_loss 5292/10000 [==============>...............] - ETA: 37:48 - loss: 2.7654 - regression_loss: 2.0390 - classification_loss 5293/10000 [==============>...............] - ETA: 37:48 - loss: 2.7652 - regression_loss: 2.0388 - classification_loss 5294/10000 [==============>...............] - ETA: 37:47 - loss: 2.7652 - regression_loss: 2.0388 - classification_loss 5295/10000 [==============>...............] - ETA: 37:47 - loss: 2.7649 - regression_loss: 2.0386 - classification_loss 5296/10000 [==============>...............] - ETA: 37:46 - loss: 2.7648 - regression_loss: 2.0385 - classification_loss 5297/10000 [==============>...............] - ETA: 37:46 - loss: 2.7645 - regression_loss: 2.0383 - classification_loss 5298/10000 [==============>...............] - ETA: 37:45 - loss: 2.7643 - regression_loss: 2.0381 - classification_loss 5299/10000 [==============>...............] - ETA: 37:45 - loss: 2.7642 - regression_loss: 2.0381 - classification_loss 5300/10000 [==============>...............] - ETA: 37:44 - loss: 2.7641 - regression_loss: 2.0380 - classification_loss 5301/10000 [==============>...............] - ETA: 37:44 - loss: 2.7641 - regression_loss: 2.0380 - classification_loss 5302/10000 [==============>...............] - ETA: 37:43 - loss: 2.7640 - regression_loss: 2.0379 - classification_loss 5303/10000 [==============>...............] - ETA: 37:43 - loss: 2.7641 - regression_loss: 2.0381 - classification_loss 5304/10000 [==============>...............] - ETA: 37:42 - loss: 2.7640 - regression_loss: 2.0381 - classification_loss 5305/10000 [==============>...............] - ETA: 37:42 - loss: 2.7640 - regression_loss: 2.0381 - classification_loss 5306/10000 [==============>...............] - ETA: 37:41 - loss: 2.7638 - regression_loss: 2.0379 - classification_loss 5307/10000 [==============>...............] - ETA: 37:41 - loss: 2.7638 - regression_loss: 2.0379 - classification_loss 5308/10000 [==============>...............] - ETA: 37:40 - loss: 2.7637 - regression_loss: 2.0379 - classification_loss 5309/10000 [==============>...............] - ETA: 37:40 - loss: 2.7639 - regression_loss: 2.0380 - classification_loss 5310/10000 [==============>...............] - ETA: 37:39 - loss: 2.7637 - regression_loss: 2.0378 - classification_loss 5311/10000 [==============>...............] - ETA: 37:39 - loss: 2.7637 - regression_loss: 2.0378 - classification_loss 5312/10000 [==============>...............] - ETA: 37:38 - loss: 2.7637 - regression_loss: 2.0378 - classification_loss 5313/10000 [==============>...............] - ETA: 37:38 - loss: 2.7637 - regression_loss: 2.0378 - classification_loss 5314/10000 [==============>...............] - ETA: 37:37 - loss: 2.7635 - regression_loss: 2.0376 - classification_loss 5315/10000 [==============>...............] - ETA: 37:37 - loss: 2.7634 - regression_loss: 2.0375 - classification_loss 5316/10000 [==============>...............] - ETA: 37:36 - loss: 2.7633 - regression_loss: 2.0375 - classification_loss 5317/10000 [==============>...............] - ETA: 37:36 - loss: 2.7634 - regression_loss: 2.0376 - classification_loss 5318/10000 [==============>...............] - ETA: 37:35 - loss: 2.7633 - regression_loss: 2.0375 - classification_loss 5319/10000 [==============>...............] - ETA: 37:35 - loss: 2.7632 - regression_loss: 2.0375 - classification_loss 5320/10000 [==============>...............] - ETA: 37:34 - loss: 2.7633 - regression_loss: 2.0376 - classification_loss 5321/10000 [==============>...............] - ETA: 37:34 - loss: 2.7632 - regression_loss: 2.0376 - classification_loss 5322/10000 [==============>...............] - ETA: 37:33 - loss: 2.7631 - regression_loss: 2.0375 - classification_loss 5323/10000 [==============>...............] - ETA: 37:33 - loss: 2.7631 - regression_loss: 2.0375 - classification_loss 5324/10000 [==============>...............] - ETA: 37:32 - loss: 2.7631 - regression_loss: 2.0375 - classification_loss 5325/10000 [==============>...............] - ETA: 37:32 - loss: 2.7631 - regression_loss: 2.0375 - classification_loss 5326/10000 [==============>...............] - ETA: 37:31 - loss: 2.7628 - regression_loss: 2.0372 - classification_loss 5327/10000 [==============>...............] - ETA: 37:31 - loss: 2.7628 - regression_loss: 2.0372 - classification_loss 5328/10000 [==============>...............] - ETA: 37:30 - loss: 2.7627 - regression_loss: 2.0372 - classification_loss 5329/10000 [==============>...............] - ETA: 37:30 - loss: 2.7624 - regression_loss: 2.0369 - classification_loss 5330/10000 [==============>...............] - ETA: 37:29 - loss: 2.7624 - regression_loss: 2.0369 - classification_loss 5331/10000 [==============>...............] - ETA: 37:29 - loss: 2.7623 - regression_loss: 2.0369 - classification_loss 5332/10000 [==============>...............] - ETA: 37:28 - loss: 2.7621 - regression_loss: 2.0368 - classification_loss 5333/10000 [==============>...............] - ETA: 37:28 - loss: 2.7621 - regression_loss: 2.0368 - classification_loss 5334/10000 [===============>..............] - ETA: 37:27 - loss: 2.7621 - regression_loss: 2.0368 - classification_loss 5335/10000 [===============>..............] - ETA: 37:27 - loss: 2.7620 - regression_loss: 2.0368 - classification_loss 5336/10000 [===============>..............] - ETA: 37:26 - loss: 2.7619 - regression_loss: 2.0367 - classification_loss 5337/10000 [===============>..............] - ETA: 37:26 - loss: 2.7619 - regression_loss: 2.0367 - classification_loss 5338/10000 [===============>..............] - ETA: 37:25 - loss: 2.7618 - regression_loss: 2.0366 - classification_loss 5339/10000 [===============>..............] - ETA: 37:25 - loss: 2.7617 - regression_loss: 2.0365 - classification_loss 5340/10000 [===============>..............] - ETA: 37:24 - loss: 2.7616 - regression_loss: 2.0365 - classification_loss 5341/10000 [===============>..............] - ETA: 37:24 - loss: 2.7615 - regression_loss: 2.0364 - classification_loss 5342/10000 [===============>..............] - ETA: 37:23 - loss: 2.7614 - regression_loss: 2.0364 - classification_loss 5343/10000 [===============>..............] - ETA: 37:23 - loss: 2.7612 - regression_loss: 2.0362 - classification_loss 5344/10000 [===============>..............] - ETA: 37:22 - loss: 2.7611 - regression_loss: 2.0361 - classification_loss 5345/10000 [===============>..............] - ETA: 37:22 - loss: 2.7610 - regression_loss: 2.0361 - classification_loss 5346/10000 [===============>..............] - ETA: 37:21 - loss: 2.7608 - regression_loss: 2.0359 - classification_loss 5347/10000 [===============>..............] - ETA: 37:21 - loss: 2.7608 - regression_loss: 2.0359 - classification_loss 5348/10000 [===============>..............] - ETA: 37:20 - loss: 2.7608 - regression_loss: 2.0359 - classification_loss 5349/10000 [===============>..............] - ETA: 37:20 - loss: 2.7606 - regression_loss: 2.0359 - classification_loss 5350/10000 [===============>..............] - ETA: 37:19 - loss: 2.7606 - regression_loss: 2.0358 - classification_loss 5351/10000 [===============>..............] - ETA: 37:19 - loss: 2.7606 - regression_loss: 2.0358 - classification_loss 5352/10000 [===============>..............] - ETA: 37:18 - loss: 2.7603 - regression_loss: 2.0356 - classification_loss 5353/10000 [===============>..............] - ETA: 37:18 - loss: 2.7604 - regression_loss: 2.0357 - classification_loss 5354/10000 [===============>..............] - ETA: 37:17 - loss: 2.7604 - regression_loss: 2.0357 - classification_loss 5355/10000 [===============>..............] - ETA: 37:17 - loss: 2.7604 - regression_loss: 2.0357 - classification_loss 5356/10000 [===============>..............] - ETA: 37:16 - loss: 2.7605 - regression_loss: 2.0357 - classification_loss 5357/10000 [===============>..............] - ETA: 37:16 - loss: 2.7605 - regression_loss: 2.0357 - classification_loss 5358/10000 [===============>..............] - ETA: 37:15 - loss: 2.7603 - regression_loss: 2.0356 - classification_loss 5359/10000 [===============>..............] - ETA: 37:15 - loss: 2.7603 - regression_loss: 2.0356 - classification_loss 5360/10000 [===============>..............] - ETA: 37:14 - loss: 2.7603 - regression_loss: 2.0356 - classification_loss 5361/10000 [===============>..............] - ETA: 37:14 - loss: 2.7603 - regression_loss: 2.0357 - classification_loss 5362/10000 [===============>..............] - ETA: 37:13 - loss: 2.7603 - regression_loss: 2.0357 - classification_loss 5363/10000 [===============>..............] - ETA: 37:13 - loss: 2.7602 - regression_loss: 2.0357 - classification_loss 5364/10000 [===============>..............] - ETA: 37:12 - loss: 2.7601 - regression_loss: 2.0357 - classification_loss 5365/10000 [===============>..............] - ETA: 37:12 - loss: 2.7601 - regression_loss: 2.0356 - classification_loss 5366/10000 [===============>..............] - ETA: 37:11 - loss: 2.7600 - regression_loss: 2.0356 - classification_loss 5367/10000 [===============>..............] - ETA: 37:11 - loss: 2.7599 - regression_loss: 2.0355 - classification_loss 5368/10000 [===============>..............] - ETA: 37:10 - loss: 2.7598 - regression_loss: 2.0354 - classification_loss 5369/10000 [===============>..............] - ETA: 37:10 - loss: 2.7597 - regression_loss: 2.0354 - classification_loss 5370/10000 [===============>..............] - ETA: 37:09 - loss: 2.7595 - regression_loss: 2.0352 - classification_loss 5371/10000 [===============>..............] - ETA: 37:09 - loss: 2.7594 - regression_loss: 2.0351 - classification_loss 5372/10000 [===============>..............] - ETA: 37:08 - loss: 2.7593 - regression_loss: 2.0350 - classification_loss 5373/10000 [===============>..............] - ETA: 37:08 - loss: 2.7592 - regression_loss: 2.0350 - classification_loss 5374/10000 [===============>..............] - ETA: 37:07 - loss: 2.7589 - regression_loss: 2.0348 - classification_loss 5375/10000 [===============>..............] - ETA: 37:07 - loss: 2.7590 - regression_loss: 2.0348 - classification_loss 5376/10000 [===============>..............] - ETA: 37:06 - loss: 2.7588 - regression_loss: 2.0346 - classification_loss 5377/10000 [===============>..............] - ETA: 37:06 - loss: 2.7587 - regression_loss: 2.0346 - classification_loss 5378/10000 [===============>..............] - ETA: 37:05 - loss: 2.7587 - regression_loss: 2.0345 - classification_loss 5379/10000 [===============>..............] - ETA: 37:05 - loss: 2.7585 - regression_loss: 2.0343 - classification_loss 5380/10000 [===============>..............] - ETA: 37:04 - loss: 2.7584 - regression_loss: 2.0342 - classification_loss 5381/10000 [===============>..............] - ETA: 37:04 - loss: 2.7582 - regression_loss: 2.0341 - classification_loss 5382/10000 [===============>..............] - ETA: 37:03 - loss: 2.7583 - regression_loss: 2.0342 - classification_loss 5383/10000 [===============>..............] - ETA: 37:03 - loss: 2.7583 - regression_loss: 2.0342 - classification_loss 5384/10000 [===============>..............] - ETA: 37:02 - loss: 2.7582 - regression_loss: 2.0340 - classification_loss 5385/10000 [===============>..............] - ETA: 37:02 - loss: 2.7581 - regression_loss: 2.0340 - classification_loss 5386/10000 [===============>..............] - ETA: 37:01 - loss: 2.7580 - regression_loss: 2.0339 - classification_loss 5387/10000 [===============>..............] - ETA: 37:01 - loss: 2.7578 - regression_loss: 2.0338 - classification_loss 5388/10000 [===============>..............] - ETA: 37:00 - loss: 2.7578 - regression_loss: 2.0338 - classification_loss 5389/10000 [===============>..............] - ETA: 37:00 - loss: 2.7577 - regression_loss: 2.0337 - classification_loss 5390/10000 [===============>..............] - ETA: 36:59 - loss: 2.7575 - regression_loss: 2.0336 - classification_loss 5391/10000 [===============>..............] - ETA: 36:59 - loss: 2.7574 - regression_loss: 2.0335 - classification_loss 5392/10000 [===============>..............] - ETA: 36:58 - loss: 2.7577 - regression_loss: 2.0331 - classification_loss 5393/10000 [===============>..............] - ETA: 36:58 - loss: 2.7576 - regression_loss: 2.0331 - classification_loss 5394/10000 [===============>..............] - ETA: 36:57 - loss: 2.7575 - regression_loss: 2.0330 - classification_loss 5395/10000 [===============>..............] - ETA: 36:57 - loss: 2.7573 - regression_loss: 2.0329 - classification_loss 5396/10000 [===============>..............] - ETA: 36:56 - loss: 2.7572 - regression_loss: 2.0328 - classification_loss 5397/10000 [===============>..............] - ETA: 36:56 - loss: 2.7571 - regression_loss: 2.0328 - classification_loss 5398/10000 [===============>..............] - ETA: 36:56 - loss: 2.7570 - regression_loss: 2.0327 - classification_loss 5399/10000 [===============>..............] - ETA: 36:55 - loss: 2.7570 - regression_loss: 2.0327 - classification_loss 5400/10000 [===============>..............] - ETA: 36:55 - loss: 2.7568 - regression_loss: 2.0326 - classification_loss 5401/10000 [===============>..............] - ETA: 36:54 - loss: 2.7568 - regression_loss: 2.0325 - classification_loss 5402/10000 [===============>..............] - ETA: 36:54 - loss: 2.7567 - regression_loss: 2.0325 - classification_loss 5403/10000 [===============>..............] - ETA: 36:53 - loss: 2.7565 - regression_loss: 2.0323 - classification_loss 5404/10000 [===============>..............] - ETA: 36:53 - loss: 2.7563 - regression_loss: 2.0322 - classification_loss 5405/10000 [===============>..............] - ETA: 36:52 - loss: 2.7562 - regression_loss: 2.0321 - classification_loss 5406/10000 [===============>..............] - ETA: 36:52 - loss: 2.7561 - regression_loss: 2.0321 - classification_loss 5407/10000 [===============>..............] - ETA: 36:51 - loss: 2.7561 - regression_loss: 2.0321 - classification_loss 5408/10000 [===============>..............] - ETA: 36:51 - loss: 2.7561 - regression_loss: 2.0321 - classification_loss 5409/10000 [===============>..............] - ETA: 36:50 - loss: 2.7559 - regression_loss: 2.0320 - classification_loss 5410/10000 [===============>..............] - ETA: 36:49 - loss: 2.7559 - regression_loss: 2.0320 - classification_loss 5411/10000 [===============>..............] - ETA: 36:49 - loss: 2.7559 - regression_loss: 2.0320 - classification_loss 5412/10000 [===============>..............] - ETA: 36:48 - loss: 2.7558 - regression_loss: 2.0320 - classification_loss 5413/10000 [===============>..............] - ETA: 36:48 - loss: 2.7558 - regression_loss: 2.0320 - classification_loss 5414/10000 [===============>..............] - ETA: 36:48 - loss: 2.7557 - regression_loss: 2.0319 - classification_loss 5415/10000 [===============>..............] - ETA: 36:47 - loss: 2.7558 - regression_loss: 2.0320 - classification_loss 5416/10000 [===============>..............] - ETA: 36:47 - loss: 2.7558 - regression_loss: 2.0320 - classification_loss 5417/10000 [===============>..............] - ETA: 36:46 - loss: 2.7555 - regression_loss: 2.0318 - classification_loss 5418/10000 [===============>..............] - ETA: 36:45 - loss: 2.7556 - regression_loss: 2.0318 - classification_loss 5419/10000 [===============>..............] - ETA: 36:45 - loss: 2.7554 - regression_loss: 2.0316 - classification_loss 5420/10000 [===============>..............] - ETA: 36:45 - loss: 2.7553 - regression_loss: 2.0316 - classification_loss 5421/10000 [===============>..............] - ETA: 36:44 - loss: 2.7552 - regression_loss: 2.0315 - classification_loss 5422/10000 [===============>..............] - ETA: 36:43 - loss: 2.7551 - regression_loss: 2.0314 - classification_loss 5423/10000 [===============>..............] - ETA: 36:43 - loss: 2.7548 - regression_loss: 2.0313 - classification_loss 5424/10000 [===============>..............] - ETA: 36:43 - loss: 2.7546 - regression_loss: 2.0310 - classification_loss 5425/10000 [===============>..............] - ETA: 36:42 - loss: 2.7545 - regression_loss: 2.0310 - classification_loss 5426/10000 [===============>..............] - ETA: 36:42 - loss: 2.7543 - regression_loss: 2.0309 - classification_loss 5427/10000 [===============>..............] - ETA: 36:41 - loss: 2.7543 - regression_loss: 2.0309 - classification_loss 5428/10000 [===============>..............] - ETA: 36:41 - loss: 2.7543 - regression_loss: 2.0310 - classification_loss 5429/10000 [===============>..............] - ETA: 36:40 - loss: 2.7541 - regression_loss: 2.0309 - classification_loss 5430/10000 [===============>..............] - ETA: 36:39 - loss: 2.7539 - regression_loss: 2.0307 - classification_loss 5431/10000 [===============>..............] - ETA: 36:39 - loss: 2.7539 - regression_loss: 2.0307 - classification_loss 5432/10000 [===============>..............] - ETA: 36:38 - loss: 2.7538 - regression_loss: 2.0307 - classification_loss 5433/10000 [===============>..............] - ETA: 36:38 - loss: 2.7537 - regression_loss: 2.0305 - classification_loss 5434/10000 [===============>..............] - ETA: 36:37 - loss: 2.7536 - regression_loss: 2.0306 - classification_loss 5435/10000 [===============>..............] - ETA: 36:37 - loss: 2.7535 - regression_loss: 2.0304 - classification_loss 5436/10000 [===============>..............] - ETA: 36:36 - loss: 2.7536 - regression_loss: 2.0305 - classification_loss 5437/10000 [===============>..............] - ETA: 36:36 - loss: 2.7534 - regression_loss: 2.0304 - classification_loss 5438/10000 [===============>..............] - ETA: 36:36 - loss: 2.7533 - regression_loss: 2.0303 - classification_loss 5439/10000 [===============>..............] - ETA: 36:35 - loss: 2.7532 - regression_loss: 2.0302 - classification_loss 5440/10000 [===============>..............] - ETA: 36:34 - loss: 2.7533 - regression_loss: 2.0303 - classification_loss 5441/10000 [===============>..............] - ETA: 36:34 - loss: 2.7531 - regression_loss: 2.0301 - classification_loss 5442/10000 [===============>..............] - ETA: 36:33 - loss: 2.7530 - regression_loss: 2.0301 - classification_loss 5443/10000 [===============>..............] - ETA: 36:33 - loss: 2.7529 - regression_loss: 2.0300 - classification_loss 5444/10000 [===============>..............] - ETA: 36:33 - loss: 2.7528 - regression_loss: 2.0299 - classification_loss 5445/10000 [===============>..............] - ETA: 36:32 - loss: 2.7527 - regression_loss: 2.0299 - classification_loss 5446/10000 [===============>..............] - ETA: 36:32 - loss: 2.7527 - regression_loss: 2.0298 - classification_loss 5447/10000 [===============>..............] - ETA: 36:31 - loss: 2.7526 - regression_loss: 2.0298 - classification_loss 5448/10000 [===============>..............] - ETA: 36:31 - loss: 2.7525 - regression_loss: 2.0297 - classification_loss 5449/10000 [===============>..............] - ETA: 36:30 - loss: 2.7523 - regression_loss: 2.0295 - classification_loss 5450/10000 [===============>..............] - ETA: 36:30 - loss: 2.7523 - regression_loss: 2.0295 - classification_loss 5451/10000 [===============>..............] - ETA: 36:29 - loss: 2.7521 - regression_loss: 2.0294 - classification_loss 5452/10000 [===============>..............] - ETA: 36:29 - loss: 2.7520 - regression_loss: 2.0293 - classification_loss 5453/10000 [===============>..............] - ETA: 36:28 - loss: 2.7519 - regression_loss: 2.0292 - classification_loss 5454/10000 [===============>..............] - ETA: 36:28 - loss: 2.7518 - regression_loss: 2.0292 - classification_loss 5455/10000 [===============>..............] - ETA: 36:27 - loss: 2.7516 - regression_loss: 2.0290 - classification_loss 5456/10000 [===============>..............] - ETA: 36:27 - loss: 2.7514 - regression_loss: 2.0289 - classification_loss 5457/10000 [===============>..............] - ETA: 36:26 - loss: 2.7512 - regression_loss: 2.0288 - classification_loss 5458/10000 [===============>..............] - ETA: 36:26 - loss: 2.7513 - regression_loss: 2.0289 - classification_loss 5459/10000 [===============>..............] - ETA: 36:25 - loss: 2.7512 - regression_loss: 2.0289 - classification_loss 5460/10000 [===============>..............] - ETA: 36:25 - loss: 2.7511 - regression_loss: 2.0288 - classification_loss 5461/10000 [===============>..............] - ETA: 36:24 - loss: 2.7510 - regression_loss: 2.0287 - classification_loss 5462/10000 [===============>..............] - ETA: 36:24 - loss: 2.7509 - regression_loss: 2.0286 - classification_loss 5463/10000 [===============>..............] - ETA: 36:23 - loss: 2.7508 - regression_loss: 2.0285 - classification_loss 5464/10000 [===============>..............] - ETA: 36:23 - loss: 2.7507 - regression_loss: 2.0285 - classification_loss 5465/10000 [===============>..............] - ETA: 36:22 - loss: 2.7505 - regression_loss: 2.0284 - classification_loss 5466/10000 [===============>..............] - ETA: 36:22 - loss: 2.7507 - regression_loss: 2.0285 - classification_loss 5467/10000 [===============>..............] - ETA: 36:21 - loss: 2.7504 - regression_loss: 2.0283 - classification_loss 5468/10000 [===============>..............] - ETA: 36:21 - loss: 2.7503 - regression_loss: 2.0282 - classification_loss 5469/10000 [===============>..............] - ETA: 36:20 - loss: 2.7503 - regression_loss: 2.0283 - classification_loss 5470/10000 [===============>..............] - ETA: 36:20 - loss: 2.7501 - regression_loss: 2.0281 - classification_loss 5471/10000 [===============>..............] - ETA: 36:19 - loss: 2.7500 - regression_loss: 2.0281 - classification_loss 5472/10000 [===============>..............] - ETA: 36:19 - loss: 2.7498 - regression_loss: 2.0279 - classification_loss 5473/10000 [===============>..............] - ETA: 36:18 - loss: 2.7498 - regression_loss: 2.0279 - classification_loss 5474/10000 [===============>..............] - ETA: 36:18 - loss: 2.7496 - regression_loss: 2.0278 - classification_loss 5475/10000 [===============>..............] - ETA: 36:17 - loss: 2.7495 - regression_loss: 2.0278 - classification_loss 5476/10000 [===============>..............] - ETA: 36:17 - loss: 2.7492 - regression_loss: 2.0276 - classification_loss 5477/10000 [===============>..............] - ETA: 36:16 - loss: 2.7492 - regression_loss: 2.0275 - classification_loss 5478/10000 [===============>..............] - ETA: 36:16 - loss: 2.7492 - regression_loss: 2.0275 - classification_loss 5479/10000 [===============>..............] - ETA: 36:15 - loss: 2.7490 - regression_loss: 2.0274 - classification_loss 5480/10000 [===============>..............] - ETA: 36:15 - loss: 2.7490 - regression_loss: 2.0274 - classification_loss 5481/10000 [===============>..............] - ETA: 36:14 - loss: 2.7489 - regression_loss: 2.0274 - classification_loss 5482/10000 [===============>..............] - ETA: 36:14 - loss: 2.7488 - regression_loss: 2.0273 - classification_loss 5483/10000 [===============>..............] - ETA: 36:13 - loss: 2.7487 - regression_loss: 2.0272 - classification_loss 5484/10000 [===============>..............] - ETA: 36:13 - loss: 2.7484 - regression_loss: 2.0271 - classification_loss 5485/10000 [===============>..............] - ETA: 36:12 - loss: 2.7485 - regression_loss: 2.0271 - classification_loss 5486/10000 [===============>..............] - ETA: 36:12 - loss: 2.7482 - regression_loss: 2.0268 - classification_loss 5487/10000 [===============>..............] - ETA: 36:11 - loss: 2.7480 - regression_loss: 2.0267 - classification_loss 5488/10000 [===============>..............] - ETA: 36:11 - loss: 2.7480 - regression_loss: 2.0267 - classification_loss 5489/10000 [===============>..............] - ETA: 36:10 - loss: 2.7479 - regression_loss: 2.0266 - classification_loss 5490/10000 [===============>..............] - ETA: 36:10 - loss: 2.7478 - regression_loss: 2.0265 - classification_loss 5491/10000 [===============>..............] - ETA: 36:09 - loss: 2.7476 - regression_loss: 2.0264 - classification_loss 5492/10000 [===============>..............] - ETA: 36:09 - loss: 2.7475 - regression_loss: 2.0263 - classification_loss 5493/10000 [===============>..............] - ETA: 36:08 - loss: 2.7474 - regression_loss: 2.0262 - classification_loss 5494/10000 [===============>..............] - ETA: 36:08 - loss: 2.7474 - regression_loss: 2.0263 - classification_loss 5495/10000 [===============>..............] - ETA: 36:07 - loss: 2.7474 - regression_loss: 2.0263 - classification_loss 5496/10000 [===============>..............] - ETA: 36:07 - loss: 2.7472 - regression_loss: 2.0261 - classification_loss 5497/10000 [===============>..............] - ETA: 36:06 - loss: 2.7472 - regression_loss: 2.0261 - classification_loss 5498/10000 [===============>..............] - ETA: 36:06 - loss: 2.7471 - regression_loss: 2.0261 - classification_loss 5499/10000 [===============>..............] - ETA: 36:05 - loss: 2.7469 - regression_loss: 2.0259 - classification_loss 5500/10000 [===============>..............] - ETA: 36:05 - loss: 2.7468 - regression_loss: 2.0259 - classification_loss 5501/10000 [===============>..............] - ETA: 36:04 - loss: 2.7469 - regression_loss: 2.0259 - classification_loss 5502/10000 [===============>..............] - ETA: 36:04 - loss: 2.7467 - regression_loss: 2.0258 - classification_loss 5503/10000 [===============>..............] - ETA: 36:03 - loss: 2.7465 - regression_loss: 2.0257 - classification_loss 5504/10000 [===============>..............] - ETA: 36:03 - loss: 2.7465 - regression_loss: 2.0257 - classification_loss 5505/10000 [===============>..............] - ETA: 36:02 - loss: 2.7463 - regression_loss: 2.0256 - classification_loss 5506/10000 [===============>..............] - ETA: 36:02 - loss: 2.7463 - regression_loss: 2.0256 - classification_loss 5507/10000 [===============>..............] - ETA: 36:01 - loss: 2.7462 - regression_loss: 2.0256 - classification_loss 5508/10000 [===============>..............] - ETA: 36:01 - loss: 2.7461 - regression_loss: 2.0255 - classification_loss 5509/10000 [===============>..............] - ETA: 36:00 - loss: 2.7459 - regression_loss: 2.0254 - classification_loss 5510/10000 [===============>..............] - ETA: 36:00 - loss: 2.7457 - regression_loss: 2.0253 - classification_loss 5511/10000 [===============>..............] - ETA: 35:59 - loss: 2.7456 - regression_loss: 2.0252 - classification_loss 5512/10000 [===============>..............] - ETA: 35:59 - loss: 2.7455 - regression_loss: 2.0251 - classification_loss 5513/10000 [===============>..............] - ETA: 35:58 - loss: 2.7453 - regression_loss: 2.0249 - classification_loss 5514/10000 [===============>..............] - ETA: 35:58 - loss: 2.7451 - regression_loss: 2.0247 - classification_loss 5515/10000 [===============>..............] - ETA: 35:57 - loss: 2.7450 - regression_loss: 2.0247 - classification_loss 5516/10000 [===============>..............] - ETA: 35:57 - loss: 2.7449 - regression_loss: 2.0246 - classification_loss 5517/10000 [===============>..............] - ETA: 35:56 - loss: 2.7446 - regression_loss: 2.0244 - classification_loss 5518/10000 [===============>..............] - ETA: 35:56 - loss: 2.7444 - regression_loss: 2.0242 - classification_loss 5519/10000 [===============>..............] - ETA: 35:55 - loss: 2.7441 - regression_loss: 2.0241 - classification_loss 5520/10000 [===============>..............] - ETA: 35:55 - loss: 2.7439 - regression_loss: 2.0239 - classification_loss 5521/10000 [===============>..............] - ETA: 35:54 - loss: 2.7438 - regression_loss: 2.0238 - classification_loss 5522/10000 [===============>..............] - ETA: 35:54 - loss: 2.7437 - regression_loss: 2.0237 - classification_loss 5523/10000 [===============>..............] - ETA: 35:53 - loss: 2.7436 - regression_loss: 2.0237 - classification_loss 5524/10000 [===============>..............] - ETA: 35:53 - loss: 2.7436 - regression_loss: 2.0237 - classification_loss 5525/10000 [===============>..............] - ETA: 35:52 - loss: 2.7436 - regression_loss: 2.0237 - classification_loss 5526/10000 [===============>..............] - ETA: 35:52 - loss: 2.7435 - regression_loss: 2.0237 - classification_loss 5527/10000 [===============>..............] - ETA: 35:51 - loss: 2.7435 - regression_loss: 2.0237 - classification_loss 5528/10000 [===============>..............] - ETA: 35:51 - loss: 2.7436 - regression_loss: 2.0233 - classification_loss 5529/10000 [===============>..............] - ETA: 35:50 - loss: 2.7436 - regression_loss: 2.0233 - classification_loss 5530/10000 [===============>..............] - ETA: 35:50 - loss: 2.7436 - regression_loss: 2.0234 - classification_loss 5531/10000 [===============>..............] - ETA: 35:49 - loss: 2.7435 - regression_loss: 2.0233 - classification_loss 5532/10000 [===============>..............] - ETA: 35:49 - loss: 2.7433 - regression_loss: 2.0232 - classification_loss 5533/10000 [===============>..............] - ETA: 35:48 - loss: 2.7433 - regression_loss: 2.0232 - classification_loss 5534/10000 [===============>..............] - ETA: 35:48 - loss: 2.7432 - regression_loss: 2.0231 - classification_loss 5535/10000 [===============>..............] - ETA: 35:47 - loss: 2.7432 - regression_loss: 2.0231 - classification_loss 5536/10000 [===============>..............] - ETA: 35:47 - loss: 2.7431 - regression_loss: 2.0231 - classification_loss 5537/10000 [===============>..............] - ETA: 35:46 - loss: 2.7430 - regression_loss: 2.0230 - classification_loss 5538/10000 [===============>..............] - ETA: 35:46 - loss: 2.7428 - regression_loss: 2.0229 - classification_loss 5539/10000 [===============>..............] - ETA: 35:45 - loss: 2.7426 - regression_loss: 2.0228 - classification_loss 5540/10000 [===============>..............] - ETA: 35:45 - loss: 2.7424 - regression_loss: 2.0226 - classification_loss 5541/10000 [===============>..............] - ETA: 35:44 - loss: 2.7423 - regression_loss: 2.0226 - classification_loss 5542/10000 [===============>..............] - ETA: 35:44 - loss: 2.7423 - regression_loss: 2.0226 - classification_loss 5543/10000 [===============>..............] - ETA: 35:43 - loss: 2.7422 - regression_loss: 2.0225 - classification_loss 5544/10000 [===============>..............] - ETA: 35:43 - loss: 2.7419 - regression_loss: 2.0223 - classification_loss 5545/10000 [===============>..............] - ETA: 35:42 - loss: 2.7418 - regression_loss: 2.0222 - classification_loss 5546/10000 [===============>..............] - ETA: 35:42 - loss: 2.7416 - regression_loss: 2.0221 - classification_loss 5547/10000 [===============>..............] - ETA: 35:41 - loss: 2.7417 - regression_loss: 2.0222 - classification_loss 5548/10000 [===============>..............] - ETA: 35:41 - loss: 2.7417 - regression_loss: 2.0222 - classification_loss 5549/10000 [===============>..............] - ETA: 35:40 - loss: 2.7417 - regression_loss: 2.0222 - classification_loss 5550/10000 [===============>..............] - ETA: 35:40 - loss: 2.7416 - regression_loss: 2.0221 - classification_loss 5551/10000 [===============>..............] - ETA: 35:39 - loss: 2.7414 - regression_loss: 2.0220 - classification_loss 5552/10000 [===============>..............] - ETA: 35:39 - loss: 2.7413 - regression_loss: 2.0220 - classification_loss 5553/10000 [===============>..............] - ETA: 35:38 - loss: 2.7413 - regression_loss: 2.0219 - classification_loss 5554/10000 [===============>..............] - ETA: 35:38 - loss: 2.7412 - regression_loss: 2.0219 - classification_loss 5555/10000 [===============>..............] - ETA: 35:38 - loss: 2.7410 - regression_loss: 2.0218 - classification_loss 5556/10000 [===============>..............] - ETA: 35:37 - loss: 2.7409 - regression_loss: 2.0217 - classification_loss 5557/10000 [===============>..............] - ETA: 35:36 - loss: 2.7409 - regression_loss: 2.0217 - classification_loss 5558/10000 [===============>..............] - ETA: 35:36 - loss: 2.7408 - regression_loss: 2.0216 - classification_loss 5559/10000 [===============>..............] - ETA: 35:35 - loss: 2.7407 - regression_loss: 2.0216 - classification_loss 5560/10000 [===============>..............] - ETA: 35:35 - loss: 2.7406 - regression_loss: 2.0215 - classification_loss 5561/10000 [===============>..............] - ETA: 35:35 - loss: 2.7404 - regression_loss: 2.0214 - classification_loss 5562/10000 [===============>..............] - ETA: 35:34 - loss: 2.7404 - regression_loss: 2.0210 - classification_loss 5563/10000 [===============>..............] - ETA: 35:34 - loss: 2.7404 - regression_loss: 2.0210 - classification_loss 5564/10000 [===============>..............] - ETA: 35:33 - loss: 2.7403 - regression_loss: 2.0210 - classification_loss 5565/10000 [===============>..............] - ETA: 35:33 - loss: 2.7402 - regression_loss: 2.0209 - classification_loss 5566/10000 [===============>..............] - ETA: 35:32 - loss: 2.7401 - regression_loss: 2.0209 - classification_loss 5567/10000 [===============>..............] - ETA: 35:32 - loss: 2.7400 - regression_loss: 2.0209 - classification_loss 5568/10000 [===============>..............] - ETA: 35:31 - loss: 2.7399 - regression_loss: 2.0208 - classification_loss 5569/10000 [===============>..............] - ETA: 35:31 - loss: 2.7398 - regression_loss: 2.0208 - classification_loss 5570/10000 [===============>..............] - ETA: 35:30 - loss: 2.7397 - regression_loss: 2.0207 - classification_loss 5571/10000 [===============>..............] - ETA: 35:30 - loss: 2.7397 - regression_loss: 2.0207 - classification_loss 5572/10000 [===============>..............] - ETA: 35:29 - loss: 2.7396 - regression_loss: 2.0206 - classification_loss 5573/10000 [===============>..............] - ETA: 35:29 - loss: 2.7395 - regression_loss: 2.0205 - classification_loss 5574/10000 [===============>..............] - ETA: 35:28 - loss: 2.7393 - regression_loss: 2.0204 - classification_loss 5575/10000 [===============>..............] - ETA: 35:28 - loss: 2.7393 - regression_loss: 2.0204 - classification_loss 5576/10000 [===============>..............] - ETA: 35:27 - loss: 2.7392 - regression_loss: 2.0203 - classification_loss 5577/10000 [===============>..............] - ETA: 35:27 - loss: 2.7392 - regression_loss: 2.0203 - classification_loss 5578/10000 [===============>..............] - ETA: 35:26 - loss: 2.7391 - regression_loss: 2.0202 - classification_loss 5579/10000 [===============>..............] - ETA: 35:26 - loss: 2.7395 - regression_loss: 2.0205 - classification_loss 5580/10000 [===============>..............] - ETA: 35:25 - loss: 2.7393 - regression_loss: 2.0204 - classification_loss 5581/10000 [===============>..............] - ETA: 35:25 - loss: 2.7393 - regression_loss: 2.0204 - classification_loss 5582/10000 [===============>..............] - ETA: 35:24 - loss: 2.7392 - regression_loss: 2.0203 - classification_loss 5583/10000 [===============>..............] - ETA: 35:24 - loss: 2.7390 - regression_loss: 2.0202 - classification_loss 5584/10000 [===============>..............] - ETA: 35:23 - loss: 2.7389 - regression_loss: 2.0201 - classification_loss 5585/10000 [===============>..............] - ETA: 35:23 - loss: 2.7388 - regression_loss: 2.0200 - classification_loss 5586/10000 [===============>..............] - ETA: 35:22 - loss: 2.7387 - regression_loss: 2.0199 - classification_loss 5587/10000 [===============>..............] - ETA: 35:22 - loss: 2.7386 - regression_loss: 2.0199 - classification_loss 5588/10000 [===============>..............] - ETA: 35:21 - loss: 2.7384 - regression_loss: 2.0197 - classification_loss 5589/10000 [===============>..............] - ETA: 35:21 - loss: 2.7383 - regression_loss: 2.0197 - classification_loss 5590/10000 [===============>..............] - ETA: 35:20 - loss: 2.7383 - regression_loss: 2.0196 - classification_loss 5591/10000 [===============>..............] - ETA: 35:20 - loss: 2.7383 - regression_loss: 2.0197 - classification_loss 5592/10000 [===============>..............] - ETA: 35:19 - loss: 2.7381 - regression_loss: 2.0195 - classification_loss 5593/10000 [===============>..............] - ETA: 35:19 - loss: 2.7380 - regression_loss: 2.0195 - classification_loss 5594/10000 [===============>..............] - ETA: 35:18 - loss: 2.7380 - regression_loss: 2.0195 - classification_loss 5595/10000 [===============>..............] - ETA: 35:18 - loss: 2.7380 - regression_loss: 2.0195 - classification_loss 5596/10000 [===============>..............] - ETA: 35:17 - loss: 2.7379 - regression_loss: 2.0194 - classification_loss 5597/10000 [===============>..............] - ETA: 35:17 - loss: 2.7377 - regression_loss: 2.0193 - classification_loss 5598/10000 [===============>..............] - ETA: 35:16 - loss: 2.7377 - regression_loss: 2.0194 - classification_loss 5599/10000 [===============>..............] - ETA: 35:16 - loss: 2.7378 - regression_loss: 2.0194 - classification_loss 5600/10000 [===============>..............] - ETA: 35:15 - loss: 2.7377 - regression_loss: 2.0194 - classification_loss 5601/10000 [===============>..............] - ETA: 35:15 - loss: 2.7376 - regression_loss: 2.0193 - classification_loss 5602/10000 [===============>..............] - ETA: 35:14 - loss: 2.7375 - regression_loss: 2.0192 - classification_loss 5603/10000 [===============>..............] - ETA: 35:14 - loss: 2.7375 - regression_loss: 2.0192 - classification_loss 5604/10000 [===============>..............] - ETA: 35:13 - loss: 2.7375 - regression_loss: 2.0192 - classification_loss 5605/10000 [===============>..............] - ETA: 35:13 - loss: 2.7374 - regression_loss: 2.0192 - classification_loss 5606/10000 [===============>..............] - ETA: 35:12 - loss: 2.7372 - regression_loss: 2.0190 - classification_loss 5607/10000 [===============>..............] - ETA: 35:12 - loss: 2.7372 - regression_loss: 2.0190 - classification_loss 5608/10000 [===============>..............] - ETA: 35:11 - loss: 2.7373 - regression_loss: 2.0191 - classification_loss 5609/10000 [===============>..............] - ETA: 35:11 - loss: 2.7372 - regression_loss: 2.0190 - classification_loss 5610/10000 [===============>..............] - ETA: 35:10 - loss: 2.7369 - regression_loss: 2.0187 - classification_loss 5611/10000 [===============>..............] - ETA: 35:10 - loss: 2.7369 - regression_loss: 2.0188 - classification_loss 5612/10000 [===============>..............] - ETA: 35:09 - loss: 2.7367 - regression_loss: 2.0186 - classification_loss 5613/10000 [===============>..............] - ETA: 35:09 - loss: 2.7367 - regression_loss: 2.0186 - classification_loss 5614/10000 [===============>..............] - ETA: 35:08 - loss: 2.7366 - regression_loss: 2.0186 - classification_loss 5615/10000 [===============>..............] - ETA: 35:08 - loss: 2.7365 - regression_loss: 2.0185 - classification_loss 5616/10000 [===============>..............] - ETA: 35:07 - loss: 2.7363 - regression_loss: 2.0183 - classification_loss 5617/10000 [===============>..............] - ETA: 35:07 - loss: 2.7361 - regression_loss: 2.0182 - classification_loss 5618/10000 [===============>..............] - ETA: 35:06 - loss: 2.7360 - regression_loss: 2.0181 - classification_loss 5619/10000 [===============>..............] - ETA: 35:06 - loss: 2.7360 - regression_loss: 2.0181 - classification_loss 5620/10000 [===============>..............] - ETA: 35:05 - loss: 2.7360 - regression_loss: 2.0181 - classification_loss 5621/10000 [===============>..............] - ETA: 35:05 - loss: 2.7360 - regression_loss: 2.0181 - classification_loss 5622/10000 [===============>..............] - ETA: 35:04 - loss: 2.7358 - regression_loss: 2.0180 - classification_loss 5623/10000 [===============>..............] - ETA: 35:04 - loss: 2.7358 - regression_loss: 2.0180 - classification_loss 5624/10000 [===============>..............] - ETA: 35:03 - loss: 2.7355 - regression_loss: 2.0178 - classification_loss 5625/10000 [===============>..............] - ETA: 35:03 - loss: 2.7355 - regression_loss: 2.0177 - classification_loss 5626/10000 [===============>..............] - ETA: 35:02 - loss: 2.7355 - regression_loss: 2.0178 - classification_loss 5627/10000 [===============>..............] - ETA: 35:02 - loss: 2.7354 - regression_loss: 2.0177 - classification_loss 5628/10000 [===============>..............] - ETA: 35:01 - loss: 2.7352 - regression_loss: 2.0176 - classification_loss 5629/10000 [===============>..............] - ETA: 35:01 - loss: 2.7350 - regression_loss: 2.0175 - classification_loss 5630/10000 [===============>..............] - ETA: 35:00 - loss: 2.7349 - regression_loss: 2.0174 - classification_loss 5631/10000 [===============>..............] - ETA: 35:00 - loss: 2.7348 - regression_loss: 2.0173 - classification_loss 5632/10000 [===============>..............] - ETA: 34:59 - loss: 2.7346 - regression_loss: 2.0172 - classification_loss 5633/10000 [===============>..............] - ETA: 34:59 - loss: 2.7345 - regression_loss: 2.0171 - classification_loss 5634/10000 [===============>..............] - ETA: 34:58 - loss: 2.7344 - regression_loss: 2.0170 - classification_loss 5635/10000 [===============>..............] - ETA: 34:58 - loss: 2.7343 - regression_loss: 2.0169 - classification_loss 5636/10000 [===============>..............] - ETA: 34:57 - loss: 2.7342 - regression_loss: 2.0169 - classification_loss 5637/10000 [===============>..............] - ETA: 34:57 - loss: 2.7341 - regression_loss: 2.0168 - classification_loss 5638/10000 [===============>..............] - ETA: 34:56 - loss: 2.7340 - regression_loss: 2.0167 - classification_loss 5639/10000 [===============>..............] - ETA: 34:56 - loss: 2.7339 - regression_loss: 2.0167 - classification_loss 5640/10000 [===============>..............] - ETA: 34:55 - loss: 2.7337 - regression_loss: 2.0165 - classification_loss 5641/10000 [===============>..............] - ETA: 34:55 - loss: 2.7337 - regression_loss: 2.0165 - classification_loss 5642/10000 [===============>..............] - ETA: 34:54 - loss: 2.7337 - regression_loss: 2.0165 - classification_loss 5643/10000 [===============>..............] - ETA: 34:54 - loss: 2.7337 - regression_loss: 2.0165 - classification_loss 5644/10000 [===============>..............] - ETA: 34:53 - loss: 2.7336 - regression_loss: 2.0165 - classification_loss 5645/10000 [===============>..............] - ETA: 34:53 - loss: 2.7334 - regression_loss: 2.0164 - classification_loss 5646/10000 [===============>..............] - ETA: 34:52 - loss: 2.7333 - regression_loss: 2.0163 - classification_loss 5647/10000 [===============>..............] - ETA: 34:52 - loss: 2.7332 - regression_loss: 2.0162 - classification_loss 5648/10000 [===============>..............] - ETA: 34:51 - loss: 2.7330 - regression_loss: 2.0160 - classification_loss 5649/10000 [===============>..............] - ETA: 34:51 - loss: 2.7330 - regression_loss: 2.0160 - classification_loss 5650/10000 [===============>..............] - ETA: 34:50 - loss: 2.7329 - regression_loss: 2.0159 - classification_loss 5651/10000 [===============>..............] - ETA: 34:50 - loss: 2.7327 - regression_loss: 2.0158 - classification_loss 5652/10000 [===============>..............] - ETA: 34:49 - loss: 2.7326 - regression_loss: 2.0158 - classification_loss 5653/10000 [===============>..............] - ETA: 34:49 - loss: 2.7326 - regression_loss: 2.0158 - classification_loss 5654/10000 [===============>..............] - ETA: 34:48 - loss: 2.7328 - regression_loss: 2.0159 - classification_loss 5655/10000 [===============>..............] - ETA: 34:48 - loss: 2.7326 - regression_loss: 2.0158 - classification_loss 5656/10000 [===============>..............] - ETA: 34:47 - loss: 2.7323 - regression_loss: 2.0156 - classification_loss 5657/10000 [===============>..............] - ETA: 34:47 - loss: 2.7322 - regression_loss: 2.0155 - classification_loss 5658/10000 [===============>..............] - ETA: 34:46 - loss: 2.7321 - regression_loss: 2.0154 - classification_loss 5659/10000 [===============>..............] - ETA: 34:46 - loss: 2.7318 - regression_loss: 2.0152 - classification_loss 5660/10000 [===============>..............] - ETA: 34:45 - loss: 2.7316 - regression_loss: 2.0151 - classification_loss 5661/10000 [===============>..............] - ETA: 34:45 - loss: 2.7314 - regression_loss: 2.0150 - classification_loss 5662/10000 [===============>..............] - ETA: 34:44 - loss: 2.7314 - regression_loss: 2.0150 - classification_loss 5663/10000 [===============>..............] - ETA: 34:44 - loss: 2.7314 - regression_loss: 2.0150 - classification_loss 5664/10000 [===============>..............] - ETA: 34:43 - loss: 2.7313 - regression_loss: 2.0149 - classification_loss 5665/10000 [===============>..............] - ETA: 34:43 - loss: 2.7312 - regression_loss: 2.0148 - classification_loss 5666/10000 [===============>..............] - ETA: 34:43 - loss: 2.7311 - regression_loss: 2.0147 - classification_loss 5667/10000 [================>.............] - ETA: 34:42 - loss: 2.7310 - regression_loss: 2.0147 - classification_loss 5668/10000 [================>.............] - ETA: 34:42 - loss: 2.7310 - regression_loss: 2.0147 - classification_loss 5669/10000 [================>.............] - ETA: 34:41 - loss: 2.7309 - regression_loss: 2.0147 - classification_loss 5670/10000 [================>.............] - ETA: 34:41 - loss: 2.7309 - regression_loss: 2.0146 - classification_loss 5671/10000 [================>.............] - ETA: 34:40 - loss: 2.7308 - regression_loss: 2.0146 - classification_loss 5672/10000 [================>.............] - ETA: 34:40 - loss: 2.7307 - regression_loss: 2.0145 - classification_loss 5673/10000 [================>.............] - ETA: 34:39 - loss: 2.7306 - regression_loss: 2.0145 - classification_loss 5674/10000 [================>.............] - ETA: 34:39 - loss: 2.7305 - regression_loss: 2.0144 - classification_loss 5675/10000 [================>.............] - ETA: 34:38 - loss: 2.7302 - regression_loss: 2.0142 - classification_loss 5676/10000 [================>.............] - ETA: 34:38 - loss: 2.7302 - regression_loss: 2.0143 - classification_loss 5677/10000 [================>.............] - ETA: 34:37 - loss: 2.7300 - regression_loss: 2.0141 - classification_loss 5678/10000 [================>.............] - ETA: 34:36 - loss: 2.7298 - regression_loss: 2.0139 - classification_loss 5679/10000 [================>.............] - ETA: 34:36 - loss: 2.7296 - regression_loss: 2.0138 - classification_loss 5680/10000 [================>.............] - ETA: 34:35 - loss: 2.7297 - regression_loss: 2.0139 - classification_loss 5681/10000 [================>.............] - ETA: 34:35 - loss: 2.7295 - regression_loss: 2.0138 - classification_loss 5682/10000 [================>.............] - ETA: 34:34 - loss: 2.7295 - regression_loss: 2.0137 - classification_loss 5683/10000 [================>.............] - ETA: 34:34 - loss: 2.7294 - regression_loss: 2.0137 - classification_loss 5684/10000 [================>.............] - ETA: 34:33 - loss: 2.7292 - regression_loss: 2.0136 - classification_loss 5685/10000 [================>.............] - ETA: 34:33 - loss: 2.7290 - regression_loss: 2.0135 - classification_loss 5686/10000 [================>.............] - ETA: 34:32 - loss: 2.7291 - regression_loss: 2.0135 - classification_loss 5687/10000 [================>.............] - ETA: 34:32 - loss: 2.7289 - regression_loss: 2.0134 - classification_loss 5688/10000 [================>.............] - ETA: 34:32 - loss: 2.7289 - regression_loss: 2.0133 - classification_loss 5689/10000 [================>.............] - ETA: 34:31 - loss: 2.7288 - regression_loss: 2.0133 - classification_loss 5690/10000 [================>.............] - ETA: 34:31 - loss: 2.7287 - regression_loss: 2.0132 - classification_loss 5691/10000 [================>.............] - ETA: 34:30 - loss: 2.7287 - regression_loss: 2.0132 - classification_loss 5692/10000 [================>.............] - ETA: 34:30 - loss: 2.7286 - regression_loss: 2.0131 - classification_loss 5693/10000 [================>.............] - ETA: 34:29 - loss: 2.7286 - regression_loss: 2.0131 - classification_loss 5694/10000 [================>.............] - ETA: 34:29 - loss: 2.7284 - regression_loss: 2.0130 - classification_loss 5695/10000 [================>.............] - ETA: 34:28 - loss: 2.7284 - regression_loss: 2.0130 - classification_loss 5696/10000 [================>.............] - ETA: 34:28 - loss: 2.7282 - regression_loss: 2.0128 - classification_loss 5697/10000 [================>.............] - ETA: 34:27 - loss: 2.7282 - regression_loss: 2.0128 - classification_loss 5698/10000 [================>.............] - ETA: 34:27 - loss: 2.7281 - regression_loss: 2.0127 - classification_loss 5699/10000 [================>.............] - ETA: 34:26 - loss: 2.7280 - regression_loss: 2.0128 - classification_loss 5700/10000 [================>.............] - ETA: 34:26 - loss: 2.7279 - regression_loss: 2.0126 - classification_loss 5701/10000 [================>.............] - ETA: 34:25 - loss: 2.7278 - regression_loss: 2.0125 - classification_loss 5702/10000 [================>.............] - ETA: 34:25 - loss: 2.7277 - regression_loss: 2.0125 - classification_loss 5703/10000 [================>.............] - ETA: 34:24 - loss: 2.7278 - regression_loss: 2.0125 - classification_loss 5704/10000 [================>.............] - ETA: 34:24 - loss: 2.7277 - regression_loss: 2.0125 - classification_loss 5705/10000 [================>.............] - ETA: 34:23 - loss: 2.7277 - regression_loss: 2.0126 - classification_loss 5706/10000 [================>.............] - ETA: 34:23 - loss: 2.7277 - regression_loss: 2.0125 - classification_loss 5707/10000 [================>.............] - ETA: 34:22 - loss: 2.7278 - regression_loss: 2.0126 - classification_loss 5708/10000 [================>.............] - ETA: 34:22 - loss: 2.7277 - regression_loss: 2.0126 - classification_loss 5709/10000 [================>.............] - ETA: 34:21 - loss: 2.7278 - regression_loss: 2.0127 - classification_loss 5710/10000 [================>.............] - ETA: 34:21 - loss: 2.7278 - regression_loss: 2.0127 - classification_loss 5711/10000 [================>.............] - ETA: 34:20 - loss: 2.7277 - regression_loss: 2.0126 - classification_loss 5712/10000 [================>.............] - ETA: 34:20 - loss: 2.7276 - regression_loss: 2.0126 - classification_loss 5713/10000 [================>.............] - ETA: 34:19 - loss: 2.7275 - regression_loss: 2.0125 - classification_loss 5714/10000 [================>.............] - ETA: 34:19 - loss: 2.7273 - regression_loss: 2.0124 - classification_loss 5715/10000 [================>.............] - ETA: 34:18 - loss: 2.7271 - regression_loss: 2.0123 - classification_loss 5716/10000 [================>.............] - ETA: 34:18 - loss: 2.7269 - regression_loss: 2.0121 - classification_loss 5717/10000 [================>.............] - ETA: 34:17 - loss: 2.7268 - regression_loss: 2.0120 - classification_loss 5718/10000 [================>.............] - ETA: 34:17 - loss: 2.7267 - regression_loss: 2.0120 - classification_loss 5719/10000 [================>.............] - ETA: 34:16 - loss: 2.7265 - regression_loss: 2.0119 - classification_loss 5720/10000 [================>.............] - ETA: 34:16 - loss: 2.7264 - regression_loss: 2.0118 - classification_loss 5721/10000 [================>.............] - ETA: 34:15 - loss: 2.7265 - regression_loss: 2.0118 - classification_loss 5722/10000 [================>.............] - ETA: 34:15 - loss: 2.7262 - regression_loss: 2.0116 - classification_loss 5723/10000 [================>.............] - ETA: 34:14 - loss: 2.7262 - regression_loss: 2.0116 - classification_loss 5724/10000 [================>.............] - ETA: 34:14 - loss: 2.7260 - regression_loss: 2.0114 - classification_loss 5725/10000 [================>.............] - ETA: 34:13 - loss: 2.7258 - regression_loss: 2.0113 - classification_loss 5726/10000 [================>.............] - ETA: 34:13 - loss: 2.7255 - regression_loss: 2.0110 - classification_loss 5727/10000 [================>.............] - ETA: 34:12 - loss: 2.7254 - regression_loss: 2.0109 - classification_loss 5728/10000 [================>.............] - ETA: 34:12 - loss: 2.7253 - regression_loss: 2.0109 - classification_loss 5729/10000 [================>.............] - ETA: 34:11 - loss: 2.7252 - regression_loss: 2.0108 - classification_loss 5730/10000 [================>.............] - ETA: 34:11 - loss: 2.7250 - regression_loss: 2.0106 - classification_loss 5731/10000 [================>.............] - ETA: 34:10 - loss: 2.7249 - regression_loss: 2.0106 - classification_loss 5732/10000 [================>.............] - ETA: 34:10 - loss: 2.7248 - regression_loss: 2.0105 - classification_loss 5733/10000 [================>.............] - ETA: 34:09 - loss: 2.7247 - regression_loss: 2.0105 - classification_loss 5734/10000 [================>.............] - ETA: 34:09 - loss: 2.7247 - regression_loss: 2.0105 - classification_loss 5735/10000 [================>.............] - ETA: 34:08 - loss: 2.7245 - regression_loss: 2.0103 - classification_loss 5736/10000 [================>.............] - ETA: 34:08 - loss: 2.7245 - regression_loss: 2.0104 - classification_loss 5737/10000 [================>.............] - ETA: 34:07 - loss: 2.7242 - regression_loss: 2.0101 - classification_loss 5738/10000 [================>.............] - ETA: 34:07 - loss: 2.7242 - regression_loss: 2.0101 - classification_loss 5739/10000 [================>.............] - ETA: 34:06 - loss: 2.7242 - regression_loss: 2.0101 - classification_loss 5740/10000 [================>.............] - ETA: 34:06 - loss: 2.7240 - regression_loss: 2.0100 - classification_loss 5741/10000 [================>.............] - ETA: 34:05 - loss: 2.7238 - regression_loss: 2.0098 - classification_loss 5742/10000 [================>.............] - ETA: 34:05 - loss: 2.7238 - regression_loss: 2.0098 - classification_loss 5743/10000 [================>.............] - ETA: 34:04 - loss: 2.7237 - regression_loss: 2.0098 - classification_loss 5744/10000 [================>.............] - ETA: 34:04 - loss: 2.7236 - regression_loss: 2.0097 - classification_loss 5745/10000 [================>.............] - ETA: 34:03 - loss: 2.7235 - regression_loss: 2.0096 - classification_loss 5746/10000 [================>.............] - ETA: 34:03 - loss: 2.7234 - regression_loss: 2.0096 - classification_loss 5747/10000 [================>.............] - ETA: 34:02 - loss: 2.7231 - regression_loss: 2.0094 - classification_loss 5748/10000 [================>.............] - ETA: 34:02 - loss: 2.7229 - regression_loss: 2.0092 - classification_loss 5749/10000 [================>.............] - ETA: 34:01 - loss: 2.7227 - regression_loss: 2.0090 - classification_loss 5750/10000 [================>.............] - ETA: 34:01 - loss: 2.7227 - regression_loss: 2.0091 - classification_loss 5751/10000 [================>.............] - ETA: 34:00 - loss: 2.7227 - regression_loss: 2.0090 - classification_loss 5752/10000 [================>.............] - ETA: 34:00 - loss: 2.7226 - regression_loss: 2.0090 - classification_loss 5753/10000 [================>.............] - ETA: 34:00 - loss: 2.7224 - regression_loss: 2.0089 - classification_loss 5754/10000 [================>.............] - ETA: 33:59 - loss: 2.7224 - regression_loss: 2.0089 - classification_loss 5755/10000 [================>.............] - ETA: 33:59 - loss: 2.7222 - regression_loss: 2.0087 - classification_loss 5756/10000 [================>.............] - ETA: 33:58 - loss: 2.7221 - regression_loss: 2.0086 - classification_loss 5757/10000 [================>.............] - ETA: 33:58 - loss: 2.7220 - regression_loss: 2.0086 - classification_loss 5758/10000 [================>.............] - ETA: 33:57 - loss: 2.7218 - regression_loss: 2.0085 - classification_loss 5759/10000 [================>.............] - ETA: 33:56 - loss: 2.7217 - regression_loss: 2.0084 - classification_loss 5760/10000 [================>.............] - ETA: 33:56 - loss: 2.7216 - regression_loss: 2.0083 - classification_loss 5761/10000 [================>.............] - ETA: 33:56 - loss: 2.7215 - regression_loss: 2.0082 - classification_loss 5762/10000 [================>.............] - ETA: 33:55 - loss: 2.7214 - regression_loss: 2.0082 - classification_loss 5763/10000 [================>.............] - ETA: 33:55 - loss: 2.7214 - regression_loss: 2.0081 - classification_loss 5764/10000 [================>.............] - ETA: 33:54 - loss: 2.7213 - regression_loss: 2.0081 - classification_loss 5765/10000 [================>.............] - ETA: 33:54 - loss: 2.7213 - regression_loss: 2.0081 - classification_loss 5766/10000 [================>.............] - ETA: 33:53 - loss: 2.7211 - regression_loss: 2.0080 - classification_loss 5767/10000 [================>.............] - ETA: 33:53 - loss: 2.7212 - regression_loss: 2.0081 - classification_loss 5768/10000 [================>.............] - ETA: 33:52 - loss: 2.7211 - regression_loss: 2.0080 - classification_loss 5769/10000 [================>.............] - ETA: 33:52 - loss: 2.7210 - regression_loss: 2.0079 - classification_loss 5770/10000 [================>.............] - ETA: 33:51 - loss: 2.7208 - regression_loss: 2.0078 - classification_loss 5771/10000 [================>.............] - ETA: 33:51 - loss: 2.7208 - regression_loss: 2.0078 - classification_loss 5772/10000 [================>.............] - ETA: 33:50 - loss: 2.7208 - regression_loss: 2.0078 - classification_loss 5773/10000 [================>.............] - ETA: 33:50 - loss: 2.7207 - regression_loss: 2.0077 - classification_loss 5774/10000 [================>.............] - ETA: 33:49 - loss: 2.7206 - regression_loss: 2.0076 - classification_loss 5775/10000 [================>.............] - ETA: 33:49 - loss: 2.7205 - regression_loss: 2.0076 - classification_loss 5776/10000 [================>.............] - ETA: 33:48 - loss: 2.7203 - regression_loss: 2.0074 - classification_loss 5777/10000 [================>.............] - ETA: 33:48 - loss: 2.7202 - regression_loss: 2.0074 - classification_loss 5778/10000 [================>.............] - ETA: 33:47 - loss: 2.7200 - regression_loss: 2.0072 - classification_loss 5779/10000 [================>.............] - ETA: 33:47 - loss: 2.7197 - regression_loss: 2.0070 - classification_loss 5780/10000 [================>.............] - ETA: 33:46 - loss: 2.7196 - regression_loss: 2.0070 - classification_loss 5781/10000 [================>.............] - ETA: 33:46 - loss: 2.7196 - regression_loss: 2.0069 - classification_loss 5782/10000 [================>.............] - ETA: 33:45 - loss: 2.7193 - regression_loss: 2.0067 - classification_loss 5783/10000 [================>.............] - ETA: 33:45 - loss: 2.7192 - regression_loss: 2.0066 - classification_loss 5784/10000 [================>.............] - ETA: 33:44 - loss: 2.7190 - regression_loss: 2.0064 - classification_loss 5785/10000 [================>.............] - ETA: 33:44 - loss: 2.7189 - regression_loss: 2.0063 - classification_loss 5786/10000 [================>.............] - ETA: 33:43 - loss: 2.7189 - regression_loss: 2.0063 - classification_loss 5787/10000 [================>.............] - ETA: 33:43 - loss: 2.7189 - regression_loss: 2.0063 - classification_loss 5788/10000 [================>.............] - ETA: 33:42 - loss: 2.7187 - regression_loss: 2.0061 - classification_loss 5789/10000 [================>.............] - ETA: 33:42 - loss: 2.7186 - regression_loss: 2.0061 - classification_loss 5790/10000 [================>.............] - ETA: 33:41 - loss: 2.7184 - regression_loss: 2.0059 - classification_loss 5791/10000 [================>.............] - ETA: 33:41 - loss: 2.7181 - regression_loss: 2.0057 - classification_loss 5792/10000 [================>.............] - ETA: 33:40 - loss: 2.7180 - regression_loss: 2.0056 - classification_loss 5793/10000 [================>.............] - ETA: 33:40 - loss: 2.7181 - regression_loss: 2.0057 - classification_loss 5794/10000 [================>.............] - ETA: 33:39 - loss: 2.7178 - regression_loss: 2.0054 - classification_loss 5795/10000 [================>.............] - ETA: 33:39 - loss: 2.7178 - regression_loss: 2.0054 - classification_loss 5796/10000 [================>.............] - ETA: 33:38 - loss: 2.7178 - regression_loss: 2.0055 - classification_loss 5797/10000 [================>.............] - ETA: 33:38 - loss: 2.7178 - regression_loss: 2.0055 - classification_loss 5798/10000 [================>.............] - ETA: 33:37 - loss: 2.7177 - regression_loss: 2.0054 - classification_loss 5799/10000 [================>.............] - ETA: 33:37 - loss: 2.7177 - regression_loss: 2.0054 - classification_loss 5800/10000 [================>.............] - ETA: 33:36 - loss: 2.7175 - regression_loss: 2.0052 - classification_loss 5801/10000 [================>.............] - ETA: 33:36 - loss: 2.7176 - regression_loss: 2.0053 - classification_loss 5802/10000 [================>.............] - ETA: 33:35 - loss: 2.7177 - regression_loss: 2.0053 - classification_loss 5803/10000 [================>.............] - ETA: 33:35 - loss: 2.7176 - regression_loss: 2.0053 - classification_loss 5804/10000 [================>.............] - ETA: 33:34 - loss: 2.7175 - regression_loss: 2.0052 - classification_loss 5805/10000 [================>.............] - ETA: 33:34 - loss: 2.7175 - regression_loss: 2.0053 - classification_loss 5806/10000 [================>.............] - ETA: 33:33 - loss: 2.7175 - regression_loss: 2.0053 - classification_loss 5807/10000 [================>.............] - ETA: 33:33 - loss: 2.7174 - regression_loss: 2.0052 - classification_loss 5808/10000 [================>.............] - ETA: 33:32 - loss: 2.7174 - regression_loss: 2.0052 - classification_loss 5809/10000 [================>.............] - ETA: 33:32 - loss: 2.7173 - regression_loss: 2.0051 - classification_loss 5810/10000 [================>.............] - ETA: 33:31 - loss: 2.7171 - regression_loss: 2.0049 - classification_loss 5811/10000 [================>.............] - ETA: 33:31 - loss: 2.7171 - regression_loss: 2.0049 - classification_loss 5812/10000 [================>.............] - ETA: 33:30 - loss: 2.7170 - regression_loss: 2.0049 - classification_loss 5813/10000 [================>.............] - ETA: 33:30 - loss: 2.7169 - regression_loss: 2.0048 - classification_loss 5814/10000 [================>.............] - ETA: 33:29 - loss: 2.7166 - regression_loss: 2.0046 - classification_loss 5815/10000 [================>.............] - ETA: 33:29 - loss: 2.7165 - regression_loss: 2.0045 - classification_loss 5816/10000 [================>.............] - ETA: 33:28 - loss: 2.7165 - regression_loss: 2.0045 - classification_loss 5817/10000 [================>.............] - ETA: 33:28 - loss: 2.7164 - regression_loss: 2.0045 - classification_loss 5818/10000 [================>.............] - ETA: 33:28 - loss: 2.7163 - regression_loss: 2.0044 - classification_loss 5819/10000 [================>.............] - ETA: 33:27 - loss: 2.7162 - regression_loss: 2.0044 - classification_loss 5820/10000 [================>.............] - ETA: 33:27 - loss: 2.7161 - regression_loss: 2.0043 - classification_loss 5821/10000 [================>.............] - ETA: 33:26 - loss: 2.7161 - regression_loss: 2.0044 - classification_loss 5822/10000 [================>.............] - ETA: 33:26 - loss: 2.7161 - regression_loss: 2.0043 - classification_loss 5823/10000 [================>.............] - ETA: 33:25 - loss: 2.7160 - regression_loss: 2.0043 - classification_loss 5824/10000 [================>.............] - ETA: 33:25 - loss: 2.7157 - regression_loss: 2.0041 - classification_loss 5825/10000 [================>.............] - ETA: 33:24 - loss: 2.7156 - regression_loss: 2.0039 - classification_loss 5826/10000 [================>.............] - ETA: 33:24 - loss: 2.7155 - regression_loss: 2.0039 - classification_loss 5827/10000 [================>.............] - ETA: 33:23 - loss: 2.7156 - regression_loss: 2.0040 - classification_loss 5828/10000 [================>.............] - ETA: 33:23 - loss: 2.7155 - regression_loss: 2.0040 - classification_loss 5829/10000 [================>.............] - ETA: 33:22 - loss: 2.7155 - regression_loss: 2.0039 - classification_loss 5830/10000 [================>.............] - ETA: 33:22 - loss: 2.7153 - regression_loss: 2.0038 - classification_loss 5831/10000 [================>.............] - ETA: 33:21 - loss: 2.7153 - regression_loss: 2.0037 - classification_loss 5832/10000 [================>.............] - ETA: 33:21 - loss: 2.7153 - regression_loss: 2.0037 - classification_loss 5833/10000 [================>.............] - ETA: 33:20 - loss: 2.7152 - regression_loss: 2.0037 - classification_loss 5834/10000 [================>.............] - ETA: 33:20 - loss: 2.7153 - regression_loss: 2.0038 - classification_loss 5835/10000 [================>.............] - ETA: 33:19 - loss: 2.7153 - regression_loss: 2.0038 - classification_loss 5836/10000 [================>.............] - ETA: 33:19 - loss: 2.7152 - regression_loss: 2.0038 - classification_loss 5837/10000 [================>.............] - ETA: 33:18 - loss: 2.7151 - regression_loss: 2.0037 - classification_loss 5838/10000 [================>.............] - ETA: 33:18 - loss: 2.7148 - regression_loss: 2.0035 - classification_loss 5839/10000 [================>.............] - ETA: 33:17 - loss: 2.7146 - regression_loss: 2.0033 - classification_loss 5840/10000 [================>.............] - ETA: 33:17 - loss: 2.7145 - regression_loss: 2.0033 - classification_loss 5841/10000 [================>.............] - ETA: 33:16 - loss: 2.7145 - regression_loss: 2.0033 - classification_loss 5842/10000 [================>.............] - ETA: 33:16 - loss: 2.7144 - regression_loss: 2.0032 - classification_loss 5843/10000 [================>.............] - ETA: 33:15 - loss: 2.7144 - regression_loss: 2.0032 - classification_loss 5844/10000 [================>.............] - ETA: 33:15 - loss: 2.7143 - regression_loss: 2.0031 - classification_loss 5845/10000 [================>.............] - ETA: 33:14 - loss: 2.7142 - regression_loss: 2.0030 - classification_loss 5846/10000 [================>.............] - ETA: 33:14 - loss: 2.7141 - regression_loss: 2.0030 - classification_loss 5847/10000 [================>.............] - ETA: 33:13 - loss: 2.7139 - regression_loss: 2.0028 - classification_loss 5848/10000 [================>.............] - ETA: 33:13 - loss: 2.7139 - regression_loss: 2.0029 - classification_loss 5849/10000 [================>.............] - ETA: 33:12 - loss: 2.7138 - regression_loss: 2.0028 - classification_loss 5850/10000 [================>.............] - ETA: 33:12 - loss: 2.7136 - regression_loss: 2.0027 - classification_loss 5851/10000 [================>.............] - ETA: 33:11 - loss: 2.7133 - regression_loss: 2.0025 - classification_loss 5852/10000 [================>.............] - ETA: 33:11 - loss: 2.7131 - regression_loss: 2.0023 - classification_loss 5853/10000 [================>.............] - ETA: 33:10 - loss: 2.7131 - regression_loss: 2.0023 - classification_loss 5854/10000 [================>.............] - ETA: 33:10 - loss: 2.7130 - regression_loss: 2.0022 - classification_loss 5855/10000 [================>.............] - ETA: 33:09 - loss: 2.7129 - regression_loss: 2.0022 - classification_loss 5856/10000 [================>.............] - ETA: 33:09 - loss: 2.7131 - regression_loss: 2.0023 - classification_loss 5857/10000 [================>.............] - ETA: 33:08 - loss: 2.7131 - regression_loss: 2.0024 - classification_loss 5858/10000 [================>.............] - ETA: 33:08 - loss: 2.7130 - regression_loss: 2.0023 - classification_loss 5859/10000 [================>.............] - ETA: 33:07 - loss: 2.7129 - regression_loss: 2.0023 - classification_loss 5860/10000 [================>.............] - ETA: 33:07 - loss: 2.7129 - regression_loss: 2.0022 - classification_loss 5861/10000 [================>.............] - ETA: 33:06 - loss: 2.7128 - regression_loss: 2.0021 - classification_loss 5862/10000 [================>.............] - ETA: 33:06 - loss: 2.7127 - regression_loss: 2.0020 - classification_loss 5863/10000 [================>.............] - ETA: 33:05 - loss: 2.7126 - regression_loss: 2.0020 - classification_loss 5864/10000 [================>.............] - ETA: 33:05 - loss: 2.7124 - regression_loss: 2.0019 - classification_loss 5865/10000 [================>.............] - ETA: 33:04 - loss: 2.7125 - regression_loss: 2.0019 - classification_loss 5866/10000 [================>.............] - ETA: 33:04 - loss: 2.7123 - regression_loss: 2.0018 - classification_loss 5867/10000 [================>.............] - ETA: 33:03 - loss: 2.7122 - regression_loss: 2.0018 - classification_loss 5868/10000 [================>.............] - ETA: 33:03 - loss: 2.7120 - regression_loss: 2.0017 - classification_loss 5869/10000 [================>.............] - ETA: 33:02 - loss: 2.7120 - regression_loss: 2.0017 - classification_loss 5870/10000 [================>.............] - ETA: 33:02 - loss: 2.7118 - regression_loss: 2.0015 - classification_loss 5871/10000 [================>.............] - ETA: 33:01 - loss: 2.7117 - regression_loss: 2.0015 - classification_loss 5872/10000 [================>.............] - ETA: 33:01 - loss: 2.7117 - regression_loss: 2.0015 - classification_loss 5873/10000 [================>.............] - ETA: 33:00 - loss: 2.7116 - regression_loss: 2.0014 - classification_loss 5874/10000 [================>.............] - ETA: 33:00 - loss: 2.7115 - regression_loss: 2.0013 - classification_loss 5875/10000 [================>.............] - ETA: 32:59 - loss: 2.7114 - regression_loss: 2.0013 - classification_loss 5876/10000 [================>.............] - ETA: 32:59 - loss: 2.7113 - regression_loss: 2.0012 - classification_loss 5877/10000 [================>.............] - ETA: 32:58 - loss: 2.7110 - regression_loss: 2.0009 - classification_loss 5878/10000 [================>.............] - ETA: 32:58 - loss: 2.7109 - regression_loss: 2.0009 - classification_loss 5879/10000 [================>.............] - ETA: 32:57 - loss: 2.7108 - regression_loss: 2.0009 - classification_loss 5880/10000 [================>.............] - ETA: 32:57 - loss: 2.7106 - regression_loss: 2.0007 - classification_loss 5881/10000 [================>.............] - ETA: 32:56 - loss: 2.7103 - regression_loss: 2.0005 - classification_loss 5882/10000 [================>.............] - ETA: 32:56 - loss: 2.7102 - regression_loss: 2.0004 - classification_loss 5883/10000 [================>.............] - ETA: 32:55 - loss: 2.7100 - regression_loss: 2.0002 - classification_loss 5884/10000 [================>.............] - ETA: 32:55 - loss: 2.7098 - regression_loss: 2.0001 - classification_loss 5885/10000 [================>.............] - ETA: 32:54 - loss: 2.7098 - regression_loss: 2.0001 - classification_loss 5886/10000 [================>.............] - ETA: 32:54 - loss: 2.7097 - regression_loss: 2.0000 - classification_loss 5887/10000 [================>.............] - ETA: 32:53 - loss: 2.7096 - regression_loss: 2.0000 - classification_loss 5888/10000 [================>.............] - ETA: 32:53 - loss: 2.7094 - regression_loss: 1.9998 - classification_loss 5889/10000 [================>.............] - ETA: 32:52 - loss: 2.7092 - regression_loss: 1.9997 - classification_loss 5890/10000 [================>.............] - ETA: 32:52 - loss: 2.7090 - regression_loss: 1.9995 - classification_loss 5891/10000 [================>.............] - ETA: 32:51 - loss: 2.7088 - regression_loss: 1.9993 - classification_loss 5892/10000 [================>.............] - ETA: 32:51 - loss: 2.7090 - regression_loss: 1.9995 - classification_loss 5893/10000 [================>.............] - ETA: 32:50 - loss: 2.7088 - regression_loss: 1.9993 - classification_loss 5894/10000 [================>.............] - ETA: 32:50 - loss: 2.7086 - regression_loss: 1.9992 - classification_loss 5895/10000 [================>.............] - ETA: 32:49 - loss: 2.7085 - regression_loss: 1.9991 - classification_loss 5896/10000 [================>.............] - ETA: 32:49 - loss: 2.7084 - regression_loss: 1.9991 - classification_loss 5897/10000 [================>.............] - ETA: 32:48 - loss: 2.7082 - regression_loss: 1.9989 - classification_loss 5898/10000 [================>.............] - ETA: 32:48 - loss: 2.7080 - regression_loss: 1.9987 - classification_loss 5899/10000 [================>.............] - ETA: 32:47 - loss: 2.7079 - regression_loss: 1.9987 - classification_loss 5900/10000 [================>.............] - ETA: 32:47 - loss: 2.7080 - regression_loss: 1.9987 - classification_loss 5901/10000 [================>.............] - ETA: 32:46 - loss: 2.7079 - regression_loss: 1.9987 - classification_loss 5902/10000 [================>.............] - ETA: 32:46 - loss: 2.7080 - regression_loss: 1.9987 - classification_loss 5903/10000 [================>.............] - ETA: 32:45 - loss: 2.7079 - regression_loss: 1.9987 - classification_loss 5904/10000 [================>.............] - ETA: 32:45 - loss: 2.7079 - regression_loss: 1.9987 - classification_loss 5905/10000 [================>.............] - ETA: 32:44 - loss: 2.7076 - regression_loss: 1.9984 - classification_loss 5906/10000 [================>.............] - ETA: 32:44 - loss: 2.7076 - regression_loss: 1.9984 - classification_loss 5907/10000 [================>.............] - ETA: 32:43 - loss: 2.7074 - regression_loss: 1.9983 - classification_loss 5908/10000 [================>.............] - ETA: 32:43 - loss: 2.7075 - regression_loss: 1.9984 - classification_loss 5909/10000 [================>.............] - ETA: 32:42 - loss: 2.7074 - regression_loss: 1.9983 - classification_loss 5910/10000 [================>.............] - ETA: 32:42 - loss: 2.7071 - regression_loss: 1.9982 - classification_loss 5911/10000 [================>.............] - ETA: 32:41 - loss: 2.7070 - regression_loss: 1.9981 - classification_loss 5912/10000 [================>.............] - ETA: 32:41 - loss: 2.7069 - regression_loss: 1.9980 - classification_loss 5913/10000 [================>.............] - ETA: 32:40 - loss: 2.7067 - regression_loss: 1.9978 - classification_loss 5914/10000 [================>.............] - ETA: 32:40 - loss: 2.7067 - regression_loss: 1.9978 - classification_loss 5915/10000 [================>.............] - ETA: 32:39 - loss: 2.7065 - regression_loss: 1.9977 - classification_loss 5916/10000 [================>.............] - ETA: 32:39 - loss: 2.7065 - regression_loss: 1.9977 - classification_loss 5917/10000 [================>.............] - ETA: 32:38 - loss: 2.7063 - regression_loss: 1.9976 - classification_loss 5918/10000 [================>.............] - ETA: 32:38 - loss: 2.7063 - regression_loss: 1.9976 - classification_loss 5919/10000 [================>.............] - ETA: 32:37 - loss: 2.7063 - regression_loss: 1.9976 - classification_loss 5920/10000 [================>.............] - ETA: 32:37 - loss: 2.7063 - regression_loss: 1.9976 - classification_loss 5921/10000 [================>.............] - ETA: 32:37 - loss: 2.7062 - regression_loss: 1.9976 - classification_loss 5922/10000 [================>.............] - ETA: 32:36 - loss: 2.7061 - regression_loss: 1.9975 - classification_loss 5923/10000 [================>.............] - ETA: 32:36 - loss: 2.7060 - regression_loss: 1.9974 - classification_loss 5924/10000 [================>.............] - ETA: 32:35 - loss: 2.7060 - regression_loss: 1.9975 - classification_loss 5925/10000 [================>.............] - ETA: 32:35 - loss: 2.7058 - regression_loss: 1.9972 - classification_loss 5926/10000 [================>.............] - ETA: 32:34 - loss: 2.7058 - regression_loss: 1.9973 - classification_loss 5927/10000 [================>.............] - ETA: 32:34 - loss: 2.7058 - regression_loss: 1.9973 - classification_loss 5928/10000 [================>.............] - ETA: 32:33 - loss: 2.7058 - regression_loss: 1.9973 - classification_loss 5929/10000 [================>.............] - ETA: 32:33 - loss: 2.7056 - regression_loss: 1.9972 - classification_loss 5930/10000 [================>.............] - ETA: 32:32 - loss: 2.7055 - regression_loss: 1.9972 - classification_loss 5931/10000 [================>.............] - ETA: 32:32 - loss: 2.7055 - regression_loss: 1.9972 - classification_loss 5932/10000 [================>.............] - ETA: 32:31 - loss: 2.7054 - regression_loss: 1.9971 - classification_loss 5933/10000 [================>.............] - ETA: 32:31 - loss: 2.7052 - regression_loss: 1.9969 - classification_loss 5934/10000 [================>.............] - ETA: 32:30 - loss: 2.7053 - regression_loss: 1.9969 - classification_loss 5935/10000 [================>.............] - ETA: 32:30 - loss: 2.7053 - regression_loss: 1.9969 - classification_loss 5936/10000 [================>.............] - ETA: 32:29 - loss: 2.7052 - regression_loss: 1.9969 - classification_loss 5937/10000 [================>.............] - ETA: 32:29 - loss: 2.7050 - regression_loss: 1.9967 - classification_loss 5938/10000 [================>.............] - ETA: 32:28 - loss: 2.7051 - regression_loss: 1.9968 - classification_loss 5939/10000 [================>.............] - ETA: 32:28 - loss: 2.7049 - regression_loss: 1.9967 - classification_loss 5940/10000 [================>.............] - ETA: 32:27 - loss: 2.7049 - regression_loss: 1.9967 - classification_loss 5941/10000 [================>.............] - ETA: 32:27 - loss: 2.7049 - regression_loss: 1.9967 - classification_loss 5942/10000 [================>.............] - ETA: 32:26 - loss: 2.7047 - regression_loss: 1.9966 - classification_loss 5943/10000 [================>.............] - ETA: 32:26 - loss: 2.7047 - regression_loss: 1.9966 - classification_loss 5944/10000 [================>.............] - ETA: 32:25 - loss: 2.7046 - regression_loss: 1.9966 - classification_loss 5945/10000 [================>.............] - ETA: 32:25 - loss: 2.7044 - regression_loss: 1.9964 - classification_loss 5946/10000 [================>.............] - ETA: 32:24 - loss: 2.7043 - regression_loss: 1.9963 - classification_loss 5947/10000 [================>.............] - ETA: 32:24 - loss: 2.7042 - regression_loss: 1.9962 - classification_loss 5948/10000 [================>.............] - ETA: 32:23 - loss: 2.7042 - regression_loss: 1.9961 - classification_loss 5949/10000 [================>.............] - ETA: 32:23 - loss: 2.7040 - regression_loss: 1.9960 - classification_loss 5950/10000 [================>.............] - ETA: 32:22 - loss: 2.7039 - regression_loss: 1.9959 - classification_loss 5951/10000 [================>.............] - ETA: 32:22 - loss: 2.7039 - regression_loss: 1.9960 - classification_loss 5952/10000 [================>.............] - ETA: 32:21 - loss: 2.7037 - regression_loss: 1.9958 - classification_loss 5953/10000 [================>.............] - ETA: 32:21 - loss: 2.7037 - regression_loss: 1.9958 - classification_loss 5954/10000 [================>.............] - ETA: 32:20 - loss: 2.7034 - regression_loss: 1.9956 - classification_loss 5955/10000 [================>.............] - ETA: 32:20 - loss: 2.7033 - regression_loss: 1.9956 - classification_loss 5956/10000 [================>.............] - ETA: 32:19 - loss: 2.7034 - regression_loss: 1.9955 - classification_loss 5957/10000 [================>.............] - ETA: 32:19 - loss: 2.7033 - regression_loss: 1.9955 - classification_loss 5958/10000 [================>.............] - ETA: 32:18 - loss: 2.7033 - regression_loss: 1.9956 - classification_loss 5959/10000 [================>.............] - ETA: 32:18 - loss: 2.7030 - regression_loss: 1.9953 - classification_loss 5960/10000 [================>.............] - ETA: 32:17 - loss: 2.7029 - regression_loss: 1.9953 - classification_loss 5961/10000 [================>.............] - ETA: 32:17 - loss: 2.7028 - regression_loss: 1.9952 - classification_loss 5962/10000 [================>.............] - ETA: 32:16 - loss: 2.7029 - regression_loss: 1.9953 - classification_loss 5963/10000 [================>.............] - ETA: 32:16 - loss: 2.7028 - regression_loss: 1.9953 - classification_loss 5964/10000 [================>.............] - ETA: 32:15 - loss: 2.7028 - regression_loss: 1.9953 - classification_loss 5965/10000 [================>.............] - ETA: 32:15 - loss: 2.7027 - regression_loss: 1.9953 - classification_loss 5966/10000 [================>.............] - ETA: 32:14 - loss: 2.7026 - regression_loss: 1.9952 - classification_loss 5967/10000 [================>.............] - ETA: 32:14 - loss: 2.7025 - regression_loss: 1.9951 - classification_loss 5968/10000 [================>.............] - ETA: 32:13 - loss: 2.7025 - regression_loss: 1.9951 - classification_loss 5969/10000 [================>.............] - ETA: 32:13 - loss: 2.7023 - regression_loss: 1.9949 - classification_loss 5970/10000 [================>.............] - ETA: 32:12 - loss: 2.7021 - regression_loss: 1.9949 - classification_loss 5971/10000 [================>.............] - ETA: 32:12 - loss: 2.7021 - regression_loss: 1.9948 - classification_loss 5972/10000 [================>.............] - ETA: 32:11 - loss: 2.7019 - regression_loss: 1.9947 - classification_loss 5973/10000 [================>.............] - ETA: 32:11 - loss: 2.7019 - regression_loss: 1.9948 - classification_loss 5974/10000 [================>.............] - ETA: 32:10 - loss: 2.7019 - regression_loss: 1.9948 - classification_loss 5975/10000 [================>.............] - ETA: 32:10 - loss: 2.7019 - regression_loss: 1.9948 - classification_loss 5976/10000 [================>.............] - ETA: 32:09 - loss: 2.7016 - regression_loss: 1.9946 - classification_loss 5977/10000 [================>.............] - ETA: 32:09 - loss: 2.7014 - regression_loss: 1.9944 - classification_loss 5978/10000 [================>.............] - ETA: 32:08 - loss: 2.7015 - regression_loss: 1.9945 - classification_loss 5979/10000 [================>.............] - ETA: 32:08 - loss: 2.7013 - regression_loss: 1.9943 - classification_loss 5980/10000 [================>.............] - ETA: 32:07 - loss: 2.7013 - regression_loss: 1.9943 - classification_loss 5981/10000 [================>.............] - ETA: 32:07 - loss: 2.7011 - regression_loss: 1.9942 - classification_loss 5982/10000 [================>.............] - ETA: 32:06 - loss: 2.7011 - regression_loss: 1.9941 - classification_loss 5983/10000 [================>.............] - ETA: 32:06 - loss: 2.7010 - regression_loss: 1.9941 - classification_loss 5984/10000 [================>.............] - ETA: 32:05 - loss: 2.7010 - regression_loss: 1.9941 - classification_loss 5985/10000 [================>.............] - ETA: 32:05 - loss: 2.7008 - regression_loss: 1.9939 - classification_loss 5986/10000 [================>.............] - ETA: 32:04 - loss: 2.7009 - regression_loss: 1.9940 - classification_loss 5987/10000 [================>.............] - ETA: 32:04 - loss: 2.7009 - regression_loss: 1.9939 - classification_loss 5988/10000 [================>.............] - ETA: 32:03 - loss: 2.7008 - regression_loss: 1.9939 - classification_loss 5989/10000 [================>.............] - ETA: 32:03 - loss: 2.7007 - regression_loss: 1.9938 - classification_loss 5990/10000 [================>.............] - ETA: 32:02 - loss: 2.7006 - regression_loss: 1.9937 - classification_loss 5991/10000 [================>.............] - ETA: 32:02 - loss: 2.7006 - regression_loss: 1.9937 - classification_loss 5992/10000 [================>.............] - ETA: 32:01 - loss: 2.7005 - regression_loss: 1.9937 - classification_loss 5993/10000 [================>.............] - ETA: 32:01 - loss: 2.7004 - regression_loss: 1.9935 - classification_loss 5994/10000 [================>.............] - ETA: 32:00 - loss: 2.7003 - regression_loss: 1.9935 - classification_loss 5995/10000 [================>.............] - ETA: 32:00 - loss: 2.7002 - regression_loss: 1.9934 - classification_loss 5996/10000 [================>.............] - ETA: 31:59 - loss: 2.7001 - regression_loss: 1.9933 - classification_loss 5997/10000 [================>.............] - ETA: 31:59 - loss: 2.7000 - regression_loss: 1.9932 - classification_loss 5998/10000 [================>.............] - ETA: 31:58 - loss: 2.6998 - regression_loss: 1.9931 - classification_loss 5999/10000 [================>.............] - ETA: 31:58 - loss: 2.6997 - regression_loss: 1.9930 - classification_loss 6000/10000 [=================>............] - ETA: 31:57 - loss: 2.6995 - regression_loss: 1.9929 - classification_loss 6001/10000 [=================>............] - ETA: 31:57 - loss: 2.6993 - regression_loss: 1.9926 - classification_loss 6002/10000 [=================>............] - ETA: 31:56 - loss: 2.6992 - regression_loss: 1.9926 - classification_loss 6003/10000 [=================>............] - ETA: 31:56 - loss: 2.6992 - regression_loss: 1.9926 - classification_loss 6004/10000 [=================>............] - ETA: 31:55 - loss: 2.6990 - regression_loss: 1.9925 - classification_loss 6005/10000 [=================>............] - ETA: 31:55 - loss: 2.6990 - regression_loss: 1.9926 - classification_loss 6006/10000 [=================>............] - ETA: 31:54 - loss: 2.6990 - regression_loss: 1.9926 - classification_loss 6007/10000 [=================>............] - ETA: 31:54 - loss: 2.6989 - regression_loss: 1.9925 - classification_loss 6008/10000 [=================>............] - ETA: 31:53 - loss: 2.6987 - regression_loss: 1.9923 - classification_loss 6009/10000 [=================>............] - ETA: 31:53 - loss: 2.6985 - regression_loss: 1.9922 - classification_loss 6010/10000 [=================>............] - ETA: 31:52 - loss: 2.6984 - regression_loss: 1.9920 - classification_loss 6011/10000 [=================>............] - ETA: 31:52 - loss: 2.6983 - regression_loss: 1.9920 - classification_loss 6012/10000 [=================>............] - ETA: 31:51 - loss: 2.6983 - regression_loss: 1.9920 - classification_loss 6013/10000 [=================>............] - ETA: 31:51 - loss: 2.6983 - regression_loss: 1.9920 - classification_loss 6014/10000 [=================>............] - ETA: 31:50 - loss: 2.6983 - regression_loss: 1.9920 - classification_loss 6015/10000 [=================>............] - ETA: 31:50 - loss: 2.6980 - regression_loss: 1.9918 - classification_loss 6016/10000 [=================>............] - ETA: 31:50 - loss: 2.6980 - regression_loss: 1.9918 - classification_loss 6017/10000 [=================>............] - ETA: 31:49 - loss: 2.6978 - regression_loss: 1.9917 - classification_loss 6018/10000 [=================>............] - ETA: 31:49 - loss: 2.6976 - regression_loss: 1.9915 - classification_loss 6019/10000 [=================>............] - ETA: 31:48 - loss: 2.6975 - regression_loss: 1.9915 - classification_loss 6020/10000 [=================>............] - ETA: 31:48 - loss: 2.6975 - regression_loss: 1.9915 - classification_loss 6021/10000 [=================>............] - ETA: 31:47 - loss: 2.6975 - regression_loss: 1.9914 - classification_loss 6022/10000 [=================>............] - ETA: 31:47 - loss: 2.6973 - regression_loss: 1.9913 - classification_loss 6023/10000 [=================>............] - ETA: 31:46 - loss: 2.6971 - regression_loss: 1.9912 - classification_loss 6024/10000 [=================>............] - ETA: 31:46 - loss: 2.6970 - regression_loss: 1.9911 - classification_loss 6025/10000 [=================>............] - ETA: 31:45 - loss: 2.6969 - regression_loss: 1.9910 - classification_loss 6026/10000 [=================>............] - ETA: 31:45 - loss: 2.6968 - regression_loss: 1.9909 - classification_loss 6027/10000 [=================>............] - ETA: 31:44 - loss: 2.6969 - regression_loss: 1.9909 - classification_loss 6028/10000 [=================>............] - ETA: 31:44 - loss: 2.6969 - regression_loss: 1.9910 - classification_loss 6029/10000 [=================>............] - ETA: 31:43 - loss: 2.6967 - regression_loss: 1.9908 - classification_loss 6030/10000 [=================>............] - ETA: 31:43 - loss: 2.6967 - regression_loss: 1.9908 - classification_loss 6031/10000 [=================>............] - ETA: 31:42 - loss: 2.6966 - regression_loss: 1.9908 - classification_loss 6032/10000 [=================>............] - ETA: 31:42 - loss: 2.6967 - regression_loss: 1.9909 - classification_loss 6033/10000 [=================>............] - ETA: 31:41 - loss: 2.6966 - regression_loss: 1.9908 - classification_loss 6034/10000 [=================>............] - ETA: 31:41 - loss: 2.6964 - regression_loss: 1.9906 - classification_loss 6035/10000 [=================>............] - ETA: 31:40 - loss: 2.6963 - regression_loss: 1.9906 - classification_loss 6036/10000 [=================>............] - ETA: 31:40 - loss: 2.6962 - regression_loss: 1.9906 - classification_loss 6037/10000 [=================>............] - ETA: 31:39 - loss: 2.6962 - regression_loss: 1.9905 - classification_loss 6038/10000 [=================>............] - ETA: 31:39 - loss: 2.6959 - regression_loss: 1.9903 - classification_loss 6039/10000 [=================>............] - ETA: 31:38 - loss: 2.6957 - regression_loss: 1.9902 - classification_loss 6040/10000 [=================>............] - ETA: 31:38 - loss: 2.6957 - regression_loss: 1.9901 - classification_loss 6041/10000 [=================>............] - ETA: 31:37 - loss: 2.6957 - regression_loss: 1.9902 - classification_loss 6042/10000 [=================>............] - ETA: 31:37 - loss: 2.6959 - regression_loss: 1.9903 - classification_loss 6043/10000 [=================>............] - ETA: 31:36 - loss: 2.6958 - regression_loss: 1.9902 - classification_loss 6044/10000 [=================>............] - ETA: 31:36 - loss: 2.6957 - regression_loss: 1.9901 - classification_loss 6045/10000 [=================>............] - ETA: 31:35 - loss: 2.6956 - regression_loss: 1.9901 - classification_loss 6046/10000 [=================>............] - ETA: 31:35 - loss: 2.6956 - regression_loss: 1.9901 - classification_loss 6047/10000 [=================>............] - ETA: 31:34 - loss: 2.6954 - regression_loss: 1.9900 - classification_loss 6048/10000 [=================>............] - ETA: 31:34 - loss: 2.6954 - regression_loss: 1.9900 - classification_loss 6049/10000 [=================>............] - ETA: 31:33 - loss: 2.6952 - regression_loss: 1.9898 - classification_loss 6050/10000 [=================>............] - ETA: 31:33 - loss: 2.6951 - regression_loss: 1.9898 - classification_loss 6051/10000 [=================>............] - ETA: 31:32 - loss: 2.6950 - regression_loss: 1.9897 - classification_loss 6052/10000 [=================>............] - ETA: 31:32 - loss: 2.6949 - regression_loss: 1.9897 - classification_loss 6053/10000 [=================>............] - ETA: 31:31 - loss: 2.6949 - regression_loss: 1.9897 - classification_loss 6054/10000 [=================>............] - ETA: 31:31 - loss: 2.6948 - regression_loss: 1.9896 - classification_loss 6055/10000 [=================>............] - ETA: 31:30 - loss: 2.6947 - regression_loss: 1.9895 - classification_loss 6056/10000 [=================>............] - ETA: 31:30 - loss: 2.6946 - regression_loss: 1.9894 - classification_loss 6057/10000 [=================>............] - ETA: 31:29 - loss: 2.6946 - regression_loss: 1.9894 - classification_loss 6058/10000 [=================>............] - ETA: 31:29 - loss: 2.6945 - regression_loss: 1.9894 - classification_loss 6059/10000 [=================>............] - ETA: 31:28 - loss: 2.6944 - regression_loss: 1.9893 - classification_loss 6060/10000 [=================>............] - ETA: 31:28 - loss: 2.6944 - regression_loss: 1.9894 - classification_loss 6061/10000 [=================>............] - ETA: 31:27 - loss: 2.6942 - regression_loss: 1.9892 - classification_loss 6062/10000 [=================>............] - ETA: 31:27 - loss: 2.6941 - regression_loss: 1.9891 - classification_loss 6063/10000 [=================>............] - ETA: 31:26 - loss: 2.6941 - regression_loss: 1.9892 - classification_loss 6064/10000 [=================>............] - ETA: 31:26 - loss: 2.6940 - regression_loss: 1.9891 - classification_loss 6065/10000 [=================>............] - ETA: 31:25 - loss: 2.6938 - regression_loss: 1.9890 - classification_loss 6066/10000 [=================>............] - ETA: 31:25 - loss: 2.6938 - regression_loss: 1.9889 - classification_loss 6067/10000 [=================>............] - ETA: 31:24 - loss: 2.6937 - regression_loss: 1.9889 - classification_loss 6068/10000 [=================>............] - ETA: 31:24 - loss: 2.6937 - regression_loss: 1.9889 - classification_loss 6069/10000 [=================>............] - ETA: 31:23 - loss: 2.6936 - regression_loss: 1.9888 - classification_loss 6070/10000 [=================>............] - ETA: 31:23 - loss: 2.6935 - regression_loss: 1.9888 - classification_loss 6071/10000 [=================>............] - ETA: 31:22 - loss: 2.6934 - regression_loss: 1.9888 - classification_loss 6072/10000 [=================>............] - ETA: 31:22 - loss: 2.6933 - regression_loss: 1.9886 - classification_loss 6073/10000 [=================>............] - ETA: 31:21 - loss: 2.6932 - regression_loss: 1.9886 - classification_loss 6074/10000 [=================>............] - ETA: 31:21 - loss: 2.6931 - regression_loss: 1.9885 - classification_loss 6075/10000 [=================>............] - ETA: 31:21 - loss: 2.6930 - regression_loss: 1.9885 - classification_loss 6076/10000 [=================>............] - ETA: 31:20 - loss: 2.6930 - regression_loss: 1.9884 - classification_loss 6077/10000 [=================>............] - ETA: 31:20 - loss: 2.6931 - regression_loss: 1.9885 - classification_loss 6078/10000 [=================>............] - ETA: 31:19 - loss: 2.6930 - regression_loss: 1.9885 - classification_loss 6079/10000 [=================>............] - ETA: 31:19 - loss: 2.6931 - regression_loss: 1.9886 - classification_loss 6080/10000 [=================>............] - ETA: 31:18 - loss: 2.6932 - regression_loss: 1.9886 - classification_loss 6081/10000 [=================>............] - ETA: 31:18 - loss: 2.6931 - regression_loss: 1.9886 - classification_loss 6082/10000 [=================>............] - ETA: 31:17 - loss: 2.6929 - regression_loss: 1.9884 - classification_loss 6083/10000 [=================>............] - ETA: 31:17 - loss: 2.6929 - regression_loss: 1.9884 - classification_loss 6084/10000 [=================>............] - ETA: 31:16 - loss: 2.6928 - regression_loss: 1.9884 - classification_loss 6085/10000 [=================>............] - ETA: 31:16 - loss: 2.6929 - regression_loss: 1.9885 - classification_loss 6086/10000 [=================>............] - ETA: 31:15 - loss: 2.6927 - regression_loss: 1.9884 - classification_loss 6087/10000 [=================>............] - ETA: 31:15 - loss: 2.6927 - regression_loss: 1.9884 - classification_loss 6088/10000 [=================>............] - ETA: 31:14 - loss: 2.6925 - regression_loss: 1.9882 - classification_loss 6089/10000 [=================>............] - ETA: 31:14 - loss: 2.6923 - regression_loss: 1.9881 - classification_loss 6090/10000 [=================>............] - ETA: 31:13 - loss: 2.6923 - regression_loss: 1.9880 - classification_loss 6091/10000 [=================>............] - ETA: 31:13 - loss: 2.6922 - regression_loss: 1.9880 - classification_loss 6092/10000 [=================>............] - ETA: 31:12 - loss: 2.6922 - regression_loss: 1.9880 - classification_loss 6093/10000 [=================>............] - ETA: 31:12 - loss: 2.6921 - regression_loss: 1.9879 - classification_loss 6094/10000 [=================>............] - ETA: 31:11 - loss: 2.6919 - regression_loss: 1.9878 - classification_loss 6095/10000 [=================>............] - ETA: 31:11 - loss: 2.6918 - regression_loss: 1.9877 - classification_loss 6096/10000 [=================>............] - ETA: 31:10 - loss: 2.6917 - regression_loss: 1.9877 - classification_loss 6097/10000 [=================>............] - ETA: 31:10 - loss: 2.6917 - regression_loss: 1.9877 - classification_loss 6098/10000 [=================>............] - ETA: 31:09 - loss: 2.6917 - regression_loss: 1.9876 - classification_loss 6099/10000 [=================>............] - ETA: 31:09 - loss: 2.6914 - regression_loss: 1.9875 - classification_loss 6100/10000 [=================>............] - ETA: 31:08 - loss: 2.6914 - regression_loss: 1.9875 - classification_loss 6101/10000 [=================>............] - ETA: 31:08 - loss: 2.6914 - regression_loss: 1.9875 - classification_loss 6102/10000 [=================>............] - ETA: 31:07 - loss: 2.6914 - regression_loss: 1.9875 - classification_loss 6103/10000 [=================>............] - ETA: 31:07 - loss: 2.6912 - regression_loss: 1.9874 - classification_loss 6104/10000 [=================>............] - ETA: 31:06 - loss: 2.6912 - regression_loss: 1.9875 - classification_loss 6105/10000 [=================>............] - ETA: 31:06 - loss: 2.6912 - regression_loss: 1.9875 - classification_loss 6106/10000 [=================>............] - ETA: 31:05 - loss: 2.6911 - regression_loss: 1.9874 - classification_loss 6107/10000 [=================>............] - ETA: 31:05 - loss: 2.6911 - regression_loss: 1.9874 - classification_loss 6108/10000 [=================>............] - ETA: 31:04 - loss: 2.6910 - regression_loss: 1.9874 - classification_loss 6109/10000 [=================>............] - ETA: 31:04 - loss: 2.6909 - regression_loss: 1.9873 - classification_loss 6110/10000 [=================>............] - ETA: 31:03 - loss: 2.6908 - regression_loss: 1.9872 - classification_loss 6111/10000 [=================>............] - ETA: 31:03 - loss: 2.6908 - regression_loss: 1.9872 - classification_loss 6112/10000 [=================>............] - ETA: 31:02 - loss: 2.6907 - regression_loss: 1.9871 - classification_loss 6113/10000 [=================>............] - ETA: 31:02 - loss: 2.6906 - regression_loss: 1.9870 - classification_loss 6114/10000 [=================>............] - ETA: 31:01 - loss: 2.6905 - regression_loss: 1.9870 - classification_loss 6115/10000 [=================>............] - ETA: 31:01 - loss: 2.6905 - regression_loss: 1.9870 - classification_loss 6116/10000 [=================>............] - ETA: 31:00 - loss: 2.6903 - regression_loss: 1.9869 - classification_loss 6117/10000 [=================>............] - ETA: 31:00 - loss: 2.6903 - regression_loss: 1.9868 - classification_loss 6118/10000 [=================>............] - ETA: 30:59 - loss: 2.6902 - regression_loss: 1.9867 - classification_loss 6119/10000 [=================>............] - ETA: 30:59 - loss: 2.6902 - regression_loss: 1.9867 - classification_loss 6120/10000 [=================>............] - ETA: 30:58 - loss: 2.6899 - regression_loss: 1.9865 - classification_loss 6121/10000 [=================>............] - ETA: 30:58 - loss: 2.6899 - regression_loss: 1.9866 - classification_loss 6122/10000 [=================>............] - ETA: 30:57 - loss: 2.6898 - regression_loss: 1.9865 - classification_loss 6123/10000 [=================>............] - ETA: 30:57 - loss: 2.6897 - regression_loss: 1.9864 - classification_loss 6124/10000 [=================>............] - ETA: 30:56 - loss: 2.6896 - regression_loss: 1.9863 - classification_loss 6125/10000 [=================>............] - ETA: 30:56 - loss: 2.6895 - regression_loss: 1.9863 - classification_loss 6126/10000 [=================>............] - ETA: 30:55 - loss: 2.6896 - regression_loss: 1.9863 - classification_loss 6127/10000 [=================>............] - ETA: 30:55 - loss: 2.6895 - regression_loss: 1.9863 - classification_loss 6128/10000 [=================>............] - ETA: 30:55 - loss: 2.6895 - regression_loss: 1.9864 - classification_loss 6129/10000 [=================>............] - ETA: 30:54 - loss: 2.6894 - regression_loss: 1.9863 - classification_loss 6130/10000 [=================>............] - ETA: 30:54 - loss: 2.6895 - regression_loss: 1.9864 - classification_loss 6131/10000 [=================>............] - ETA: 30:53 - loss: 2.6894 - regression_loss: 1.9863 - classification_loss 6132/10000 [=================>............] - ETA: 30:53 - loss: 2.6892 - regression_loss: 1.9862 - classification_loss 6133/10000 [=================>............] - ETA: 30:52 - loss: 2.6891 - regression_loss: 1.9861 - classification_loss 6134/10000 [=================>............] - ETA: 30:52 - loss: 2.6891 - regression_loss: 1.9861 - classification_loss 6135/10000 [=================>............] - ETA: 30:51 - loss: 2.6889 - regression_loss: 1.9860 - classification_loss 6136/10000 [=================>............] - ETA: 30:51 - loss: 2.6888 - regression_loss: 1.9859 - classification_loss 6137/10000 [=================>............] - ETA: 30:50 - loss: 2.6886 - regression_loss: 1.9857 - classification_loss 6138/10000 [=================>............] - ETA: 30:50 - loss: 2.6884 - regression_loss: 1.9856 - classification_loss 6139/10000 [=================>............] - ETA: 30:49 - loss: 2.6883 - regression_loss: 1.9855 - classification_loss 6140/10000 [=================>............] - ETA: 30:49 - loss: 2.6882 - regression_loss: 1.9855 - classification_loss 6141/10000 [=================>............] - ETA: 30:48 - loss: 2.6882 - regression_loss: 1.9856 - classification_loss 6142/10000 [=================>............] - ETA: 30:48 - loss: 2.6881 - regression_loss: 1.9855 - classification_loss 6143/10000 [=================>............] - ETA: 30:47 - loss: 2.6880 - regression_loss: 1.9854 - classification_loss 6144/10000 [=================>............] - ETA: 30:47 - loss: 2.6879 - regression_loss: 1.9854 - classification_loss 6145/10000 [=================>............] - ETA: 30:46 - loss: 2.6881 - regression_loss: 1.9855 - classification_loss 6146/10000 [=================>............] - ETA: 30:46 - loss: 2.6880 - regression_loss: 1.9855 - classification_loss 6147/10000 [=================>............] - ETA: 30:45 - loss: 2.6879 - regression_loss: 1.9854 - classification_loss 6148/10000 [=================>............] - ETA: 30:45 - loss: 2.6877 - regression_loss: 1.9852 - classification_loss 6149/10000 [=================>............] - ETA: 30:44 - loss: 2.6876 - regression_loss: 1.9851 - classification_loss 6150/10000 [=================>............] - ETA: 30:44 - loss: 2.6874 - regression_loss: 1.9850 - classification_loss 6151/10000 [=================>............] - ETA: 30:43 - loss: 2.6875 - regression_loss: 1.9851 - classification_loss 6152/10000 [=================>............] - ETA: 30:43 - loss: 2.6875 - regression_loss: 1.9851 - classification_loss 6153/10000 [=================>............] - ETA: 30:42 - loss: 2.6874 - regression_loss: 1.9850 - classification_loss 6154/10000 [=================>............] - ETA: 30:42 - loss: 2.6872 - regression_loss: 1.9849 - classification_loss 6155/10000 [=================>............] - ETA: 30:41 - loss: 2.6873 - regression_loss: 1.9850 - classification_loss 6156/10000 [=================>............] - ETA: 30:41 - loss: 2.6872 - regression_loss: 1.9848 - classification_loss 6157/10000 [=================>............] - ETA: 30:40 - loss: 2.6871 - regression_loss: 1.9848 - classification_loss 6158/10000 [=================>............] - ETA: 30:40 - loss: 2.6872 - regression_loss: 1.9848 - classification_loss 6159/10000 [=================>............] - ETA: 30:39 - loss: 2.6871 - regression_loss: 1.9848 - classification_loss 6160/10000 [=================>............] - ETA: 30:39 - loss: 2.6870 - regression_loss: 1.9847 - classification_loss 6161/10000 [=================>............] - ETA: 30:38 - loss: 2.6870 - regression_loss: 1.9848 - classification_loss 6162/10000 [=================>............] - ETA: 30:38 - loss: 2.6870 - regression_loss: 1.9848 - classification_loss 6163/10000 [=================>............] - ETA: 30:37 - loss: 2.6869 - regression_loss: 1.9848 - classification_loss 6164/10000 [=================>............] - ETA: 30:37 - loss: 2.6868 - regression_loss: 1.9847 - classification_loss 6165/10000 [=================>............] - ETA: 30:37 - loss: 2.6867 - regression_loss: 1.9846 - classification_loss 6166/10000 [=================>............] - ETA: 30:36 - loss: 2.6865 - regression_loss: 1.9845 - classification_loss 6167/10000 [=================>............] - ETA: 30:36 - loss: 2.6864 - regression_loss: 1.9844 - classification_loss 6168/10000 [=================>............] - ETA: 30:35 - loss: 2.6863 - regression_loss: 1.9844 - classification_loss 6169/10000 [=================>............] - ETA: 30:35 - loss: 2.6864 - regression_loss: 1.9844 - classification_loss 6170/10000 [=================>............] - ETA: 30:34 - loss: 2.6862 - regression_loss: 1.9843 - classification_loss 6171/10000 [=================>............] - ETA: 30:34 - loss: 2.6860 - regression_loss: 1.9841 - classification_loss 6172/10000 [=================>............] - ETA: 30:33 - loss: 2.6861 - regression_loss: 1.9842 - classification_loss 6173/10000 [=================>............] - ETA: 30:33 - loss: 2.6859 - regression_loss: 1.9840 - classification_loss 6174/10000 [=================>............] - ETA: 30:32 - loss: 2.6858 - regression_loss: 1.9840 - classification_loss 6175/10000 [=================>............] - ETA: 30:32 - loss: 2.6858 - regression_loss: 1.9840 - classification_loss 6176/10000 [=================>............] - ETA: 30:31 - loss: 2.6857 - regression_loss: 1.9839 - classification_loss 6177/10000 [=================>............] - ETA: 30:31 - loss: 2.6857 - regression_loss: 1.9839 - classification_loss 6178/10000 [=================>............] - ETA: 30:30 - loss: 2.6858 - regression_loss: 1.9840 - classification_loss 6179/10000 [=================>............] - ETA: 30:30 - loss: 2.6859 - regression_loss: 1.9842 - classification_loss 6180/10000 [=================>............] - ETA: 30:29 - loss: 2.6858 - regression_loss: 1.9841 - classification_loss 6181/10000 [=================>............] - ETA: 30:29 - loss: 2.6859 - regression_loss: 1.9842 - classification_loss 6182/10000 [=================>............] - ETA: 30:28 - loss: 2.6858 - regression_loss: 1.9842 - classification_loss 6183/10000 [=================>............] - ETA: 30:28 - loss: 2.6858 - regression_loss: 1.9842 - classification_loss 6184/10000 [=================>............] - ETA: 30:27 - loss: 2.6859 - regression_loss: 1.9842 - classification_loss 6185/10000 [=================>............] - ETA: 30:27 - loss: 2.6858 - regression_loss: 1.9842 - classification_loss 6186/10000 [=================>............] - ETA: 30:26 - loss: 2.6857 - regression_loss: 1.9840 - classification_loss 6187/10000 [=================>............] - ETA: 30:26 - loss: 2.6858 - regression_loss: 1.9841 - classification_loss 6188/10000 [=================>............] - ETA: 30:25 - loss: 2.6856 - regression_loss: 1.9840 - classification_loss 6189/10000 [=================>............] - ETA: 30:25 - loss: 2.6856 - regression_loss: 1.9840 - classification_loss 6190/10000 [=================>............] - ETA: 30:24 - loss: 2.6856 - regression_loss: 1.9840 - classification_loss 6191/10000 [=================>............] - ETA: 30:24 - loss: 2.6855 - regression_loss: 1.9839 - classification_loss 6192/10000 [=================>............] - ETA: 30:23 - loss: 2.6855 - regression_loss: 1.9839 - classification_loss 6193/10000 [=================>............] - ETA: 30:23 - loss: 2.6856 - regression_loss: 1.9840 - classification_loss 6194/10000 [=================>............] - ETA: 30:22 - loss: 2.6855 - regression_loss: 1.9839 - classification_loss 6195/10000 [=================>............] - ETA: 30:22 - loss: 2.6853 - regression_loss: 1.9837 - classification_loss 6196/10000 [=================>............] - ETA: 30:21 - loss: 2.6853 - regression_loss: 1.9838 - classification_loss 6197/10000 [=================>............] - ETA: 30:21 - loss: 2.6852 - regression_loss: 1.9838 - classification_loss 6198/10000 [=================>............] - ETA: 30:20 - loss: 2.6852 - regression_loss: 1.9838 - classification_loss 6199/10000 [=================>............] - ETA: 30:20 - loss: 2.6853 - regression_loss: 1.9838 - classification_loss 6200/10000 [=================>............] - ETA: 30:19 - loss: 2.6852 - regression_loss: 1.9838 - classification_loss 6201/10000 [=================>............] - ETA: 30:19 - loss: 2.6851 - regression_loss: 1.9838 - classification_loss 6202/10000 [=================>............] - ETA: 30:18 - loss: 2.6850 - regression_loss: 1.9836 - classification_loss 6203/10000 [=================>............] - ETA: 30:18 - loss: 2.6850 - regression_loss: 1.9837 - classification_loss 6204/10000 [=================>............] - ETA: 30:17 - loss: 2.6851 - regression_loss: 1.9837 - classification_loss 6205/10000 [=================>............] - ETA: 30:17 - loss: 2.6850 - regression_loss: 1.9837 - classification_loss 6206/10000 [=================>............] - ETA: 30:16 - loss: 2.6850 - regression_loss: 1.9837 - classification_loss 6207/10000 [=================>............] - ETA: 30:16 - loss: 2.6850 - regression_loss: 1.9837 - classification_loss 6208/10000 [=================>............] - ETA: 30:15 - loss: 2.6849 - regression_loss: 1.9837 - classification_loss 6209/10000 [=================>............] - ETA: 30:15 - loss: 2.6849 - regression_loss: 1.9836 - classification_loss 6210/10000 [=================>............] - ETA: 30:14 - loss: 2.6846 - regression_loss: 1.9834 - classification_loss 6211/10000 [=================>............] - ETA: 30:14 - loss: 2.6845 - regression_loss: 1.9833 - classification_loss 6212/10000 [=================>............] - ETA: 30:13 - loss: 2.6843 - regression_loss: 1.9831 - classification_loss 6213/10000 [=================>............] - ETA: 30:13 - loss: 2.6842 - regression_loss: 1.9831 - classification_loss 6214/10000 [=================>............] - ETA: 30:12 - loss: 2.6842 - regression_loss: 1.9831 - classification_loss 6215/10000 [=================>............] - ETA: 30:12 - loss: 2.6841 - regression_loss: 1.9830 - classification_loss 6216/10000 [=================>............] - ETA: 30:11 - loss: 2.6840 - regression_loss: 1.9829 - classification_loss 6217/10000 [=================>............] - ETA: 30:11 - loss: 2.6841 - regression_loss: 1.9830 - classification_loss 6218/10000 [=================>............] - ETA: 30:10 - loss: 2.6841 - regression_loss: 1.9830 - classification_loss 6219/10000 [=================>............] - ETA: 30:10 - loss: 2.6839 - regression_loss: 1.9829 - classification_loss 6220/10000 [=================>............] - ETA: 30:10 - loss: 2.6839 - regression_loss: 1.9829 - classification_loss 6221/10000 [=================>............] - ETA: 30:09 - loss: 2.6838 - regression_loss: 1.9828 - classification_loss 6222/10000 [=================>............] - ETA: 30:09 - loss: 2.6838 - regression_loss: 1.9828 - classification_loss 6223/10000 [=================>............] - ETA: 30:08 - loss: 2.6836 - regression_loss: 1.9827 - classification_loss 6224/10000 [=================>............] - ETA: 30:08 - loss: 2.6836 - regression_loss: 1.9827 - classification_loss 6225/10000 [=================>............] - ETA: 30:07 - loss: 2.6834 - regression_loss: 1.9825 - classification_loss 6226/10000 [=================>............] - ETA: 30:07 - loss: 2.6834 - regression_loss: 1.9825 - classification_loss 6227/10000 [=================>............] - ETA: 30:06 - loss: 2.6833 - regression_loss: 1.9824 - classification_loss 6228/10000 [=================>............] - ETA: 30:06 - loss: 2.6832 - regression_loss: 1.9824 - classification_loss 6229/10000 [=================>............] - ETA: 30:05 - loss: 2.6831 - regression_loss: 1.9823 - classification_loss 6230/10000 [=================>............] - ETA: 30:05 - loss: 2.6832 - regression_loss: 1.9824 - classification_loss 6231/10000 [=================>............] - ETA: 30:04 - loss: 2.6831 - regression_loss: 1.9823 - classification_loss 6232/10000 [=================>............] - ETA: 30:04 - loss: 2.6830 - regression_loss: 1.9822 - classification_loss 6233/10000 [=================>............] - ETA: 30:03 - loss: 2.6830 - regression_loss: 1.9822 - classification_loss 6234/10000 [=================>............] - ETA: 30:03 - loss: 2.6829 - regression_loss: 1.9822 - classification_loss 6235/10000 [=================>............] - ETA: 30:02 - loss: 2.6828 - regression_loss: 1.9822 - classification_loss 6236/10000 [=================>............] - ETA: 30:02 - loss: 2.6827 - regression_loss: 1.9821 - classification_loss 6237/10000 [=================>............] - ETA: 30:01 - loss: 2.6827 - regression_loss: 1.9821 - classification_loss 6238/10000 [=================>............] - ETA: 30:01 - loss: 2.6825 - regression_loss: 1.9820 - classification_loss 6239/10000 [=================>............] - ETA: 30:00 - loss: 2.6824 - regression_loss: 1.9819 - classification_loss 6240/10000 [=================>............] - ETA: 30:00 - loss: 2.6822 - regression_loss: 1.9817 - classification_loss 6241/10000 [=================>............] - ETA: 29:59 - loss: 2.6821 - regression_loss: 1.9816 - classification_loss 6242/10000 [=================>............] - ETA: 29:59 - loss: 2.6819 - regression_loss: 1.9816 - classification_loss 6243/10000 [=================>............] - ETA: 29:58 - loss: 2.6819 - regression_loss: 1.9816 - classification_loss 6244/10000 [=================>............] - ETA: 29:58 - loss: 2.6819 - regression_loss: 1.9815 - classification_loss 6245/10000 [=================>............] - ETA: 29:57 - loss: 2.6819 - regression_loss: 1.9815 - classification_loss 6246/10000 [=================>............] - ETA: 29:57 - loss: 2.6817 - regression_loss: 1.9814 - classification_loss 6247/10000 [=================>............] - ETA: 29:56 - loss: 2.6816 - regression_loss: 1.9813 - classification_loss 6248/10000 [=================>............] - ETA: 29:56 - loss: 2.6815 - regression_loss: 1.9813 - classification_loss 6249/10000 [=================>............] - ETA: 29:55 - loss: 2.6815 - regression_loss: 1.9813 - classification_loss 6250/10000 [=================>............] - ETA: 29:55 - loss: 2.6813 - regression_loss: 1.9812 - classification_loss 6251/10000 [=================>............] - ETA: 29:54 - loss: 2.6814 - regression_loss: 1.9812 - classification_loss 6252/10000 [=================>............] - ETA: 29:54 - loss: 2.6813 - regression_loss: 1.9811 - classification_loss 6253/10000 [=================>............] - ETA: 29:53 - loss: 2.6811 - regression_loss: 1.9810 - classification_loss 6254/10000 [=================>............] - ETA: 29:53 - loss: 2.6810 - regression_loss: 1.9809 - classification_loss 6255/10000 [=================>............] - ETA: 29:52 - loss: 2.6810 - regression_loss: 1.9808 - classification_loss 6256/10000 [=================>............] - ETA: 29:52 - loss: 2.6809 - regression_loss: 1.9807 - classification_loss 6257/10000 [=================>............] - ETA: 29:51 - loss: 2.6808 - regression_loss: 1.9807 - classification_loss 6258/10000 [=================>............] - ETA: 29:51 - loss: 2.6807 - regression_loss: 1.9806 - classification_loss 6259/10000 [=================>............] - ETA: 29:50 - loss: 2.6805 - regression_loss: 1.9805 - classification_loss 6260/10000 [=================>............] - ETA: 29:50 - loss: 2.6804 - regression_loss: 1.9804 - classification_loss 6261/10000 [=================>............] - ETA: 29:50 - loss: 2.6805 - regression_loss: 1.9805 - classification_loss 6262/10000 [=================>............] - ETA: 29:49 - loss: 2.6804 - regression_loss: 1.9805 - classification_loss 6263/10000 [=================>............] - ETA: 29:49 - loss: 2.6803 - regression_loss: 1.9804 - classification_loss 6264/10000 [=================>............] - ETA: 29:48 - loss: 2.6801 - regression_loss: 1.9802 - classification_loss 6265/10000 [=================>............] - ETA: 29:48 - loss: 2.6800 - regression_loss: 1.9802 - classification_loss 6266/10000 [=================>............] - ETA: 29:47 - loss: 2.6800 - regression_loss: 1.9802 - classification_loss 6267/10000 [=================>............] - ETA: 29:47 - loss: 2.6798 - regression_loss: 1.9800 - classification_loss 6268/10000 [=================>............] - ETA: 29:46 - loss: 2.6798 - regression_loss: 1.9801 - classification_loss 6269/10000 [=================>............] - ETA: 29:46 - loss: 2.6798 - regression_loss: 1.9801 - classification_loss 6270/10000 [=================>............] - ETA: 29:45 - loss: 2.6797 - regression_loss: 1.9801 - classification_loss 6271/10000 [=================>............] - ETA: 29:45 - loss: 2.6797 - regression_loss: 1.9801 - classification_loss 6272/10000 [=================>............] - ETA: 29:44 - loss: 2.6797 - regression_loss: 1.9801 - classification_loss 6273/10000 [=================>............] - ETA: 29:44 - loss: 2.6796 - regression_loss: 1.9801 - classification_loss 6274/10000 [=================>............] - ETA: 29:43 - loss: 2.6795 - regression_loss: 1.9800 - classification_loss 6275/10000 [=================>............] - ETA: 29:43 - loss: 2.6794 - regression_loss: 1.9799 - classification_loss 6276/10000 [=================>............] - ETA: 29:42 - loss: 2.6794 - regression_loss: 1.9800 - classification_loss 6277/10000 [=================>............] - ETA: 29:42 - loss: 2.6794 - regression_loss: 1.9800 - classification_loss 6278/10000 [=================>............] - ETA: 29:41 - loss: 2.6794 - regression_loss: 1.9799 - classification_loss 6279/10000 [=================>............] - ETA: 29:41 - loss: 2.6794 - regression_loss: 1.9800 - classification_loss 6280/10000 [=================>............] - ETA: 29:40 - loss: 2.6793 - regression_loss: 1.9798 - classification_loss 6281/10000 [=================>............] - ETA: 29:40 - loss: 2.6793 - regression_loss: 1.9799 - classification_loss 6282/10000 [=================>............] - ETA: 29:39 - loss: 2.6792 - regression_loss: 1.9798 - classification_loss 6283/10000 [=================>............] - ETA: 29:39 - loss: 2.6792 - regression_loss: 1.9798 - classification_loss 6284/10000 [=================>............] - ETA: 29:38 - loss: 2.6792 - regression_loss: 1.9798 - classification_loss 6285/10000 [=================>............] - ETA: 29:38 - loss: 2.6790 - regression_loss: 1.9796 - classification_loss 6286/10000 [=================>............] - ETA: 29:37 - loss: 2.6789 - regression_loss: 1.9795 - classification_loss 6287/10000 [=================>............] - ETA: 29:37 - loss: 2.6788 - regression_loss: 1.9795 - classification_loss 6288/10000 [=================>............] - ETA: 29:36 - loss: 2.6786 - regression_loss: 1.9794 - classification_loss 6289/10000 [=================>............] - ETA: 29:36 - loss: 2.6785 - regression_loss: 1.9793 - classification_loss 6290/10000 [=================>............] - ETA: 29:35 - loss: 2.6784 - regression_loss: 1.9792 - classification_loss 6291/10000 [=================>............] - ETA: 29:35 - loss: 2.6783 - regression_loss: 1.9791 - classification_loss 6292/10000 [=================>............] - ETA: 29:34 - loss: 2.6781 - regression_loss: 1.9790 - classification_loss 6293/10000 [=================>............] - ETA: 29:34 - loss: 2.6781 - regression_loss: 1.9790 - classification_loss 6294/10000 [=================>............] - ETA: 29:33 - loss: 2.6780 - regression_loss: 1.9789 - classification_loss 6295/10000 [=================>............] - ETA: 29:33 - loss: 2.6779 - regression_loss: 1.9788 - classification_loss 6296/10000 [=================>............] - ETA: 29:32 - loss: 2.6778 - regression_loss: 1.9788 - classification_loss 6297/10000 [=================>............] - ETA: 29:32 - loss: 2.6777 - regression_loss: 1.9787 - classification_loss 6298/10000 [=================>............] - ETA: 29:32 - loss: 2.6776 - regression_loss: 1.9786 - classification_loss 6299/10000 [=================>............] - ETA: 29:31 - loss: 2.6778 - regression_loss: 1.9788 - classification_loss 6300/10000 [=================>............] - ETA: 29:31 - loss: 2.6778 - regression_loss: 1.9788 - classification_loss 6301/10000 [=================>............] - ETA: 29:30 - loss: 2.6777 - regression_loss: 1.9788 - classification_loss 6302/10000 [=================>............] - ETA: 29:30 - loss: 2.6777 - regression_loss: 1.9787 - classification_loss 6303/10000 [=================>............] - ETA: 29:29 - loss: 2.6776 - regression_loss: 1.9787 - classification_loss 6304/10000 [=================>............] - ETA: 29:29 - loss: 2.6774 - regression_loss: 1.9785 - classification_loss 6305/10000 [=================>............] - ETA: 29:28 - loss: 2.6774 - regression_loss: 1.9786 - classification_loss 6306/10000 [=================>............] - ETA: 29:28 - loss: 2.6771 - regression_loss: 1.9784 - classification_loss 6307/10000 [=================>............] - ETA: 29:27 - loss: 2.6771 - regression_loss: 1.9784 - classification_loss 6308/10000 [=================>............] - ETA: 29:27 - loss: 2.6771 - regression_loss: 1.9784 - classification_loss 6309/10000 [=================>............] - ETA: 29:26 - loss: 2.6771 - regression_loss: 1.9784 - classification_loss 6310/10000 [=================>............] - ETA: 29:26 - loss: 2.6771 - regression_loss: 1.9784 - classification_loss 6311/10000 [=================>............] - ETA: 29:25 - loss: 2.6770 - regression_loss: 1.9784 - classification_loss 6312/10000 [=================>............] - ETA: 29:25 - loss: 2.6768 - regression_loss: 1.9782 - classification_loss 6313/10000 [=================>............] - ETA: 29:24 - loss: 2.6767 - regression_loss: 1.9782 - classification_loss 6314/10000 [=================>............] - ETA: 29:24 - loss: 2.6766 - regression_loss: 1.9781 - classification_loss 6315/10000 [=================>............] - ETA: 29:23 - loss: 2.6765 - regression_loss: 1.9781 - classification_loss 6316/10000 [=================>............] - ETA: 29:23 - loss: 2.6765 - regression_loss: 1.9781 - classification_loss 6317/10000 [=================>............] - ETA: 29:22 - loss: 2.6766 - regression_loss: 1.9782 - classification_loss 6318/10000 [=================>............] - ETA: 29:22 - loss: 2.6764 - regression_loss: 1.9780 - classification_loss 6319/10000 [=================>............] - ETA: 29:21 - loss: 2.6765 - regression_loss: 1.9781 - classification_loss 6320/10000 [=================>............] - ETA: 29:21 - loss: 2.6764 - regression_loss: 1.9781 - classification_loss 6321/10000 [=================>............] - ETA: 29:20 - loss: 2.6766 - regression_loss: 1.9782 - classification_loss 6322/10000 [=================>............] - ETA: 29:20 - loss: 2.6764 - regression_loss: 1.9781 - classification_loss 6323/10000 [=================>............] - ETA: 29:19 - loss: 2.6764 - regression_loss: 1.9781 - classification_loss 6324/10000 [=================>............] - ETA: 29:19 - loss: 2.6763 - regression_loss: 1.9781 - classification_loss 6325/10000 [=================>............] - ETA: 29:18 - loss: 2.6763 - regression_loss: 1.9780 - classification_loss 6326/10000 [=================>............] - ETA: 29:18 - loss: 2.6761 - regression_loss: 1.9779 - classification_loss 6327/10000 [=================>............] - ETA: 29:17 - loss: 2.6759 - regression_loss: 1.9778 - classification_loss 6328/10000 [=================>............] - ETA: 29:17 - loss: 2.6759 - regression_loss: 1.9778 - classification_loss 6329/10000 [=================>............] - ETA: 29:16 - loss: 2.6759 - regression_loss: 1.9778 - classification_loss 6330/10000 [=================>............] - ETA: 29:16 - loss: 2.6757 - regression_loss: 1.9777 - classification_loss 6331/10000 [=================>............] - ETA: 29:15 - loss: 2.6757 - regression_loss: 1.9777 - classification_loss 6332/10000 [=================>............] - ETA: 29:15 - loss: 2.6757 - regression_loss: 1.9777 - classification_loss 6333/10000 [=================>............] - ETA: 29:14 - loss: 2.6755 - regression_loss: 1.9776 - classification_loss 6334/10000 [==================>...........] - ETA: 29:14 - loss: 2.6754 - regression_loss: 1.9775 - classification_loss 6335/10000 [==================>...........] - ETA: 29:13 - loss: 2.6753 - regression_loss: 1.9774 - classification_loss 6336/10000 [==================>...........] - ETA: 29:13 - loss: 2.6752 - regression_loss: 1.9774 - classification_loss 6337/10000 [==================>...........] - ETA: 29:12 - loss: 2.6751 - regression_loss: 1.9773 - classification_loss 6338/10000 [==================>...........] - ETA: 29:12 - loss: 2.6748 - regression_loss: 1.9771 - classification_loss 6339/10000 [==================>...........] - ETA: 29:11 - loss: 2.6748 - regression_loss: 1.9771 - classification_loss 6340/10000 [==================>...........] - ETA: 29:11 - loss: 2.6747 - regression_loss: 1.9770 - classification_loss 6341/10000 [==================>...........] - ETA: 29:10 - loss: 2.6745 - regression_loss: 1.9769 - classification_loss 6342/10000 [==================>...........] - ETA: 29:10 - loss: 2.6744 - regression_loss: 1.9769 - classification_loss 6343/10000 [==================>...........] - ETA: 29:09 - loss: 2.6745 - regression_loss: 1.9769 - classification_loss 6344/10000 [==================>...........] - ETA: 29:09 - loss: 2.6745 - regression_loss: 1.9769 - classification_loss 6345/10000 [==================>...........] - ETA: 29:08 - loss: 2.6743 - regression_loss: 1.9768 - classification_loss 6346/10000 [==================>...........] - ETA: 29:08 - loss: 2.6743 - regression_loss: 1.9768 - classification_loss 6347/10000 [==================>...........] - ETA: 29:07 - loss: 2.6741 - regression_loss: 1.9767 - classification_loss 6348/10000 [==================>...........] - ETA: 29:07 - loss: 2.6740 - regression_loss: 1.9767 - classification_loss 6349/10000 [==================>...........] - ETA: 29:06 - loss: 2.6739 - regression_loss: 1.9766 - classification_loss 6350/10000 [==================>...........] - ETA: 29:06 - loss: 2.6739 - regression_loss: 1.9765 - classification_loss 6351/10000 [==================>...........] - ETA: 29:05 - loss: 2.6738 - regression_loss: 1.9766 - classification_loss 6352/10000 [==================>...........] - ETA: 29:05 - loss: 2.6737 - regression_loss: 1.9765 - classification_loss 6353/10000 [==================>...........] - ETA: 29:05 - loss: 2.6736 - regression_loss: 1.9764 - classification_loss 6354/10000 [==================>...........] - ETA: 29:04 - loss: 2.6736 - regression_loss: 1.9764 - classification_loss 6355/10000 [==================>...........] - ETA: 29:04 - loss: 2.6736 - regression_loss: 1.9764 - classification_loss 6356/10000 [==================>...........] - ETA: 29:03 - loss: 2.6736 - regression_loss: 1.9764 - classification_loss 6357/10000 [==================>...........] - ETA: 29:03 - loss: 2.6735 - regression_loss: 1.9764 - classification_loss 6358/10000 [==================>...........] - ETA: 29:02 - loss: 2.6735 - regression_loss: 1.9763 - classification_loss 6359/10000 [==================>...........] - ETA: 29:02 - loss: 2.6734 - regression_loss: 1.9763 - classification_loss 6360/10000 [==================>...........] - ETA: 29:01 - loss: 2.6733 - regression_loss: 1.9763 - classification_loss 6361/10000 [==================>...........] - ETA: 29:01 - loss: 2.6732 - regression_loss: 1.9762 - classification_loss 6362/10000 [==================>...........] - ETA: 29:00 - loss: 2.6729 - regression_loss: 1.9760 - classification_loss 6363/10000 [==================>...........] - ETA: 29:00 - loss: 2.6732 - regression_loss: 1.9762 - classification_loss 6364/10000 [==================>...........] - ETA: 28:59 - loss: 2.6731 - regression_loss: 1.9761 - classification_loss 6365/10000 [==================>...........] - ETA: 28:59 - loss: 2.6731 - regression_loss: 1.9761 - classification_loss 6366/10000 [==================>...........] - ETA: 28:58 - loss: 2.6731 - regression_loss: 1.9762 - classification_loss 6367/10000 [==================>...........] - ETA: 28:58 - loss: 2.6729 - regression_loss: 1.9760 - classification_loss 6368/10000 [==================>...........] - ETA: 28:57 - loss: 2.6729 - regression_loss: 1.9760 - classification_loss 6369/10000 [==================>...........] - ETA: 28:57 - loss: 2.6729 - regression_loss: 1.9760 - classification_loss 6370/10000 [==================>...........] - ETA: 28:56 - loss: 2.6728 - regression_loss: 1.9760 - classification_loss 6371/10000 [==================>...........] - ETA: 28:56 - loss: 2.6727 - regression_loss: 1.9759 - classification_loss 6372/10000 [==================>...........] - ETA: 28:55 - loss: 2.6727 - regression_loss: 1.9759 - classification_loss 6373/10000 [==================>...........] - ETA: 28:55 - loss: 2.6726 - regression_loss: 1.9758 - classification_loss 6374/10000 [==================>...........] - ETA: 28:54 - loss: 2.6724 - regression_loss: 1.9756 - classification_loss 6375/10000 [==================>...........] - ETA: 28:54 - loss: 2.6724 - regression_loss: 1.9756 - classification_loss 6376/10000 [==================>...........] - ETA: 28:53 - loss: 2.6723 - regression_loss: 1.9755 - classification_loss 6377/10000 [==================>...........] - ETA: 28:53 - loss: 2.6722 - regression_loss: 1.9755 - classification_loss 6378/10000 [==================>...........] - ETA: 28:52 - loss: 2.6720 - regression_loss: 1.9753 - classification_loss 6379/10000 [==================>...........] - ETA: 28:52 - loss: 2.6720 - regression_loss: 1.9753 - classification_loss 6380/10000 [==================>...........] - ETA: 28:51 - loss: 2.6720 - regression_loss: 1.9753 - classification_loss 6381/10000 [==================>...........] - ETA: 28:51 - loss: 2.6720 - regression_loss: 1.9753 - classification_loss 6382/10000 [==================>...........] - ETA: 28:50 - loss: 2.6719 - regression_loss: 1.9752 - classification_loss 6383/10000 [==================>...........] - ETA: 28:50 - loss: 2.6716 - regression_loss: 1.9751 - classification_loss 6384/10000 [==================>...........] - ETA: 28:49 - loss: 2.6716 - regression_loss: 1.9750 - classification_loss 6385/10000 [==================>...........] - ETA: 28:49 - loss: 2.6715 - regression_loss: 1.9750 - classification_loss 6386/10000 [==================>...........] - ETA: 28:48 - loss: 2.6714 - regression_loss: 1.9749 - classification_loss 6387/10000 [==================>...........] - ETA: 28:48 - loss: 2.6712 - regression_loss: 1.9747 - classification_loss 6388/10000 [==================>...........] - ETA: 28:47 - loss: 2.6710 - regression_loss: 1.9746 - classification_loss 6389/10000 [==================>...........] - ETA: 28:47 - loss: 2.6710 - regression_loss: 1.9746 - classification_loss 6390/10000 [==================>...........] - ETA: 28:46 - loss: 2.6709 - regression_loss: 1.9745 - classification_loss 6391/10000 [==================>...........] - ETA: 28:46 - loss: 2.6708 - regression_loss: 1.9745 - classification_loss 6392/10000 [==================>...........] - ETA: 28:45 - loss: 2.6707 - regression_loss: 1.9745 - classification_loss 6393/10000 [==================>...........] - ETA: 28:45 - loss: 2.6705 - regression_loss: 1.9743 - classification_loss 6394/10000 [==================>...........] - ETA: 28:44 - loss: 2.6703 - regression_loss: 1.9742 - classification_loss 6395/10000 [==================>...........] - ETA: 28:44 - loss: 2.6702 - regression_loss: 1.9741 - classification_loss 6396/10000 [==================>...........] - ETA: 28:43 - loss: 2.6700 - regression_loss: 1.9739 - classification_loss 6397/10000 [==================>...........] - ETA: 28:43 - loss: 2.6699 - regression_loss: 1.9739 - classification_loss 6398/10000 [==================>...........] - ETA: 28:42 - loss: 2.6698 - regression_loss: 1.9738 - classification_loss 6399/10000 [==================>...........] - ETA: 28:42 - loss: 2.6697 - regression_loss: 1.9738 - classification_loss 6400/10000 [==================>...........] - ETA: 28:42 - loss: 2.6696 - regression_loss: 1.9736 - classification_loss 6401/10000 [==================>...........] - ETA: 28:41 - loss: 2.6694 - regression_loss: 1.9735 - classification_loss 6402/10000 [==================>...........] - ETA: 28:41 - loss: 2.6693 - regression_loss: 1.9734 - classification_loss 6403/10000 [==================>...........] - ETA: 28:40 - loss: 2.6692 - regression_loss: 1.9734 - classification_loss 6404/10000 [==================>...........] - ETA: 28:40 - loss: 2.6690 - regression_loss: 1.9732 - classification_loss 6405/10000 [==================>...........] - ETA: 28:39 - loss: 2.6690 - regression_loss: 1.9732 - classification_loss 6406/10000 [==================>...........] - ETA: 28:39 - loss: 2.6688 - regression_loss: 1.9730 - classification_loss 6407/10000 [==================>...........] - ETA: 28:38 - loss: 2.6686 - regression_loss: 1.9729 - classification_loss 6408/10000 [==================>...........] - ETA: 28:38 - loss: 2.6686 - regression_loss: 1.9729 - classification_loss 6409/10000 [==================>...........] - ETA: 28:37 - loss: 2.6684 - regression_loss: 1.9727 - classification_loss 6410/10000 [==================>...........] - ETA: 28:37 - loss: 2.6683 - regression_loss: 1.9727 - classification_loss 6411/10000 [==================>...........] - ETA: 28:36 - loss: 2.6683 - regression_loss: 1.9727 - classification_loss 6412/10000 [==================>...........] - ETA: 28:36 - loss: 2.6681 - regression_loss: 1.9726 - classification_loss 6413/10000 [==================>...........] - ETA: 28:35 - loss: 2.6679 - regression_loss: 1.9725 - classification_loss 6414/10000 [==================>...........] - ETA: 28:35 - loss: 2.6678 - regression_loss: 1.9724 - classification_loss 6415/10000 [==================>...........] - ETA: 28:34 - loss: 2.6676 - regression_loss: 1.9722 - classification_loss 6416/10000 [==================>...........] - ETA: 28:34 - loss: 2.6677 - regression_loss: 1.9723 - classification_loss 6417/10000 [==================>...........] - ETA: 28:33 - loss: 2.6675 - regression_loss: 1.9722 - classification_loss 6418/10000 [==================>...........] - ETA: 28:33 - loss: 2.6673 - regression_loss: 1.9721 - classification_loss 6419/10000 [==================>...........] - ETA: 28:32 - loss: 2.6672 - regression_loss: 1.9720 - classification_loss 6420/10000 [==================>...........] - ETA: 28:32 - loss: 2.6671 - regression_loss: 1.9720 - classification_loss 6421/10000 [==================>...........] - ETA: 28:31 - loss: 2.6670 - regression_loss: 1.9719 - classification_loss 6422/10000 [==================>...........] - ETA: 28:31 - loss: 2.6668 - regression_loss: 1.9718 - classification_loss 6423/10000 [==================>...........] - ETA: 28:30 - loss: 2.6667 - regression_loss: 1.9717 - classification_loss 6424/10000 [==================>...........] - ETA: 28:30 - loss: 2.6665 - regression_loss: 1.9715 - classification_loss 6425/10000 [==================>...........] - ETA: 28:29 - loss: 2.6664 - regression_loss: 1.9715 - classification_loss 6426/10000 [==================>...........] - ETA: 28:29 - loss: 2.6663 - regression_loss: 1.9714 - classification_loss 6427/10000 [==================>...........] - ETA: 28:28 - loss: 2.6661 - regression_loss: 1.9713 - classification_loss 6428/10000 [==================>...........] - ETA: 28:28 - loss: 2.6660 - regression_loss: 1.9712 - classification_loss 6429/10000 [==================>...........] - ETA: 28:27 - loss: 2.6658 - regression_loss: 1.9710 - classification_loss 6430/10000 [==================>...........] - ETA: 28:27 - loss: 2.6657 - regression_loss: 1.9709 - classification_loss 6431/10000 [==================>...........] - ETA: 28:26 - loss: 2.6655 - regression_loss: 1.9708 - classification_loss 6432/10000 [==================>...........] - ETA: 28:26 - loss: 2.6655 - regression_loss: 1.9708 - classification_loss 6433/10000 [==================>...........] - ETA: 28:25 - loss: 2.6655 - regression_loss: 1.9708 - classification_loss 6434/10000 [==================>...........] - ETA: 28:25 - loss: 2.6653 - regression_loss: 1.9706 - classification_loss 6435/10000 [==================>...........] - ETA: 28:24 - loss: 2.6651 - regression_loss: 1.9705 - classification_loss 6436/10000 [==================>...........] - ETA: 28:24 - loss: 2.6650 - regression_loss: 1.9705 - classification_loss 6437/10000 [==================>...........] - ETA: 28:23 - loss: 2.6648 - regression_loss: 1.9703 - classification_loss 6438/10000 [==================>...........] - ETA: 28:23 - loss: 2.6650 - regression_loss: 1.9704 - classification_loss 6439/10000 [==================>...........] - ETA: 28:23 - loss: 2.6648 - regression_loss: 1.9703 - classification_loss 6440/10000 [==================>...........] - ETA: 28:22 - loss: 2.6647 - regression_loss: 1.9703 - classification_loss 6441/10000 [==================>...........] - ETA: 28:22 - loss: 2.6646 - regression_loss: 1.9702 - classification_loss 6442/10000 [==================>...........] - ETA: 28:21 - loss: 2.6645 - regression_loss: 1.9701 - classification_loss 6443/10000 [==================>...........] - ETA: 28:21 - loss: 2.6642 - regression_loss: 1.9699 - classification_loss 6444/10000 [==================>...........] - ETA: 28:20 - loss: 2.6640 - regression_loss: 1.9698 - classification_loss 6445/10000 [==================>...........] - ETA: 28:20 - loss: 2.6639 - regression_loss: 1.9697 - classification_loss 6446/10000 [==================>...........] - ETA: 28:19 - loss: 2.6638 - regression_loss: 1.9695 - classification_loss 6447/10000 [==================>...........] - ETA: 28:19 - loss: 2.6637 - regression_loss: 1.9694 - classification_loss 6448/10000 [==================>...........] - ETA: 28:18 - loss: 2.6634 - regression_loss: 1.9692 - classification_loss 6449/10000 [==================>...........] - ETA: 28:18 - loss: 2.6633 - regression_loss: 1.9692 - classification_loss 6450/10000 [==================>...........] - ETA: 28:17 - loss: 2.6633 - regression_loss: 1.9692 - classification_loss 6451/10000 [==================>...........] - ETA: 28:17 - loss: 2.6631 - regression_loss: 1.9691 - classification_loss 6452/10000 [==================>...........] - ETA: 28:16 - loss: 2.6631 - regression_loss: 1.9691 - classification_loss 6453/10000 [==================>...........] - ETA: 28:16 - loss: 2.6631 - regression_loss: 1.9691 - classification_loss 6454/10000 [==================>...........] - ETA: 28:15 - loss: 2.6633 - regression_loss: 1.9692 - classification_loss 6455/10000 [==================>...........] - ETA: 28:15 - loss: 2.6632 - regression_loss: 1.9691 - classification_loss 6456/10000 [==================>...........] - ETA: 28:14 - loss: 2.6632 - regression_loss: 1.9691 - classification_loss 6457/10000 [==================>...........] - ETA: 28:14 - loss: 2.6633 - regression_loss: 1.9692 - classification_loss 6458/10000 [==================>...........] - ETA: 28:13 - loss: 2.6634 - regression_loss: 1.9693 - classification_loss 6459/10000 [==================>...........] - ETA: 28:13 - loss: 2.6632 - regression_loss: 1.9691 - classification_loss 6460/10000 [==================>...........] - ETA: 28:12 - loss: 2.6630 - regression_loss: 1.9690 - classification_loss 6461/10000 [==================>...........] - ETA: 28:12 - loss: 2.6629 - regression_loss: 1.9689 - classification_loss 6462/10000 [==================>...........] - ETA: 28:11 - loss: 2.6628 - regression_loss: 1.9688 - classification_loss 6463/10000 [==================>...........] - ETA: 28:11 - loss: 2.6627 - regression_loss: 1.9687 - classification_loss 6464/10000 [==================>...........] - ETA: 28:10 - loss: 2.6627 - regression_loss: 1.9687 - classification_loss 6465/10000 [==================>...........] - ETA: 28:10 - loss: 2.6627 - regression_loss: 1.9687 - classification_loss 6466/10000 [==================>...........] - ETA: 28:09 - loss: 2.6626 - regression_loss: 1.9687 - classification_loss 6467/10000 [==================>...........] - ETA: 28:09 - loss: 2.6624 - regression_loss: 1.9686 - classification_loss 6468/10000 [==================>...........] - ETA: 28:08 - loss: 2.6623 - regression_loss: 1.9684 - classification_loss 6469/10000 [==================>...........] - ETA: 28:08 - loss: 2.6622 - regression_loss: 1.9684 - classification_loss 6470/10000 [==================>...........] - ETA: 28:07 - loss: 2.6622 - regression_loss: 1.9684 - classification_loss 6471/10000 [==================>...........] - ETA: 28:07 - loss: 2.6622 - regression_loss: 1.9684 - classification_loss 6472/10000 [==================>...........] - ETA: 28:06 - loss: 2.6621 - regression_loss: 1.9684 - classification_loss 6473/10000 [==================>...........] - ETA: 28:06 - loss: 2.6621 - regression_loss: 1.9684 - classification_loss 6474/10000 [==================>...........] - ETA: 28:05 - loss: 2.6620 - regression_loss: 1.9683 - classification_loss 6475/10000 [==================>...........] - ETA: 28:05 - loss: 2.6620 - regression_loss: 1.9683 - classification_loss 6476/10000 [==================>...........] - ETA: 28:04 - loss: 2.6620 - regression_loss: 1.9684 - classification_loss 6477/10000 [==================>...........] - ETA: 28:04 - loss: 2.6620 - regression_loss: 1.9684 - classification_loss 6478/10000 [==================>...........] - ETA: 28:04 - loss: 2.6617 - regression_loss: 1.9681 - classification_loss 6479/10000 [==================>...........] - ETA: 28:03 - loss: 2.6615 - regression_loss: 1.9680 - classification_loss 6480/10000 [==================>...........] - ETA: 28:03 - loss: 2.6613 - regression_loss: 1.9678 - classification_loss 6481/10000 [==================>...........] - ETA: 28:02 - loss: 2.6613 - regression_loss: 1.9678 - classification_loss 6482/10000 [==================>...........] - ETA: 28:02 - loss: 2.6613 - regression_loss: 1.9678 - classification_loss 6483/10000 [==================>...........] - ETA: 28:01 - loss: 2.6613 - regression_loss: 1.9679 - classification_loss 6484/10000 [==================>...........] - ETA: 28:01 - loss: 2.6613 - regression_loss: 1.9679 - classification_loss 6485/10000 [==================>...........] - ETA: 28:00 - loss: 2.6613 - regression_loss: 1.9678 - classification_loss 6486/10000 [==================>...........] - ETA: 28:00 - loss: 2.6611 - regression_loss: 1.9677 - classification_loss 6487/10000 [==================>...........] - ETA: 27:59 - loss: 2.6609 - regression_loss: 1.9675 - classification_loss 6488/10000 [==================>...........] - ETA: 27:59 - loss: 2.6608 - regression_loss: 1.9674 - classification_loss 6489/10000 [==================>...........] - ETA: 27:58 - loss: 2.6607 - regression_loss: 1.9673 - classification_loss 6490/10000 [==================>...........] - ETA: 27:58 - loss: 2.6605 - regression_loss: 1.9672 - classification_loss 6491/10000 [==================>...........] - ETA: 27:57 - loss: 2.6604 - regression_loss: 1.9671 - classification_loss 6492/10000 [==================>...........] - ETA: 27:57 - loss: 2.6603 - regression_loss: 1.9671 - classification_loss 6493/10000 [==================>...........] - ETA: 27:56 - loss: 2.6601 - regression_loss: 1.9669 - classification_loss 6494/10000 [==================>...........] - ETA: 27:56 - loss: 2.6599 - regression_loss: 1.9667 - classification_loss 6495/10000 [==================>...........] - ETA: 27:55 - loss: 2.6598 - regression_loss: 1.9667 - classification_loss 6496/10000 [==================>...........] - ETA: 27:55 - loss: 2.6597 - regression_loss: 1.9666 - classification_loss 6497/10000 [==================>...........] - ETA: 27:54 - loss: 2.6596 - regression_loss: 1.9666 - classification_loss 6498/10000 [==================>...........] - ETA: 27:54 - loss: 2.6596 - regression_loss: 1.9666 - classification_loss 6499/10000 [==================>...........] - ETA: 27:53 - loss: 2.6596 - regression_loss: 1.9666 - classification_loss 6500/10000 [==================>...........] - ETA: 27:53 - loss: 2.6595 - regression_loss: 1.9666 - classification_loss 6501/10000 [==================>...........] - ETA: 27:52 - loss: 2.6593 - regression_loss: 1.9664 - classification_loss 6502/10000 [==================>...........] - ETA: 27:52 - loss: 2.6594 - regression_loss: 1.9664 - classification_loss 6503/10000 [==================>...........] - ETA: 27:51 - loss: 2.6594 - regression_loss: 1.9664 - classification_loss 6504/10000 [==================>...........] - ETA: 27:51 - loss: 2.6593 - regression_loss: 1.9663 - classification_loss 6505/10000 [==================>...........] - ETA: 27:50 - loss: 2.6592 - regression_loss: 1.9662 - classification_loss 6506/10000 [==================>...........] - ETA: 27:50 - loss: 2.6591 - regression_loss: 1.9662 - classification_loss 6507/10000 [==================>...........] - ETA: 27:49 - loss: 2.6591 - regression_loss: 1.9661 - classification_loss 6508/10000 [==================>...........] - ETA: 27:49 - loss: 2.6589 - regression_loss: 1.9660 - classification_loss 6509/10000 [==================>...........] - ETA: 27:48 - loss: 2.6589 - regression_loss: 1.9661 - classification_loss 6510/10000 [==================>...........] - ETA: 27:48 - loss: 2.6590 - regression_loss: 1.9662 - classification_loss 6511/10000 [==================>...........] - ETA: 27:47 - loss: 2.6589 - regression_loss: 1.9661 - classification_loss 6512/10000 [==================>...........] - ETA: 27:47 - loss: 2.6589 - regression_loss: 1.9661 - classification_loss 6513/10000 [==================>...........] - ETA: 27:46 - loss: 2.6589 - regression_loss: 1.9661 - classification_loss 6514/10000 [==================>...........] - ETA: 27:46 - loss: 2.6588 - regression_loss: 1.9660 - classification_loss 6515/10000 [==================>...........] - ETA: 27:45 - loss: 2.6587 - regression_loss: 1.9659 - classification_loss 6516/10000 [==================>...........] - ETA: 27:45 - loss: 2.6585 - regression_loss: 1.9658 - classification_loss 6517/10000 [==================>...........] - ETA: 27:44 - loss: 2.6584 - regression_loss: 1.9658 - classification_loss 6518/10000 [==================>...........] - ETA: 27:44 - loss: 2.6583 - regression_loss: 1.9657 - classification_loss 6519/10000 [==================>...........] - ETA: 27:43 - loss: 2.6583 - regression_loss: 1.9657 - classification_loss 6520/10000 [==================>...........] - ETA: 27:43 - loss: 2.6582 - regression_loss: 1.9657 - classification_loss 6521/10000 [==================>...........] - ETA: 27:42 - loss: 2.6582 - regression_loss: 1.9656 - classification_loss 6522/10000 [==================>...........] - ETA: 27:42 - loss: 2.6581 - regression_loss: 1.9656 - classification_loss 6523/10000 [==================>...........] - ETA: 27:42 - loss: 2.6579 - regression_loss: 1.9655 - classification_loss 6524/10000 [==================>...........] - ETA: 27:41 - loss: 2.6578 - regression_loss: 1.9654 - classification_loss 6525/10000 [==================>...........] - ETA: 27:41 - loss: 2.6578 - regression_loss: 1.9654 - classification_loss 6526/10000 [==================>...........] - ETA: 27:40 - loss: 2.6576 - regression_loss: 1.9653 - classification_loss 6527/10000 [==================>...........] - ETA: 27:40 - loss: 2.6575 - regression_loss: 1.9652 - classification_loss 6528/10000 [==================>...........] - ETA: 27:39 - loss: 2.6575 - regression_loss: 1.9651 - classification_loss 6529/10000 [==================>...........] - ETA: 27:39 - loss: 2.6574 - regression_loss: 1.9651 - classification_loss 6530/10000 [==================>...........] - ETA: 27:38 - loss: 2.6573 - regression_loss: 1.9650 - classification_loss 6531/10000 [==================>...........] - ETA: 27:38 - loss: 2.6572 - regression_loss: 1.9649 - classification_loss 6532/10000 [==================>...........] - ETA: 27:37 - loss: 2.6571 - regression_loss: 1.9649 - classification_loss 6533/10000 [==================>...........] - ETA: 27:37 - loss: 2.6571 - regression_loss: 1.9648 - classification_loss 6534/10000 [==================>...........] - ETA: 27:36 - loss: 2.6571 - regression_loss: 1.9649 - classification_loss 6535/10000 [==================>...........] - ETA: 27:36 - loss: 2.6569 - regression_loss: 1.9647 - classification_loss 6536/10000 [==================>...........] - ETA: 27:35 - loss: 2.6568 - regression_loss: 1.9646 - classification_loss 6537/10000 [==================>...........] - ETA: 27:35 - loss: 2.6566 - regression_loss: 1.9644 - classification_loss 6538/10000 [==================>...........] - ETA: 27:34 - loss: 2.6566 - regression_loss: 1.9644 - classification_loss 6539/10000 [==================>...........] - ETA: 27:34 - loss: 2.6566 - regression_loss: 1.9644 - classification_loss 6540/10000 [==================>...........] - ETA: 27:33 - loss: 2.6565 - regression_loss: 1.9644 - classification_loss 6541/10000 [==================>...........] - ETA: 27:33 - loss: 2.6563 - regression_loss: 1.9642 - classification_loss 6542/10000 [==================>...........] - ETA: 27:32 - loss: 2.6562 - regression_loss: 1.9642 - classification_loss 6543/10000 [==================>...........] - ETA: 27:32 - loss: 2.6562 - regression_loss: 1.9642 - classification_loss 6544/10000 [==================>...........] - ETA: 27:31 - loss: 2.6561 - regression_loss: 1.9641 - classification_loss 6545/10000 [==================>...........] - ETA: 27:31 - loss: 2.6560 - regression_loss: 1.9640 - classification_loss 6546/10000 [==================>...........] - ETA: 27:30 - loss: 2.6560 - regression_loss: 1.9641 - classification_loss 6547/10000 [==================>...........] - ETA: 27:30 - loss: 2.6560 - regression_loss: 1.9641 - classification_loss 6548/10000 [==================>...........] - ETA: 27:29 - loss: 2.6559 - regression_loss: 1.9640 - classification_loss 6549/10000 [==================>...........] - ETA: 27:29 - loss: 2.6558 - regression_loss: 1.9639 - classification_loss 6550/10000 [==================>...........] - ETA: 27:28 - loss: 2.6556 - regression_loss: 1.9638 - classification_loss 6551/10000 [==================>...........] - ETA: 27:28 - loss: 2.6556 - regression_loss: 1.9638 - classification_loss 6552/10000 [==================>...........] - ETA: 27:27 - loss: 2.6556 - regression_loss: 1.9639 - classification_loss 6553/10000 [==================>...........] - ETA: 27:27 - loss: 2.6558 - regression_loss: 1.9640 - classification_loss 6554/10000 [==================>...........] - ETA: 27:26 - loss: 2.6558 - regression_loss: 1.9640 - classification_loss 6555/10000 [==================>...........] - ETA: 27:26 - loss: 2.6558 - regression_loss: 1.9640 - classification_loss 6556/10000 [==================>...........] - ETA: 27:25 - loss: 2.6558 - regression_loss: 1.9640 - classification_loss 6557/10000 [==================>...........] - ETA: 27:25 - loss: 2.6557 - regression_loss: 1.9640 - classification_loss 6558/10000 [==================>...........] - ETA: 27:24 - loss: 2.6556 - regression_loss: 1.9638 - classification_loss 6559/10000 [==================>...........] - ETA: 27:24 - loss: 2.6555 - regression_loss: 1.9638 - classification_loss 6560/10000 [==================>...........] - ETA: 27:23 - loss: 2.6555 - regression_loss: 1.9638 - classification_loss 6561/10000 [==================>...........] - ETA: 27:23 - loss: 2.6554 - regression_loss: 1.9636 - classification_loss 6562/10000 [==================>...........] - ETA: 27:22 - loss: 2.6552 - regression_loss: 1.9636 - classification_loss 6563/10000 [==================>...........] - ETA: 27:22 - loss: 2.6551 - regression_loss: 1.9635 - classification_loss 6564/10000 [==================>...........] - ETA: 27:21 - loss: 2.6550 - regression_loss: 1.9634 - classification_loss 6565/10000 [==================>...........] - ETA: 27:21 - loss: 2.6549 - regression_loss: 1.9633 - classification_loss 6566/10000 [==================>...........] - ETA: 27:21 - loss: 2.6549 - regression_loss: 1.9633 - classification_loss 6567/10000 [==================>...........] - ETA: 27:20 - loss: 2.6548 - regression_loss: 1.9633 - classification_loss 6568/10000 [==================>...........] - ETA: 27:20 - loss: 2.6548 - regression_loss: 1.9633 - classification_loss 6569/10000 [==================>...........] - ETA: 27:19 - loss: 2.6547 - regression_loss: 1.9633 - classification_loss 6570/10000 [==================>...........] - ETA: 27:19 - loss: 2.6546 - regression_loss: 1.9632 - classification_loss 6571/10000 [==================>...........] - ETA: 27:18 - loss: 2.6545 - regression_loss: 1.9632 - classification_loss 6572/10000 [==================>...........] - ETA: 27:18 - loss: 2.6545 - regression_loss: 1.9632 - classification_loss 6573/10000 [==================>...........] - ETA: 27:17 - loss: 2.6544 - regression_loss: 1.9631 - classification_loss 6574/10000 [==================>...........] - ETA: 27:17 - loss: 2.6543 - regression_loss: 1.9630 - classification_loss 6575/10000 [==================>...........] - ETA: 27:16 - loss: 2.6542 - regression_loss: 1.9630 - classification_loss 6576/10000 [==================>...........] - ETA: 27:16 - loss: 2.6542 - regression_loss: 1.9630 - classification_loss 6577/10000 [==================>...........] - ETA: 27:15 - loss: 2.6542 - regression_loss: 1.9630 - classification_loss 6578/10000 [==================>...........] - ETA: 27:15 - loss: 2.6542 - regression_loss: 1.9630 - classification_loss 6579/10000 [==================>...........] - ETA: 27:14 - loss: 2.6541 - regression_loss: 1.9629 - classification_loss 6580/10000 [==================>...........] - ETA: 27:14 - loss: 2.6540 - regression_loss: 1.9628 - classification_loss 6581/10000 [==================>...........] - ETA: 27:13 - loss: 2.6540 - regression_loss: 1.9627 - classification_loss 6582/10000 [==================>...........] - ETA: 27:13 - loss: 2.6539 - regression_loss: 1.9627 - classification_loss 6583/10000 [==================>...........] - ETA: 27:12 - loss: 2.6539 - regression_loss: 1.9627 - classification_loss 6584/10000 [==================>...........] - ETA: 27:12 - loss: 2.6539 - regression_loss: 1.9627 - classification_loss 6585/10000 [==================>...........] - ETA: 27:11 - loss: 2.6538 - regression_loss: 1.9626 - classification_loss 6586/10000 [==================>...........] - ETA: 27:11 - loss: 2.6538 - regression_loss: 1.9626 - classification_loss 6587/10000 [==================>...........] - ETA: 27:10 - loss: 2.6537 - regression_loss: 1.9625 - classification_loss 6588/10000 [==================>...........] - ETA: 27:10 - loss: 2.6536 - regression_loss: 1.9625 - classification_loss 6589/10000 [==================>...........] - ETA: 27:09 - loss: 2.6535 - regression_loss: 1.9624 - classification_loss 6590/10000 [==================>...........] - ETA: 27:09 - loss: 2.6535 - regression_loss: 1.9624 - classification_loss 6591/10000 [==================>...........] - ETA: 27:08 - loss: 2.6535 - regression_loss: 1.9624 - classification_loss 6592/10000 [==================>...........] - ETA: 27:08 - loss: 2.6535 - regression_loss: 1.9624 - classification_loss 6593/10000 [==================>...........] - ETA: 27:07 - loss: 2.6535 - regression_loss: 1.9624 - classification_loss 6594/10000 [==================>...........] - ETA: 27:07 - loss: 2.6532 - regression_loss: 1.9622 - classification_loss 6595/10000 [==================>...........] - ETA: 27:06 - loss: 2.6531 - regression_loss: 1.9620 - classification_loss 6596/10000 [==================>...........] - ETA: 27:06 - loss: 2.6531 - regression_loss: 1.9621 - classification_loss 6597/10000 [==================>...........] - ETA: 27:05 - loss: 2.6530 - regression_loss: 1.9620 - classification_loss 6598/10000 [==================>...........] - ETA: 27:05 - loss: 2.6531 - regression_loss: 1.9621 - classification_loss 6599/10000 [==================>...........] - ETA: 27:04 - loss: 2.6531 - regression_loss: 1.9621 - classification_loss 6600/10000 [==================>...........] - ETA: 27:04 - loss: 2.6530 - regression_loss: 1.9620 - classification_loss 6601/10000 [==================>...........] - ETA: 27:04 - loss: 2.6529 - regression_loss: 1.9619 - classification_loss 6602/10000 [==================>...........] - ETA: 27:03 - loss: 2.6529 - regression_loss: 1.9619 - classification_loss 6603/10000 [==================>...........] - ETA: 27:03 - loss: 2.6526 - regression_loss: 1.9617 - classification_loss 6604/10000 [==================>...........] - ETA: 27:02 - loss: 2.6526 - regression_loss: 1.9617 - classification_loss 6605/10000 [==================>...........] - ETA: 27:02 - loss: 2.6524 - regression_loss: 1.9616 - classification_loss 6606/10000 [==================>...........] - ETA: 27:01 - loss: 2.6525 - regression_loss: 1.9616 - classification_loss 6607/10000 [==================>...........] - ETA: 27:01 - loss: 2.6524 - regression_loss: 1.9616 - classification_loss 6608/10000 [==================>...........] - ETA: 27:00 - loss: 2.6523 - regression_loss: 1.9615 - classification_loss 6609/10000 [==================>...........] - ETA: 27:00 - loss: 2.6521 - regression_loss: 1.9614 - classification_loss 6610/10000 [==================>...........] - ETA: 26:59 - loss: 2.6521 - regression_loss: 1.9614 - classification_loss 6611/10000 [==================>...........] - ETA: 26:59 - loss: 2.6520 - regression_loss: 1.9613 - classification_loss 6612/10000 [==================>...........] - ETA: 26:58 - loss: 2.6521 - regression_loss: 1.9614 - classification_loss 6613/10000 [==================>...........] - ETA: 26:58 - loss: 2.6519 - regression_loss: 1.9612 - classification_loss 6614/10000 [==================>...........] - ETA: 26:57 - loss: 2.6518 - regression_loss: 1.9612 - classification_loss 6615/10000 [==================>...........] - ETA: 26:57 - loss: 2.6517 - regression_loss: 1.9611 - classification_loss 6616/10000 [==================>...........] - ETA: 26:56 - loss: 2.6517 - regression_loss: 1.9611 - classification_loss 6617/10000 [==================>...........] - ETA: 26:56 - loss: 2.6515 - regression_loss: 1.9610 - classification_loss 6618/10000 [==================>...........] - ETA: 26:55 - loss: 2.6514 - regression_loss: 1.9610 - classification_loss 6619/10000 [==================>...........] - ETA: 26:55 - loss: 2.6514 - regression_loss: 1.9610 - classification_loss 6620/10000 [==================>...........] - ETA: 26:54 - loss: 2.6515 - regression_loss: 1.9610 - classification_loss 6621/10000 [==================>...........] - ETA: 26:54 - loss: 2.6515 - regression_loss: 1.9610 - classification_loss 6622/10000 [==================>...........] - ETA: 26:53 - loss: 2.6515 - regression_loss: 1.9610 - classification_loss 6623/10000 [==================>...........] - ETA: 26:53 - loss: 2.6515 - regression_loss: 1.9610 - classification_loss 6624/10000 [==================>...........] - ETA: 26:52 - loss: 2.6514 - regression_loss: 1.9610 - classification_loss 6625/10000 [==================>...........] - ETA: 26:52 - loss: 2.6511 - regression_loss: 1.9608 - classification_loss 6626/10000 [==================>...........] - ETA: 26:51 - loss: 2.6512 - regression_loss: 1.9608 - classification_loss 6627/10000 [==================>...........] - ETA: 26:51 - loss: 2.6511 - regression_loss: 1.9608 - classification_loss 6628/10000 [==================>...........] - ETA: 26:50 - loss: 2.6509 - regression_loss: 1.9606 - classification_loss 6629/10000 [==================>...........] - ETA: 26:50 - loss: 2.6508 - regression_loss: 1.9606 - classification_loss 6630/10000 [==================>...........] - ETA: 26:49 - loss: 2.6507 - regression_loss: 1.9605 - classification_loss 6631/10000 [==================>...........] - ETA: 26:49 - loss: 2.6506 - regression_loss: 1.9604 - classification_loss 6632/10000 [==================>...........] - ETA: 26:48 - loss: 2.6504 - regression_loss: 1.9603 - classification_loss 6633/10000 [==================>...........] - ETA: 26:48 - loss: 2.6503 - regression_loss: 1.9601 - classification_loss 6634/10000 [==================>...........] - ETA: 26:47 - loss: 2.6501 - regression_loss: 1.9599 - classification_loss 6635/10000 [==================>...........] - ETA: 26:47 - loss: 2.6501 - regression_loss: 1.9599 - classification_loss 6636/10000 [==================>...........] - ETA: 26:47 - loss: 2.6500 - regression_loss: 1.9599 - classification_loss 6637/10000 [==================>...........] - ETA: 26:46 - loss: 2.6500 - regression_loss: 1.9599 - classification_loss 6638/10000 [==================>...........] - ETA: 26:46 - loss: 2.6499 - regression_loss: 1.9598 - classification_loss 6639/10000 [==================>...........] - ETA: 26:45 - loss: 2.6499 - regression_loss: 1.9598 - classification_loss 6640/10000 [==================>...........] - ETA: 26:45 - loss: 2.6498 - regression_loss: 1.9598 - classification_loss 6641/10000 [==================>...........] - ETA: 26:44 - loss: 2.6497 - regression_loss: 1.9597 - classification_loss 6642/10000 [==================>...........] - ETA: 26:44 - loss: 2.6499 - regression_loss: 1.9598 - classification_loss 6643/10000 [==================>...........] - ETA: 26:43 - loss: 2.6499 - regression_loss: 1.9599 - classification_loss 6644/10000 [==================>...........] - ETA: 26:43 - loss: 2.6497 - regression_loss: 1.9598 - classification_loss 6645/10000 [==================>...........] - ETA: 26:42 - loss: 2.6496 - regression_loss: 1.9597 - classification_loss 6646/10000 [==================>...........] - ETA: 26:42 - loss: 2.6495 - regression_loss: 1.9597 - classification_loss 6647/10000 [==================>...........] - ETA: 26:41 - loss: 2.6494 - regression_loss: 1.9595 - classification_loss 6648/10000 [==================>...........] - ETA: 26:41 - loss: 2.6494 - regression_loss: 1.9596 - classification_loss 6649/10000 [==================>...........] - ETA: 26:40 - loss: 2.6492 - regression_loss: 1.9594 - classification_loss 6650/10000 [==================>...........] - ETA: 26:40 - loss: 2.6491 - regression_loss: 1.9593 - classification_loss 6651/10000 [==================>...........] - ETA: 26:39 - loss: 2.6490 - regression_loss: 1.9593 - classification_loss 6652/10000 [==================>...........] - ETA: 26:39 - loss: 2.6491 - regression_loss: 1.9594 - classification_loss 6653/10000 [==================>...........] - ETA: 26:38 - loss: 2.6493 - regression_loss: 1.9595 - classification_loss 6654/10000 [==================>...........] - ETA: 26:38 - loss: 2.6492 - regression_loss: 1.9594 - classification_loss 6655/10000 [==================>...........] - ETA: 26:37 - loss: 2.6493 - regression_loss: 1.9595 - classification_loss 6656/10000 [==================>...........] - ETA: 26:37 - loss: 2.6492 - regression_loss: 1.9594 - classification_loss 6657/10000 [==================>...........] - ETA: 26:36 - loss: 2.6492 - regression_loss: 1.9594 - classification_loss 6658/10000 [==================>...........] - ETA: 26:36 - loss: 2.6490 - regression_loss: 1.9593 - classification_loss 6659/10000 [==================>...........] - ETA: 26:35 - loss: 2.6490 - regression_loss: 1.9593 - classification_loss 6660/10000 [==================>...........] - ETA: 26:35 - loss: 2.6489 - regression_loss: 1.9593 - classification_loss 6661/10000 [==================>...........] - ETA: 26:34 - loss: 2.6489 - regression_loss: 1.9593 - classification_loss 6662/10000 [==================>...........] - ETA: 26:34 - loss: 2.6489 - regression_loss: 1.9593 - classification_loss 6663/10000 [==================>...........] - ETA: 26:33 - loss: 2.6489 - regression_loss: 1.9593 - classification_loss 6664/10000 [==================>...........] - ETA: 26:33 - loss: 2.6489 - regression_loss: 1.9593 - classification_loss 6665/10000 [==================>...........] - ETA: 26:32 - loss: 2.6487 - regression_loss: 1.9591 - classification_loss 6666/10000 [==================>...........] - ETA: 26:32 - loss: 2.6485 - regression_loss: 1.9590 - classification_loss 6667/10000 [===================>..........] - ETA: 26:31 - loss: 2.6484 - regression_loss: 1.9587 - classification_loss 6668/10000 [===================>..........] - ETA: 26:31 - loss: 2.6485 - regression_loss: 1.9587 - classification_loss 6669/10000 [===================>..........] - ETA: 26:31 - loss: 2.6484 - regression_loss: 1.9587 - classification_loss 6670/10000 [===================>..........] - ETA: 26:30 - loss: 2.6483 - regression_loss: 1.9587 - classification_loss 6671/10000 [===================>..........] - ETA: 26:29 - loss: 2.6482 - regression_loss: 1.9585 - classification_loss 6672/10000 [===================>..........] - ETA: 26:29 - loss: 2.6481 - regression_loss: 1.9585 - classification_loss 6673/10000 [===================>..........] - ETA: 26:28 - loss: 2.6480 - regression_loss: 1.9584 - classification_loss 6674/10000 [===================>..........] - ETA: 26:28 - loss: 2.6479 - regression_loss: 1.9583 - classification_loss 6675/10000 [===================>..........] - ETA: 26:27 - loss: 2.6478 - regression_loss: 1.9584 - classification_loss 6676/10000 [===================>..........] - ETA: 26:27 - loss: 2.6477 - regression_loss: 1.9583 - classification_loss 6677/10000 [===================>..........] - ETA: 26:27 - loss: 2.6477 - regression_loss: 1.9582 - classification_loss 6678/10000 [===================>..........] - ETA: 26:26 - loss: 2.6478 - regression_loss: 1.9583 - classification_loss 6679/10000 [===================>..........] - ETA: 26:26 - loss: 2.6478 - regression_loss: 1.9584 - classification_loss 6680/10000 [===================>..........] - ETA: 26:25 - loss: 2.6478 - regression_loss: 1.9584 - classification_loss 6681/10000 [===================>..........] - ETA: 26:25 - loss: 2.6478 - regression_loss: 1.9584 - classification_loss 6682/10000 [===================>..........] - ETA: 26:24 - loss: 2.6478 - regression_loss: 1.9584 - classification_loss 6683/10000 [===================>..........] - ETA: 26:24 - loss: 2.6477 - regression_loss: 1.9583 - classification_loss 6684/10000 [===================>..........] - ETA: 26:23 - loss: 2.6476 - regression_loss: 1.9583 - classification_loss 6685/10000 [===================>..........] - ETA: 26:23 - loss: 2.6475 - regression_loss: 1.9582 - classification_loss 6686/10000 [===================>..........] - ETA: 26:22 - loss: 2.6474 - regression_loss: 1.9581 - classification_loss 6687/10000 [===================>..........] - ETA: 26:22 - loss: 2.6473 - regression_loss: 1.9580 - classification_loss 6688/10000 [===================>..........] - ETA: 26:21 - loss: 2.6471 - regression_loss: 1.9579 - classification_loss 6689/10000 [===================>..........] - ETA: 26:21 - loss: 2.6470 - regression_loss: 1.9578 - classification_loss 6690/10000 [===================>..........] - ETA: 26:20 - loss: 2.6468 - regression_loss: 1.9577 - classification_loss 6691/10000 [===================>..........] - ETA: 26:20 - loss: 2.6467 - regression_loss: 1.9576 - classification_loss 6692/10000 [===================>..........] - ETA: 26:19 - loss: 2.6466 - regression_loss: 1.9575 - classification_loss 6693/10000 [===================>..........] - ETA: 26:19 - loss: 2.6465 - regression_loss: 1.9575 - classification_loss 6694/10000 [===================>..........] - ETA: 26:18 - loss: 2.6465 - regression_loss: 1.9575 - classification_loss 6695/10000 [===================>..........] - ETA: 26:18 - loss: 2.6464 - regression_loss: 1.9574 - classification_loss 6696/10000 [===================>..........] - ETA: 26:17 - loss: 2.6463 - regression_loss: 1.9574 - classification_loss 6697/10000 [===================>..........] - ETA: 26:17 - loss: 2.6463 - regression_loss: 1.9574 - classification_loss 6698/10000 [===================>..........] - ETA: 26:16 - loss: 2.6462 - regression_loss: 1.9573 - classification_loss 6699/10000 [===================>..........] - ETA: 26:16 - loss: 2.6461 - regression_loss: 1.9573 - classification_loss 6700/10000 [===================>..........] - ETA: 26:15 - loss: 2.6461 - regression_loss: 1.9573 - classification_loss 6701/10000 [===================>..........] - ETA: 26:15 - loss: 2.6460 - regression_loss: 1.9572 - classification_loss 6702/10000 [===================>..........] - ETA: 26:14 - loss: 2.6458 - regression_loss: 1.9571 - classification_loss 6703/10000 [===================>..........] - ETA: 26:14 - loss: 2.6456 - regression_loss: 1.9569 - classification_loss 6704/10000 [===================>..........] - ETA: 26:13 - loss: 2.6455 - regression_loss: 1.9568 - classification_loss 6705/10000 [===================>..........] - ETA: 26:13 - loss: 2.6454 - regression_loss: 1.9568 - classification_loss 6706/10000 [===================>..........] - ETA: 26:13 - loss: 2.6452 - regression_loss: 1.9567 - classification_loss 6707/10000 [===================>..........] - ETA: 26:12 - loss: 2.6450 - regression_loss: 1.9566 - classification_loss 6708/10000 [===================>..........] - ETA: 26:12 - loss: 2.6449 - regression_loss: 1.9565 - classification_loss 6709/10000 [===================>..........] - ETA: 26:11 - loss: 2.6449 - regression_loss: 1.9565 - classification_loss 6710/10000 [===================>..........] - ETA: 26:11 - loss: 2.6450 - regression_loss: 1.9565 - classification_loss 6711/10000 [===================>..........] - ETA: 26:10 - loss: 2.6451 - regression_loss: 1.9566 - classification_loss 6712/10000 [===================>..........] - ETA: 26:10 - loss: 2.6450 - regression_loss: 1.9566 - classification_loss 6713/10000 [===================>..........] - ETA: 26:09 - loss: 2.6449 - regression_loss: 1.9565 - classification_loss 6714/10000 [===================>..........] - ETA: 26:09 - loss: 2.6448 - regression_loss: 1.9564 - classification_loss 6715/10000 [===================>..........] - ETA: 26:08 - loss: 2.6448 - regression_loss: 1.9564 - classification_loss 6716/10000 [===================>..........] - ETA: 26:08 - loss: 2.6445 - regression_loss: 1.9562 - classification_loss 6717/10000 [===================>..........] - ETA: 26:07 - loss: 2.6444 - regression_loss: 1.9562 - classification_loss 6718/10000 [===================>..........] - ETA: 26:07 - loss: 2.6445 - regression_loss: 1.9562 - classification_loss 6719/10000 [===================>..........] - ETA: 26:06 - loss: 2.6443 - regression_loss: 1.9561 - classification_loss 6720/10000 [===================>..........] - ETA: 26:06 - loss: 2.6443 - regression_loss: 1.9561 - classification_loss 6721/10000 [===================>..........] - ETA: 26:05 - loss: 2.6442 - regression_loss: 1.9560 - classification_loss 6722/10000 [===================>..........] - ETA: 26:05 - loss: 2.6441 - regression_loss: 1.9560 - classification_loss 6723/10000 [===================>..........] - ETA: 26:04 - loss: 2.6440 - regression_loss: 1.9560 - classification_loss 6724/10000 [===================>..........] - ETA: 26:04 - loss: 2.6439 - regression_loss: 1.9559 - classification_loss 6725/10000 [===================>..........] - ETA: 26:03 - loss: 2.6438 - regression_loss: 1.9558 - classification_loss 6726/10000 [===================>..........] - ETA: 26:03 - loss: 2.6437 - regression_loss: 1.9558 - classification_loss 6727/10000 [===================>..........] - ETA: 26:02 - loss: 2.6436 - regression_loss: 1.9557 - classification_loss 6728/10000 [===================>..........] - ETA: 26:02 - loss: 2.6434 - regression_loss: 1.9555 - classification_loss 6729/10000 [===================>..........] - ETA: 26:01 - loss: 2.6433 - regression_loss: 1.9555 - classification_loss 6730/10000 [===================>..........] - ETA: 26:01 - loss: 2.6433 - regression_loss: 1.9555 - classification_loss 6731/10000 [===================>..........] - ETA: 26:00 - loss: 2.6433 - regression_loss: 1.9555 - classification_loss 6732/10000 [===================>..........] - ETA: 26:00 - loss: 2.6433 - regression_loss: 1.9555 - classification_loss 6733/10000 [===================>..........] - ETA: 25:59 - loss: 2.6430 - regression_loss: 1.9553 - classification_loss 6734/10000 [===================>..........] - ETA: 25:59 - loss: 2.6429 - regression_loss: 1.9552 - classification_loss 6735/10000 [===================>..........] - ETA: 25:58 - loss: 2.6428 - regression_loss: 1.9551 - classification_loss 6736/10000 [===================>..........] - ETA: 25:58 - loss: 2.6425 - regression_loss: 1.9548 - classification_loss 6737/10000 [===================>..........] - ETA: 25:57 - loss: 2.6424 - regression_loss: 1.9548 - classification_loss 6738/10000 [===================>..........] - ETA: 25:57 - loss: 2.6422 - regression_loss: 1.9547 - classification_loss 6739/10000 [===================>..........] - ETA: 25:56 - loss: 2.6423 - regression_loss: 1.9547 - classification_loss 6740/10000 [===================>..........] - ETA: 25:56 - loss: 2.6422 - regression_loss: 1.9546 - classification_loss 6741/10000 [===================>..........] - ETA: 25:56 - loss: 2.6421 - regression_loss: 1.9546 - classification_loss 6742/10000 [===================>..........] - ETA: 25:55 - loss: 2.6420 - regression_loss: 1.9545 - classification_loss 6743/10000 [===================>..........] - ETA: 25:55 - loss: 2.6420 - regression_loss: 1.9544 - classification_loss 6744/10000 [===================>..........] - ETA: 25:54 - loss: 2.6419 - regression_loss: 1.9543 - classification_loss 6745/10000 [===================>..........] - ETA: 25:54 - loss: 2.6419 - regression_loss: 1.9544 - classification_loss 6746/10000 [===================>..........] - ETA: 25:53 - loss: 2.6419 - regression_loss: 1.9544 - classification_loss 6747/10000 [===================>..........] - ETA: 25:53 - loss: 2.6417 - regression_loss: 1.9542 - classification_loss 6748/10000 [===================>..........] - ETA: 25:52 - loss: 2.6416 - regression_loss: 1.9542 - classification_loss 6749/10000 [===================>..........] - ETA: 25:52 - loss: 2.6414 - regression_loss: 1.9540 - classification_loss 6750/10000 [===================>..........] - ETA: 25:51 - loss: 2.6413 - regression_loss: 1.9539 - classification_loss 6751/10000 [===================>..........] - ETA: 25:51 - loss: 2.6413 - regression_loss: 1.9540 - classification_loss 6752/10000 [===================>..........] - ETA: 25:50 - loss: 2.6411 - regression_loss: 1.9538 - classification_loss 6753/10000 [===================>..........] - ETA: 25:50 - loss: 2.6410 - regression_loss: 1.9538 - classification_loss 6754/10000 [===================>..........] - ETA: 25:49 - loss: 2.6409 - regression_loss: 1.9537 - classification_loss 6755/10000 [===================>..........] - ETA: 25:49 - loss: 2.6408 - regression_loss: 1.9537 - classification_loss 6756/10000 [===================>..........] - ETA: 25:48 - loss: 2.6408 - regression_loss: 1.9536 - classification_loss 6757/10000 [===================>..........] - ETA: 25:48 - loss: 2.6406 - regression_loss: 1.9535 - classification_loss 6758/10000 [===================>..........] - ETA: 25:47 - loss: 2.6407 - regression_loss: 1.9535 - classification_loss 6759/10000 [===================>..........] - ETA: 25:47 - loss: 2.6405 - regression_loss: 1.9534 - classification_loss 6760/10000 [===================>..........] - ETA: 25:46 - loss: 2.6404 - regression_loss: 1.9533 - classification_loss 6761/10000 [===================>..........] - ETA: 25:46 - loss: 2.6403 - regression_loss: 1.9533 - classification_loss 6762/10000 [===================>..........] - ETA: 25:45 - loss: 2.6404 - regression_loss: 1.9533 - classification_loss 6763/10000 [===================>..........] - ETA: 25:45 - loss: 2.6403 - regression_loss: 1.9533 - classification_loss 6764/10000 [===================>..........] - ETA: 25:44 - loss: 2.6403 - regression_loss: 1.9532 - classification_loss 6765/10000 [===================>..........] - ETA: 25:44 - loss: 2.6402 - regression_loss: 1.9532 - classification_loss 6766/10000 [===================>..........] - ETA: 25:43 - loss: 2.6400 - regression_loss: 1.9530 - classification_loss 6767/10000 [===================>..........] - ETA: 25:43 - loss: 2.6399 - regression_loss: 1.9529 - classification_loss 6768/10000 [===================>..........] - ETA: 25:42 - loss: 2.6398 - regression_loss: 1.9528 - classification_loss 6769/10000 [===================>..........] - ETA: 25:42 - loss: 2.6395 - regression_loss: 1.9526 - classification_loss 6770/10000 [===================>..........] - ETA: 25:41 - loss: 2.6396 - regression_loss: 1.9526 - classification_loss 6771/10000 [===================>..........] - ETA: 25:41 - loss: 2.6396 - regression_loss: 1.9527 - classification_loss 6772/10000 [===================>..........] - ETA: 25:40 - loss: 2.6397 - regression_loss: 1.9528 - classification_loss 6773/10000 [===================>..........] - ETA: 25:40 - loss: 2.6397 - regression_loss: 1.9529 - classification_loss 6774/10000 [===================>..........] - ETA: 25:40 - loss: 2.6398 - regression_loss: 1.9529 - classification_loss 6775/10000 [===================>..........] - ETA: 25:39 - loss: 2.6397 - regression_loss: 1.9528 - classification_loss 6776/10000 [===================>..........] - ETA: 25:39 - loss: 2.6397 - regression_loss: 1.9529 - classification_loss 6777/10000 [===================>..........] - ETA: 25:38 - loss: 2.6396 - regression_loss: 1.9528 - classification_loss 6778/10000 [===================>..........] - ETA: 25:38 - loss: 2.6395 - regression_loss: 1.9527 - classification_loss 6779/10000 [===================>..........] - ETA: 25:37 - loss: 2.6394 - regression_loss: 1.9527 - classification_loss 6780/10000 [===================>..........] - ETA: 25:37 - loss: 2.6394 - regression_loss: 1.9527 - classification_loss 6781/10000 [===================>..........] - ETA: 25:36 - loss: 2.6393 - regression_loss: 1.9526 - classification_loss 6782/10000 [===================>..........] - ETA: 25:36 - loss: 2.6392 - regression_loss: 1.9526 - classification_loss 6783/10000 [===================>..........] - ETA: 25:35 - loss: 2.6392 - regression_loss: 1.9526 - classification_loss 6784/10000 [===================>..........] - ETA: 25:35 - loss: 2.6391 - regression_loss: 1.9526 - classification_loss 6785/10000 [===================>..........] - ETA: 25:34 - loss: 2.6391 - regression_loss: 1.9525 - classification_loss 6786/10000 [===================>..........] - ETA: 25:34 - loss: 2.6390 - regression_loss: 1.9525 - classification_loss 6787/10000 [===================>..........] - ETA: 25:33 - loss: 2.6389 - regression_loss: 1.9524 - classification_loss 6788/10000 [===================>..........] - ETA: 25:33 - loss: 2.6388 - regression_loss: 1.9524 - classification_loss 6789/10000 [===================>..........] - ETA: 25:32 - loss: 2.6388 - regression_loss: 1.9524 - classification_loss 6790/10000 [===================>..........] - ETA: 25:32 - loss: 2.6387 - regression_loss: 1.9524 - classification_loss 6791/10000 [===================>..........] - ETA: 25:31 - loss: 2.6386 - regression_loss: 1.9523 - classification_loss 6792/10000 [===================>..........] - ETA: 25:31 - loss: 2.6386 - regression_loss: 1.9523 - classification_loss 6793/10000 [===================>..........] - ETA: 25:30 - loss: 2.6387 - regression_loss: 1.9523 - classification_loss 6794/10000 [===================>..........] - ETA: 25:30 - loss: 2.6386 - regression_loss: 1.9523 - classification_loss 6795/10000 [===================>..........] - ETA: 25:29 - loss: 2.6384 - regression_loss: 1.9521 - classification_loss 6796/10000 [===================>..........] - ETA: 25:29 - loss: 2.6383 - regression_loss: 1.9520 - classification_loss 6797/10000 [===================>..........] - ETA: 25:28 - loss: 2.6381 - regression_loss: 1.9519 - classification_loss 6798/10000 [===================>..........] - ETA: 25:28 - loss: 2.6381 - regression_loss: 1.9519 - classification_loss 6799/10000 [===================>..........] - ETA: 25:27 - loss: 2.6382 - regression_loss: 1.9520 - classification_loss 6800/10000 [===================>..........] - ETA: 25:27 - loss: 2.6382 - regression_loss: 1.9520 - classification_loss 6801/10000 [===================>..........] - ETA: 25:26 - loss: 2.6383 - regression_loss: 1.9521 - classification_loss 6802/10000 [===================>..........] - ETA: 25:26 - loss: 2.6383 - regression_loss: 1.9521 - classification_loss 6803/10000 [===================>..........] - ETA: 25:26 - loss: 2.6385 - regression_loss: 1.9522 - classification_loss 6804/10000 [===================>..........] - ETA: 25:25 - loss: 2.6385 - regression_loss: 1.9522 - classification_loss 6805/10000 [===================>..........] - ETA: 25:25 - loss: 2.6383 - regression_loss: 1.9521 - classification_loss 6806/10000 [===================>..........] - ETA: 25:24 - loss: 2.6383 - regression_loss: 1.9521 - classification_loss 6807/10000 [===================>..........] - ETA: 25:24 - loss: 2.6384 - regression_loss: 1.9522 - classification_loss 6808/10000 [===================>..........] - ETA: 25:23 - loss: 2.6384 - regression_loss: 1.9522 - classification_loss 6809/10000 [===================>..........] - ETA: 25:23 - loss: 2.6384 - regression_loss: 1.9522 - classification_loss 6810/10000 [===================>..........] - ETA: 25:22 - loss: 2.6383 - regression_loss: 1.9521 - classification_loss 6811/10000 [===================>..........] - ETA: 25:22 - loss: 2.6382 - regression_loss: 1.9521 - classification_loss 6812/10000 [===================>..........] - ETA: 25:21 - loss: 2.6381 - regression_loss: 1.9521 - classification_loss 6813/10000 [===================>..........] - ETA: 25:21 - loss: 2.6381 - regression_loss: 1.9520 - classification_loss 6814/10000 [===================>..........] - ETA: 25:20 - loss: 2.6380 - regression_loss: 1.9520 - classification_loss 6815/10000 [===================>..........] - ETA: 25:20 - loss: 2.6380 - regression_loss: 1.9519 - classification_loss 6816/10000 [===================>..........] - ETA: 25:19 - loss: 2.6379 - regression_loss: 1.9519 - classification_loss 6817/10000 [===================>..........] - ETA: 25:19 - loss: 2.6378 - regression_loss: 1.9518 - classification_loss 6818/10000 [===================>..........] - ETA: 25:18 - loss: 2.6376 - regression_loss: 1.9516 - classification_loss 6819/10000 [===================>..........] - ETA: 25:18 - loss: 2.6375 - regression_loss: 1.9515 - classification_loss 6820/10000 [===================>..........] - ETA: 25:17 - loss: 2.6374 - regression_loss: 1.9515 - classification_loss 6821/10000 [===================>..........] - ETA: 25:17 - loss: 2.6373 - regression_loss: 1.9514 - classification_loss 6822/10000 [===================>..........] - ETA: 25:16 - loss: 2.6373 - regression_loss: 1.9514 - classification_loss 6823/10000 [===================>..........] - ETA: 25:16 - loss: 2.6373 - regression_loss: 1.9515 - classification_loss 6824/10000 [===================>..........] - ETA: 25:15 - loss: 2.6373 - regression_loss: 1.9515 - classification_loss 6825/10000 [===================>..........] - ETA: 25:15 - loss: 2.6372 - regression_loss: 1.9514 - classification_loss 6826/10000 [===================>..........] - ETA: 25:14 - loss: 2.6371 - regression_loss: 1.9513 - classification_loss 6827/10000 [===================>..........] - ETA: 25:14 - loss: 2.6369 - regression_loss: 1.9512 - classification_loss 6828/10000 [===================>..........] - ETA: 25:13 - loss: 2.6366 - regression_loss: 1.9510 - classification_loss 6829/10000 [===================>..........] - ETA: 25:13 - loss: 2.6365 - regression_loss: 1.9509 - classification_loss 6830/10000 [===================>..........] - ETA: 25:12 - loss: 2.6364 - regression_loss: 1.9509 - classification_loss 6831/10000 [===================>..........] - ETA: 25:12 - loss: 2.6365 - regression_loss: 1.9509 - classification_loss 6832/10000 [===================>..........] - ETA: 25:11 - loss: 2.6365 - regression_loss: 1.9510 - classification_loss 6833/10000 [===================>..........] - ETA: 25:11 - loss: 2.6365 - regression_loss: 1.9509 - classification_loss 6834/10000 [===================>..........] - ETA: 25:11 - loss: 2.6365 - regression_loss: 1.9510 - classification_loss 6835/10000 [===================>..........] - ETA: 25:10 - loss: 2.6364 - regression_loss: 1.9509 - classification_loss 6836/10000 [===================>..........] - ETA: 25:10 - loss: 2.6364 - regression_loss: 1.9509 - classification_loss 6837/10000 [===================>..........] - ETA: 25:09 - loss: 2.6364 - regression_loss: 1.9509 - classification_loss 6838/10000 [===================>..........] - ETA: 25:09 - loss: 2.6363 - regression_loss: 1.9508 - classification_loss 6839/10000 [===================>..........] - ETA: 25:08 - loss: 2.6362 - regression_loss: 1.9508 - classification_loss 6840/10000 [===================>..........] - ETA: 25:08 - loss: 2.6362 - regression_loss: 1.9507 - classification_loss 6841/10000 [===================>..........] - ETA: 25:07 - loss: 2.6360 - regression_loss: 1.9506 - classification_loss 6842/10000 [===================>..........] - ETA: 25:07 - loss: 2.6359 - regression_loss: 1.9505 - classification_loss 6843/10000 [===================>..........] - ETA: 25:06 - loss: 2.6358 - regression_loss: 1.9504 - classification_loss 6844/10000 [===================>..........] - ETA: 25:06 - loss: 2.6357 - regression_loss: 1.9504 - classification_loss 6845/10000 [===================>..........] - ETA: 25:05 - loss: 2.6357 - regression_loss: 1.9504 - classification_loss 6846/10000 [===================>..........] - ETA: 25:05 - loss: 2.6357 - regression_loss: 1.9505 - classification_loss 6847/10000 [===================>..........] - ETA: 25:04 - loss: 2.6356 - regression_loss: 1.9503 - classification_loss 6848/10000 [===================>..........] - ETA: 25:04 - loss: 2.6356 - regression_loss: 1.9503 - classification_loss 6849/10000 [===================>..........] - ETA: 25:03 - loss: 2.6354 - regression_loss: 1.9501 - classification_loss 6850/10000 [===================>..........] - ETA: 25:03 - loss: 2.6354 - regression_loss: 1.9502 - classification_loss 6851/10000 [===================>..........] - ETA: 25:02 - loss: 2.6354 - regression_loss: 1.9502 - classification_loss 6852/10000 [===================>..........] - ETA: 25:02 - loss: 2.6353 - regression_loss: 1.9501 - classification_loss 6853/10000 [===================>..........] - ETA: 25:01 - loss: 2.6351 - regression_loss: 1.9499 - classification_loss 6854/10000 [===================>..........] - ETA: 25:01 - loss: 2.6350 - regression_loss: 1.9499 - classification_loss 6855/10000 [===================>..........] - ETA: 25:00 - loss: 2.6349 - regression_loss: 1.9498 - classification_loss 6856/10000 [===================>..........] - ETA: 25:00 - loss: 2.6348 - regression_loss: 1.9497 - classification_loss 6857/10000 [===================>..........] - ETA: 24:59 - loss: 2.6347 - regression_loss: 1.9497 - classification_loss 6858/10000 [===================>..........] - ETA: 24:59 - loss: 2.6347 - regression_loss: 1.9497 - classification_loss 6859/10000 [===================>..........] - ETA: 24:58 - loss: 2.6347 - regression_loss: 1.9496 - classification_loss 6860/10000 [===================>..........] - ETA: 24:58 - loss: 2.6347 - regression_loss: 1.9496 - classification_loss 6861/10000 [===================>..........] - ETA: 24:57 - loss: 2.6347 - regression_loss: 1.9497 - classification_loss 6862/10000 [===================>..........] - ETA: 24:57 - loss: 2.6346 - regression_loss: 1.9496 - classification_loss 6863/10000 [===================>..........] - ETA: 24:57 - loss: 2.6345 - regression_loss: 1.9496 - classification_loss 6864/10000 [===================>..........] - ETA: 24:56 - loss: 2.6345 - regression_loss: 1.9495 - classification_loss 6865/10000 [===================>..........] - ETA: 24:56 - loss: 2.6343 - regression_loss: 1.9494 - classification_loss 6866/10000 [===================>..........] - ETA: 24:55 - loss: 2.6342 - regression_loss: 1.9493 - classification_loss 6867/10000 [===================>..........] - ETA: 24:55 - loss: 2.6342 - regression_loss: 1.9493 - classification_loss 6868/10000 [===================>..........] - ETA: 24:54 - loss: 2.6341 - regression_loss: 1.9492 - classification_loss 6869/10000 [===================>..........] - ETA: 24:54 - loss: 2.6339 - regression_loss: 1.9491 - classification_loss 6870/10000 [===================>..........] - ETA: 24:53 - loss: 2.6339 - regression_loss: 1.9491 - classification_loss 6871/10000 [===================>..........] - ETA: 24:53 - loss: 2.6338 - regression_loss: 1.9491 - classification_loss 6872/10000 [===================>..........] - ETA: 24:52 - loss: 2.6338 - regression_loss: 1.9491 - classification_loss 6873/10000 [===================>..........] - ETA: 24:52 - loss: 2.6337 - regression_loss: 1.9491 - classification_loss 6874/10000 [===================>..........] - ETA: 24:51 - loss: 2.6337 - regression_loss: 1.9490 - classification_loss 6875/10000 [===================>..........] - ETA: 24:51 - loss: 2.6335 - regression_loss: 1.9489 - classification_loss 6876/10000 [===================>..........] - ETA: 24:50 - loss: 2.6335 - regression_loss: 1.9489 - classification_loss 6877/10000 [===================>..........] - ETA: 24:50 - loss: 2.6334 - regression_loss: 1.9488 - classification_loss 6878/10000 [===================>..........] - ETA: 24:49 - loss: 2.6333 - regression_loss: 1.9487 - classification_loss 6879/10000 [===================>..........] - ETA: 24:49 - loss: 2.6331 - regression_loss: 1.9485 - classification_loss 6880/10000 [===================>..........] - ETA: 24:48 - loss: 2.6331 - regression_loss: 1.9485 - classification_loss 6881/10000 [===================>..........] - ETA: 24:48 - loss: 2.6330 - regression_loss: 1.9485 - classification_loss 6882/10000 [===================>..........] - ETA: 24:47 - loss: 2.6329 - regression_loss: 1.9484 - classification_loss 6883/10000 [===================>..........] - ETA: 24:47 - loss: 2.6327 - regression_loss: 1.9483 - classification_loss 6884/10000 [===================>..........] - ETA: 24:46 - loss: 2.6327 - regression_loss: 1.9482 - classification_loss 6885/10000 [===================>..........] - ETA: 24:46 - loss: 2.6326 - regression_loss: 1.9482 - classification_loss 6886/10000 [===================>..........] - ETA: 24:45 - loss: 2.6326 - regression_loss: 1.9482 - classification_loss 6887/10000 [===================>..........] - ETA: 24:45 - loss: 2.6325 - regression_loss: 1.9482 - classification_loss 6888/10000 [===================>..........] - ETA: 24:44 - loss: 2.6324 - regression_loss: 1.9481 - classification_loss 6889/10000 [===================>..........] - ETA: 24:44 - loss: 2.6323 - regression_loss: 1.9480 - classification_loss 6890/10000 [===================>..........] - ETA: 24:43 - loss: 2.6322 - regression_loss: 1.9480 - classification_loss 6891/10000 [===================>..........] - ETA: 24:43 - loss: 2.6320 - regression_loss: 1.9478 - classification_loss 6892/10000 [===================>..........] - ETA: 24:42 - loss: 2.6320 - regression_loss: 1.9478 - classification_loss 6893/10000 [===================>..........] - ETA: 24:42 - loss: 2.6320 - regression_loss: 1.9479 - classification_loss 6894/10000 [===================>..........] - ETA: 24:41 - loss: 2.6320 - regression_loss: 1.9479 - classification_loss 6895/10000 [===================>..........] - ETA: 24:41 - loss: 2.6320 - regression_loss: 1.9479 - classification_loss 6896/10000 [===================>..........] - ETA: 24:40 - loss: 2.6320 - regression_loss: 1.9479 - classification_loss 6897/10000 [===================>..........] - ETA: 24:40 - loss: 2.6320 - regression_loss: 1.9479 - classification_loss 6898/10000 [===================>..........] - ETA: 24:40 - loss: 2.6318 - regression_loss: 1.9477 - classification_loss 6899/10000 [===================>..........] - ETA: 24:39 - loss: 2.6318 - regression_loss: 1.9477 - classification_loss 6900/10000 [===================>..........] - ETA: 24:39 - loss: 2.6317 - regression_loss: 1.9478 - classification_loss 6901/10000 [===================>..........] - ETA: 24:38 - loss: 2.6318 - regression_loss: 1.9478 - classification_loss 6902/10000 [===================>..........] - ETA: 24:38 - loss: 2.6317 - regression_loss: 1.9477 - classification_loss 6903/10000 [===================>..........] - ETA: 24:37 - loss: 2.6317 - regression_loss: 1.9477 - classification_loss 6904/10000 [===================>..........] - ETA: 24:37 - loss: 2.6315 - regression_loss: 1.9475 - classification_loss 6905/10000 [===================>..........] - ETA: 24:36 - loss: 2.6314 - regression_loss: 1.9475 - classification_loss 6906/10000 [===================>..........] - ETA: 24:36 - loss: 2.6314 - regression_loss: 1.9476 - classification_loss 6907/10000 [===================>..........] - ETA: 24:35 - loss: 2.6314 - regression_loss: 1.9476 - classification_loss 6908/10000 [===================>..........] - ETA: 24:35 - loss: 2.6313 - regression_loss: 1.9475 - classification_loss 6909/10000 [===================>..........] - ETA: 24:34 - loss: 2.6312 - regression_loss: 1.9475 - classification_loss 6910/10000 [===================>..........] - ETA: 24:34 - loss: 2.6311 - regression_loss: 1.9474 - classification_loss 6911/10000 [===================>..........] - ETA: 24:33 - loss: 2.6310 - regression_loss: 1.9473 - classification_loss 6912/10000 [===================>..........] - ETA: 24:33 - loss: 2.6311 - regression_loss: 1.9474 - classification_loss 6913/10000 [===================>..........] - ETA: 24:32 - loss: 2.6310 - regression_loss: 1.9473 - classification_loss 6914/10000 [===================>..........] - ETA: 24:32 - loss: 2.6308 - regression_loss: 1.9471 - classification_loss 6915/10000 [===================>..........] - ETA: 24:31 - loss: 2.6306 - regression_loss: 1.9470 - classification_loss 6916/10000 [===================>..........] - ETA: 24:31 - loss: 2.6305 - regression_loss: 1.9469 - classification_loss 6917/10000 [===================>..........] - ETA: 24:30 - loss: 2.6304 - regression_loss: 1.9469 - classification_loss 6918/10000 [===================>..........] - ETA: 24:30 - loss: 2.6303 - regression_loss: 1.9468 - classification_loss 6919/10000 [===================>..........] - ETA: 24:29 - loss: 2.6301 - regression_loss: 1.9467 - classification_loss 6920/10000 [===================>..........] - ETA: 24:29 - loss: 2.6300 - regression_loss: 1.9466 - classification_loss 6921/10000 [===================>..........] - ETA: 24:28 - loss: 2.6299 - regression_loss: 1.9466 - classification_loss 6922/10000 [===================>..........] - ETA: 24:28 - loss: 2.6299 - regression_loss: 1.9465 - classification_loss 6923/10000 [===================>..........] - ETA: 24:27 - loss: 2.6299 - regression_loss: 1.9465 - classification_loss 6924/10000 [===================>..........] - ETA: 24:27 - loss: 2.6297 - regression_loss: 1.9464 - classification_loss 6925/10000 [===================>..........] - ETA: 24:26 - loss: 2.6296 - regression_loss: 1.9462 - classification_loss 6926/10000 [===================>..........] - ETA: 24:26 - loss: 2.6295 - regression_loss: 1.9461 - classification_loss 6927/10000 [===================>..........] - ETA: 24:25 - loss: 2.6294 - regression_loss: 1.9461 - classification_loss 6928/10000 [===================>..........] - ETA: 24:25 - loss: 2.6293 - regression_loss: 1.9461 - classification_loss 6929/10000 [===================>..........] - ETA: 24:25 - loss: 2.6294 - regression_loss: 1.9461 - classification_loss 6930/10000 [===================>..........] - ETA: 24:24 - loss: 2.6293 - regression_loss: 1.9461 - classification_loss 6931/10000 [===================>..........] - ETA: 24:24 - loss: 2.6292 - regression_loss: 1.9460 - classification_loss 6932/10000 [===================>..........] - ETA: 24:23 - loss: 2.6291 - regression_loss: 1.9458 - classification_loss 6933/10000 [===================>..........] - ETA: 24:23 - loss: 2.6290 - regression_loss: 1.9458 - classification_loss 6934/10000 [===================>..........] - ETA: 24:22 - loss: 2.6290 - regression_loss: 1.9458 - classification_loss 6935/10000 [===================>..........] - ETA: 24:22 - loss: 2.6289 - regression_loss: 1.9457 - classification_loss 6936/10000 [===================>..........] - ETA: 24:21 - loss: 2.6288 - regression_loss: 1.9457 - classification_loss 6937/10000 [===================>..........] - ETA: 24:21 - loss: 2.6288 - regression_loss: 1.9457 - classification_loss 6938/10000 [===================>..........] - ETA: 24:20 - loss: 2.6287 - regression_loss: 1.9456 - classification_loss 6939/10000 [===================>..........] - ETA: 24:20 - loss: 2.6286 - regression_loss: 1.9456 - classification_loss 6940/10000 [===================>..........] - ETA: 24:19 - loss: 2.6283 - regression_loss: 1.9454 - classification_loss 6941/10000 [===================>..........] - ETA: 24:19 - loss: 2.6281 - regression_loss: 1.9452 - classification_loss 6942/10000 [===================>..........] - ETA: 24:18 - loss: 2.6280 - regression_loss: 1.9452 - classification_loss 6943/10000 [===================>..........] - ETA: 24:18 - loss: 2.6281 - regression_loss: 1.9452 - classification_loss 6944/10000 [===================>..........] - ETA: 24:17 - loss: 2.6280 - regression_loss: 1.9452 - classification_loss 6945/10000 [===================>..........] - ETA: 24:17 - loss: 2.6281 - regression_loss: 1.9453 - classification_loss 6946/10000 [===================>..........] - ETA: 24:16 - loss: 2.6279 - regression_loss: 1.9451 - classification_loss 6947/10000 [===================>..........] - ETA: 24:16 - loss: 2.6279 - regression_loss: 1.9451 - classification_loss 6948/10000 [===================>..........] - ETA: 24:15 - loss: 2.6278 - regression_loss: 1.9450 - classification_loss 6949/10000 [===================>..........] - ETA: 24:15 - loss: 2.6277 - regression_loss: 1.9450 - classification_loss 6950/10000 [===================>..........] - ETA: 24:14 - loss: 2.6277 - regression_loss: 1.9450 - classification_loss 6951/10000 [===================>..........] - ETA: 24:14 - loss: 2.6277 - regression_loss: 1.9451 - classification_loss 6952/10000 [===================>..........] - ETA: 24:13 - loss: 2.6276 - regression_loss: 1.9450 - classification_loss 6953/10000 [===================>..........] - ETA: 24:13 - loss: 2.6275 - regression_loss: 1.9450 - classification_loss 6954/10000 [===================>..........] - ETA: 24:12 - loss: 2.6274 - regression_loss: 1.9449 - classification_loss 6955/10000 [===================>..........] - ETA: 24:12 - loss: 2.6272 - regression_loss: 1.9447 - classification_loss 6956/10000 [===================>..........] - ETA: 24:11 - loss: 2.6271 - regression_loss: 1.9447 - classification_loss 6957/10000 [===================>..........] - ETA: 24:11 - loss: 2.6269 - regression_loss: 1.9445 - classification_loss 6958/10000 [===================>..........] - ETA: 24:11 - loss: 2.6269 - regression_loss: 1.9445 - classification_loss 6959/10000 [===================>..........] - ETA: 24:10 - loss: 2.6268 - regression_loss: 1.9444 - classification_loss 6960/10000 [===================>..........] - ETA: 24:10 - loss: 2.6268 - regression_loss: 1.9445 - classification_loss 6961/10000 [===================>..........] - ETA: 24:09 - loss: 2.6267 - regression_loss: 1.9444 - classification_loss 6962/10000 [===================>..........] - ETA: 24:09 - loss: 2.6266 - regression_loss: 1.9443 - classification_loss 6963/10000 [===================>..........] - ETA: 24:08 - loss: 2.6266 - regression_loss: 1.9443 - classification_loss 6964/10000 [===================>..........] - ETA: 24:08 - loss: 2.6265 - regression_loss: 1.9442 - classification_loss 6965/10000 [===================>..........] - ETA: 24:07 - loss: 2.6265 - regression_loss: 1.9443 - classification_loss 6966/10000 [===================>..........] - ETA: 24:07 - loss: 2.6266 - regression_loss: 1.9444 - classification_loss 6967/10000 [===================>..........] - ETA: 24:06 - loss: 2.6265 - regression_loss: 1.9442 - classification_loss 6968/10000 [===================>..........] - ETA: 24:06 - loss: 2.6264 - regression_loss: 1.9442 - classification_loss 6969/10000 [===================>..........] - ETA: 24:05 - loss: 2.6262 - regression_loss: 1.9441 - classification_loss 6970/10000 [===================>..........] - ETA: 24:05 - loss: 2.6261 - regression_loss: 1.9440 - classification_loss 6971/10000 [===================>..........] - ETA: 24:04 - loss: 2.6261 - regression_loss: 1.9440 - classification_loss 6972/10000 [===================>..........] - ETA: 24:04 - loss: 2.6261 - regression_loss: 1.9440 - classification_loss 6973/10000 [===================>..........] - ETA: 24:03 - loss: 2.6259 - regression_loss: 1.9439 - classification_loss 6974/10000 [===================>..........] - ETA: 24:03 - loss: 2.6258 - regression_loss: 1.9437 - classification_loss 6975/10000 [===================>..........] - ETA: 24:02 - loss: 2.6258 - regression_loss: 1.9438 - classification_loss 6976/10000 [===================>..........] - ETA: 24:02 - loss: 2.6256 - regression_loss: 1.9436 - classification_loss 6977/10000 [===================>..........] - ETA: 24:01 - loss: 2.6254 - regression_loss: 1.9434 - classification_loss 6978/10000 [===================>..........] - ETA: 24:01 - loss: 2.6254 - regression_loss: 1.9434 - classification_loss 6979/10000 [===================>..........] - ETA: 24:00 - loss: 2.6253 - regression_loss: 1.9433 - classification_loss 6980/10000 [===================>..........] - ETA: 24:00 - loss: 2.6252 - regression_loss: 1.9433 - classification_loss 6981/10000 [===================>..........] - ETA: 23:59 - loss: 2.6252 - regression_loss: 1.9433 - classification_loss 6982/10000 [===================>..........] - ETA: 23:59 - loss: 2.6252 - regression_loss: 1.9433 - classification_loss 6983/10000 [===================>..........] - ETA: 23:58 - loss: 2.6251 - regression_loss: 1.9432 - classification_loss 6984/10000 [===================>..........] - ETA: 23:58 - loss: 2.6249 - regression_loss: 1.9431 - classification_loss 6985/10000 [===================>..........] - ETA: 23:57 - loss: 2.6248 - regression_loss: 1.9428 - classification_loss 6986/10000 [===================>..........] - ETA: 23:57 - loss: 2.6246 - regression_loss: 1.9427 - classification_loss 6987/10000 [===================>..........] - ETA: 23:57 - loss: 2.6246 - regression_loss: 1.9427 - classification_loss 6988/10000 [===================>..........] - ETA: 23:56 - loss: 2.6244 - regression_loss: 1.9425 - classification_loss 6989/10000 [===================>..........] - ETA: 23:56 - loss: 2.6244 - regression_loss: 1.9426 - classification_loss 6990/10000 [===================>..........] - ETA: 23:55 - loss: 2.6242 - regression_loss: 1.9424 - classification_loss 6991/10000 [===================>..........] - ETA: 23:55 - loss: 2.6240 - regression_loss: 1.9423 - classification_loss 6992/10000 [===================>..........] - ETA: 23:54 - loss: 2.6240 - regression_loss: 1.9423 - classification_loss 6993/10000 [===================>..........] - ETA: 23:54 - loss: 2.6239 - regression_loss: 1.9423 - classification_loss 6994/10000 [===================>..........] - ETA: 23:53 - loss: 2.6240 - regression_loss: 1.9424 - classification_loss 6995/10000 [===================>..........] - ETA: 23:53 - loss: 2.6239 - regression_loss: 1.9423 - classification_loss 6996/10000 [===================>..........] - ETA: 23:52 - loss: 2.6238 - regression_loss: 1.9423 - classification_loss 6997/10000 [===================>..........] - ETA: 23:52 - loss: 2.6237 - regression_loss: 1.9422 - classification_loss 6998/10000 [===================>..........] - ETA: 23:51 - loss: 2.6236 - regression_loss: 1.9421 - classification_loss 6999/10000 [===================>..........] - ETA: 23:51 - loss: 2.6234 - regression_loss: 1.9419 - classification_loss 7000/10000 [====================>.........] - ETA: 23:50 - loss: 2.6233 - regression_loss: 1.9418 - classification_loss 7001/10000 [====================>.........] - ETA: 23:50 - loss: 2.6233 - regression_loss: 1.9419 - classification_loss 7002/10000 [====================>.........] - ETA: 23:49 - loss: 2.6233 - regression_loss: 1.9419 - classification_loss 7003/10000 [====================>.........] - ETA: 23:49 - loss: 2.6233 - regression_loss: 1.9419 - classification_loss 7004/10000 [====================>.........] - ETA: 23:48 - loss: 2.6232 - regression_loss: 1.9418 - classification_loss 7005/10000 [====================>.........] - ETA: 23:48 - loss: 2.6232 - regression_loss: 1.9418 - classification_loss 7006/10000 [====================>.........] - ETA: 23:47 - loss: 2.6231 - regression_loss: 1.9417 - classification_loss 7007/10000 [====================>.........] - ETA: 23:47 - loss: 2.6230 - regression_loss: 1.9417 - classification_loss 7008/10000 [====================>.........] - ETA: 23:46 - loss: 2.6228 - regression_loss: 1.9416 - classification_loss 7009/10000 [====================>.........] - ETA: 23:46 - loss: 2.6229 - regression_loss: 1.9416 - classification_loss 7010/10000 [====================>.........] - ETA: 23:45 - loss: 2.6229 - regression_loss: 1.9416 - classification_loss 7011/10000 [====================>.........] - ETA: 23:45 - loss: 2.6228 - regression_loss: 1.9416 - classification_loss 7012/10000 [====================>.........] - ETA: 23:44 - loss: 2.6227 - regression_loss: 1.9415 - classification_loss 7013/10000 [====================>.........] - ETA: 23:44 - loss: 2.6226 - regression_loss: 1.9414 - classification_loss 7014/10000 [====================>.........] - ETA: 23:43 - loss: 2.6227 - regression_loss: 1.9415 - classification_loss 7015/10000 [====================>.........] - ETA: 23:43 - loss: 2.6226 - regression_loss: 1.9415 - classification_loss 7016/10000 [====================>.........] - ETA: 23:43 - loss: 2.6226 - regression_loss: 1.9414 - classification_loss 7017/10000 [====================>.........] - ETA: 23:42 - loss: 2.6224 - regression_loss: 1.9413 - classification_loss 7018/10000 [====================>.........] - ETA: 23:42 - loss: 2.6223 - regression_loss: 1.9412 - classification_loss 7019/10000 [====================>.........] - ETA: 23:41 - loss: 2.6221 - regression_loss: 1.9411 - classification_loss 7020/10000 [====================>.........] - ETA: 23:41 - loss: 2.6220 - regression_loss: 1.9410 - classification_loss 7021/10000 [====================>.........] - ETA: 23:40 - loss: 2.6219 - regression_loss: 1.9409 - classification_loss 7022/10000 [====================>.........] - ETA: 23:40 - loss: 2.6218 - regression_loss: 1.9408 - classification_loss 7023/10000 [====================>.........] - ETA: 23:39 - loss: 2.6217 - regression_loss: 1.9407 - classification_loss 7024/10000 [====================>.........] - ETA: 23:39 - loss: 2.6217 - regression_loss: 1.9408 - classification_loss 7025/10000 [====================>.........] - ETA: 23:38 - loss: 2.6216 - regression_loss: 1.9407 - classification_loss 7026/10000 [====================>.........] - ETA: 23:38 - loss: 2.6216 - regression_loss: 1.9407 - classification_loss 7027/10000 [====================>.........] - ETA: 23:37 - loss: 2.6216 - regression_loss: 1.9408 - classification_loss 7028/10000 [====================>.........] - ETA: 23:37 - loss: 2.6214 - regression_loss: 1.9407 - classification_loss 7029/10000 [====================>.........] - ETA: 23:36 - loss: 2.6213 - regression_loss: 1.9405 - classification_loss 7030/10000 [====================>.........] - ETA: 23:36 - loss: 2.6212 - regression_loss: 1.9405 - classification_loss 7031/10000 [====================>.........] - ETA: 23:35 - loss: 2.6210 - regression_loss: 1.9404 - classification_loss 7032/10000 [====================>.........] - ETA: 23:35 - loss: 2.6210 - regression_loss: 1.9403 - classification_loss 7033/10000 [====================>.........] - ETA: 23:34 - loss: 2.6209 - regression_loss: 1.9403 - classification_loss 7034/10000 [====================>.........] - ETA: 23:34 - loss: 2.6210 - regression_loss: 1.9404 - classification_loss 7035/10000 [====================>.........] - ETA: 23:33 - loss: 2.6208 - regression_loss: 1.9402 - classification_loss 7036/10000 [====================>.........] - ETA: 23:33 - loss: 2.6209 - regression_loss: 1.9403 - classification_loss 7037/10000 [====================>.........] - ETA: 23:32 - loss: 2.6210 - regression_loss: 1.9404 - classification_loss 7038/10000 [====================>.........] - ETA: 23:32 - loss: 2.6209 - regression_loss: 1.9404 - classification_loss 7039/10000 [====================>.........] - ETA: 23:31 - loss: 2.6208 - regression_loss: 1.9403 - classification_loss 7040/10000 [====================>.........] - ETA: 23:31 - loss: 2.6206 - regression_loss: 1.9402 - classification_loss 7041/10000 [====================>.........] - ETA: 23:30 - loss: 2.6205 - regression_loss: 1.9401 - classification_loss 7042/10000 [====================>.........] - ETA: 23:30 - loss: 2.6204 - regression_loss: 1.9400 - classification_loss 7043/10000 [====================>.........] - ETA: 23:29 - loss: 2.6205 - regression_loss: 1.9400 - classification_loss 7044/10000 [====================>.........] - ETA: 23:29 - loss: 2.6205 - regression_loss: 1.9401 - classification_loss 7045/10000 [====================>.........] - ETA: 23:28 - loss: 2.6205 - regression_loss: 1.9401 - classification_loss 7046/10000 [====================>.........] - ETA: 23:28 - loss: 2.6202 - regression_loss: 1.9399 - classification_loss 7047/10000 [====================>.........] - ETA: 23:28 - loss: 2.6203 - regression_loss: 1.9400 - classification_loss 7048/10000 [====================>.........] - ETA: 23:27 - loss: 2.6201 - regression_loss: 1.9398 - classification_loss 7049/10000 [====================>.........] - ETA: 23:27 - loss: 2.6200 - regression_loss: 1.9398 - classification_loss 7050/10000 [====================>.........] - ETA: 23:26 - loss: 2.6198 - regression_loss: 1.9396 - classification_loss 7051/10000 [====================>.........] - ETA: 23:26 - loss: 2.6196 - regression_loss: 1.9395 - classification_loss 7052/10000 [====================>.........] - ETA: 23:25 - loss: 2.6196 - regression_loss: 1.9395 - classification_loss 7053/10000 [====================>.........] - ETA: 23:25 - loss: 2.6196 - regression_loss: 1.9394 - classification_loss 7054/10000 [====================>.........] - ETA: 23:24 - loss: 2.6195 - regression_loss: 1.9394 - classification_loss 7055/10000 [====================>.........] - ETA: 23:24 - loss: 2.6195 - regression_loss: 1.9394 - classification_loss 7056/10000 [====================>.........] - ETA: 23:23 - loss: 2.6193 - regression_loss: 1.9393 - classification_loss 7057/10000 [====================>.........] - ETA: 23:23 - loss: 2.6192 - regression_loss: 1.9393 - classification_loss 7058/10000 [====================>.........] - ETA: 23:22 - loss: 2.6193 - regression_loss: 1.9394 - classification_loss 7059/10000 [====================>.........] - ETA: 23:22 - loss: 2.6192 - regression_loss: 1.9393 - classification_loss 7060/10000 [====================>.........] - ETA: 23:21 - loss: 2.6192 - regression_loss: 1.9393 - classification_loss 7061/10000 [====================>.........] - ETA: 23:21 - loss: 2.6190 - regression_loss: 1.9392 - classification_loss 7062/10000 [====================>.........] - ETA: 23:20 - loss: 2.6189 - regression_loss: 1.9391 - classification_loss 7063/10000 [====================>.........] - ETA: 23:20 - loss: 2.6188 - regression_loss: 1.9390 - classification_loss 7064/10000 [====================>.........] - ETA: 23:19 - loss: 2.6188 - regression_loss: 1.9391 - classification_loss 7065/10000 [====================>.........] - ETA: 23:19 - loss: 2.6186 - regression_loss: 1.9389 - classification_loss 7066/10000 [====================>.........] - ETA: 23:18 - loss: 2.6186 - regression_loss: 1.9389 - classification_loss 7067/10000 [====================>.........] - ETA: 23:18 - loss: 2.6186 - regression_loss: 1.9389 - classification_loss 7068/10000 [====================>.........] - ETA: 23:17 - loss: 2.6185 - regression_loss: 1.9388 - classification_loss 7069/10000 [====================>.........] - ETA: 23:17 - loss: 2.6185 - regression_loss: 1.9388 - classification_loss 7070/10000 [====================>.........] - ETA: 23:16 - loss: 2.6183 - regression_loss: 1.9387 - classification_loss 7071/10000 [====================>.........] - ETA: 23:16 - loss: 2.6182 - regression_loss: 1.9386 - classification_loss 7072/10000 [====================>.........] - ETA: 23:15 - loss: 2.6182 - regression_loss: 1.9387 - classification_loss 7073/10000 [====================>.........] - ETA: 23:15 - loss: 2.6182 - regression_loss: 1.9386 - classification_loss 7074/10000 [====================>.........] - ETA: 23:14 - loss: 2.6180 - regression_loss: 1.9385 - classification_loss 7075/10000 [====================>.........] - ETA: 23:14 - loss: 2.6178 - regression_loss: 1.9384 - classification_loss 7076/10000 [====================>.........] - ETA: 23:14 - loss: 2.6178 - regression_loss: 1.9383 - classification_loss 7077/10000 [====================>.........] - ETA: 23:13 - loss: 2.6178 - regression_loss: 1.9384 - classification_loss 7078/10000 [====================>.........] - ETA: 23:13 - loss: 2.6177 - regression_loss: 1.9383 - classification_loss 7079/10000 [====================>.........] - ETA: 23:12 - loss: 2.6176 - regression_loss: 1.9382 - classification_loss 7080/10000 [====================>.........] - ETA: 23:12 - loss: 2.6176 - regression_loss: 1.9382 - classification_loss 7081/10000 [====================>.........] - ETA: 23:11 - loss: 2.6175 - regression_loss: 1.9382 - classification_loss 7082/10000 [====================>.........] - ETA: 23:11 - loss: 2.6174 - regression_loss: 1.9381 - classification_loss 7083/10000 [====================>.........] - ETA: 23:10 - loss: 2.6174 - regression_loss: 1.9381 - classification_loss 7084/10000 [====================>.........] - ETA: 23:10 - loss: 2.6172 - regression_loss: 1.9380 - classification_loss 7085/10000 [====================>.........] - ETA: 23:09 - loss: 2.6170 - regression_loss: 1.9379 - classification_loss 7086/10000 [====================>.........] - ETA: 23:09 - loss: 2.6169 - regression_loss: 1.9378 - classification_loss 7087/10000 [====================>.........] - ETA: 23:08 - loss: 2.6167 - regression_loss: 1.9376 - classification_loss 7088/10000 [====================>.........] - ETA: 23:08 - loss: 2.6167 - regression_loss: 1.9376 - classification_loss 7089/10000 [====================>.........] - ETA: 23:07 - loss: 2.6167 - regression_loss: 1.9376 - classification_loss 7090/10000 [====================>.........] - ETA: 23:07 - loss: 2.6165 - regression_loss: 1.9375 - classification_loss 7091/10000 [====================>.........] - ETA: 23:06 - loss: 2.6165 - regression_loss: 1.9375 - classification_loss 7092/10000 [====================>.........] - ETA: 23:06 - loss: 2.6165 - regression_loss: 1.9375 - classification_loss 7093/10000 [====================>.........] - ETA: 23:05 - loss: 2.6165 - regression_loss: 1.9375 - classification_loss 7094/10000 [====================>.........] - ETA: 23:05 - loss: 2.6165 - regression_loss: 1.9375 - classification_loss 7095/10000 [====================>.........] - ETA: 23:04 - loss: 2.6164 - regression_loss: 1.9374 - classification_loss 7096/10000 [====================>.........] - ETA: 23:04 - loss: 2.6164 - regression_loss: 1.9374 - classification_loss 7097/10000 [====================>.........] - ETA: 23:03 - loss: 2.6164 - regression_loss: 1.9375 - classification_loss 7098/10000 [====================>.........] - ETA: 23:03 - loss: 2.6162 - regression_loss: 1.9373 - classification_loss 7099/10000 [====================>.........] - ETA: 23:02 - loss: 2.6162 - regression_loss: 1.9372 - classification_loss 7100/10000 [====================>.........] - ETA: 23:02 - loss: 2.6161 - regression_loss: 1.9372 - classification_loss 7101/10000 [====================>.........] - ETA: 23:01 - loss: 2.6160 - regression_loss: 1.9371 - classification_loss 7102/10000 [====================>.........] - ETA: 23:01 - loss: 2.6159 - regression_loss: 1.9371 - classification_loss 7103/10000 [====================>.........] - ETA: 23:01 - loss: 2.6160 - regression_loss: 1.9372 - classification_loss 7104/10000 [====================>.........] - ETA: 23:00 - loss: 2.6159 - regression_loss: 1.9371 - classification_loss 7105/10000 [====================>.........] - ETA: 23:00 - loss: 2.6157 - regression_loss: 1.9370 - classification_loss 7106/10000 [====================>.........] - ETA: 22:59 - loss: 2.6156 - regression_loss: 1.9369 - classification_loss 7107/10000 [====================>.........] - ETA: 22:59 - loss: 2.6156 - regression_loss: 1.9370 - classification_loss 7108/10000 [====================>.........] - ETA: 22:58 - loss: 2.6156 - regression_loss: 1.9369 - classification_loss 7109/10000 [====================>.........] - ETA: 22:58 - loss: 2.6155 - regression_loss: 1.9369 - classification_loss 7110/10000 [====================>.........] - ETA: 22:57 - loss: 2.6154 - regression_loss: 1.9368 - classification_loss 7111/10000 [====================>.........] - ETA: 22:57 - loss: 2.6154 - regression_loss: 1.9368 - classification_loss 7112/10000 [====================>.........] - ETA: 22:56 - loss: 2.6153 - regression_loss: 1.9367 - classification_loss 7113/10000 [====================>.........] - ETA: 22:56 - loss: 2.6151 - regression_loss: 1.9366 - classification_loss 7114/10000 [====================>.........] - ETA: 22:55 - loss: 2.6151 - regression_loss: 1.9367 - classification_loss 7115/10000 [====================>.........] - ETA: 22:55 - loss: 2.6152 - regression_loss: 1.9367 - classification_loss 7116/10000 [====================>.........] - ETA: 22:54 - loss: 2.6151 - regression_loss: 1.9366 - classification_loss 7117/10000 [====================>.........] - ETA: 22:54 - loss: 2.6151 - regression_loss: 1.9366 - classification_loss 7118/10000 [====================>.........] - ETA: 22:53 - loss: 2.6150 - regression_loss: 1.9366 - classification_loss 7119/10000 [====================>.........] - ETA: 22:53 - loss: 2.6148 - regression_loss: 1.9364 - classification_loss 7120/10000 [====================>.........] - ETA: 22:52 - loss: 2.6147 - regression_loss: 1.9364 - classification_loss 7121/10000 [====================>.........] - ETA: 22:52 - loss: 2.6147 - regression_loss: 1.9364 - classification_loss 7122/10000 [====================>.........] - ETA: 22:51 - loss: 2.6145 - regression_loss: 1.9362 - classification_loss 7123/10000 [====================>.........] - ETA: 22:51 - loss: 2.6144 - regression_loss: 1.9361 - classification_loss 7124/10000 [====================>.........] - ETA: 22:50 - loss: 2.6143 - regression_loss: 1.9360 - classification_loss 7125/10000 [====================>.........] - ETA: 22:50 - loss: 2.6145 - regression_loss: 1.9362 - classification_loss 7126/10000 [====================>.........] - ETA: 22:49 - loss: 2.6143 - regression_loss: 1.9360 - classification_loss 7127/10000 [====================>.........] - ETA: 22:49 - loss: 2.6143 - regression_loss: 1.9360 - classification_loss 7128/10000 [====================>.........] - ETA: 22:48 - loss: 2.6142 - regression_loss: 1.9359 - classification_loss 7129/10000 [====================>.........] - ETA: 22:48 - loss: 2.6141 - regression_loss: 1.9359 - classification_loss 7130/10000 [====================>.........] - ETA: 22:47 - loss: 2.6142 - regression_loss: 1.9360 - classification_loss 7131/10000 [====================>.........] - ETA: 22:47 - loss: 2.6140 - regression_loss: 1.9359 - classification_loss 7132/10000 [====================>.........] - ETA: 22:46 - loss: 2.6140 - regression_loss: 1.9359 - classification_loss 7133/10000 [====================>.........] - ETA: 22:46 - loss: 2.6140 - regression_loss: 1.9359 - classification_loss 7134/10000 [====================>.........] - ETA: 22:45 - loss: 2.6140 - regression_loss: 1.9359 - classification_loss 7135/10000 [====================>.........] - ETA: 22:45 - loss: 2.6139 - regression_loss: 1.9358 - classification_loss 7136/10000 [====================>.........] - ETA: 22:45 - loss: 2.6137 - regression_loss: 1.9357 - classification_loss 7137/10000 [====================>.........] - ETA: 22:44 - loss: 2.6138 - regression_loss: 1.9357 - classification_loss 7138/10000 [====================>.........] - ETA: 22:44 - loss: 2.6137 - regression_loss: 1.9356 - classification_loss 7139/10000 [====================>.........] - ETA: 22:43 - loss: 2.6135 - regression_loss: 1.9354 - classification_loss 7140/10000 [====================>.........] - ETA: 22:43 - loss: 2.6133 - regression_loss: 1.9353 - classification_loss 7141/10000 [====================>.........] - ETA: 22:42 - loss: 2.6132 - regression_loss: 1.9351 - classification_loss 7142/10000 [====================>.........] - ETA: 22:42 - loss: 2.6132 - regression_loss: 1.9352 - classification_loss 7143/10000 [====================>.........] - ETA: 22:41 - loss: 2.6133 - regression_loss: 1.9352 - classification_loss 7144/10000 [====================>.........] - ETA: 22:41 - loss: 2.6131 - regression_loss: 1.9351 - classification_loss 7145/10000 [====================>.........] - ETA: 22:40 - loss: 2.6131 - regression_loss: 1.9350 - classification_loss 7146/10000 [====================>.........] - ETA: 22:40 - loss: 2.6129 - regression_loss: 1.9349 - classification_loss 7147/10000 [====================>.........] - ETA: 22:39 - loss: 2.6129 - regression_loss: 1.9349 - classification_loss 7148/10000 [====================>.........] - ETA: 22:39 - loss: 2.6129 - regression_loss: 1.9349 - classification_loss 7149/10000 [====================>.........] - ETA: 22:38 - loss: 2.6129 - regression_loss: 1.9350 - classification_loss 7150/10000 [====================>.........] - ETA: 22:38 - loss: 2.6128 - regression_loss: 1.9349 - classification_loss 7151/10000 [====================>.........] - ETA: 22:37 - loss: 2.6126 - regression_loss: 1.9348 - classification_loss 7152/10000 [====================>.........] - ETA: 22:37 - loss: 2.6126 - regression_loss: 1.9347 - classification_loss 7153/10000 [====================>.........] - ETA: 22:36 - loss: 2.6125 - regression_loss: 1.9347 - classification_loss 7154/10000 [====================>.........] - ETA: 22:36 - loss: 2.6123 - regression_loss: 1.9346 - classification_loss 7155/10000 [====================>.........] - ETA: 22:35 - loss: 2.6121 - regression_loss: 1.9345 - classification_loss 7156/10000 [====================>.........] - ETA: 22:35 - loss: 2.6121 - regression_loss: 1.9344 - classification_loss 7157/10000 [====================>.........] - ETA: 22:34 - loss: 2.6119 - regression_loss: 1.9343 - classification_loss 7158/10000 [====================>.........] - ETA: 22:34 - loss: 2.6118 - regression_loss: 1.9342 - classification_loss 7159/10000 [====================>.........] - ETA: 22:33 - loss: 2.6116 - regression_loss: 1.9340 - classification_loss 7160/10000 [====================>.........] - ETA: 22:33 - loss: 2.6116 - regression_loss: 1.9340 - classification_loss 7161/10000 [====================>.........] - ETA: 22:32 - loss: 2.6114 - regression_loss: 1.9339 - classification_loss 7162/10000 [====================>.........] - ETA: 22:32 - loss: 2.6113 - regression_loss: 1.9338 - classification_loss 7163/10000 [====================>.........] - ETA: 22:31 - loss: 2.6112 - regression_loss: 1.9338 - classification_loss 7164/10000 [====================>.........] - ETA: 22:31 - loss: 2.6112 - regression_loss: 1.9338 - classification_loss 7165/10000 [====================>.........] - ETA: 22:31 - loss: 2.6112 - regression_loss: 1.9338 - classification_loss 7166/10000 [====================>.........] - ETA: 22:30 - loss: 2.6110 - regression_loss: 1.9337 - classification_loss 7167/10000 [====================>.........] - ETA: 22:30 - loss: 2.6109 - regression_loss: 1.9336 - classification_loss 7168/10000 [====================>.........] - ETA: 22:29 - loss: 2.6108 - regression_loss: 1.9335 - classification_loss 7169/10000 [====================>.........] - ETA: 22:29 - loss: 2.6108 - regression_loss: 1.9335 - classification_loss 7170/10000 [====================>.........] - ETA: 22:28 - loss: 2.6107 - regression_loss: 1.9335 - classification_loss 7171/10000 [====================>.........] - ETA: 22:28 - loss: 2.6106 - regression_loss: 1.9334 - classification_loss 7172/10000 [====================>.........] - ETA: 22:27 - loss: 2.6104 - regression_loss: 1.9333 - classification_loss 7173/10000 [====================>.........] - ETA: 22:27 - loss: 2.6104 - regression_loss: 1.9332 - classification_loss 7174/10000 [====================>.........] - ETA: 22:26 - loss: 2.6102 - regression_loss: 1.9331 - classification_loss 7175/10000 [====================>.........] - ETA: 22:26 - loss: 2.6102 - regression_loss: 1.9331 - classification_loss 7176/10000 [====================>.........] - ETA: 22:25 - loss: 2.6102 - regression_loss: 1.9331 - classification_loss 7177/10000 [====================>.........] - ETA: 22:25 - loss: 2.6100 - regression_loss: 1.9330 - classification_loss 7178/10000 [====================>.........] - ETA: 22:24 - loss: 2.6101 - regression_loss: 1.9330 - classification_loss 7179/10000 [====================>.........] - ETA: 22:24 - loss: 2.6101 - regression_loss: 1.9330 - classification_loss 7180/10000 [====================>.........] - ETA: 22:23 - loss: 2.6101 - regression_loss: 1.9331 - classification_loss 7181/10000 [====================>.........] - ETA: 22:23 - loss: 2.6101 - regression_loss: 1.9331 - classification_loss 7182/10000 [====================>.........] - ETA: 22:22 - loss: 2.6100 - regression_loss: 1.9330 - classification_loss 7183/10000 [====================>.........] - ETA: 22:22 - loss: 2.6099 - regression_loss: 1.9330 - classification_loss 7184/10000 [====================>.........] - ETA: 22:21 - loss: 2.6097 - regression_loss: 1.9328 - classification_loss 7185/10000 [====================>.........] - ETA: 22:21 - loss: 2.6095 - regression_loss: 1.9327 - classification_loss 7186/10000 [====================>.........] - ETA: 22:20 - loss: 2.6095 - regression_loss: 1.9327 - classification_loss 7187/10000 [====================>.........] - ETA: 22:20 - loss: 2.6096 - regression_loss: 1.9327 - classification_loss 7188/10000 [====================>.........] - ETA: 22:19 - loss: 2.6095 - regression_loss: 1.9327 - classification_loss 7189/10000 [====================>.........] - ETA: 22:19 - loss: 2.6095 - regression_loss: 1.9326 - classification_loss 7190/10000 [====================>.........] - ETA: 22:18 - loss: 2.6094 - regression_loss: 1.9326 - classification_loss 7191/10000 [====================>.........] - ETA: 22:18 - loss: 2.6094 - regression_loss: 1.9327 - classification_loss 7192/10000 [====================>.........] - ETA: 22:18 - loss: 2.6095 - regression_loss: 1.9327 - classification_loss 7193/10000 [====================>.........] - ETA: 22:17 - loss: 2.6096 - regression_loss: 1.9328 - classification_loss 7194/10000 [====================>.........] - ETA: 22:17 - loss: 2.6095 - regression_loss: 1.9328 - classification_loss 7195/10000 [====================>.........] - ETA: 22:16 - loss: 2.6094 - regression_loss: 1.9328 - classification_loss 7196/10000 [====================>.........] - ETA: 22:16 - loss: 2.6095 - regression_loss: 1.9328 - classification_loss 7197/10000 [====================>.........] - ETA: 22:15 - loss: 2.6094 - regression_loss: 1.9327 - classification_loss 7198/10000 [====================>.........] - ETA: 22:15 - loss: 2.6094 - regression_loss: 1.9328 - classification_loss 7199/10000 [====================>.........] - ETA: 22:14 - loss: 2.6092 - regression_loss: 1.9326 - classification_loss 7200/10000 [====================>.........] - ETA: 22:14 - loss: 2.6091 - regression_loss: 1.9325 - classification_loss 7201/10000 [====================>.........] - ETA: 22:13 - loss: 2.6090 - regression_loss: 1.9325 - classification_loss 7202/10000 [====================>.........] - ETA: 22:13 - loss: 2.6089 - regression_loss: 1.9324 - classification_loss 7203/10000 [====================>.........] - ETA: 22:12 - loss: 2.6089 - regression_loss: 1.9324 - classification_loss 7204/10000 [====================>.........] - ETA: 22:12 - loss: 2.6088 - regression_loss: 1.9324 - classification_loss 7205/10000 [====================>.........] - ETA: 22:11 - loss: 2.6089 - regression_loss: 1.9324 - classification_loss 7206/10000 [====================>.........] - ETA: 22:11 - loss: 2.6089 - regression_loss: 1.9324 - classification_loss 7207/10000 [====================>.........] - ETA: 22:10 - loss: 2.6087 - regression_loss: 1.9323 - classification_loss 7208/10000 [====================>.........] - ETA: 22:10 - loss: 2.6086 - regression_loss: 1.9322 - classification_loss 7209/10000 [====================>.........] - ETA: 22:09 - loss: 2.6085 - regression_loss: 1.9321 - classification_loss 7210/10000 [====================>.........] - ETA: 22:09 - loss: 2.6084 - regression_loss: 1.9320 - classification_loss 7211/10000 [====================>.........] - ETA: 22:08 - loss: 2.6083 - regression_loss: 1.9320 - classification_loss 7212/10000 [====================>.........] - ETA: 22:08 - loss: 2.6083 - regression_loss: 1.9319 - classification_loss 7213/10000 [====================>.........] - ETA: 22:07 - loss: 2.6082 - regression_loss: 1.9319 - classification_loss 7214/10000 [====================>.........] - ETA: 22:07 - loss: 2.6081 - regression_loss: 1.9318 - classification_loss 7215/10000 [====================>.........] - ETA: 22:06 - loss: 2.6081 - regression_loss: 1.9318 - classification_loss 7216/10000 [====================>.........] - ETA: 22:06 - loss: 2.6080 - regression_loss: 1.9317 - classification_loss 7217/10000 [====================>.........] - ETA: 22:05 - loss: 2.6080 - regression_loss: 1.9317 - classification_loss 7218/10000 [====================>.........] - ETA: 22:05 - loss: 2.6079 - regression_loss: 1.9316 - classification_loss 7219/10000 [====================>.........] - ETA: 22:04 - loss: 2.6077 - regression_loss: 1.9315 - classification_loss 7220/10000 [====================>.........] - ETA: 22:04 - loss: 2.6076 - regression_loss: 1.9314 - classification_loss 7221/10000 [====================>.........] - ETA: 22:04 - loss: 2.6076 - regression_loss: 1.9314 - classification_loss 7222/10000 [====================>.........] - ETA: 22:03 - loss: 2.6073 - regression_loss: 1.9312 - classification_loss 7223/10000 [====================>.........] - ETA: 22:03 - loss: 2.6072 - regression_loss: 1.9312 - classification_loss 7224/10000 [====================>.........] - ETA: 22:02 - loss: 2.6072 - regression_loss: 1.9312 - classification_loss 7225/10000 [====================>.........] - ETA: 22:02 - loss: 2.6073 - regression_loss: 1.9313 - classification_loss 7226/10000 [====================>.........] - ETA: 22:01 - loss: 2.6072 - regression_loss: 1.9312 - classification_loss 7227/10000 [====================>.........] - ETA: 22:01 - loss: 2.6071 - regression_loss: 1.9312 - classification_loss 7228/10000 [====================>.........] - ETA: 22:00 - loss: 2.6071 - regression_loss: 1.9311 - classification_loss 7229/10000 [====================>.........] - ETA: 22:00 - loss: 2.6069 - regression_loss: 1.9310 - classification_loss 7230/10000 [====================>.........] - ETA: 21:59 - loss: 2.6069 - regression_loss: 1.9310 - classification_loss 7231/10000 [====================>.........] - ETA: 21:59 - loss: 2.6068 - regression_loss: 1.9310 - classification_loss 7232/10000 [====================>.........] - ETA: 21:58 - loss: 2.6068 - regression_loss: 1.9309 - classification_loss 7233/10000 [====================>.........] - ETA: 21:58 - loss: 2.6067 - regression_loss: 1.9309 - classification_loss 7234/10000 [====================>.........] - ETA: 21:57 - loss: 2.6067 - regression_loss: 1.9309 - classification_loss 7235/10000 [====================>.........] - ETA: 21:57 - loss: 2.6066 - regression_loss: 1.9308 - classification_loss 7236/10000 [====================>.........] - ETA: 21:56 - loss: 2.6065 - regression_loss: 1.9308 - classification_loss 7237/10000 [====================>.........] - ETA: 21:56 - loss: 2.6063 - regression_loss: 1.9307 - classification_loss 7238/10000 [====================>.........] - ETA: 21:55 - loss: 2.6061 - regression_loss: 1.9305 - classification_loss 7239/10000 [====================>.........] - ETA: 21:55 - loss: 2.6061 - regression_loss: 1.9305 - classification_loss 7240/10000 [====================>.........] - ETA: 21:54 - loss: 2.6061 - regression_loss: 1.9305 - classification_loss 7241/10000 [====================>.........] - ETA: 21:54 - loss: 2.6062 - regression_loss: 1.9305 - classification_loss 7242/10000 [====================>.........] - ETA: 21:53 - loss: 2.6061 - regression_loss: 1.9305 - classification_loss 7243/10000 [====================>.........] - ETA: 21:53 - loss: 2.6060 - regression_loss: 1.9305 - classification_loss 7244/10000 [====================>.........] - ETA: 21:52 - loss: 2.6059 - regression_loss: 1.9304 - classification_loss 7245/10000 [====================>.........] - ETA: 21:52 - loss: 2.6059 - regression_loss: 1.9303 - classification_loss 7246/10000 [====================>.........] - ETA: 21:51 - loss: 2.6057 - regression_loss: 1.9302 - classification_loss 7247/10000 [====================>.........] - ETA: 21:51 - loss: 2.6057 - regression_loss: 1.9302 - classification_loss 7248/10000 [====================>.........] - ETA: 21:50 - loss: 2.6056 - regression_loss: 1.9302 - classification_loss 7249/10000 [====================>.........] - ETA: 21:50 - loss: 2.6054 - regression_loss: 1.9301 - classification_loss 7250/10000 [====================>.........] - ETA: 21:49 - loss: 2.6052 - regression_loss: 1.9299 - classification_loss 7251/10000 [====================>.........] - ETA: 21:49 - loss: 2.6051 - regression_loss: 1.9297 - classification_loss 7252/10000 [====================>.........] - ETA: 21:49 - loss: 2.6050 - regression_loss: 1.9297 - classification_loss 7253/10000 [====================>.........] - ETA: 21:48 - loss: 2.6049 - regression_loss: 1.9296 - classification_loss 7254/10000 [====================>.........] - ETA: 21:48 - loss: 2.6050 - regression_loss: 1.9296 - classification_loss 7255/10000 [====================>.........] - ETA: 21:47 - loss: 2.6047 - regression_loss: 1.9294 - classification_loss 7256/10000 [====================>.........] - ETA: 21:47 - loss: 2.6046 - regression_loss: 1.9294 - classification_loss 7257/10000 [====================>.........] - ETA: 21:46 - loss: 2.6046 - regression_loss: 1.9294 - classification_loss 7258/10000 [====================>.........] - ETA: 21:46 - loss: 2.6045 - regression_loss: 1.9293 - classification_loss 7259/10000 [====================>.........] - ETA: 21:45 - loss: 2.6044 - regression_loss: 1.9292 - classification_loss 7260/10000 [====================>.........] - ETA: 21:45 - loss: 2.6041 - regression_loss: 1.9290 - classification_loss 7261/10000 [====================>.........] - ETA: 21:44 - loss: 2.6040 - regression_loss: 1.9289 - classification_loss 7262/10000 [====================>.........] - ETA: 21:44 - loss: 2.6039 - regression_loss: 1.9288 - classification_loss 7263/10000 [====================>.........] - ETA: 21:43 - loss: 2.6038 - regression_loss: 1.9287 - classification_loss 7264/10000 [====================>.........] - ETA: 21:43 - loss: 2.6037 - regression_loss: 1.9286 - classification_loss 7265/10000 [====================>.........] - ETA: 21:42 - loss: 2.6036 - regression_loss: 1.9286 - classification_loss 7266/10000 [====================>.........] - ETA: 21:42 - loss: 2.6036 - regression_loss: 1.9286 - classification_loss 7267/10000 [====================>.........] - ETA: 21:41 - loss: 2.6035 - regression_loss: 1.9286 - classification_loss 7268/10000 [====================>.........] - ETA: 21:41 - loss: 2.6035 - regression_loss: 1.9286 - classification_loss 7269/10000 [====================>.........] - ETA: 21:40 - loss: 2.6035 - regression_loss: 1.9286 - classification_loss 7270/10000 [====================>.........] - ETA: 21:40 - loss: 2.6035 - regression_loss: 1.9286 - classification_loss 7271/10000 [====================>.........] - ETA: 21:39 - loss: 2.6033 - regression_loss: 1.9284 - classification_loss 7272/10000 [====================>.........] - ETA: 21:39 - loss: 2.6032 - regression_loss: 1.9284 - classification_loss 7273/10000 [====================>.........] - ETA: 21:38 - loss: 2.6032 - regression_loss: 1.9284 - classification_loss 7274/10000 [====================>.........] - ETA: 21:38 - loss: 2.6031 - regression_loss: 1.9283 - classification_loss 7275/10000 [====================>.........] - ETA: 21:37 - loss: 2.6031 - regression_loss: 1.9284 - classification_loss 7276/10000 [====================>.........] - ETA: 21:37 - loss: 2.6031 - regression_loss: 1.9284 - classification_loss 7277/10000 [====================>.........] - ETA: 21:36 - loss: 2.6030 - regression_loss: 1.9282 - classification_loss 7278/10000 [====================>.........] - ETA: 21:36 - loss: 2.6028 - regression_loss: 1.9281 - classification_loss 7279/10000 [====================>.........] - ETA: 21:36 - loss: 2.6030 - regression_loss: 1.9282 - classification_loss 7280/10000 [====================>.........] - ETA: 21:35 - loss: 2.6029 - regression_loss: 1.9282 - classification_loss 7281/10000 [====================>.........] - ETA: 21:35 - loss: 2.6027 - regression_loss: 1.9281 - classification_loss 7282/10000 [====================>.........] - ETA: 21:34 - loss: 2.6026 - regression_loss: 1.9279 - classification_loss 7283/10000 [====================>.........] - ETA: 21:34 - loss: 2.6025 - regression_loss: 1.9279 - classification_loss 7284/10000 [====================>.........] - ETA: 21:33 - loss: 2.6023 - regression_loss: 1.9278 - classification_loss 7285/10000 [====================>.........] - ETA: 21:33 - loss: 2.6022 - regression_loss: 1.9277 - classification_loss 7286/10000 [====================>.........] - ETA: 21:32 - loss: 2.6022 - regression_loss: 1.9277 - classification_loss 7287/10000 [====================>.........] - ETA: 21:32 - loss: 2.6022 - regression_loss: 1.9277 - classification_loss 7288/10000 [====================>.........] - ETA: 21:31 - loss: 2.6021 - regression_loss: 1.9276 - classification_loss 7289/10000 [====================>.........] - ETA: 21:31 - loss: 2.6020 - regression_loss: 1.9275 - classification_loss 7290/10000 [====================>.........] - ETA: 21:30 - loss: 2.6019 - regression_loss: 1.9274 - classification_loss 7291/10000 [====================>.........] - ETA: 21:30 - loss: 2.6019 - regression_loss: 1.9274 - classification_loss 7292/10000 [====================>.........] - ETA: 21:29 - loss: 2.6020 - regression_loss: 1.9276 - classification_loss 7293/10000 [====================>.........] - ETA: 21:29 - loss: 2.6020 - regression_loss: 1.9276 - classification_loss 7294/10000 [====================>.........] - ETA: 21:28 - loss: 2.6018 - regression_loss: 1.9273 - classification_loss 7295/10000 [====================>.........] - ETA: 21:28 - loss: 2.6017 - regression_loss: 1.9273 - classification_loss 7296/10000 [====================>.........] - ETA: 21:27 - loss: 2.6016 - regression_loss: 1.9272 - classification_loss 7297/10000 [====================>.........] - ETA: 21:27 - loss: 2.6015 - regression_loss: 1.9272 - classification_loss 7298/10000 [====================>.........] - ETA: 21:26 - loss: 2.6014 - regression_loss: 1.9271 - classification_loss 7299/10000 [====================>.........] - ETA: 21:26 - loss: 2.6014 - regression_loss: 1.9270 - classification_loss 7300/10000 [====================>.........] - ETA: 21:25 - loss: 2.6013 - regression_loss: 1.9270 - classification_loss 7301/10000 [====================>.........] - ETA: 21:25 - loss: 2.6013 - regression_loss: 1.9269 - classification_loss 7302/10000 [====================>.........] - ETA: 21:24 - loss: 2.6012 - regression_loss: 1.9270 - classification_loss 7303/10000 [====================>.........] - ETA: 21:24 - loss: 2.6012 - regression_loss: 1.9270 - classification_loss 7304/10000 [====================>.........] - ETA: 21:23 - loss: 2.6012 - regression_loss: 1.9270 - classification_loss 7305/10000 [====================>.........] - ETA: 21:23 - loss: 2.6011 - regression_loss: 1.9270 - classification_loss 7306/10000 [====================>.........] - ETA: 21:23 - loss: 2.6010 - regression_loss: 1.9268 - classification_loss 7307/10000 [====================>.........] - ETA: 21:22 - loss: 2.6010 - regression_loss: 1.9269 - classification_loss 7308/10000 [====================>.........] - ETA: 21:22 - loss: 2.6010 - regression_loss: 1.9269 - classification_loss 7309/10000 [====================>.........] - ETA: 21:21 - loss: 2.6010 - regression_loss: 1.9269 - classification_loss 7310/10000 [====================>.........] - ETA: 21:21 - loss: 2.6010 - regression_loss: 1.9269 - classification_loss 7311/10000 [====================>.........] - ETA: 21:20 - loss: 2.6009 - regression_loss: 1.9269 - classification_loss 7312/10000 [====================>.........] - ETA: 21:20 - loss: 2.6008 - regression_loss: 1.9268 - classification_loss 7313/10000 [====================>.........] - ETA: 21:19 - loss: 2.6008 - regression_loss: 1.9267 - classification_loss 7314/10000 [====================>.........] - ETA: 21:19 - loss: 2.6006 - regression_loss: 1.9266 - classification_loss 7315/10000 [====================>.........] - ETA: 21:18 - loss: 2.6005 - regression_loss: 1.9266 - classification_loss 7316/10000 [====================>.........] - ETA: 21:18 - loss: 2.6005 - regression_loss: 1.9265 - classification_loss 7317/10000 [====================>.........] - ETA: 21:17 - loss: 2.6004 - regression_loss: 1.9265 - classification_loss 7318/10000 [====================>.........] - ETA: 21:17 - loss: 2.6003 - regression_loss: 1.9264 - classification_loss 7319/10000 [====================>.........] - ETA: 21:16 - loss: 2.6003 - regression_loss: 1.9264 - classification_loss 7320/10000 [====================>.........] - ETA: 21:16 - loss: 2.6001 - regression_loss: 1.9263 - classification_loss 7321/10000 [====================>.........] - ETA: 21:15 - loss: 2.6002 - regression_loss: 1.9263 - classification_loss 7322/10000 [====================>.........] - ETA: 21:15 - loss: 2.6001 - regression_loss: 1.9262 - classification_loss 7323/10000 [====================>.........] - ETA: 21:14 - loss: 2.5999 - regression_loss: 1.9262 - classification_loss 7324/10000 [====================>.........] - ETA: 21:14 - loss: 2.5998 - regression_loss: 1.9261 - classification_loss 7325/10000 [====================>.........] - ETA: 21:13 - loss: 2.5997 - regression_loss: 1.9259 - classification_loss 7326/10000 [====================>.........] - ETA: 21:13 - loss: 2.5995 - regression_loss: 1.9258 - classification_loss 7327/10000 [====================>.........] - ETA: 21:12 - loss: 2.5995 - regression_loss: 1.9258 - classification_loss 7328/10000 [====================>.........] - ETA: 21:12 - loss: 2.5996 - regression_loss: 1.9258 - classification_loss 7329/10000 [====================>.........] - ETA: 21:11 - loss: 2.5995 - regression_loss: 1.9258 - classification_loss 7330/10000 [====================>.........] - ETA: 21:11 - loss: 2.5995 - regression_loss: 1.9258 - classification_loss 7331/10000 [====================>.........] - ETA: 21:10 - loss: 2.5993 - regression_loss: 1.9256 - classification_loss 7332/10000 [====================>.........] - ETA: 21:10 - loss: 2.5992 - regression_loss: 1.9256 - classification_loss 7333/10000 [====================>.........] - ETA: 21:10 - loss: 2.5992 - regression_loss: 1.9256 - classification_loss 7334/10000 [=====================>........] - ETA: 21:09 - loss: 2.5991 - regression_loss: 1.9255 - classification_loss 7335/10000 [=====================>........] - ETA: 21:09 - loss: 2.5990 - regression_loss: 1.9253 - classification_loss 7336/10000 [=====================>........] - ETA: 21:08 - loss: 2.5988 - regression_loss: 1.9252 - classification_loss 7337/10000 [=====================>........] - ETA: 21:08 - loss: 2.5987 - regression_loss: 1.9251 - classification_loss 7338/10000 [=====================>........] - ETA: 21:07 - loss: 2.5985 - regression_loss: 1.9250 - classification_loss 7339/10000 [=====================>........] - ETA: 21:07 - loss: 2.5985 - regression_loss: 1.9250 - classification_loss 7340/10000 [=====================>........] - ETA: 21:06 - loss: 2.5985 - regression_loss: 1.9251 - classification_loss 7341/10000 [=====================>........] - ETA: 21:06 - loss: 2.5985 - regression_loss: 1.9251 - classification_loss 7342/10000 [=====================>........] - ETA: 21:05 - loss: 2.5985 - regression_loss: 1.9251 - classification_loss 7343/10000 [=====================>........] - ETA: 21:05 - loss: 2.5985 - regression_loss: 1.9251 - classification_loss 7344/10000 [=====================>........] - ETA: 21:04 - loss: 2.5984 - regression_loss: 1.9250 - classification_loss 7345/10000 [=====================>........] - ETA: 21:04 - loss: 2.5983 - regression_loss: 1.9250 - classification_loss 7346/10000 [=====================>........] - ETA: 21:03 - loss: 2.5982 - regression_loss: 1.9249 - classification_loss 7347/10000 [=====================>........] - ETA: 21:03 - loss: 2.5982 - regression_loss: 1.9249 - classification_loss 7348/10000 [=====================>........] - ETA: 21:02 - loss: 2.5982 - regression_loss: 1.9249 - classification_loss 7349/10000 [=====================>........] - ETA: 21:02 - loss: 2.5981 - regression_loss: 1.9249 - classification_loss 7350/10000 [=====================>........] - ETA: 21:01 - loss: 2.5980 - regression_loss: 1.9248 - classification_loss 7351/10000 [=====================>........] - ETA: 21:01 - loss: 2.5980 - regression_loss: 1.9248 - classification_loss 7352/10000 [=====================>........] - ETA: 21:00 - loss: 2.5978 - regression_loss: 1.9247 - classification_loss 7353/10000 [=====================>........] - ETA: 21:00 - loss: 2.5978 - regression_loss: 1.9247 - classification_loss 7354/10000 [=====================>........] - ETA: 20:59 - loss: 2.5977 - regression_loss: 1.9246 - classification_loss 7355/10000 [=====================>........] - ETA: 20:59 - loss: 2.5976 - regression_loss: 1.9246 - classification_loss 7356/10000 [=====================>........] - ETA: 20:59 - loss: 2.5977 - regression_loss: 1.9246 - classification_loss 7357/10000 [=====================>........] - ETA: 20:58 - loss: 2.5976 - regression_loss: 1.9246 - classification_loss 7358/10000 [=====================>........] - ETA: 20:58 - loss: 2.5975 - regression_loss: 1.9245 - classification_loss 7359/10000 [=====================>........] - ETA: 20:57 - loss: 2.5976 - regression_loss: 1.9245 - classification_loss 7360/10000 [=====================>........] - ETA: 20:57 - loss: 2.5975 - regression_loss: 1.9245 - classification_loss 7361/10000 [=====================>........] - ETA: 20:56 - loss: 2.5977 - regression_loss: 1.9246 - classification_loss 7362/10000 [=====================>........] - ETA: 20:56 - loss: 2.5978 - regression_loss: 1.9247 - classification_loss 7363/10000 [=====================>........] - ETA: 20:55 - loss: 2.5977 - regression_loss: 1.9247 - classification_loss 7364/10000 [=====================>........] - ETA: 20:55 - loss: 2.5976 - regression_loss: 1.9246 - classification_loss 7365/10000 [=====================>........] - ETA: 20:54 - loss: 2.5975 - regression_loss: 1.9245 - classification_loss 7366/10000 [=====================>........] - ETA: 20:54 - loss: 2.5974 - regression_loss: 1.9244 - classification_loss 7367/10000 [=====================>........] - ETA: 20:53 - loss: 2.5973 - regression_loss: 1.9243 - classification_loss 7368/10000 [=====================>........] - ETA: 20:53 - loss: 2.5972 - regression_loss: 1.9242 - classification_loss 7369/10000 [=====================>........] - ETA: 20:52 - loss: 2.5971 - regression_loss: 1.9242 - classification_loss 7370/10000 [=====================>........] - ETA: 20:52 - loss: 2.5969 - regression_loss: 1.9240 - classification_loss 7371/10000 [=====================>........] - ETA: 20:51 - loss: 2.5969 - regression_loss: 1.9240 - classification_loss 7372/10000 [=====================>........] - ETA: 20:51 - loss: 2.5969 - regression_loss: 1.9240 - classification_loss 7373/10000 [=====================>........] - ETA: 20:50 - loss: 2.5969 - regression_loss: 1.9240 - classification_loss 7374/10000 [=====================>........] - ETA: 20:50 - loss: 2.5968 - regression_loss: 1.9239 - classification_loss 7375/10000 [=====================>........] - ETA: 20:49 - loss: 2.5968 - regression_loss: 1.9239 - classification_loss 7376/10000 [=====================>........] - ETA: 20:49 - loss: 2.5966 - regression_loss: 1.9238 - classification_loss 7377/10000 [=====================>........] - ETA: 20:48 - loss: 2.5966 - regression_loss: 1.9238 - classification_loss 7378/10000 [=====================>........] - ETA: 20:48 - loss: 2.5965 - regression_loss: 1.9237 - classification_loss 7379/10000 [=====================>........] - ETA: 20:47 - loss: 2.5964 - regression_loss: 1.9235 - classification_loss 7380/10000 [=====================>........] - ETA: 20:47 - loss: 2.5963 - regression_loss: 1.9235 - classification_loss 7381/10000 [=====================>........] - ETA: 20:46 - loss: 2.5962 - regression_loss: 1.9235 - classification_loss 7382/10000 [=====================>........] - ETA: 20:46 - loss: 2.5961 - regression_loss: 1.9234 - classification_loss 7383/10000 [=====================>........] - ETA: 20:45 - loss: 2.5960 - regression_loss: 1.9232 - classification_loss 7384/10000 [=====================>........] - ETA: 20:45 - loss: 2.5960 - regression_loss: 1.9232 - classification_loss 7385/10000 [=====================>........] - ETA: 20:45 - loss: 2.5959 - regression_loss: 1.9232 - classification_loss 7386/10000 [=====================>........] - ETA: 20:44 - loss: 2.5959 - regression_loss: 1.9231 - classification_loss 7387/10000 [=====================>........] - ETA: 20:44 - loss: 2.5957 - regression_loss: 1.9230 - classification_loss 7388/10000 [=====================>........] - ETA: 20:43 - loss: 2.5957 - regression_loss: 1.9230 - classification_loss 7389/10000 [=====================>........] - ETA: 20:43 - loss: 2.5956 - regression_loss: 1.9230 - classification_loss 7390/10000 [=====================>........] - ETA: 20:42 - loss: 2.5954 - regression_loss: 1.9228 - classification_loss 7391/10000 [=====================>........] - ETA: 20:42 - loss: 2.5953 - regression_loss: 1.9227 - classification_loss 7392/10000 [=====================>........] - ETA: 20:41 - loss: 2.5953 - regression_loss: 1.9227 - classification_loss 7393/10000 [=====================>........] - ETA: 20:41 - loss: 2.5953 - regression_loss: 1.9227 - classification_loss 7394/10000 [=====================>........] - ETA: 20:40 - loss: 2.5953 - regression_loss: 1.9227 - classification_loss 7395/10000 [=====================>........] - ETA: 20:40 - loss: 2.5952 - regression_loss: 1.9227 - classification_loss 7396/10000 [=====================>........] - ETA: 20:39 - loss: 2.5951 - regression_loss: 1.9226 - classification_loss 7397/10000 [=====================>........] - ETA: 20:39 - loss: 2.5950 - regression_loss: 1.9225 - classification_loss 7398/10000 [=====================>........] - ETA: 20:38 - loss: 2.5951 - regression_loss: 1.9226 - classification_loss 7399/10000 [=====================>........] - ETA: 20:38 - loss: 2.5949 - regression_loss: 1.9225 - classification_loss 7400/10000 [=====================>........] - ETA: 20:37 - loss: 2.5947 - regression_loss: 1.9223 - classification_loss 7401/10000 [=====================>........] - ETA: 20:37 - loss: 2.5948 - regression_loss: 1.9223 - classification_loss 7402/10000 [=====================>........] - ETA: 20:36 - loss: 2.5948 - regression_loss: 1.9223 - classification_loss 7403/10000 [=====================>........] - ETA: 20:36 - loss: 2.5947 - regression_loss: 1.9223 - classification_loss 7404/10000 [=====================>........] - ETA: 20:35 - loss: 2.5947 - regression_loss: 1.9223 - classification_loss 7405/10000 [=====================>........] - ETA: 20:35 - loss: 2.5946 - regression_loss: 1.9222 - classification_loss 7406/10000 [=====================>........] - ETA: 20:34 - loss: 2.5945 - regression_loss: 1.9222 - classification_loss 7407/10000 [=====================>........] - ETA: 20:34 - loss: 2.5943 - regression_loss: 1.9220 - classification_loss 7408/10000 [=====================>........] - ETA: 20:33 - loss: 2.5943 - regression_loss: 1.9220 - classification_loss 7409/10000 [=====================>........] - ETA: 20:33 - loss: 2.5943 - regression_loss: 1.9220 - classification_loss 7410/10000 [=====================>........] - ETA: 20:32 - loss: 2.5942 - regression_loss: 1.9220 - classification_loss 7411/10000 [=====================>........] - ETA: 20:32 - loss: 2.5942 - regression_loss: 1.9220 - classification_loss 7412/10000 [=====================>........] - ETA: 20:32 - loss: 2.5940 - regression_loss: 1.9218 - classification_loss 7413/10000 [=====================>........] - ETA: 20:31 - loss: 2.5940 - regression_loss: 1.9219 - classification_loss 7414/10000 [=====================>........] - ETA: 20:31 - loss: 2.5940 - regression_loss: 1.9219 - classification_loss 7415/10000 [=====================>........] - ETA: 20:30 - loss: 2.5940 - regression_loss: 1.9219 - classification_loss 7416/10000 [=====================>........] - ETA: 20:30 - loss: 2.5939 - regression_loss: 1.9218 - classification_loss 7417/10000 [=====================>........] - ETA: 20:29 - loss: 2.5938 - regression_loss: 1.9217 - classification_loss 7418/10000 [=====================>........] - ETA: 20:29 - loss: 2.5937 - regression_loss: 1.9217 - classification_loss 7419/10000 [=====================>........] - ETA: 20:28 - loss: 2.5936 - regression_loss: 1.9216 - classification_loss 7420/10000 [=====================>........] - ETA: 20:28 - loss: 2.5934 - regression_loss: 1.9215 - classification_loss 7421/10000 [=====================>........] - ETA: 20:27 - loss: 2.5934 - regression_loss: 1.9215 - classification_loss 7422/10000 [=====================>........] - ETA: 20:27 - loss: 2.5934 - regression_loss: 1.9215 - classification_loss 7423/10000 [=====================>........] - ETA: 20:26 - loss: 2.5933 - regression_loss: 1.9215 - classification_loss 7424/10000 [=====================>........] - ETA: 20:26 - loss: 2.5931 - regression_loss: 1.9213 - classification_loss 7425/10000 [=====================>........] - ETA: 20:25 - loss: 2.5930 - regression_loss: 1.9212 - classification_loss 7426/10000 [=====================>........] - ETA: 20:25 - loss: 2.5930 - regression_loss: 1.9212 - classification_loss 7427/10000 [=====================>........] - ETA: 20:24 - loss: 2.5930 - regression_loss: 1.9212 - classification_loss 7428/10000 [=====================>........] - ETA: 20:24 - loss: 2.5929 - regression_loss: 1.9211 - classification_loss 7429/10000 [=====================>........] - ETA: 20:23 - loss: 2.5929 - regression_loss: 1.9211 - classification_loss 7430/10000 [=====================>........] - ETA: 20:23 - loss: 2.5929 - regression_loss: 1.9211 - classification_loss 7431/10000 [=====================>........] - ETA: 20:22 - loss: 2.5928 - regression_loss: 1.9211 - classification_loss 7432/10000 [=====================>........] - ETA: 20:22 - loss: 2.5927 - regression_loss: 1.9209 - classification_loss 7433/10000 [=====================>........] - ETA: 20:21 - loss: 2.5926 - regression_loss: 1.9209 - classification_loss 7434/10000 [=====================>........] - ETA: 20:21 - loss: 2.5925 - regression_loss: 1.9208 - classification_loss 7435/10000 [=====================>........] - ETA: 20:20 - loss: 2.5924 - regression_loss: 1.9208 - classification_loss 7436/10000 [=====================>........] - ETA: 20:20 - loss: 2.5924 - regression_loss: 1.9207 - classification_loss 7437/10000 [=====================>........] - ETA: 20:20 - loss: 2.5923 - regression_loss: 1.9207 - classification_loss 7438/10000 [=====================>........] - ETA: 20:19 - loss: 2.5921 - regression_loss: 1.9205 - classification_loss 7439/10000 [=====================>........] - ETA: 20:19 - loss: 2.5920 - regression_loss: 1.9204 - classification_loss 7440/10000 [=====================>........] - ETA: 20:18 - loss: 2.5919 - regression_loss: 1.9203 - classification_loss 7441/10000 [=====================>........] - ETA: 20:18 - loss: 2.5920 - regression_loss: 1.9204 - classification_loss 7442/10000 [=====================>........] - ETA: 20:17 - loss: 2.5919 - regression_loss: 1.9204 - classification_loss 7443/10000 [=====================>........] - ETA: 20:17 - loss: 2.5919 - regression_loss: 1.9203 - classification_loss 7444/10000 [=====================>........] - ETA: 20:16 - loss: 2.5919 - regression_loss: 1.9203 - classification_loss 7445/10000 [=====================>........] - ETA: 20:16 - loss: 2.5920 - regression_loss: 1.9204 - classification_loss 7446/10000 [=====================>........] - ETA: 20:15 - loss: 2.5919 - regression_loss: 1.9203 - classification_loss 7447/10000 [=====================>........] - ETA: 20:15 - loss: 2.5918 - regression_loss: 1.9203 - classification_loss 7448/10000 [=====================>........] - ETA: 20:14 - loss: 2.5919 - regression_loss: 1.9203 - classification_loss 7449/10000 [=====================>........] - ETA: 20:14 - loss: 2.5918 - regression_loss: 1.9203 - classification_loss 7450/10000 [=====================>........] - ETA: 20:13 - loss: 2.5918 - regression_loss: 1.9203 - classification_loss 7451/10000 [=====================>........] - ETA: 20:13 - loss: 2.5917 - regression_loss: 1.9203 - classification_loss 7452/10000 [=====================>........] - ETA: 20:12 - loss: 2.5917 - regression_loss: 1.9203 - classification_loss 7453/10000 [=====================>........] - ETA: 20:12 - loss: 2.5917 - regression_loss: 1.9203 - classification_loss 7454/10000 [=====================>........] - ETA: 20:11 - loss: 2.5916 - regression_loss: 1.9202 - classification_loss 7455/10000 [=====================>........] - ETA: 20:11 - loss: 2.5916 - regression_loss: 1.9202 - classification_loss 7456/10000 [=====================>........] - ETA: 20:10 - loss: 2.5915 - regression_loss: 1.9202 - classification_loss 7457/10000 [=====================>........] - ETA: 20:10 - loss: 2.5914 - regression_loss: 1.9201 - classification_loss 7458/10000 [=====================>........] - ETA: 20:09 - loss: 2.5913 - regression_loss: 1.9200 - classification_loss 7459/10000 [=====================>........] - ETA: 20:09 - loss: 2.5912 - regression_loss: 1.9199 - classification_loss 7460/10000 [=====================>........] - ETA: 20:09 - loss: 2.5911 - regression_loss: 1.9199 - classification_loss 7461/10000 [=====================>........] - ETA: 20:08 - loss: 2.5909 - regression_loss: 1.9197 - classification_loss 7462/10000 [=====================>........] - ETA: 20:08 - loss: 2.5908 - regression_loss: 1.9196 - classification_loss 7463/10000 [=====================>........] - ETA: 20:07 - loss: 2.5907 - regression_loss: 1.9195 - classification_loss 7464/10000 [=====================>........] - ETA: 20:07 - loss: 2.5906 - regression_loss: 1.9195 - classification_loss 7465/10000 [=====================>........] - ETA: 20:06 - loss: 2.5905 - regression_loss: 1.9195 - classification_loss 7466/10000 [=====================>........] - ETA: 20:06 - loss: 2.5905 - regression_loss: 1.9194 - classification_loss 7467/10000 [=====================>........] - ETA: 20:05 - loss: 2.5905 - regression_loss: 1.9194 - classification_loss 7468/10000 [=====================>........] - ETA: 20:05 - loss: 2.5905 - regression_loss: 1.9194 - classification_loss 7469/10000 [=====================>........] - ETA: 20:04 - loss: 2.5905 - regression_loss: 1.9194 - classification_loss 7470/10000 [=====================>........] - ETA: 20:04 - loss: 2.5904 - regression_loss: 1.9193 - classification_loss 7471/10000 [=====================>........] - ETA: 20:03 - loss: 2.5903 - regression_loss: 1.9193 - classification_loss 7472/10000 [=====================>........] - ETA: 20:03 - loss: 2.5903 - regression_loss: 1.9193 - classification_loss 7473/10000 [=====================>........] - ETA: 20:02 - loss: 2.5902 - regression_loss: 1.9192 - classification_loss 7474/10000 [=====================>........] - ETA: 20:02 - loss: 2.5900 - regression_loss: 1.9191 - classification_loss 7475/10000 [=====================>........] - ETA: 20:01 - loss: 2.5898 - regression_loss: 1.9189 - classification_loss 7476/10000 [=====================>........] - ETA: 20:01 - loss: 2.5898 - regression_loss: 1.9189 - classification_loss 7477/10000 [=====================>........] - ETA: 20:00 - loss: 2.5897 - regression_loss: 1.9189 - classification_loss 7478/10000 [=====================>........] - ETA: 20:00 - loss: 2.5896 - regression_loss: 1.9188 - classification_loss 7479/10000 [=====================>........] - ETA: 19:59 - loss: 2.5895 - regression_loss: 1.9188 - classification_loss 7480/10000 [=====================>........] - ETA: 19:59 - loss: 2.5894 - regression_loss: 1.9187 - classification_loss 7481/10000 [=====================>........] - ETA: 19:58 - loss: 2.5895 - regression_loss: 1.9188 - classification_loss 7482/10000 [=====================>........] - ETA: 19:58 - loss: 2.5895 - regression_loss: 1.9188 - classification_loss 7483/10000 [=====================>........] - ETA: 19:57 - loss: 2.5893 - regression_loss: 1.9186 - classification_loss 7484/10000 [=====================>........] - ETA: 19:57 - loss: 2.5892 - regression_loss: 1.9185 - classification_loss 7485/10000 [=====================>........] - ETA: 19:56 - loss: 2.5892 - regression_loss: 1.9186 - classification_loss 7486/10000 [=====================>........] - ETA: 19:56 - loss: 2.5892 - regression_loss: 1.9186 - classification_loss 7487/10000 [=====================>........] - ETA: 19:56 - loss: 2.5891 - regression_loss: 1.9185 - classification_loss 7488/10000 [=====================>........] - ETA: 19:55 - loss: 2.5889 - regression_loss: 1.9183 - classification_loss 7489/10000 [=====================>........] - ETA: 19:55 - loss: 2.5889 - regression_loss: 1.9184 - classification_loss 7490/10000 [=====================>........] - ETA: 19:54 - loss: 2.5889 - regression_loss: 1.9183 - classification_loss 7491/10000 [=====================>........] - ETA: 19:54 - loss: 2.5888 - regression_loss: 1.9183 - classification_loss 7492/10000 [=====================>........] - ETA: 19:53 - loss: 2.5888 - regression_loss: 1.9183 - classification_loss 7493/10000 [=====================>........] - ETA: 19:53 - loss: 2.5886 - regression_loss: 1.9182 - classification_loss 7494/10000 [=====================>........] - ETA: 19:52 - loss: 2.5885 - regression_loss: 1.9181 - classification_loss 7495/10000 [=====================>........] - ETA: 19:52 - loss: 2.5884 - regression_loss: 1.9181 - classification_loss 7496/10000 [=====================>........] - ETA: 19:51 - loss: 2.5884 - regression_loss: 1.9181 - classification_loss 7497/10000 [=====================>........] - ETA: 19:51 - loss: 2.5883 - regression_loss: 1.9181 - classification_loss 7498/10000 [=====================>........] - ETA: 19:50 - loss: 2.5882 - regression_loss: 1.9180 - classification_loss 7499/10000 [=====================>........] - ETA: 19:50 - loss: 2.5881 - regression_loss: 1.9179 - classification_loss 7500/10000 [=====================>........] - ETA: 19:49 - loss: 2.5880 - regression_loss: 1.9178 - classification_loss 7501/10000 [=====================>........] - ETA: 19:49 - loss: 2.5879 - regression_loss: 1.9178 - classification_loss 7502/10000 [=====================>........] - ETA: 19:48 - loss: 2.5878 - regression_loss: 1.9177 - classification_loss 7503/10000 [=====================>........] - ETA: 19:48 - loss: 2.5879 - regression_loss: 1.9177 - classification_loss 7504/10000 [=====================>........] - ETA: 19:47 - loss: 2.5878 - regression_loss: 1.9176 - classification_loss 7505/10000 [=====================>........] - ETA: 19:47 - loss: 2.5878 - regression_loss: 1.9176 - classification_loss 7506/10000 [=====================>........] - ETA: 19:46 - loss: 2.5878 - regression_loss: 1.9176 - classification_loss 7507/10000 [=====================>........] - ETA: 19:46 - loss: 2.5878 - regression_loss: 1.9176 - classification_loss 7508/10000 [=====================>........] - ETA: 19:45 - loss: 2.5878 - regression_loss: 1.9176 - classification_loss 7509/10000 [=====================>........] - ETA: 19:45 - loss: 2.5876 - regression_loss: 1.9175 - classification_loss 7510/10000 [=====================>........] - ETA: 19:45 - loss: 2.5877 - regression_loss: 1.9175 - classification_loss 7511/10000 [=====================>........] - ETA: 19:44 - loss: 2.5875 - regression_loss: 1.9174 - classification_loss 7512/10000 [=====================>........] - ETA: 19:44 - loss: 2.5874 - regression_loss: 1.9173 - classification_loss 7513/10000 [=====================>........] - ETA: 19:43 - loss: 2.5873 - regression_loss: 1.9172 - classification_loss 7514/10000 [=====================>........] - ETA: 19:43 - loss: 2.5873 - regression_loss: 1.9172 - classification_loss 7515/10000 [=====================>........] - ETA: 19:42 - loss: 2.5872 - regression_loss: 1.9171 - classification_loss 7516/10000 [=====================>........] - ETA: 19:42 - loss: 2.5872 - regression_loss: 1.9171 - classification_loss 7517/10000 [=====================>........] - ETA: 19:41 - loss: 2.5872 - regression_loss: 1.9171 - classification_loss 7518/10000 [=====================>........] - ETA: 19:41 - loss: 2.5872 - regression_loss: 1.9171 - classification_loss 7519/10000 [=====================>........] - ETA: 19:40 - loss: 2.5871 - regression_loss: 1.9171 - classification_loss 7520/10000 [=====================>........] - ETA: 19:40 - loss: 2.5870 - regression_loss: 1.9171 - classification_loss 7521/10000 [=====================>........] - ETA: 19:39 - loss: 2.5871 - regression_loss: 1.9171 - classification_loss 7522/10000 [=====================>........] - ETA: 19:39 - loss: 2.5870 - regression_loss: 1.9171 - classification_loss 7523/10000 [=====================>........] - ETA: 19:38 - loss: 2.5869 - regression_loss: 1.9170 - classification_loss 7524/10000 [=====================>........] - ETA: 19:38 - loss: 2.5868 - regression_loss: 1.9169 - classification_loss 7525/10000 [=====================>........] - ETA: 19:37 - loss: 2.5867 - regression_loss: 1.9168 - classification_loss 7526/10000 [=====================>........] - ETA: 19:37 - loss: 2.5866 - regression_loss: 1.9167 - classification_loss 7527/10000 [=====================>........] - ETA: 19:36 - loss: 2.5864 - regression_loss: 1.9166 - classification_loss 7528/10000 [=====================>........] - ETA: 19:36 - loss: 2.5864 - regression_loss: 1.9166 - classification_loss 7529/10000 [=====================>........] - ETA: 19:35 - loss: 2.5863 - regression_loss: 1.9166 - classification_loss 7530/10000 [=====================>........] - ETA: 19:35 - loss: 2.5862 - regression_loss: 1.9164 - classification_loss 7531/10000 [=====================>........] - ETA: 19:34 - loss: 2.5861 - regression_loss: 1.9164 - classification_loss 7532/10000 [=====================>........] - ETA: 19:34 - loss: 2.5861 - regression_loss: 1.9164 - classification_loss 7533/10000 [=====================>........] - ETA: 19:33 - loss: 2.5860 - regression_loss: 1.9163 - classification_loss 7534/10000 [=====================>........] - ETA: 19:33 - loss: 2.5858 - regression_loss: 1.9162 - classification_loss 7535/10000 [=====================>........] - ETA: 19:32 - loss: 2.5858 - regression_loss: 1.9162 - classification_loss 7536/10000 [=====================>........] - ETA: 19:32 - loss: 2.5858 - regression_loss: 1.9161 - classification_loss 7537/10000 [=====================>........] - ETA: 19:31 - loss: 2.5857 - regression_loss: 1.9161 - classification_loss 7538/10000 [=====================>........] - ETA: 19:31 - loss: 2.5858 - regression_loss: 1.9161 - classification_loss 7539/10000 [=====================>........] - ETA: 19:31 - loss: 2.5858 - regression_loss: 1.9162 - classification_loss 7540/10000 [=====================>........] - ETA: 19:30 - loss: 2.5856 - regression_loss: 1.9160 - classification_loss 7541/10000 [=====================>........] - ETA: 19:30 - loss: 2.5856 - regression_loss: 1.9160 - classification_loss 7542/10000 [=====================>........] - ETA: 19:29 - loss: 2.5854 - regression_loss: 1.9159 - classification_loss 7543/10000 [=====================>........] - ETA: 19:29 - loss: 2.5853 - regression_loss: 1.9158 - classification_loss 7544/10000 [=====================>........] - ETA: 19:28 - loss: 2.5853 - regression_loss: 1.9157 - classification_loss 7545/10000 [=====================>........] - ETA: 19:28 - loss: 2.5852 - regression_loss: 1.9157 - classification_loss 7546/10000 [=====================>........] - ETA: 19:27 - loss: 2.5851 - regression_loss: 1.9157 - classification_loss 7547/10000 [=====================>........] - ETA: 19:27 - loss: 2.5852 - regression_loss: 1.9157 - classification_loss 7548/10000 [=====================>........] - ETA: 19:26 - loss: 2.5850 - regression_loss: 1.9156 - classification_loss 7549/10000 [=====================>........] - ETA: 19:26 - loss: 2.5850 - regression_loss: 1.9156 - classification_loss 7550/10000 [=====================>........] - ETA: 19:25 - loss: 2.5849 - regression_loss: 1.9155 - classification_loss 7551/10000 [=====================>........] - ETA: 19:25 - loss: 2.5849 - regression_loss: 1.9155 - classification_loss 7552/10000 [=====================>........] - ETA: 19:24 - loss: 2.5848 - regression_loss: 1.9155 - classification_loss 7553/10000 [=====================>........] - ETA: 19:24 - loss: 2.5847 - regression_loss: 1.9154 - classification_loss 7554/10000 [=====================>........] - ETA: 19:23 - loss: 2.5845 - regression_loss: 1.9153 - classification_loss 7555/10000 [=====================>........] - ETA: 19:23 - loss: 2.5845 - regression_loss: 1.9153 - classification_loss 7556/10000 [=====================>........] - ETA: 19:22 - loss: 2.5844 - regression_loss: 1.9151 - classification_loss 7557/10000 [=====================>........] - ETA: 19:22 - loss: 2.5844 - regression_loss: 1.9151 - classification_loss 7558/10000 [=====================>........] - ETA: 19:21 - loss: 2.5843 - regression_loss: 1.9151 - classification_loss 7559/10000 [=====================>........] - ETA: 19:21 - loss: 2.5843 - regression_loss: 1.9151 - classification_loss 7560/10000 [=====================>........] - ETA: 19:21 - loss: 2.5842 - regression_loss: 1.9151 - classification_loss 7561/10000 [=====================>........] - ETA: 19:20 - loss: 2.5842 - regression_loss: 1.9150 - classification_loss 7562/10000 [=====================>........] - ETA: 19:20 - loss: 2.5841 - regression_loss: 1.9150 - classification_loss 7563/10000 [=====================>........] - ETA: 19:19 - loss: 2.5841 - regression_loss: 1.9150 - classification_loss 7564/10000 [=====================>........] - ETA: 19:19 - loss: 2.5839 - regression_loss: 1.9149 - classification_loss 7565/10000 [=====================>........] - ETA: 19:18 - loss: 2.5839 - regression_loss: 1.9149 - classification_loss 7566/10000 [=====================>........] - ETA: 19:18 - loss: 2.5841 - regression_loss: 1.9150 - classification_loss 7567/10000 [=====================>........] - ETA: 19:17 - loss: 2.5840 - regression_loss: 1.9150 - classification_loss 7568/10000 [=====================>........] - ETA: 19:17 - loss: 2.5840 - regression_loss: 1.9150 - classification_loss 7569/10000 [=====================>........] - ETA: 19:16 - loss: 2.5839 - regression_loss: 1.9149 - classification_loss 7570/10000 [=====================>........] - ETA: 19:16 - loss: 2.5838 - regression_loss: 1.9148 - classification_loss 7571/10000 [=====================>........] - ETA: 19:15 - loss: 2.5837 - regression_loss: 1.9149 - classification_loss 7572/10000 [=====================>........] - ETA: 19:15 - loss: 2.5836 - regression_loss: 1.9147 - classification_loss 7573/10000 [=====================>........] - ETA: 19:14 - loss: 2.5835 - regression_loss: 1.9147 - classification_loss 7574/10000 [=====================>........] - ETA: 19:14 - loss: 2.5834 - regression_loss: 1.9145 - classification_loss 7575/10000 [=====================>........] - ETA: 19:13 - loss: 2.5832 - regression_loss: 1.9144 - classification_loss 7576/10000 [=====================>........] - ETA: 19:13 - loss: 2.5831 - regression_loss: 1.9144 - classification_loss 7577/10000 [=====================>........] - ETA: 19:12 - loss: 2.5831 - regression_loss: 1.9144 - classification_loss 7578/10000 [=====================>........] - ETA: 19:12 - loss: 2.5831 - regression_loss: 1.9144 - classification_loss 7579/10000 [=====================>........] - ETA: 19:11 - loss: 2.5830 - regression_loss: 1.9144 - classification_loss 7580/10000 [=====================>........] - ETA: 19:11 - loss: 2.5830 - regression_loss: 1.9143 - classification_loss 7581/10000 [=====================>........] - ETA: 19:10 - loss: 2.5828 - regression_loss: 1.9141 - classification_loss 7582/10000 [=====================>........] - ETA: 19:10 - loss: 2.5826 - regression_loss: 1.9141 - classification_loss 7583/10000 [=====================>........] - ETA: 19:09 - loss: 2.5826 - regression_loss: 1.9141 - classification_loss 7584/10000 [=====================>........] - ETA: 19:09 - loss: 2.5825 - regression_loss: 1.9140 - classification_loss 7585/10000 [=====================>........] - ETA: 19:08 - loss: 2.5825 - regression_loss: 1.9139 - classification_loss 7586/10000 [=====================>........] - ETA: 19:08 - loss: 2.5825 - regression_loss: 1.9139 - classification_loss 7587/10000 [=====================>........] - ETA: 19:07 - loss: 2.5824 - regression_loss: 1.9138 - classification_loss 7588/10000 [=====================>........] - ETA: 19:07 - loss: 2.5824 - regression_loss: 1.9138 - classification_loss 7589/10000 [=====================>........] - ETA: 19:07 - loss: 2.5822 - regression_loss: 1.9137 - classification_loss 7590/10000 [=====================>........] - ETA: 19:06 - loss: 2.5821 - regression_loss: 1.9137 - classification_loss 7591/10000 [=====================>........] - ETA: 19:06 - loss: 2.5822 - regression_loss: 1.9137 - classification_loss 7592/10000 [=====================>........] - ETA: 19:05 - loss: 2.5821 - regression_loss: 1.9137 - classification_loss 7593/10000 [=====================>........] - ETA: 19:05 - loss: 2.5821 - regression_loss: 1.9137 - classification_loss 7594/10000 [=====================>........] - ETA: 19:04 - loss: 2.5820 - regression_loss: 1.9136 - classification_loss 7595/10000 [=====================>........] - ETA: 19:04 - loss: 2.5818 - regression_loss: 1.9135 - classification_loss 7596/10000 [=====================>........] - ETA: 19:03 - loss: 2.5818 - regression_loss: 1.9134 - classification_loss 7597/10000 [=====================>........] - ETA: 19:03 - loss: 2.5817 - regression_loss: 1.9134 - classification_loss 7598/10000 [=====================>........] - ETA: 19:02 - loss: 2.5815 - regression_loss: 1.9132 - classification_loss 7599/10000 [=====================>........] - ETA: 19:02 - loss: 2.5815 - regression_loss: 1.9132 - classification_loss 7600/10000 [=====================>........] - ETA: 19:01 - loss: 2.5815 - regression_loss: 1.9132 - classification_loss 7601/10000 [=====================>........] - ETA: 19:01 - loss: 2.5813 - regression_loss: 1.9131 - classification_loss 7602/10000 [=====================>........] - ETA: 19:00 - loss: 2.5813 - regression_loss: 1.9131 - classification_loss 7603/10000 [=====================>........] - ETA: 19:00 - loss: 2.5812 - regression_loss: 1.9130 - classification_loss 7604/10000 [=====================>........] - ETA: 18:59 - loss: 2.5811 - regression_loss: 1.9130 - classification_loss 7605/10000 [=====================>........] - ETA: 18:59 - loss: 2.5810 - regression_loss: 1.9129 - classification_loss 7606/10000 [=====================>........] - ETA: 18:58 - loss: 2.5810 - regression_loss: 1.9129 - classification_loss 7607/10000 [=====================>........] - ETA: 18:58 - loss: 2.5810 - regression_loss: 1.9129 - classification_loss 7608/10000 [=====================>........] - ETA: 18:57 - loss: 2.5809 - regression_loss: 1.9129 - classification_loss 7609/10000 [=====================>........] - ETA: 18:57 - loss: 2.5809 - regression_loss: 1.9128 - classification_loss 7610/10000 [=====================>........] - ETA: 18:56 - loss: 2.5809 - regression_loss: 1.9128 - classification_loss 7611/10000 [=====================>........] - ETA: 18:56 - loss: 2.5808 - regression_loss: 1.9128 - classification_loss 7612/10000 [=====================>........] - ETA: 18:55 - loss: 2.5807 - regression_loss: 1.9126 - classification_loss 7613/10000 [=====================>........] - ETA: 18:55 - loss: 2.5806 - regression_loss: 1.9126 - classification_loss 7614/10000 [=====================>........] - ETA: 18:55 - loss: 2.5806 - regression_loss: 1.9126 - classification_loss 7615/10000 [=====================>........] - ETA: 18:54 - loss: 2.5805 - regression_loss: 1.9125 - classification_loss 7616/10000 [=====================>........] - ETA: 18:54 - loss: 2.5804 - regression_loss: 1.9124 - classification_loss 7617/10000 [=====================>........] - ETA: 18:53 - loss: 2.5803 - regression_loss: 1.9124 - classification_loss 7618/10000 [=====================>........] - ETA: 18:53 - loss: 2.5803 - regression_loss: 1.9124 - classification_loss 7619/10000 [=====================>........] - ETA: 18:52 - loss: 2.5803 - regression_loss: 1.9124 - classification_loss 7620/10000 [=====================>........] - ETA: 18:52 - loss: 2.5801 - regression_loss: 1.9123 - classification_loss 7621/10000 [=====================>........] - ETA: 18:51 - loss: 2.5800 - regression_loss: 1.9121 - classification_loss 7622/10000 [=====================>........] - ETA: 18:51 - loss: 2.5799 - regression_loss: 1.9119 - classification_loss 7623/10000 [=====================>........] - ETA: 18:50 - loss: 2.5799 - regression_loss: 1.9119 - classification_loss 7624/10000 [=====================>........] - ETA: 18:50 - loss: 2.5800 - regression_loss: 1.9120 - classification_loss 7625/10000 [=====================>........] - ETA: 18:49 - loss: 2.5799 - regression_loss: 1.9120 - classification_loss 7626/10000 [=====================>........] - ETA: 18:49 - loss: 2.5798 - regression_loss: 1.9119 - classification_loss 7627/10000 [=====================>........] - ETA: 18:48 - loss: 2.5798 - regression_loss: 1.9119 - classification_loss 7628/10000 [=====================>........] - ETA: 18:48 - loss: 2.5797 - regression_loss: 1.9118 - classification_loss 7629/10000 [=====================>........] - ETA: 18:47 - loss: 2.5797 - regression_loss: 1.9118 - classification_loss 7630/10000 [=====================>........] - ETA: 18:47 - loss: 2.5797 - regression_loss: 1.9118 - classification_loss 7631/10000 [=====================>........] - ETA: 18:46 - loss: 2.5797 - regression_loss: 1.9119 - classification_loss 7632/10000 [=====================>........] - ETA: 18:46 - loss: 2.5796 - regression_loss: 1.9118 - classification_loss 7633/10000 [=====================>........] - ETA: 18:45 - loss: 2.5795 - regression_loss: 1.9117 - classification_loss 7634/10000 [=====================>........] - ETA: 18:45 - loss: 2.5793 - regression_loss: 1.9115 - classification_loss 7635/10000 [=====================>........] - ETA: 18:44 - loss: 2.5792 - regression_loss: 1.9115 - classification_loss 7636/10000 [=====================>........] - ETA: 18:44 - loss: 2.5791 - regression_loss: 1.9113 - classification_loss 7637/10000 [=====================>........] - ETA: 18:43 - loss: 2.5791 - regression_loss: 1.9114 - classification_loss 7638/10000 [=====================>........] - ETA: 18:43 - loss: 2.5790 - regression_loss: 1.9113 - classification_loss 7639/10000 [=====================>........] - ETA: 18:42 - loss: 2.5789 - regression_loss: 1.9112 - classification_loss 7640/10000 [=====================>........] - ETA: 18:42 - loss: 2.5788 - regression_loss: 1.9111 - classification_loss 7641/10000 [=====================>........] - ETA: 18:41 - loss: 2.5788 - regression_loss: 1.9112 - classification_loss 7642/10000 [=====================>........] - ETA: 18:41 - loss: 2.5789 - regression_loss: 1.9112 - classification_loss 7643/10000 [=====================>........] - ETA: 18:41 - loss: 2.5788 - regression_loss: 1.9112 - classification_loss 7644/10000 [=====================>........] - ETA: 18:40 - loss: 2.5787 - regression_loss: 1.9111 - classification_loss 7645/10000 [=====================>........] - ETA: 18:40 - loss: 2.5787 - regression_loss: 1.9111 - classification_loss 7646/10000 [=====================>........] - ETA: 18:39 - loss: 2.5786 - regression_loss: 1.9111 - classification_loss 7647/10000 [=====================>........] - ETA: 18:39 - loss: 2.5785 - regression_loss: 1.9110 - classification_loss 7648/10000 [=====================>........] - ETA: 18:38 - loss: 2.5784 - regression_loss: 1.9110 - classification_loss 7649/10000 [=====================>........] - ETA: 18:38 - loss: 2.5782 - regression_loss: 1.9108 - classification_loss 7650/10000 [=====================>........] - ETA: 18:37 - loss: 2.5782 - regression_loss: 1.9108 - classification_loss 7651/10000 [=====================>........] - ETA: 18:37 - loss: 2.5782 - regression_loss: 1.9108 - classification_loss 7652/10000 [=====================>........] - ETA: 18:36 - loss: 2.5780 - regression_loss: 1.9107 - classification_loss 7653/10000 [=====================>........] - ETA: 18:36 - loss: 2.5781 - regression_loss: 1.9108 - classification_loss 7654/10000 [=====================>........] - ETA: 18:35 - loss: 2.5781 - regression_loss: 1.9107 - classification_loss 7655/10000 [=====================>........] - ETA: 18:35 - loss: 2.5780 - regression_loss: 1.9106 - classification_loss 7656/10000 [=====================>........] - ETA: 18:34 - loss: 2.5779 - regression_loss: 1.9106 - classification_loss 7657/10000 [=====================>........] - ETA: 18:34 - loss: 2.5778 - regression_loss: 1.9106 - classification_loss 7658/10000 [=====================>........] - ETA: 18:33 - loss: 2.5777 - regression_loss: 1.9104 - classification_loss 7659/10000 [=====================>........] - ETA: 18:33 - loss: 2.5776 - regression_loss: 1.9104 - classification_loss 7660/10000 [=====================>........] - ETA: 18:32 - loss: 2.5775 - regression_loss: 1.9104 - classification_loss 7661/10000 [=====================>........] - ETA: 18:32 - loss: 2.5776 - regression_loss: 1.9105 - classification_loss 7662/10000 [=====================>........] - ETA: 18:31 - loss: 2.5775 - regression_loss: 1.9104 - classification_loss 7663/10000 [=====================>........] - ETA: 18:31 - loss: 2.5774 - regression_loss: 1.9103 - classification_loss 7664/10000 [=====================>........] - ETA: 18:30 - loss: 2.5774 - regression_loss: 1.9103 - classification_loss 7665/10000 [=====================>........] - ETA: 18:30 - loss: 2.5773 - regression_loss: 1.9103 - classification_loss 7666/10000 [=====================>........] - ETA: 18:29 - loss: 2.5772 - regression_loss: 1.9102 - classification_loss 7667/10000 [======================>.......] - ETA: 18:29 - loss: 2.5770 - regression_loss: 1.9101 - classification_loss 7668/10000 [======================>.......] - ETA: 18:29 - loss: 2.5770 - regression_loss: 1.9101 - classification_loss 7669/10000 [======================>.......] - ETA: 18:28 - loss: 2.5768 - regression_loss: 1.9100 - classification_loss 7670/10000 [======================>.......] - ETA: 18:28 - loss: 2.5768 - regression_loss: 1.9099 - classification_loss 7671/10000 [======================>.......] - ETA: 18:27 - loss: 2.5768 - regression_loss: 1.9099 - classification_loss 7672/10000 [======================>.......] - ETA: 18:27 - loss: 2.5767 - regression_loss: 1.9099 - classification_loss 7673/10000 [======================>.......] - ETA: 18:26 - loss: 2.5765 - regression_loss: 1.9097 - classification_loss 7674/10000 [======================>.......] - ETA: 18:26 - loss: 2.5765 - regression_loss: 1.9097 - classification_loss 7675/10000 [======================>.......] - ETA: 18:25 - loss: 2.5764 - regression_loss: 1.9097 - classification_loss 7676/10000 [======================>.......] - ETA: 18:25 - loss: 2.5763 - regression_loss: 1.9096 - classification_loss 7677/10000 [======================>.......] - ETA: 18:24 - loss: 2.5761 - regression_loss: 1.9094 - classification_loss 7678/10000 [======================>.......] - ETA: 18:24 - loss: 2.5760 - regression_loss: 1.9094 - classification_loss 7679/10000 [======================>.......] - ETA: 18:23 - loss: 2.5759 - regression_loss: 1.9092 - classification_loss 7680/10000 [======================>.......] - ETA: 18:23 - loss: 2.5758 - regression_loss: 1.9092 - classification_loss 7681/10000 [======================>.......] - ETA: 18:22 - loss: 2.5758 - regression_loss: 1.9091 - classification_loss 7682/10000 [======================>.......] - ETA: 18:22 - loss: 2.5756 - regression_loss: 1.9090 - classification_loss 7683/10000 [======================>.......] - ETA: 18:21 - loss: 2.5756 - regression_loss: 1.9090 - classification_loss 7684/10000 [======================>.......] - ETA: 18:21 - loss: 2.5755 - regression_loss: 1.9090 - classification_loss 7685/10000 [======================>.......] - ETA: 18:20 - loss: 2.5754 - regression_loss: 1.9089 - classification_loss 7686/10000 [======================>.......] - ETA: 18:20 - loss: 2.5754 - regression_loss: 1.9089 - classification_loss 7687/10000 [======================>.......] - ETA: 18:19 - loss: 2.5753 - regression_loss: 1.9089 - classification_loss 7688/10000 [======================>.......] - ETA: 18:19 - loss: 2.5752 - regression_loss: 1.9088 - classification_loss 7689/10000 [======================>.......] - ETA: 18:18 - loss: 2.5751 - regression_loss: 1.9087 - classification_loss 7690/10000 [======================>.......] - ETA: 18:18 - loss: 2.5750 - regression_loss: 1.9087 - classification_loss 7691/10000 [======================>.......] - ETA: 18:17 - loss: 2.5750 - regression_loss: 1.9087 - classification_loss 7692/10000 [======================>.......] - ETA: 18:17 - loss: 2.5750 - regression_loss: 1.9087 - classification_loss 7693/10000 [======================>.......] - ETA: 18:17 - loss: 2.5749 - regression_loss: 1.9087 - classification_loss 7694/10000 [======================>.......] - ETA: 18:16 - loss: 2.5750 - regression_loss: 1.9087 - classification_loss 7695/10000 [======================>.......] - ETA: 18:16 - loss: 2.5749 - regression_loss: 1.9087 - classification_loss 7696/10000 [======================>.......] - ETA: 18:15 - loss: 2.5749 - regression_loss: 1.9086 - classification_loss 7697/10000 [======================>.......] - ETA: 18:15 - loss: 2.5749 - regression_loss: 1.9087 - classification_loss 7698/10000 [======================>.......] - ETA: 18:14 - loss: 2.5748 - regression_loss: 1.9086 - classification_loss 7699/10000 [======================>.......] - ETA: 18:14 - loss: 2.5747 - regression_loss: 1.9085 - classification_loss 7700/10000 [======================>.......] - ETA: 18:13 - loss: 2.5746 - regression_loss: 1.9084 - classification_loss 7701/10000 [======================>.......] - ETA: 18:13 - loss: 2.5745 - regression_loss: 1.9083 - classification_loss 7702/10000 [======================>.......] - ETA: 18:12 - loss: 2.5744 - regression_loss: 1.9083 - classification_loss 7703/10000 [======================>.......] - ETA: 18:12 - loss: 2.5745 - regression_loss: 1.9083 - classification_loss 7704/10000 [======================>.......] - ETA: 18:11 - loss: 2.5745 - regression_loss: 1.9084 - classification_loss 7705/10000 [======================>.......] - ETA: 18:11 - loss: 2.5745 - regression_loss: 1.9081 - classification_loss 7706/10000 [======================>.......] - ETA: 18:10 - loss: 2.5744 - regression_loss: 1.9080 - classification_loss 7707/10000 [======================>.......] - ETA: 18:10 - loss: 2.5742 - regression_loss: 1.9079 - classification_loss 7708/10000 [======================>.......] - ETA: 18:09 - loss: 2.5743 - regression_loss: 1.9080 - classification_loss 7709/10000 [======================>.......] - ETA: 18:09 - loss: 2.5742 - regression_loss: 1.9079 - classification_loss 7710/10000 [======================>.......] - ETA: 18:08 - loss: 2.5741 - regression_loss: 1.9079 - classification_loss 7711/10000 [======================>.......] - ETA: 18:08 - loss: 2.5740 - regression_loss: 1.9078 - classification_loss 7712/10000 [======================>.......] - ETA: 18:07 - loss: 2.5738 - regression_loss: 1.9076 - classification_loss 7713/10000 [======================>.......] - ETA: 18:07 - loss: 2.5737 - regression_loss: 1.9075 - classification_loss 7714/10000 [======================>.......] - ETA: 18:06 - loss: 2.5736 - regression_loss: 1.9075 - classification_loss 7715/10000 [======================>.......] - ETA: 18:06 - loss: 2.5737 - regression_loss: 1.9076 - classification_loss 7716/10000 [======================>.......] - ETA: 18:06 - loss: 2.5736 - regression_loss: 1.9074 - classification_loss 7717/10000 [======================>.......] - ETA: 18:05 - loss: 2.5736 - regression_loss: 1.9074 - classification_loss 7718/10000 [======================>.......] - ETA: 18:05 - loss: 2.5735 - regression_loss: 1.9074 - classification_loss 7719/10000 [======================>.......] - ETA: 18:04 - loss: 2.5734 - regression_loss: 1.9073 - classification_loss 7720/10000 [======================>.......] - ETA: 18:04 - loss: 2.5734 - regression_loss: 1.9073 - classification_loss 7721/10000 [======================>.......] - ETA: 18:03 - loss: 2.5734 - regression_loss: 1.9074 - classification_loss 7722/10000 [======================>.......] - ETA: 18:03 - loss: 2.5734 - regression_loss: 1.9074 - classification_loss 7723/10000 [======================>.......] - ETA: 18:02 - loss: 2.5733 - regression_loss: 1.9073 - classification_loss 7724/10000 [======================>.......] - ETA: 18:02 - loss: 2.5732 - regression_loss: 1.9072 - classification_loss 7725/10000 [======================>.......] - ETA: 18:01 - loss: 2.5732 - regression_loss: 1.9072 - classification_loss 7726/10000 [======================>.......] - ETA: 18:01 - loss: 2.5731 - regression_loss: 1.9072 - classification_loss 7727/10000 [======================>.......] - ETA: 18:00 - loss: 2.5731 - regression_loss: 1.9072 - classification_loss 7728/10000 [======================>.......] - ETA: 18:00 - loss: 2.5730 - regression_loss: 1.9071 - classification_loss 7729/10000 [======================>.......] - ETA: 17:59 - loss: 2.5729 - regression_loss: 1.9071 - classification_loss 7730/10000 [======================>.......] - ETA: 17:59 - loss: 2.5728 - regression_loss: 1.9070 - classification_loss 7731/10000 [======================>.......] - ETA: 17:58 - loss: 2.5727 - regression_loss: 1.9069 - classification_loss 7732/10000 [======================>.......] - ETA: 17:58 - loss: 2.5726 - regression_loss: 1.9068 - classification_loss 7733/10000 [======================>.......] - ETA: 17:57 - loss: 2.5726 - regression_loss: 1.9068 - classification_loss 7734/10000 [======================>.......] - ETA: 17:57 - loss: 2.5726 - regression_loss: 1.9068 - classification_loss 7735/10000 [======================>.......] - ETA: 17:56 - loss: 2.5726 - regression_loss: 1.9068 - classification_loss 7736/10000 [======================>.......] - ETA: 17:56 - loss: 2.5726 - regression_loss: 1.9069 - classification_loss 7737/10000 [======================>.......] - ETA: 17:55 - loss: 2.5726 - regression_loss: 1.9069 - classification_loss 7738/10000 [======================>.......] - ETA: 17:55 - loss: 2.5725 - regression_loss: 1.9068 - classification_loss 7739/10000 [======================>.......] - ETA: 17:54 - loss: 2.5725 - regression_loss: 1.9069 - classification_loss 7740/10000 [======================>.......] - ETA: 17:54 - loss: 2.5725 - regression_loss: 1.9068 - classification_loss 7741/10000 [======================>.......] - ETA: 17:54 - loss: 2.5726 - regression_loss: 1.9069 - classification_loss 7742/10000 [======================>.......] - ETA: 17:53 - loss: 2.5725 - regression_loss: 1.9068 - classification_loss 7743/10000 [======================>.......] - ETA: 17:53 - loss: 2.5724 - regression_loss: 1.9068 - classification_loss 7744/10000 [======================>.......] - ETA: 17:52 - loss: 2.5723 - regression_loss: 1.9067 - classification_loss 7745/10000 [======================>.......] - ETA: 17:52 - loss: 2.5722 - regression_loss: 1.9066 - classification_loss 7746/10000 [======================>.......] - ETA: 17:51 - loss: 2.5721 - regression_loss: 1.9066 - classification_loss 7747/10000 [======================>.......] - ETA: 17:51 - loss: 2.5721 - regression_loss: 1.9066 - classification_loss 7748/10000 [======================>.......] - ETA: 17:50 - loss: 2.5720 - regression_loss: 1.9065 - classification_loss 7749/10000 [======================>.......] - ETA: 17:50 - loss: 2.5718 - regression_loss: 1.9064 - classification_loss 7750/10000 [======================>.......] - ETA: 17:49 - loss: 2.5718 - regression_loss: 1.9064 - classification_loss 7751/10000 [======================>.......] - ETA: 17:49 - loss: 2.5718 - regression_loss: 1.9064 - classification_loss 7752/10000 [======================>.......] - ETA: 17:48 - loss: 2.5717 - regression_loss: 1.9063 - classification_loss 7753/10000 [======================>.......] - ETA: 17:48 - loss: 2.5716 - regression_loss: 1.9063 - classification_loss 7754/10000 [======================>.......] - ETA: 17:47 - loss: 2.5717 - regression_loss: 1.9063 - classification_loss 7755/10000 [======================>.......] - ETA: 17:47 - loss: 2.5717 - regression_loss: 1.9063 - classification_loss 7756/10000 [======================>.......] - ETA: 17:46 - loss: 2.5715 - regression_loss: 1.9062 - classification_loss 7757/10000 [======================>.......] - ETA: 17:46 - loss: 2.5714 - regression_loss: 1.9061 - classification_loss 7758/10000 [======================>.......] - ETA: 17:45 - loss: 2.5715 - regression_loss: 1.9061 - classification_loss 7759/10000 [======================>.......] - ETA: 17:45 - loss: 2.5714 - regression_loss: 1.9061 - classification_loss 7760/10000 [======================>.......] - ETA: 17:44 - loss: 2.5713 - regression_loss: 1.9060 - classification_loss 7761/10000 [======================>.......] - ETA: 17:44 - loss: 2.5714 - regression_loss: 1.9061 - classification_loss 7762/10000 [======================>.......] - ETA: 17:43 - loss: 2.5713 - regression_loss: 1.9060 - classification_loss 7763/10000 [======================>.......] - ETA: 17:43 - loss: 2.5714 - regression_loss: 1.9057 - classification_loss 7764/10000 [======================>.......] - ETA: 17:42 - loss: 2.5713 - regression_loss: 1.9056 - classification_loss 7765/10000 [======================>.......] - ETA: 17:42 - loss: 2.5713 - regression_loss: 1.9057 - classification_loss 7766/10000 [======================>.......] - ETA: 17:42 - loss: 2.5712 - regression_loss: 1.9056 - classification_loss 7767/10000 [======================>.......] - ETA: 17:41 - loss: 2.5712 - regression_loss: 1.9055 - classification_loss 7768/10000 [======================>.......] - ETA: 17:41 - loss: 2.5711 - regression_loss: 1.9055 - classification_loss 7769/10000 [======================>.......] - ETA: 17:40 - loss: 2.5709 - regression_loss: 1.9053 - classification_loss 7770/10000 [======================>.......] - ETA: 17:40 - loss: 2.5707 - regression_loss: 1.9052 - classification_loss 7771/10000 [======================>.......] - ETA: 17:39 - loss: 2.5707 - regression_loss: 1.9052 - classification_loss 7772/10000 [======================>.......] - ETA: 17:39 - loss: 2.5706 - regression_loss: 1.9051 - classification_loss 7773/10000 [======================>.......] - ETA: 17:38 - loss: 2.5705 - regression_loss: 1.9050 - classification_loss 7774/10000 [======================>.......] - ETA: 17:38 - loss: 2.5703 - regression_loss: 1.9049 - classification_loss 7775/10000 [======================>.......] - ETA: 17:37 - loss: 2.5703 - regression_loss: 1.9049 - classification_loss 7776/10000 [======================>.......] - ETA: 17:37 - loss: 2.5703 - regression_loss: 1.9048 - classification_loss 7777/10000 [======================>.......] - ETA: 17:36 - loss: 2.5701 - regression_loss: 1.9047 - classification_loss 7778/10000 [======================>.......] - ETA: 17:36 - loss: 2.5702 - regression_loss: 1.9047 - classification_loss 7779/10000 [======================>.......] - ETA: 17:35 - loss: 2.5700 - regression_loss: 1.9046 - classification_loss 7780/10000 [======================>.......] - ETA: 17:35 - loss: 2.5699 - regression_loss: 1.9045 - classification_loss 7781/10000 [======================>.......] - ETA: 17:34 - loss: 2.5699 - regression_loss: 1.9046 - classification_loss 7782/10000 [======================>.......] - ETA: 17:34 - loss: 2.5700 - regression_loss: 1.9046 - classification_loss 7783/10000 [======================>.......] - ETA: 17:33 - loss: 2.5701 - regression_loss: 1.9047 - classification_loss 7784/10000 [======================>.......] - ETA: 17:33 - loss: 2.5699 - regression_loss: 1.9045 - classification_loss 7785/10000 [======================>.......] - ETA: 17:32 - loss: 2.5698 - regression_loss: 1.9044 - classification_loss 7786/10000 [======================>.......] - ETA: 17:32 - loss: 2.5698 - regression_loss: 1.9045 - classification_loss 7787/10000 [======================>.......] - ETA: 17:32 - loss: 2.5699 - regression_loss: 1.9045 - classification_loss 7788/10000 [======================>.......] - ETA: 17:31 - loss: 2.5698 - regression_loss: 1.9044 - classification_loss 7789/10000 [======================>.......] - ETA: 17:31 - loss: 2.5697 - regression_loss: 1.9044 - classification_loss 7790/10000 [======================>.......] - ETA: 17:30 - loss: 2.5697 - regression_loss: 1.9044 - classification_loss 7791/10000 [======================>.......] - ETA: 17:30 - loss: 2.5696 - regression_loss: 1.9044 - classification_loss 7792/10000 [======================>.......] - ETA: 17:29 - loss: 2.5698 - regression_loss: 1.9045 - classification_loss 7793/10000 [======================>.......] - ETA: 17:29 - loss: 2.5698 - regression_loss: 1.9045 - classification_loss 7794/10000 [======================>.......] - ETA: 17:28 - loss: 2.5697 - regression_loss: 1.9045 - classification_loss 7795/10000 [======================>.......] - ETA: 17:28 - loss: 2.5698 - regression_loss: 1.9046 - classification_loss 7796/10000 [======================>.......] - ETA: 17:27 - loss: 2.5697 - regression_loss: 1.9045 - classification_loss 7797/10000 [======================>.......] - ETA: 17:27 - loss: 2.5697 - regression_loss: 1.9045 - classification_loss 7798/10000 [======================>.......] - ETA: 17:26 - loss: 2.5696 - regression_loss: 1.9045 - classification_loss 7799/10000 [======================>.......] - ETA: 17:26 - loss: 2.5695 - regression_loss: 1.9044 - classification_loss 7800/10000 [======================>.......] - ETA: 17:25 - loss: 2.5694 - regression_loss: 1.9043 - classification_loss 7801/10000 [======================>.......] - ETA: 17:25 - loss: 2.5693 - regression_loss: 1.9042 - classification_loss 7802/10000 [======================>.......] - ETA: 17:24 - loss: 2.5693 - regression_loss: 1.9042 - classification_loss 7803/10000 [======================>.......] - ETA: 17:24 - loss: 2.5691 - regression_loss: 1.9041 - classification_loss 7804/10000 [======================>.......] - ETA: 17:23 - loss: 2.5690 - regression_loss: 1.9040 - classification_loss 7805/10000 [======================>.......] - ETA: 17:23 - loss: 2.5690 - regression_loss: 1.9040 - classification_loss 7806/10000 [======================>.......] - ETA: 17:22 - loss: 2.5689 - regression_loss: 1.9039 - classification_loss 7807/10000 [======================>.......] - ETA: 17:22 - loss: 2.5687 - regression_loss: 1.9038 - classification_loss 7808/10000 [======================>.......] - ETA: 17:21 - loss: 2.5686 - regression_loss: 1.9037 - classification_loss 7809/10000 [======================>.......] - ETA: 17:21 - loss: 2.5685 - regression_loss: 1.9037 - classification_loss 7810/10000 [======================>.......] - ETA: 17:21 - loss: 2.5685 - regression_loss: 1.9036 - classification_loss 7811/10000 [======================>.......] - ETA: 17:20 - loss: 2.5685 - regression_loss: 1.9037 - classification_loss 7812/10000 [======================>.......] - ETA: 17:20 - loss: 2.5686 - regression_loss: 1.9037 - classification_loss 7813/10000 [======================>.......] - ETA: 17:19 - loss: 2.5684 - regression_loss: 1.9036 - classification_loss 7814/10000 [======================>.......] - ETA: 17:19 - loss: 2.5683 - regression_loss: 1.9035 - classification_loss 7815/10000 [======================>.......] - ETA: 17:18 - loss: 2.5683 - regression_loss: 1.9035 - classification_loss 7816/10000 [======================>.......] - ETA: 17:18 - loss: 2.5683 - regression_loss: 1.9035 - classification_loss 7817/10000 [======================>.......] - ETA: 17:17 - loss: 2.5681 - regression_loss: 1.9034 - classification_loss 7818/10000 [======================>.......] - ETA: 17:17 - loss: 2.5681 - regression_loss: 1.9034 - classification_loss 7819/10000 [======================>.......] - ETA: 17:16 - loss: 2.5681 - regression_loss: 1.9035 - classification_loss 7820/10000 [======================>.......] - ETA: 17:16 - loss: 2.5681 - regression_loss: 1.9034 - classification_loss 7821/10000 [======================>.......] - ETA: 17:15 - loss: 2.5680 - regression_loss: 1.9033 - classification_loss 7822/10000 [======================>.......] - ETA: 17:15 - loss: 2.5679 - regression_loss: 1.9033 - classification_loss 7823/10000 [======================>.......] - ETA: 17:14 - loss: 2.5678 - regression_loss: 1.9033 - classification_loss 7824/10000 [======================>.......] - ETA: 17:14 - loss: 2.5678 - regression_loss: 1.9032 - classification_loss 7825/10000 [======================>.......] - ETA: 17:13 - loss: 2.5677 - regression_loss: 1.9032 - classification_loss 7826/10000 [======================>.......] - ETA: 17:13 - loss: 2.5676 - regression_loss: 1.9031 - classification_loss 7827/10000 [======================>.......] - ETA: 17:12 - loss: 2.5677 - regression_loss: 1.9031 - classification_loss 7828/10000 [======================>.......] - ETA: 17:12 - loss: 2.5676 - regression_loss: 1.9031 - classification_loss 7829/10000 [======================>.......] - ETA: 17:11 - loss: 2.5676 - regression_loss: 1.9030 - classification_loss 7830/10000 [======================>.......] - ETA: 17:11 - loss: 2.5675 - regression_loss: 1.9030 - classification_loss 7831/10000 [======================>.......] - ETA: 17:10 - loss: 2.5675 - regression_loss: 1.9030 - classification_loss 7832/10000 [======================>.......] - ETA: 17:10 - loss: 2.5675 - regression_loss: 1.9030 - classification_loss 7833/10000 [======================>.......] - ETA: 17:09 - loss: 2.5674 - regression_loss: 1.9029 - classification_loss 7834/10000 [======================>.......] - ETA: 17:09 - loss: 2.5674 - regression_loss: 1.9030 - classification_loss 7835/10000 [======================>.......] - ETA: 17:09 - loss: 2.5673 - regression_loss: 1.9029 - classification_loss 7836/10000 [======================>.......] - ETA: 17:08 - loss: 2.5673 - regression_loss: 1.9029 - classification_loss 7837/10000 [======================>.......] - ETA: 17:08 - loss: 2.5670 - regression_loss: 1.9027 - classification_loss 7838/10000 [======================>.......] - ETA: 17:07 - loss: 2.5670 - regression_loss: 1.9027 - classification_loss 7839/10000 [======================>.......] - ETA: 17:07 - loss: 2.5669 - regression_loss: 1.9026 - classification_loss 7840/10000 [======================>.......] - ETA: 17:06 - loss: 2.5669 - regression_loss: 1.9027 - classification_loss 7841/10000 [======================>.......] - ETA: 17:06 - loss: 2.5668 - regression_loss: 1.9026 - classification_loss 7842/10000 [======================>.......] - ETA: 17:05 - loss: 2.5667 - regression_loss: 1.9026 - classification_loss 7843/10000 [======================>.......] - ETA: 17:05 - loss: 2.5668 - regression_loss: 1.9026 - classification_loss 7844/10000 [======================>.......] - ETA: 17:04 - loss: 2.5668 - regression_loss: 1.9026 - classification_loss 7845/10000 [======================>.......] - ETA: 17:04 - loss: 2.5666 - regression_loss: 1.9025 - classification_loss 7846/10000 [======================>.......] - ETA: 17:03 - loss: 2.5665 - regression_loss: 1.9024 - classification_loss 7847/10000 [======================>.......] - ETA: 17:03 - loss: 2.5666 - regression_loss: 1.9024 - classification_loss 7848/10000 [======================>.......] - ETA: 17:02 - loss: 2.5665 - regression_loss: 1.9024 - classification_loss 7849/10000 [======================>.......] - ETA: 17:02 - loss: 2.5664 - regression_loss: 1.9023 - classification_loss 7850/10000 [======================>.......] - ETA: 17:01 - loss: 2.5664 - regression_loss: 1.9023 - classification_loss 7851/10000 [======================>.......] - ETA: 17:01 - loss: 2.5662 - regression_loss: 1.9022 - classification_loss 7852/10000 [======================>.......] - ETA: 17:00 - loss: 2.5662 - regression_loss: 1.9022 - classification_loss 7853/10000 [======================>.......] - ETA: 17:00 - loss: 2.5661 - regression_loss: 1.9021 - classification_loss 7854/10000 [======================>.......] - ETA: 16:59 - loss: 2.5660 - regression_loss: 1.9020 - classification_loss 7855/10000 [======================>.......] - ETA: 16:59 - loss: 2.5658 - regression_loss: 1.9019 - classification_loss 7856/10000 [======================>.......] - ETA: 16:59 - loss: 2.5657 - regression_loss: 1.9018 - classification_loss 7857/10000 [======================>.......] - ETA: 16:58 - loss: 2.5655 - regression_loss: 1.9017 - classification_loss 7858/10000 [======================>.......] - ETA: 16:58 - loss: 2.5656 - regression_loss: 1.9017 - classification_loss 7859/10000 [======================>.......] - ETA: 16:57 - loss: 2.5655 - regression_loss: 1.9017 - classification_loss 7860/10000 [======================>.......] - ETA: 16:57 - loss: 2.5653 - regression_loss: 1.9015 - classification_loss 7861/10000 [======================>.......] - ETA: 16:56 - loss: 2.5653 - regression_loss: 1.9014 - classification_loss 7862/10000 [======================>.......] - ETA: 16:56 - loss: 2.5652 - regression_loss: 1.9014 - classification_loss 7863/10000 [======================>.......] - ETA: 16:55 - loss: 2.5651 - regression_loss: 1.9014 - classification_loss 7864/10000 [======================>.......] - ETA: 16:55 - loss: 2.5649 - regression_loss: 1.9012 - classification_loss 7865/10000 [======================>.......] - ETA: 16:54 - loss: 2.5649 - regression_loss: 1.9012 - classification_loss 7866/10000 [======================>.......] - ETA: 16:54 - loss: 2.5649 - regression_loss: 1.9012 - classification_loss 7867/10000 [======================>.......] - ETA: 16:53 - loss: 2.5648 - regression_loss: 1.9011 - classification_loss 7868/10000 [======================>.......] - ETA: 16:53 - loss: 2.5647 - regression_loss: 1.9011 - classification_loss 7869/10000 [======================>.......] - ETA: 16:52 - loss: 2.5646 - regression_loss: 1.9010 - classification_loss 7870/10000 [======================>.......] - ETA: 16:52 - loss: 2.5646 - regression_loss: 1.9010 - classification_loss 7871/10000 [======================>.......] - ETA: 16:51 - loss: 2.5645 - regression_loss: 1.9009 - classification_loss 7872/10000 [======================>.......] - ETA: 16:51 - loss: 2.5645 - regression_loss: 1.9009 - classification_loss 7873/10000 [======================>.......] - ETA: 16:50 - loss: 2.5644 - regression_loss: 1.9009 - classification_loss 7874/10000 [======================>.......] - ETA: 16:50 - loss: 2.5643 - regression_loss: 1.9008 - classification_loss 7875/10000 [======================>.......] - ETA: 16:49 - loss: 2.5642 - regression_loss: 1.9008 - classification_loss 7876/10000 [======================>.......] - ETA: 16:49 - loss: 2.5641 - regression_loss: 1.9007 - classification_loss 7877/10000 [======================>.......] - ETA: 16:48 - loss: 2.5640 - regression_loss: 1.9006 - classification_loss 7878/10000 [======================>.......] - ETA: 16:48 - loss: 2.5639 - regression_loss: 1.9006 - classification_loss 7879/10000 [======================>.......] - ETA: 16:47 - loss: 2.5638 - regression_loss: 1.9006 - classification_loss 7880/10000 [======================>.......] - ETA: 16:47 - loss: 2.5638 - regression_loss: 1.9005 - classification_loss 7881/10000 [======================>.......] - ETA: 16:47 - loss: 2.5637 - regression_loss: 1.9005 - classification_loss 7882/10000 [======================>.......] - ETA: 16:46 - loss: 2.5637 - regression_loss: 1.9005 - classification_loss 7883/10000 [======================>.......] - ETA: 16:46 - loss: 2.5636 - regression_loss: 1.9004 - classification_loss 7884/10000 [======================>.......] - ETA: 16:45 - loss: 2.5634 - regression_loss: 1.9003 - classification_loss 7885/10000 [======================>.......] - ETA: 16:45 - loss: 2.5633 - regression_loss: 1.9002 - classification_loss 7886/10000 [======================>.......] - ETA: 16:44 - loss: 2.5633 - regression_loss: 1.9002 - classification_loss 7887/10000 [======================>.......] - ETA: 16:44 - loss: 2.5631 - regression_loss: 1.9000 - classification_loss 7888/10000 [======================>.......] - ETA: 16:43 - loss: 2.5630 - regression_loss: 1.9000 - classification_loss 7889/10000 [======================>.......] - ETA: 16:43 - loss: 2.5631 - regression_loss: 1.9000 - classification_loss 7890/10000 [======================>.......] - ETA: 16:42 - loss: 2.5630 - regression_loss: 1.8999 - classification_loss 7891/10000 [======================>.......] - ETA: 16:42 - loss: 2.5629 - regression_loss: 1.8999 - classification_loss 7892/10000 [======================>.......] - ETA: 16:41 - loss: 2.5628 - regression_loss: 1.8998 - classification_loss 7893/10000 [======================>.......] - ETA: 16:41 - loss: 2.5627 - regression_loss: 1.8998 - classification_loss 7894/10000 [======================>.......] - ETA: 16:40 - loss: 2.5627 - regression_loss: 1.8998 - classification_loss 7895/10000 [======================>.......] - ETA: 16:40 - loss: 2.5627 - regression_loss: 1.8998 - classification_loss 7896/10000 [======================>.......] - ETA: 16:39 - loss: 2.5627 - regression_loss: 1.8997 - classification_loss 7897/10000 [======================>.......] - ETA: 16:39 - loss: 2.5626 - regression_loss: 1.8997 - classification_loss 7898/10000 [======================>.......] - ETA: 16:38 - loss: 2.5627 - regression_loss: 1.8998 - classification_loss 7899/10000 [======================>.......] - ETA: 16:38 - loss: 2.5627 - regression_loss: 1.8997 - classification_loss 7900/10000 [======================>.......] - ETA: 16:37 - loss: 2.5627 - regression_loss: 1.8998 - classification_loss 7901/10000 [======================>.......] - ETA: 16:37 - loss: 2.5627 - regression_loss: 1.8998 - classification_loss 7902/10000 [======================>.......] - ETA: 16:36 - loss: 2.5626 - regression_loss: 1.8997 - classification_loss 7903/10000 [======================>.......] - ETA: 16:36 - loss: 2.5625 - regression_loss: 1.8997 - classification_loss 7904/10000 [======================>.......] - ETA: 16:35 - loss: 2.5625 - regression_loss: 1.8996 - classification_loss 7905/10000 [======================>.......] - ETA: 16:35 - loss: 2.5624 - regression_loss: 1.8996 - classification_loss 7906/10000 [======================>.......] - ETA: 16:34 - loss: 2.5622 - regression_loss: 1.8994 - classification_loss 7907/10000 [======================>.......] - ETA: 16:34 - loss: 2.5622 - regression_loss: 1.8994 - classification_loss 7908/10000 [======================>.......] - ETA: 16:34 - loss: 2.5622 - regression_loss: 1.8994 - classification_loss 7909/10000 [======================>.......] - ETA: 16:33 - loss: 2.5621 - regression_loss: 1.8994 - classification_loss 7910/10000 [======================>.......] - ETA: 16:33 - loss: 2.5622 - regression_loss: 1.8994 - classification_loss 7911/10000 [======================>.......] - ETA: 16:32 - loss: 2.5622 - regression_loss: 1.8994 - classification_loss 7912/10000 [======================>.......] - ETA: 16:32 - loss: 2.5620 - regression_loss: 1.8992 - classification_loss 7913/10000 [======================>.......] - ETA: 16:31 - loss: 2.5620 - regression_loss: 1.8992 - classification_loss 7914/10000 [======================>.......] - ETA: 16:31 - loss: 2.5620 - regression_loss: 1.8991 - classification_loss 7915/10000 [======================>.......] - ETA: 16:30 - loss: 2.5619 - regression_loss: 1.8991 - classification_loss 7916/10000 [======================>.......] - ETA: 16:30 - loss: 2.5619 - regression_loss: 1.8991 - classification_loss 7917/10000 [======================>.......] - ETA: 16:29 - loss: 2.5619 - regression_loss: 1.8992 - classification_loss 7918/10000 [======================>.......] - ETA: 16:29 - loss: 2.5619 - regression_loss: 1.8992 - classification_loss 7919/10000 [======================>.......] - ETA: 16:28 - loss: 2.5619 - regression_loss: 1.8991 - classification_loss 7920/10000 [======================>.......] - ETA: 16:28 - loss: 2.5619 - regression_loss: 1.8991 - classification_loss 7921/10000 [======================>.......] - ETA: 16:27 - loss: 2.5619 - regression_loss: 1.8992 - classification_loss 7922/10000 [======================>.......] - ETA: 16:27 - loss: 2.5617 - regression_loss: 1.8990 - classification_loss 7923/10000 [======================>.......] - ETA: 16:26 - loss: 2.5617 - regression_loss: 1.8991 - classification_loss 7924/10000 [======================>.......] - ETA: 16:26 - loss: 2.5614 - regression_loss: 1.8988 - classification_loss 7925/10000 [======================>.......] - ETA: 16:25 - loss: 2.5614 - regression_loss: 1.8989 - classification_loss 7926/10000 [======================>.......] - ETA: 16:25 - loss: 2.5614 - regression_loss: 1.8988 - classification_loss 7927/10000 [======================>.......] - ETA: 16:24 - loss: 2.5613 - regression_loss: 1.8988 - classification_loss 7928/10000 [======================>.......] - ETA: 16:24 - loss: 2.5613 - regression_loss: 1.8988 - classification_loss 7929/10000 [======================>.......] - ETA: 16:23 - loss: 2.5612 - regression_loss: 1.8987 - classification_loss 7930/10000 [======================>.......] - ETA: 16:23 - loss: 2.5612 - regression_loss: 1.8987 - classification_loss 7931/10000 [======================>.......] - ETA: 16:22 - loss: 2.5611 - regression_loss: 1.8987 - classification_loss 7932/10000 [======================>.......] - ETA: 16:22 - loss: 2.5611 - regression_loss: 1.8987 - classification_loss 7933/10000 [======================>.......] - ETA: 16:21 - loss: 2.5611 - regression_loss: 1.8987 - classification_loss 7934/10000 [======================>.......] - ETA: 16:21 - loss: 2.5611 - regression_loss: 1.8988 - classification_loss 7935/10000 [======================>.......] - ETA: 16:20 - loss: 2.5610 - regression_loss: 1.8987 - classification_loss 7936/10000 [======================>.......] - ETA: 16:20 - loss: 2.5612 - regression_loss: 1.8989 - classification_loss 7937/10000 [======================>.......] - ETA: 16:20 - loss: 2.5611 - regression_loss: 1.8988 - classification_loss 7938/10000 [======================>.......] - ETA: 16:19 - loss: 2.5609 - regression_loss: 1.8987 - classification_loss 7939/10000 [======================>.......] - ETA: 16:19 - loss: 2.5609 - regression_loss: 1.8987 - classification_loss 7940/10000 [======================>.......] - ETA: 16:18 - loss: 2.5608 - regression_loss: 1.8987 - classification_loss 7941/10000 [======================>.......] - ETA: 16:18 - loss: 2.5607 - regression_loss: 1.8986 - classification_loss 7942/10000 [======================>.......] - ETA: 16:17 - loss: 2.5606 - regression_loss: 1.8985 - classification_loss 7943/10000 [======================>.......] - ETA: 16:17 - loss: 2.5605 - regression_loss: 1.8984 - classification_loss 7944/10000 [======================>.......] - ETA: 16:16 - loss: 2.5605 - regression_loss: 1.8984 - classification_loss 7945/10000 [======================>.......] - ETA: 16:16 - loss: 2.5603 - regression_loss: 1.8983 - classification_loss 7946/10000 [======================>.......] - ETA: 16:15 - loss: 2.5601 - regression_loss: 1.8981 - classification_loss 7947/10000 [======================>.......] - ETA: 16:15 - loss: 2.5601 - regression_loss: 1.8981 - classification_loss 7948/10000 [======================>.......] - ETA: 16:14 - loss: 2.5601 - regression_loss: 1.8981 - classification_loss 7949/10000 [======================>.......] - ETA: 16:14 - loss: 2.5601 - regression_loss: 1.8982 - classification_loss 7950/10000 [======================>.......] - ETA: 16:13 - loss: 2.5600 - regression_loss: 1.8981 - classification_loss 7951/10000 [======================>.......] - ETA: 16:13 - loss: 2.5600 - regression_loss: 1.8981 - classification_loss 7952/10000 [======================>.......] - ETA: 16:12 - loss: 2.5599 - regression_loss: 1.8981 - classification_loss 7953/10000 [======================>.......] - ETA: 16:12 - loss: 2.5599 - regression_loss: 1.8980 - classification_loss 7954/10000 [======================>.......] - ETA: 16:11 - loss: 2.5598 - regression_loss: 1.8979 - classification_loss 7955/10000 [======================>.......] - ETA: 16:11 - loss: 2.5596 - regression_loss: 1.8978 - classification_loss 7956/10000 [======================>.......] - ETA: 16:10 - loss: 2.5596 - regression_loss: 1.8978 - classification_loss 7957/10000 [======================>.......] - ETA: 16:10 - loss: 2.5597 - regression_loss: 1.8979 - classification_loss 7958/10000 [======================>.......] - ETA: 16:09 - loss: 2.5595 - regression_loss: 1.8978 - classification_loss 7959/10000 [======================>.......] - ETA: 16:09 - loss: 2.5595 - regression_loss: 1.8978 - classification_loss 7960/10000 [======================>.......] - ETA: 16:08 - loss: 2.5595 - regression_loss: 1.8978 - classification_loss 7961/10000 [======================>.......] - ETA: 16:08 - loss: 2.5595 - regression_loss: 1.8978 - classification_loss 7962/10000 [======================>.......] - ETA: 16:08 - loss: 2.5594 - regression_loss: 1.8977 - classification_loss 7963/10000 [======================>.......] - ETA: 16:07 - loss: 2.5593 - regression_loss: 1.8977 - classification_loss 7964/10000 [======================>.......] - ETA: 16:07 - loss: 2.5592 - regression_loss: 1.8975 - classification_loss 7965/10000 [======================>.......] - ETA: 16:06 - loss: 2.5590 - regression_loss: 1.8974 - classification_loss 7966/10000 [======================>.......] - ETA: 16:06 - loss: 2.5591 - regression_loss: 1.8975 - classification_loss 7967/10000 [======================>.......] - ETA: 16:05 - loss: 2.5590 - regression_loss: 1.8974 - classification_loss 7968/10000 [======================>.......] - ETA: 16:05 - loss: 2.5589 - regression_loss: 1.8974 - classification_loss 7969/10000 [======================>.......] - ETA: 16:04 - loss: 2.5590 - regression_loss: 1.8974 - classification_loss 7970/10000 [======================>.......] - ETA: 16:04 - loss: 2.5589 - regression_loss: 1.8973 - classification_loss 7971/10000 [======================>.......] - ETA: 16:03 - loss: 2.5588 - regression_loss: 1.8973 - classification_loss 7972/10000 [======================>.......] - ETA: 16:03 - loss: 2.5587 - regression_loss: 1.8971 - classification_loss 7973/10000 [======================>.......] - ETA: 16:02 - loss: 2.5586 - regression_loss: 1.8971 - classification_loss 7974/10000 [======================>.......] - ETA: 16:02 - loss: 2.5586 - regression_loss: 1.8971 - classification_loss 7975/10000 [======================>.......] - ETA: 16:01 - loss: 2.5584 - regression_loss: 1.8970 - classification_loss 7976/10000 [======================>.......] - ETA: 16:01 - loss: 2.5583 - regression_loss: 1.8970 - classification_loss 7977/10000 [======================>.......] - ETA: 16:00 - loss: 2.5583 - regression_loss: 1.8969 - classification_loss 7978/10000 [======================>.......] - ETA: 16:00 - loss: 2.5583 - regression_loss: 1.8969 - classification_loss 7979/10000 [======================>.......] - ETA: 15:59 - loss: 2.5581 - regression_loss: 1.8967 - classification_loss 7980/10000 [======================>.......] - ETA: 15:59 - loss: 2.5581 - regression_loss: 1.8968 - classification_loss 7981/10000 [======================>.......] - ETA: 15:58 - loss: 2.5581 - regression_loss: 1.8968 - classification_loss 7982/10000 [======================>.......] - ETA: 15:58 - loss: 2.5580 - regression_loss: 1.8968 - classification_loss 7983/10000 [======================>.......] - ETA: 15:57 - loss: 2.5580 - regression_loss: 1.8968 - classification_loss 7984/10000 [======================>.......] - ETA: 15:57 - loss: 2.5580 - regression_loss: 1.8968 - classification_loss 7985/10000 [======================>.......] - ETA: 15:57 - loss: 2.5580 - regression_loss: 1.8968 - classification_loss 7986/10000 [======================>.......] - ETA: 15:56 - loss: 2.5579 - regression_loss: 1.8967 - classification_loss 7987/10000 [======================>.......] - ETA: 15:56 - loss: 2.5580 - regression_loss: 1.8968 - classification_loss 7988/10000 [======================>.......] - ETA: 15:55 - loss: 2.5581 - regression_loss: 1.8969 - classification_loss 7989/10000 [======================>.......] - ETA: 15:55 - loss: 2.5580 - regression_loss: 1.8968 - classification_loss 7990/10000 [======================>.......] - ETA: 15:54 - loss: 2.5579 - regression_loss: 1.8967 - classification_loss 7991/10000 [======================>.......] - ETA: 15:54 - loss: 2.5579 - regression_loss: 1.8967 - classification_loss 7992/10000 [======================>.......] - ETA: 15:53 - loss: 2.5579 - regression_loss: 1.8967 - classification_loss 7993/10000 [======================>.......] - ETA: 15:53 - loss: 2.5578 - regression_loss: 1.8967 - classification_loss 7994/10000 [======================>.......] - ETA: 15:52 - loss: 2.5577 - regression_loss: 1.8966 - classification_loss 7995/10000 [======================>.......] - ETA: 15:52 - loss: 2.5575 - regression_loss: 1.8965 - classification_loss 7996/10000 [======================>.......] - ETA: 15:51 - loss: 2.5573 - regression_loss: 1.8963 - classification_loss 7997/10000 [======================>.......] - ETA: 15:51 - loss: 2.5573 - regression_loss: 1.8963 - classification_loss 7998/10000 [======================>.......] - ETA: 15:50 - loss: 2.5572 - regression_loss: 1.8962 - classification_loss 7999/10000 [======================>.......] - ETA: 15:50 - loss: 2.5571 - regression_loss: 1.8962 - classification_loss 8000/10000 [=======================>......] - ETA: 15:49 - loss: 2.5570 - regression_loss: 1.8960 - classification_loss 8001/10000 [=======================>......] - ETA: 15:49 - loss: 2.5570 - regression_loss: 1.8960 - classification_loss 8002/10000 [=======================>......] - ETA: 15:48 - loss: 2.5568 - regression_loss: 1.8960 - classification_loss 8003/10000 [=======================>......] - ETA: 15:48 - loss: 2.5569 - regression_loss: 1.8960 - classification_loss 8004/10000 [=======================>......] - ETA: 15:47 - loss: 2.5567 - regression_loss: 1.8959 - classification_loss 8005/10000 [=======================>......] - ETA: 15:47 - loss: 2.5568 - regression_loss: 1.8960 - classification_loss 8006/10000 [=======================>......] - ETA: 15:47 - loss: 2.5567 - regression_loss: 1.8959 - classification_loss 8007/10000 [=======================>......] - ETA: 15:46 - loss: 2.5566 - regression_loss: 1.8958 - classification_loss 8008/10000 [=======================>......] - ETA: 15:46 - loss: 2.5566 - regression_loss: 1.8959 - classification_loss 8009/10000 [=======================>......] - ETA: 15:45 - loss: 2.5566 - regression_loss: 1.8959 - classification_loss 8010/10000 [=======================>......] - ETA: 15:45 - loss: 2.5565 - regression_loss: 1.8958 - classification_loss 8011/10000 [=======================>......] - ETA: 15:44 - loss: 2.5564 - regression_loss: 1.8957 - classification_loss 8012/10000 [=======================>......] - ETA: 15:44 - loss: 2.5563 - regression_loss: 1.8957 - classification_loss 8013/10000 [=======================>......] - ETA: 15:43 - loss: 2.5563 - regression_loss: 1.8956 - classification_loss 8014/10000 [=======================>......] - ETA: 15:43 - loss: 2.5562 - regression_loss: 1.8956 - classification_loss 8015/10000 [=======================>......] - ETA: 15:42 - loss: 2.5561 - regression_loss: 1.8955 - classification_loss 8016/10000 [=======================>......] - ETA: 15:42 - loss: 2.5561 - regression_loss: 1.8955 - classification_loss 8017/10000 [=======================>......] - ETA: 15:41 - loss: 2.5560 - regression_loss: 1.8954 - classification_loss 8018/10000 [=======================>......] - ETA: 15:41 - loss: 2.5558 - regression_loss: 1.8953 - classification_loss 8019/10000 [=======================>......] - ETA: 15:40 - loss: 2.5558 - regression_loss: 1.8953 - classification_loss 8020/10000 [=======================>......] - ETA: 15:40 - loss: 2.5557 - regression_loss: 1.8952 - classification_loss 8021/10000 [=======================>......] - ETA: 15:39 - loss: 2.5557 - regression_loss: 1.8952 - classification_loss 8022/10000 [=======================>......] - ETA: 15:39 - loss: 2.5556 - regression_loss: 1.8951 - classification_loss 8023/10000 [=======================>......] - ETA: 15:38 - loss: 2.5556 - regression_loss: 1.8951 - classification_loss 8024/10000 [=======================>......] - ETA: 15:38 - loss: 2.5555 - regression_loss: 1.8951 - classification_loss 8025/10000 [=======================>......] - ETA: 15:37 - loss: 2.5555 - regression_loss: 1.8950 - classification_loss 8026/10000 [=======================>......] - ETA: 15:37 - loss: 2.5553 - regression_loss: 1.8949 - classification_loss 8027/10000 [=======================>......] - ETA: 15:36 - loss: 2.5553 - regression_loss: 1.8949 - classification_loss 8028/10000 [=======================>......] - ETA: 15:36 - loss: 2.5552 - regression_loss: 1.8948 - classification_loss 8029/10000 [=======================>......] - ETA: 15:35 - loss: 2.5552 - regression_loss: 1.8948 - classification_loss 8030/10000 [=======================>......] - ETA: 15:35 - loss: 2.5551 - regression_loss: 1.8948 - classification_loss 8031/10000 [=======================>......] - ETA: 15:35 - loss: 2.5550 - regression_loss: 1.8947 - classification_loss 8032/10000 [=======================>......] - ETA: 15:34 - loss: 2.5550 - regression_loss: 1.8947 - classification_loss 8033/10000 [=======================>......] - ETA: 15:34 - loss: 2.5549 - regression_loss: 1.8946 - classification_loss 8034/10000 [=======================>......] - ETA: 15:33 - loss: 2.5549 - regression_loss: 1.8946 - classification_loss 8035/10000 [=======================>......] - ETA: 15:33 - loss: 2.5548 - regression_loss: 1.8946 - classification_loss 8036/10000 [=======================>......] - ETA: 15:32 - loss: 2.5547 - regression_loss: 1.8946 - classification_loss 8037/10000 [=======================>......] - ETA: 15:32 - loss: 2.5546 - regression_loss: 1.8945 - classification_loss 8038/10000 [=======================>......] - ETA: 15:31 - loss: 2.5546 - regression_loss: 1.8945 - classification_loss 8039/10000 [=======================>......] - ETA: 15:31 - loss: 2.5546 - regression_loss: 1.8945 - classification_loss 8040/10000 [=======================>......] - ETA: 15:30 - loss: 2.5545 - regression_loss: 1.8945 - classification_loss 8041/10000 [=======================>......] - ETA: 15:30 - loss: 2.5545 - regression_loss: 1.8945 - classification_loss 8042/10000 [=======================>......] - ETA: 15:29 - loss: 2.5545 - regression_loss: 1.8945 - classification_loss 8043/10000 [=======================>......] - ETA: 15:29 - loss: 2.5545 - regression_loss: 1.8946 - classification_loss 8044/10000 [=======================>......] - ETA: 15:28 - loss: 2.5545 - regression_loss: 1.8945 - classification_loss 8045/10000 [=======================>......] - ETA: 15:28 - loss: 2.5545 - regression_loss: 1.8945 - classification_loss 8046/10000 [=======================>......] - ETA: 15:27 - loss: 2.5543 - regression_loss: 1.8944 - classification_loss 8047/10000 [=======================>......] - ETA: 15:27 - loss: 2.5543 - regression_loss: 1.8944 - classification_loss 8048/10000 [=======================>......] - ETA: 15:26 - loss: 2.5543 - regression_loss: 1.8944 - classification_loss 8049/10000 [=======================>......] - ETA: 15:26 - loss: 2.5542 - regression_loss: 1.8944 - classification_loss 8050/10000 [=======================>......] - ETA: 15:25 - loss: 2.5542 - regression_loss: 1.8943 - classification_loss 8051/10000 [=======================>......] - ETA: 15:25 - loss: 2.5541 - regression_loss: 1.8943 - classification_loss 8052/10000 [=======================>......] - ETA: 15:24 - loss: 2.5542 - regression_loss: 1.8944 - classification_loss 8053/10000 [=======================>......] - ETA: 15:24 - loss: 2.5541 - regression_loss: 1.8944 - classification_loss 8054/10000 [=======================>......] - ETA: 15:24 - loss: 2.5540 - regression_loss: 1.8943 - classification_loss 8055/10000 [=======================>......] - ETA: 15:23 - loss: 2.5539 - regression_loss: 1.8942 - classification_loss 8056/10000 [=======================>......] - ETA: 15:23 - loss: 2.5539 - regression_loss: 1.8942 - classification_loss 8057/10000 [=======================>......] - ETA: 15:22 - loss: 2.5539 - regression_loss: 1.8942 - classification_loss 8058/10000 [=======================>......] - ETA: 15:22 - loss: 2.5539 - regression_loss: 1.8942 - classification_loss 8059/10000 [=======================>......] - ETA: 15:21 - loss: 2.5539 - regression_loss: 1.8942 - classification_loss 8060/10000 [=======================>......] - ETA: 15:21 - loss: 2.5538 - regression_loss: 1.8942 - classification_loss 8061/10000 [=======================>......] - ETA: 15:20 - loss: 2.5537 - regression_loss: 1.8941 - classification_loss 8062/10000 [=======================>......] - ETA: 15:20 - loss: 2.5538 - regression_loss: 1.8942 - classification_loss 8063/10000 [=======================>......] - ETA: 15:19 - loss: 2.5537 - regression_loss: 1.8941 - classification_loss 8064/10000 [=======================>......] - ETA: 15:19 - loss: 2.5537 - regression_loss: 1.8941 - classification_loss 8065/10000 [=======================>......] - ETA: 15:18 - loss: 2.5537 - regression_loss: 1.8941 - classification_loss 8066/10000 [=======================>......] - ETA: 15:18 - loss: 2.5537 - regression_loss: 1.8941 - classification_loss 8067/10000 [=======================>......] - ETA: 15:17 - loss: 2.5535 - regression_loss: 1.8940 - classification_loss 8068/10000 [=======================>......] - ETA: 15:17 - loss: 2.5534 - regression_loss: 1.8939 - classification_loss 8069/10000 [=======================>......] - ETA: 15:16 - loss: 2.5534 - regression_loss: 1.8939 - classification_loss 8070/10000 [=======================>......] - ETA: 15:16 - loss: 2.5534 - regression_loss: 1.8940 - classification_loss 8071/10000 [=======================>......] - ETA: 15:15 - loss: 2.5535 - regression_loss: 1.8940 - classification_loss 8072/10000 [=======================>......] - ETA: 15:15 - loss: 2.5536 - regression_loss: 1.8941 - classification_loss 8073/10000 [=======================>......] - ETA: 15:14 - loss: 2.5535 - regression_loss: 1.8941 - classification_loss 8074/10000 [=======================>......] - ETA: 15:14 - loss: 2.5535 - regression_loss: 1.8941 - classification_loss 8075/10000 [=======================>......] - ETA: 15:13 - loss: 2.5534 - regression_loss: 1.8940 - classification_loss 8076/10000 [=======================>......] - ETA: 15:13 - loss: 2.5534 - regression_loss: 1.8940 - classification_loss 8077/10000 [=======================>......] - ETA: 15:13 - loss: 2.5534 - regression_loss: 1.8940 - classification_loss 8078/10000 [=======================>......] - ETA: 15:12 - loss: 2.5534 - regression_loss: 1.8940 - classification_loss 8079/10000 [=======================>......] - ETA: 15:12 - loss: 2.5532 - regression_loss: 1.8939 - classification_loss 8080/10000 [=======================>......] - ETA: 15:11 - loss: 2.5531 - regression_loss: 1.8938 - classification_loss 8081/10000 [=======================>......] - ETA: 15:11 - loss: 2.5530 - regression_loss: 1.8938 - classification_loss 8082/10000 [=======================>......] - ETA: 15:10 - loss: 2.5529 - regression_loss: 1.8937 - classification_loss 8083/10000 [=======================>......] - ETA: 15:10 - loss: 2.5529 - regression_loss: 1.8937 - classification_loss 8084/10000 [=======================>......] - ETA: 15:09 - loss: 2.5529 - regression_loss: 1.8937 - classification_loss 8085/10000 [=======================>......] - ETA: 15:09 - loss: 2.5530 - regression_loss: 1.8938 - classification_loss 8086/10000 [=======================>......] - ETA: 15:08 - loss: 2.5529 - regression_loss: 1.8938 - classification_loss 8087/10000 [=======================>......] - ETA: 15:08 - loss: 2.5528 - regression_loss: 1.8937 - classification_loss 8088/10000 [=======================>......] - ETA: 15:07 - loss: 2.5528 - regression_loss: 1.8937 - classification_loss 8089/10000 [=======================>......] - ETA: 15:07 - loss: 2.5527 - regression_loss: 1.8937 - classification_loss 8090/10000 [=======================>......] - ETA: 15:06 - loss: 2.5527 - regression_loss: 1.8936 - classification_loss 8091/10000 [=======================>......] - ETA: 15:06 - loss: 2.5527 - regression_loss: 1.8936 - classification_loss 8092/10000 [=======================>......] - ETA: 15:05 - loss: 2.5526 - regression_loss: 1.8936 - classification_loss 8093/10000 [=======================>......] - ETA: 15:05 - loss: 2.5526 - regression_loss: 1.8935 - classification_loss 8094/10000 [=======================>......] - ETA: 15:04 - loss: 2.5525 - regression_loss: 1.8935 - classification_loss 8095/10000 [=======================>......] - ETA: 15:04 - loss: 2.5524 - regression_loss: 1.8935 - classification_loss 8096/10000 [=======================>......] - ETA: 15:03 - loss: 2.5523 - regression_loss: 1.8934 - classification_loss 8097/10000 [=======================>......] - ETA: 15:03 - loss: 2.5522 - regression_loss: 1.8933 - classification_loss 8098/10000 [=======================>......] - ETA: 15:03 - loss: 2.5522 - regression_loss: 1.8933 - classification_loss 8099/10000 [=======================>......] - ETA: 15:02 - loss: 2.5522 - regression_loss: 1.8933 - classification_loss 8100/10000 [=======================>......] - ETA: 15:02 - loss: 2.5521 - regression_loss: 1.8933 - classification_loss 8101/10000 [=======================>......] - ETA: 15:01 - loss: 2.5521 - regression_loss: 1.8933 - classification_loss 8102/10000 [=======================>......] - ETA: 15:01 - loss: 2.5522 - regression_loss: 1.8933 - classification_loss 8103/10000 [=======================>......] - ETA: 15:00 - loss: 2.5522 - regression_loss: 1.8933 - classification_loss 8104/10000 [=======================>......] - ETA: 15:00 - loss: 2.5520 - regression_loss: 1.8932 - classification_loss 8105/10000 [=======================>......] - ETA: 14:59 - loss: 2.5519 - regression_loss: 1.8932 - classification_loss 8106/10000 [=======================>......] - ETA: 14:59 - loss: 2.5518 - regression_loss: 1.8930 - classification_loss 8107/10000 [=======================>......] - ETA: 14:58 - loss: 2.5517 - regression_loss: 1.8929 - classification_loss 8108/10000 [=======================>......] - ETA: 14:58 - loss: 2.5515 - regression_loss: 1.8928 - classification_loss 8109/10000 [=======================>......] - ETA: 14:57 - loss: 2.5515 - regression_loss: 1.8928 - classification_loss 8110/10000 [=======================>......] - ETA: 14:57 - loss: 2.5515 - regression_loss: 1.8929 - classification_loss 8111/10000 [=======================>......] - ETA: 14:56 - loss: 2.5515 - regression_loss: 1.8929 - classification_loss 8112/10000 [=======================>......] - ETA: 14:56 - loss: 2.5516 - regression_loss: 1.8929 - classification_loss 8113/10000 [=======================>......] - ETA: 14:55 - loss: 2.5515 - regression_loss: 1.8929 - classification_loss 8114/10000 [=======================>......] - ETA: 14:55 - loss: 2.5515 - regression_loss: 1.8929 - classification_loss 8115/10000 [=======================>......] - ETA: 14:54 - loss: 2.5513 - regression_loss: 1.8928 - classification_loss 8116/10000 [=======================>......] - ETA: 14:54 - loss: 2.5512 - regression_loss: 1.8927 - classification_loss 8117/10000 [=======================>......] - ETA: 14:53 - loss: 2.5512 - regression_loss: 1.8927 - classification_loss 8118/10000 [=======================>......] - ETA: 14:53 - loss: 2.5511 - regression_loss: 1.8927 - classification_loss 8119/10000 [=======================>......] - ETA: 14:52 - loss: 2.5512 - regression_loss: 1.8928 - classification_loss 8120/10000 [=======================>......] - ETA: 14:52 - loss: 2.5511 - regression_loss: 1.8927 - classification_loss 8121/10000 [=======================>......] - ETA: 14:52 - loss: 2.5510 - regression_loss: 1.8926 - classification_loss 8122/10000 [=======================>......] - ETA: 14:51 - loss: 2.5509 - regression_loss: 1.8925 - classification_loss 8123/10000 [=======================>......] - ETA: 14:51 - loss: 2.5508 - regression_loss: 1.8924 - classification_loss 8124/10000 [=======================>......] - ETA: 14:50 - loss: 2.5507 - regression_loss: 1.8924 - classification_loss 8125/10000 [=======================>......] - ETA: 14:50 - loss: 2.5505 - regression_loss: 1.8923 - classification_loss 8126/10000 [=======================>......] - ETA: 14:49 - loss: 2.5505 - regression_loss: 1.8922 - classification_loss 8127/10000 [=======================>......] - ETA: 14:49 - loss: 2.5504 - regression_loss: 1.8922 - classification_loss 8128/10000 [=======================>......] - ETA: 14:48 - loss: 2.5504 - regression_loss: 1.8922 - classification_loss 8129/10000 [=======================>......] - ETA: 14:48 - loss: 2.5505 - regression_loss: 1.8923 - classification_loss 8130/10000 [=======================>......] - ETA: 14:47 - loss: 2.5505 - regression_loss: 1.8923 - classification_loss 8131/10000 [=======================>......] - ETA: 14:47 - loss: 2.5504 - regression_loss: 1.8923 - classification_loss 8132/10000 [=======================>......] - ETA: 14:46 - loss: 2.5503 - regression_loss: 1.8922 - classification_loss 8133/10000 [=======================>......] - ETA: 14:46 - loss: 2.5503 - regression_loss: 1.8922 - classification_loss 8134/10000 [=======================>......] - ETA: 14:45 - loss: 2.5501 - regression_loss: 1.8921 - classification_loss 8135/10000 [=======================>......] - ETA: 14:45 - loss: 2.5500 - regression_loss: 1.8920 - classification_loss 8136/10000 [=======================>......] - ETA: 14:44 - loss: 2.5501 - regression_loss: 1.8921 - classification_loss 8137/10000 [=======================>......] - ETA: 14:44 - loss: 2.5501 - regression_loss: 1.8921 - classification_loss 8138/10000 [=======================>......] - ETA: 14:43 - loss: 2.5500 - regression_loss: 1.8920 - classification_loss 8139/10000 [=======================>......] - ETA: 14:43 - loss: 2.5499 - regression_loss: 1.8919 - classification_loss 8140/10000 [=======================>......] - ETA: 14:42 - loss: 2.5498 - regression_loss: 1.8919 - classification_loss 8141/10000 [=======================>......] - ETA: 14:42 - loss: 2.5498 - regression_loss: 1.8919 - classification_loss 8142/10000 [=======================>......] - ETA: 14:41 - loss: 2.5497 - regression_loss: 1.8917 - classification_loss 8143/10000 [=======================>......] - ETA: 14:41 - loss: 2.5496 - regression_loss: 1.8917 - classification_loss 8144/10000 [=======================>......] - ETA: 14:40 - loss: 2.5496 - regression_loss: 1.8917 - classification_loss 8145/10000 [=======================>......] - ETA: 14:40 - loss: 2.5496 - regression_loss: 1.8917 - classification_loss 8146/10000 [=======================>......] - ETA: 14:40 - loss: 2.5496 - regression_loss: 1.8917 - classification_loss 8147/10000 [=======================>......] - ETA: 14:39 - loss: 2.5495 - regression_loss: 1.8917 - classification_loss 8148/10000 [=======================>......] - ETA: 14:39 - loss: 2.5494 - regression_loss: 1.8916 - classification_loss 8149/10000 [=======================>......] - ETA: 14:38 - loss: 2.5495 - regression_loss: 1.8917 - classification_loss 8150/10000 [=======================>......] - ETA: 14:38 - loss: 2.5496 - regression_loss: 1.8918 - classification_loss 8151/10000 [=======================>......] - ETA: 14:37 - loss: 2.5495 - regression_loss: 1.8917 - classification_loss 8152/10000 [=======================>......] - ETA: 14:37 - loss: 2.5495 - regression_loss: 1.8918 - classification_loss 8153/10000 [=======================>......] - ETA: 14:36 - loss: 2.5494 - regression_loss: 1.8917 - classification_loss 8154/10000 [=======================>......] - ETA: 14:36 - loss: 2.5494 - regression_loss: 1.8917 - classification_loss 8155/10000 [=======================>......] - ETA: 14:35 - loss: 2.5494 - regression_loss: 1.8917 - classification_loss 8156/10000 [=======================>......] - ETA: 14:35 - loss: 2.5494 - regression_loss: 1.8917 - classification_loss 8157/10000 [=======================>......] - ETA: 14:34 - loss: 2.5494 - regression_loss: 1.8917 - classification_loss 8158/10000 [=======================>......] - ETA: 14:34 - loss: 2.5493 - regression_loss: 1.8917 - classification_loss 8159/10000 [=======================>......] - ETA: 14:33 - loss: 2.5492 - regression_loss: 1.8916 - classification_loss 8160/10000 [=======================>......] - ETA: 14:33 - loss: 2.5491 - regression_loss: 1.8916 - classification_loss 8161/10000 [=======================>......] - ETA: 14:32 - loss: 2.5491 - regression_loss: 1.8916 - classification_loss 8162/10000 [=======================>......] - ETA: 14:32 - loss: 2.5489 - regression_loss: 1.8914 - classification_loss 8163/10000 [=======================>......] - ETA: 14:31 - loss: 2.5487 - regression_loss: 1.8912 - classification_loss 8164/10000 [=======================>......] - ETA: 14:31 - loss: 2.5487 - regression_loss: 1.8912 - classification_loss 8165/10000 [=======================>......] - ETA: 14:30 - loss: 2.5487 - regression_loss: 1.8912 - classification_loss 8166/10000 [=======================>......] - ETA: 14:30 - loss: 2.5486 - regression_loss: 1.8912 - classification_loss 8167/10000 [=======================>......] - ETA: 14:29 - loss: 2.5485 - regression_loss: 1.8911 - classification_loss 8168/10000 [=======================>......] - ETA: 14:29 - loss: 2.5483 - regression_loss: 1.8909 - classification_loss 8169/10000 [=======================>......] - ETA: 14:29 - loss: 2.5482 - regression_loss: 1.8909 - classification_loss 8170/10000 [=======================>......] - ETA: 14:28 - loss: 2.5482 - regression_loss: 1.8909 - classification_loss 8171/10000 [=======================>......] - ETA: 14:28 - loss: 2.5482 - regression_loss: 1.8909 - classification_loss 8172/10000 [=======================>......] - ETA: 14:27 - loss: 2.5482 - regression_loss: 1.8909 - classification_loss 8173/10000 [=======================>......] - ETA: 14:27 - loss: 2.5481 - regression_loss: 1.8909 - classification_loss 8174/10000 [=======================>......] - ETA: 14:26 - loss: 2.5481 - regression_loss: 1.8909 - classification_loss 8175/10000 [=======================>......] - ETA: 14:26 - loss: 2.5480 - regression_loss: 1.8908 - classification_loss 8176/10000 [=======================>......] - ETA: 14:25 - loss: 2.5478 - regression_loss: 1.8907 - classification_loss 8177/10000 [=======================>......] - ETA: 14:25 - loss: 2.5477 - regression_loss: 1.8906 - classification_loss 8178/10000 [=======================>......] - ETA: 14:24 - loss: 2.5477 - regression_loss: 1.8906 - classification_loss 8179/10000 [=======================>......] - ETA: 14:24 - loss: 2.5476 - regression_loss: 1.8906 - classification_loss 8180/10000 [=======================>......] - ETA: 14:23 - loss: 2.5476 - regression_loss: 1.8906 - classification_loss 8181/10000 [=======================>......] - ETA: 14:23 - loss: 2.5474 - regression_loss: 1.8904 - classification_loss 8182/10000 [=======================>......] - ETA: 14:22 - loss: 2.5473 - regression_loss: 1.8904 - classification_loss 8183/10000 [=======================>......] - ETA: 14:22 - loss: 2.5472 - regression_loss: 1.8903 - classification_loss 8184/10000 [=======================>......] - ETA: 14:21 - loss: 2.5471 - regression_loss: 1.8902 - classification_loss 8185/10000 [=======================>......] - ETA: 14:21 - loss: 2.5471 - regression_loss: 1.8902 - classification_loss 8186/10000 [=======================>......] - ETA: 14:20 - loss: 2.5471 - regression_loss: 1.8901 - classification_loss 8187/10000 [=======================>......] - ETA: 14:20 - loss: 2.5469 - regression_loss: 1.8900 - classification_loss 8188/10000 [=======================>......] - ETA: 14:19 - loss: 2.5467 - regression_loss: 1.8899 - classification_loss 8189/10000 [=======================>......] - ETA: 14:19 - loss: 2.5467 - regression_loss: 1.8899 - classification_loss 8190/10000 [=======================>......] - ETA: 14:18 - loss: 2.5468 - regression_loss: 1.8899 - classification_loss 8191/10000 [=======================>......] - ETA: 14:18 - loss: 2.5466 - regression_loss: 1.8898 - classification_loss 8192/10000 [=======================>......] - ETA: 14:17 - loss: 2.5465 - regression_loss: 1.8897 - classification_loss 8193/10000 [=======================>......] - ETA: 14:17 - loss: 2.5465 - regression_loss: 1.8897 - classification_loss 8194/10000 [=======================>......] - ETA: 14:17 - loss: 2.5465 - regression_loss: 1.8897 - classification_loss 8195/10000 [=======================>......] - ETA: 14:16 - loss: 2.5464 - regression_loss: 1.8897 - classification_loss 8196/10000 [=======================>......] - ETA: 14:16 - loss: 2.5463 - regression_loss: 1.8896 - classification_loss 8197/10000 [=======================>......] - ETA: 14:15 - loss: 2.5463 - regression_loss: 1.8896 - classification_loss 8198/10000 [=======================>......] - ETA: 14:15 - loss: 2.5463 - regression_loss: 1.8896 - classification_loss 8199/10000 [=======================>......] - ETA: 14:14 - loss: 2.5463 - regression_loss: 1.8896 - classification_loss 8200/10000 [=======================>......] - ETA: 14:14 - loss: 2.5461 - regression_loss: 1.8894 - classification_loss 8201/10000 [=======================>......] - ETA: 14:13 - loss: 2.5460 - regression_loss: 1.8894 - classification_loss 8202/10000 [=======================>......] - ETA: 14:13 - loss: 2.5459 - regression_loss: 1.8893 - classification_loss 8203/10000 [=======================>......] - ETA: 14:12 - loss: 2.5458 - regression_loss: 1.8892 - classification_loss 8204/10000 [=======================>......] - ETA: 14:12 - loss: 2.5457 - regression_loss: 1.8892 - classification_loss 8205/10000 [=======================>......] - ETA: 14:11 - loss: 2.5457 - regression_loss: 1.8892 - classification_loss 8206/10000 [=======================>......] - ETA: 14:11 - loss: 2.5456 - regression_loss: 1.8891 - classification_loss 8207/10000 [=======================>......] - ETA: 14:10 - loss: 2.5455 - regression_loss: 1.8891 - classification_loss 8208/10000 [=======================>......] - ETA: 14:10 - loss: 2.5455 - regression_loss: 1.8890 - classification_loss 8209/10000 [=======================>......] - ETA: 14:09 - loss: 2.5454 - regression_loss: 1.8889 - classification_loss 8210/10000 [=======================>......] - ETA: 14:09 - loss: 2.5453 - regression_loss: 1.8888 - classification_loss 8211/10000 [=======================>......] - ETA: 14:08 - loss: 2.5453 - regression_loss: 1.8888 - classification_loss 8212/10000 [=======================>......] - ETA: 14:08 - loss: 2.5452 - regression_loss: 1.8888 - classification_loss 8213/10000 [=======================>......] - ETA: 14:07 - loss: 2.5452 - regression_loss: 1.8888 - classification_loss 8214/10000 [=======================>......] - ETA: 14:07 - loss: 2.5450 - regression_loss: 1.8887 - classification_loss 8215/10000 [=======================>......] - ETA: 14:07 - loss: 2.5451 - regression_loss: 1.8887 - classification_loss 8216/10000 [=======================>......] - ETA: 14:06 - loss: 2.5450 - regression_loss: 1.8887 - classification_loss 8217/10000 [=======================>......] - ETA: 14:06 - loss: 2.5449 - regression_loss: 1.8886 - classification_loss 8218/10000 [=======================>......] - ETA: 14:05 - loss: 2.5449 - regression_loss: 1.8886 - classification_loss 8219/10000 [=======================>......] - ETA: 14:05 - loss: 2.5448 - regression_loss: 1.8885 - classification_loss 8220/10000 [=======================>......] - ETA: 14:04 - loss: 2.5447 - regression_loss: 1.8885 - classification_loss 8221/10000 [=======================>......] - ETA: 14:04 - loss: 2.5446 - regression_loss: 1.8884 - classification_loss 8222/10000 [=======================>......] - ETA: 14:03 - loss: 2.5445 - regression_loss: 1.8883 - classification_loss 8223/10000 [=======================>......] - ETA: 14:03 - loss: 2.5445 - regression_loss: 1.8883 - classification_loss 8224/10000 [=======================>......] - ETA: 14:02 - loss: 2.5443 - regression_loss: 1.8882 - classification_loss 8225/10000 [=======================>......] - ETA: 14:02 - loss: 2.5443 - regression_loss: 1.8882 - classification_loss 8226/10000 [=======================>......] - ETA: 14:01 - loss: 2.5442 - regression_loss: 1.8881 - classification_loss 8227/10000 [=======================>......] - ETA: 14:01 - loss: 2.5440 - regression_loss: 1.8880 - classification_loss 8228/10000 [=======================>......] - ETA: 14:00 - loss: 2.5439 - regression_loss: 1.8879 - classification_loss 8229/10000 [=======================>......] - ETA: 14:00 - loss: 2.5439 - regression_loss: 1.8879 - classification_loss 8230/10000 [=======================>......] - ETA: 13:59 - loss: 2.5439 - regression_loss: 1.8878 - classification_loss 8231/10000 [=======================>......] - ETA: 13:59 - loss: 2.5438 - regression_loss: 1.8878 - classification_loss 8232/10000 [=======================>......] - ETA: 13:58 - loss: 2.5438 - regression_loss: 1.8878 - classification_loss 8233/10000 [=======================>......] - ETA: 13:58 - loss: 2.5437 - regression_loss: 1.8877 - classification_loss 8234/10000 [=======================>......] - ETA: 13:57 - loss: 2.5437 - regression_loss: 1.8877 - classification_loss 8235/10000 [=======================>......] - ETA: 13:57 - loss: 2.5436 - regression_loss: 1.8877 - classification_loss 8236/10000 [=======================>......] - ETA: 13:56 - loss: 2.5437 - regression_loss: 1.8877 - classification_loss 8237/10000 [=======================>......] - ETA: 13:56 - loss: 2.5436 - regression_loss: 1.8877 - classification_loss 8238/10000 [=======================>......] - ETA: 13:55 - loss: 2.5436 - regression_loss: 1.8876 - classification_loss 8239/10000 [=======================>......] - ETA: 13:55 - loss: 2.5435 - regression_loss: 1.8876 - classification_loss 8240/10000 [=======================>......] - ETA: 13:55 - loss: 2.5434 - regression_loss: 1.8876 - classification_loss 8241/10000 [=======================>......] - ETA: 13:54 - loss: 2.5433 - regression_loss: 1.8875 - classification_loss 8242/10000 [=======================>......] - ETA: 13:54 - loss: 2.5432 - regression_loss: 1.8873 - classification_loss 8243/10000 [=======================>......] - ETA: 13:53 - loss: 2.5430 - regression_loss: 1.8872 - classification_loss 8244/10000 [=======================>......] - ETA: 13:53 - loss: 2.5429 - regression_loss: 1.8872 - classification_loss 8245/10000 [=======================>......] - ETA: 13:52 - loss: 2.5430 - regression_loss: 1.8872 - classification_loss 8246/10000 [=======================>......] - ETA: 13:52 - loss: 2.5430 - regression_loss: 1.8872 - classification_loss 8247/10000 [=======================>......] - ETA: 13:51 - loss: 2.5429 - regression_loss: 1.8872 - classification_loss 8248/10000 [=======================>......] - ETA: 13:51 - loss: 2.5429 - regression_loss: 1.8872 - classification_loss 8249/10000 [=======================>......] - ETA: 13:50 - loss: 2.5428 - regression_loss: 1.8872 - classification_loss 8250/10000 [=======================>......] - ETA: 13:50 - loss: 2.5428 - regression_loss: 1.8871 - classification_loss 8251/10000 [=======================>......] - ETA: 13:49 - loss: 2.5427 - regression_loss: 1.8871 - classification_loss 8252/10000 [=======================>......] - ETA: 13:49 - loss: 2.5427 - regression_loss: 1.8870 - classification_loss 8253/10000 [=======================>......] - ETA: 13:48 - loss: 2.5426 - regression_loss: 1.8870 - classification_loss 8254/10000 [=======================>......] - ETA: 13:48 - loss: 2.5427 - regression_loss: 1.8871 - classification_loss 8255/10000 [=======================>......] - ETA: 13:47 - loss: 2.5427 - regression_loss: 1.8871 - classification_loss 8256/10000 [=======================>......] - ETA: 13:47 - loss: 2.5426 - regression_loss: 1.8870 - classification_loss 8257/10000 [=======================>......] - ETA: 13:46 - loss: 2.5425 - regression_loss: 1.8869 - classification_loss 8258/10000 [=======================>......] - ETA: 13:46 - loss: 2.5425 - regression_loss: 1.8869 - classification_loss 8259/10000 [=======================>......] - ETA: 13:45 - loss: 2.5424 - regression_loss: 1.8868 - classification_loss 8260/10000 [=======================>......] - ETA: 13:45 - loss: 2.5424 - regression_loss: 1.8869 - classification_loss 8261/10000 [=======================>......] - ETA: 13:45 - loss: 2.5425 - regression_loss: 1.8869 - classification_loss 8262/10000 [=======================>......] - ETA: 13:44 - loss: 2.5425 - regression_loss: 1.8869 - classification_loss 8263/10000 [=======================>......] - ETA: 13:44 - loss: 2.5424 - regression_loss: 1.8869 - classification_loss 8264/10000 [=======================>......] - ETA: 13:43 - loss: 2.5423 - regression_loss: 1.8868 - classification_loss 8265/10000 [=======================>......] - ETA: 13:43 - loss: 2.5422 - regression_loss: 1.8867 - classification_loss 8266/10000 [=======================>......] - ETA: 13:42 - loss: 2.5421 - regression_loss: 1.8867 - classification_loss 8267/10000 [=======================>......] - ETA: 13:42 - loss: 2.5421 - regression_loss: 1.8867 - classification_loss 8268/10000 [=======================>......] - ETA: 13:41 - loss: 2.5420 - regression_loss: 1.8866 - classification_loss 8269/10000 [=======================>......] - ETA: 13:41 - loss: 2.5419 - regression_loss: 1.8865 - classification_loss 8270/10000 [=======================>......] - ETA: 13:40 - loss: 2.5419 - regression_loss: 1.8865 - classification_loss 8271/10000 [=======================>......] - ETA: 13:40 - loss: 2.5418 - regression_loss: 1.8864 - classification_loss 8272/10000 [=======================>......] - ETA: 13:39 - loss: 2.5417 - regression_loss: 1.8864 - classification_loss 8273/10000 [=======================>......] - ETA: 13:39 - loss: 2.5417 - regression_loss: 1.8864 - classification_loss 8274/10000 [=======================>......] - ETA: 13:38 - loss: 2.5416 - regression_loss: 1.8862 - classification_loss 8275/10000 [=======================>......] - ETA: 13:38 - loss: 2.5415 - regression_loss: 1.8862 - classification_loss 8276/10000 [=======================>......] - ETA: 13:37 - loss: 2.5413 - regression_loss: 1.8860 - classification_loss 8277/10000 [=======================>......] - ETA: 13:37 - loss: 2.5413 - regression_loss: 1.8861 - classification_loss 8278/10000 [=======================>......] - ETA: 13:36 - loss: 2.5413 - regression_loss: 1.8861 - classification_loss 8279/10000 [=======================>......] - ETA: 13:36 - loss: 2.5414 - regression_loss: 1.8861 - classification_loss 8280/10000 [=======================>......] - ETA: 13:35 - loss: 2.5413 - regression_loss: 1.8861 - classification_loss 8281/10000 [=======================>......] - ETA: 13:35 - loss: 2.5413 - regression_loss: 1.8861 - classification_loss 8282/10000 [=======================>......] - ETA: 13:35 - loss: 2.5412 - regression_loss: 1.8859 - classification_loss 8283/10000 [=======================>......] - ETA: 13:34 - loss: 2.5411 - regression_loss: 1.8858 - classification_loss 8284/10000 [=======================>......] - ETA: 13:34 - loss: 2.5410 - regression_loss: 1.8858 - classification_loss 8285/10000 [=======================>......] - ETA: 13:33 - loss: 2.5410 - regression_loss: 1.8858 - classification_loss 8286/10000 [=======================>......] - ETA: 13:33 - loss: 2.5409 - regression_loss: 1.8857 - classification_loss 8287/10000 [=======================>......] - ETA: 13:32 - loss: 2.5408 - regression_loss: 1.8856 - classification_loss 8288/10000 [=======================>......] - ETA: 13:32 - loss: 2.5407 - regression_loss: 1.8856 - classification_loss 8289/10000 [=======================>......] - ETA: 13:31 - loss: 2.5406 - regression_loss: 1.8854 - classification_loss 8290/10000 [=======================>......] - ETA: 13:31 - loss: 2.5405 - regression_loss: 1.8854 - classification_loss 8291/10000 [=======================>......] - ETA: 13:30 - loss: 2.5404 - regression_loss: 1.8853 - classification_loss 8292/10000 [=======================>......] - ETA: 13:30 - loss: 2.5403 - regression_loss: 1.8852 - classification_loss 8293/10000 [=======================>......] - ETA: 13:29 - loss: 2.5402 - regression_loss: 1.8852 - classification_loss 8294/10000 [=======================>......] - ETA: 13:29 - loss: 2.5402 - regression_loss: 1.8852 - classification_loss 8295/10000 [=======================>......] - ETA: 13:28 - loss: 2.5402 - regression_loss: 1.8852 - classification_loss 8296/10000 [=======================>......] - ETA: 13:28 - loss: 2.5400 - regression_loss: 1.8850 - classification_loss 8297/10000 [=======================>......] - ETA: 13:27 - loss: 2.5400 - regression_loss: 1.8851 - classification_loss 8298/10000 [=======================>......] - ETA: 13:27 - loss: 2.5399 - regression_loss: 1.8850 - classification_loss 8299/10000 [=======================>......] - ETA: 13:26 - loss: 2.5398 - regression_loss: 1.8849 - classification_loss 8300/10000 [=======================>......] - ETA: 13:26 - loss: 2.5397 - regression_loss: 1.8848 - classification_loss 8301/10000 [=======================>......] - ETA: 13:25 - loss: 2.5395 - regression_loss: 1.8847 - classification_loss 8302/10000 [=======================>......] - ETA: 13:25 - loss: 2.5395 - regression_loss: 1.8847 - classification_loss 8303/10000 [=======================>......] - ETA: 13:25 - loss: 2.5394 - regression_loss: 1.8846 - classification_loss 8304/10000 [=======================>......] - ETA: 13:24 - loss: 2.5394 - regression_loss: 1.8845 - classification_loss 8305/10000 [=======================>......] - ETA: 13:24 - loss: 2.5394 - regression_loss: 1.8845 - classification_loss 8306/10000 [=======================>......] - ETA: 13:23 - loss: 2.5392 - regression_loss: 1.8844 - classification_loss 8307/10000 [=======================>......] - ETA: 13:23 - loss: 2.5392 - regression_loss: 1.8844 - classification_loss 8308/10000 [=======================>......] - ETA: 13:22 - loss: 2.5391 - regression_loss: 1.8843 - classification_loss 8309/10000 [=======================>......] - ETA: 13:22 - loss: 2.5390 - regression_loss: 1.8842 - classification_loss 8310/10000 [=======================>......] - ETA: 13:21 - loss: 2.5389 - regression_loss: 1.8842 - classification_loss 8311/10000 [=======================>......] - ETA: 13:21 - loss: 2.5388 - regression_loss: 1.8841 - classification_loss 8312/10000 [=======================>......] - ETA: 13:20 - loss: 2.5387 - regression_loss: 1.8840 - classification_loss 8313/10000 [=======================>......] - ETA: 13:20 - loss: 2.5387 - regression_loss: 1.8840 - classification_loss 8314/10000 [=======================>......] - ETA: 13:19 - loss: 2.5385 - regression_loss: 1.8839 - classification_loss 8315/10000 [=======================>......] - ETA: 13:19 - loss: 2.5385 - regression_loss: 1.8838 - classification_loss 8316/10000 [=======================>......] - ETA: 13:18 - loss: 2.5385 - regression_loss: 1.8838 - classification_loss 8317/10000 [=======================>......] - ETA: 13:18 - loss: 2.5385 - regression_loss: 1.8838 - classification_loss 8318/10000 [=======================>......] - ETA: 13:17 - loss: 2.5384 - regression_loss: 1.8838 - classification_loss 8319/10000 [=======================>......] - ETA: 13:17 - loss: 2.5383 - regression_loss: 1.8837 - classification_loss 8320/10000 [=======================>......] - ETA: 13:16 - loss: 2.5383 - regression_loss: 1.8837 - classification_loss 8321/10000 [=======================>......] - ETA: 13:16 - loss: 2.5382 - regression_loss: 1.8837 - classification_loss 8322/10000 [=======================>......] - ETA: 13:15 - loss: 2.5383 - regression_loss: 1.8837 - classification_loss 8323/10000 [=======================>......] - ETA: 13:15 - loss: 2.5383 - regression_loss: 1.8837 - classification_loss 8324/10000 [=======================>......] - ETA: 13:14 - loss: 2.5382 - regression_loss: 1.8837 - classification_loss 8325/10000 [=======================>......] - ETA: 13:14 - loss: 2.5382 - regression_loss: 1.8837 - classification_loss 8326/10000 [=======================>......] - ETA: 13:14 - loss: 2.5381 - regression_loss: 1.8837 - classification_loss 8327/10000 [=======================>......] - ETA: 13:13 - loss: 2.5381 - regression_loss: 1.8837 - classification_loss 8328/10000 [=======================>......] - ETA: 13:13 - loss: 2.5381 - regression_loss: 1.8837 - classification_loss 8329/10000 [=======================>......] - ETA: 13:12 - loss: 2.5381 - regression_loss: 1.8836 - classification_loss 8330/10000 [=======================>......] - ETA: 13:12 - loss: 2.5381 - regression_loss: 1.8836 - classification_loss 8331/10000 [=======================>......] - ETA: 13:11 - loss: 2.5380 - regression_loss: 1.8836 - classification_loss 8332/10000 [=======================>......] - ETA: 13:11 - loss: 2.5379 - regression_loss: 1.8835 - classification_loss 8333/10000 [=======================>......] - ETA: 13:10 - loss: 2.5378 - regression_loss: 1.8834 - classification_loss 8334/10000 [========================>.....] - ETA: 13:10 - loss: 2.5376 - regression_loss: 1.8833 - classification_loss 8335/10000 [========================>.....] - ETA: 13:09 - loss: 2.5376 - regression_loss: 1.8833 - classification_loss 8336/10000 [========================>.....] - ETA: 13:09 - loss: 2.5375 - regression_loss: 1.8832 - classification_loss 8337/10000 [========================>.....] - ETA: 13:08 - loss: 2.5375 - regression_loss: 1.8832 - classification_loss 8338/10000 [========================>.....] - ETA: 13:08 - loss: 2.5374 - regression_loss: 1.8831 - classification_loss 8339/10000 [========================>.....] - ETA: 13:07 - loss: 2.5373 - regression_loss: 1.8830 - classification_loss 8340/10000 [========================>.....] - ETA: 13:07 - loss: 2.5371 - regression_loss: 1.8829 - classification_loss 8341/10000 [========================>.....] - ETA: 13:06 - loss: 2.5372 - regression_loss: 1.8830 - classification_loss 8342/10000 [========================>.....] - ETA: 13:06 - loss: 2.5371 - regression_loss: 1.8829 - classification_loss 8343/10000 [========================>.....] - ETA: 13:05 - loss: 2.5370 - regression_loss: 1.8828 - classification_loss 8344/10000 [========================>.....] - ETA: 13:05 - loss: 2.5369 - regression_loss: 1.8827 - classification_loss 8345/10000 [========================>.....] - ETA: 13:04 - loss: 2.5368 - regression_loss: 1.8827 - classification_loss 8346/10000 [========================>.....] - ETA: 13:04 - loss: 2.5368 - regression_loss: 1.8827 - classification_loss 8347/10000 [========================>.....] - ETA: 13:03 - loss: 2.5368 - regression_loss: 1.8827 - classification_loss 8348/10000 [========================>.....] - ETA: 13:03 - loss: 2.5368 - regression_loss: 1.8827 - classification_loss 8349/10000 [========================>.....] - ETA: 13:03 - loss: 2.5367 - regression_loss: 1.8826 - classification_loss 8350/10000 [========================>.....] - ETA: 13:02 - loss: 2.5368 - regression_loss: 1.8826 - classification_loss 8351/10000 [========================>.....] - ETA: 13:02 - loss: 2.5366 - regression_loss: 1.8825 - classification_loss 8352/10000 [========================>.....] - ETA: 13:01 - loss: 2.5366 - regression_loss: 1.8825 - classification_loss 8353/10000 [========================>.....] - ETA: 13:01 - loss: 2.5366 - regression_loss: 1.8825 - classification_loss 8354/10000 [========================>.....] - ETA: 13:00 - loss: 2.5365 - regression_loss: 1.8825 - classification_loss 8355/10000 [========================>.....] - ETA: 13:00 - loss: 2.5364 - regression_loss: 1.8824 - classification_loss 8356/10000 [========================>.....] - ETA: 12:59 - loss: 2.5363 - regression_loss: 1.8824 - classification_loss 8357/10000 [========================>.....] - ETA: 12:59 - loss: 2.5363 - regression_loss: 1.8824 - classification_loss 8358/10000 [========================>.....] - ETA: 12:58 - loss: 2.5362 - regression_loss: 1.8823 - classification_loss 8359/10000 [========================>.....] - ETA: 12:58 - loss: 2.5361 - regression_loss: 1.8822 - classification_loss 8360/10000 [========================>.....] - ETA: 12:57 - loss: 2.5360 - regression_loss: 1.8822 - classification_loss 8361/10000 [========================>.....] - ETA: 12:57 - loss: 2.5360 - regression_loss: 1.8822 - classification_loss 8362/10000 [========================>.....] - ETA: 12:56 - loss: 2.5361 - regression_loss: 1.8822 - classification_loss 8363/10000 [========================>.....] - ETA: 12:56 - loss: 2.5359 - regression_loss: 1.8821 - classification_loss 8364/10000 [========================>.....] - ETA: 12:55 - loss: 2.5359 - regression_loss: 1.8820 - classification_loss 8365/10000 [========================>.....] - ETA: 12:55 - loss: 2.5358 - regression_loss: 1.8820 - classification_loss 8366/10000 [========================>.....] - ETA: 12:54 - loss: 2.5358 - regression_loss: 1.8820 - classification_loss 8367/10000 [========================>.....] - ETA: 12:54 - loss: 2.5357 - regression_loss: 1.8819 - classification_loss 8368/10000 [========================>.....] - ETA: 12:53 - loss: 2.5356 - regression_loss: 1.8819 - classification_loss 8369/10000 [========================>.....] - ETA: 12:53 - loss: 2.5357 - regression_loss: 1.8819 - classification_loss 8370/10000 [========================>.....] - ETA: 12:53 - loss: 2.5356 - regression_loss: 1.8819 - classification_loss 8371/10000 [========================>.....] - ETA: 12:52 - loss: 2.5355 - regression_loss: 1.8818 - classification_loss 8372/10000 [========================>.....] - ETA: 12:52 - loss: 2.5356 - regression_loss: 1.8818 - classification_loss 8373/10000 [========================>.....] - ETA: 12:51 - loss: 2.5356 - regression_loss: 1.8818 - classification_loss 8374/10000 [========================>.....] - ETA: 12:51 - loss: 2.5355 - regression_loss: 1.8819 - classification_loss 8375/10000 [========================>.....] - ETA: 12:50 - loss: 2.5355 - regression_loss: 1.8819 - classification_loss 8376/10000 [========================>.....] - ETA: 12:50 - loss: 2.5354 - regression_loss: 1.8818 - classification_loss 8377/10000 [========================>.....] - ETA: 12:49 - loss: 2.5353 - regression_loss: 1.8817 - classification_loss 8378/10000 [========================>.....] - ETA: 12:49 - loss: 2.5353 - regression_loss: 1.8817 - classification_loss 8379/10000 [========================>.....] - ETA: 12:48 - loss: 2.5353 - regression_loss: 1.8817 - classification_loss 8380/10000 [========================>.....] - ETA: 12:48 - loss: 2.5351 - regression_loss: 1.8816 - classification_loss 8381/10000 [========================>.....] - ETA: 12:47 - loss: 2.5351 - regression_loss: 1.8816 - classification_loss 8382/10000 [========================>.....] - ETA: 12:47 - loss: 2.5351 - regression_loss: 1.8816 - classification_loss 8383/10000 [========================>.....] - ETA: 12:46 - loss: 2.5350 - regression_loss: 1.8815 - classification_loss 8384/10000 [========================>.....] - ETA: 12:46 - loss: 2.5348 - regression_loss: 1.8814 - classification_loss 8385/10000 [========================>.....] - ETA: 12:45 - loss: 2.5347 - regression_loss: 1.8813 - classification_loss 8386/10000 [========================>.....] - ETA: 12:45 - loss: 2.5346 - regression_loss: 1.8812 - classification_loss 8387/10000 [========================>.....] - ETA: 12:44 - loss: 2.5345 - regression_loss: 1.8811 - classification_loss 8388/10000 [========================>.....] - ETA: 12:44 - loss: 2.5344 - regression_loss: 1.8810 - classification_loss 8389/10000 [========================>.....] - ETA: 12:44 - loss: 2.5342 - regression_loss: 1.8810 - classification_loss 8390/10000 [========================>.....] - ETA: 12:43 - loss: 2.5342 - regression_loss: 1.8810 - classification_loss 8391/10000 [========================>.....] - ETA: 12:43 - loss: 2.5342 - regression_loss: 1.8809 - classification_loss 8392/10000 [========================>.....] - ETA: 12:42 - loss: 2.5340 - regression_loss: 1.8808 - classification_loss 8393/10000 [========================>.....] - ETA: 12:42 - loss: 2.5340 - regression_loss: 1.8808 - classification_loss 8394/10000 [========================>.....] - ETA: 12:41 - loss: 2.5340 - regression_loss: 1.8808 - classification_loss 8395/10000 [========================>.....] - ETA: 12:41 - loss: 2.5340 - regression_loss: 1.8808 - classification_loss 8396/10000 [========================>.....] - ETA: 12:40 - loss: 2.5340 - regression_loss: 1.8808 - classification_loss 8397/10000 [========================>.....] - ETA: 12:40 - loss: 2.5340 - regression_loss: 1.8808 - classification_loss 8398/10000 [========================>.....] - ETA: 12:39 - loss: 2.5338 - regression_loss: 1.8806 - classification_loss 8399/10000 [========================>.....] - ETA: 12:39 - loss: 2.5336 - regression_loss: 1.8805 - classification_loss 8400/10000 [========================>.....] - ETA: 12:38 - loss: 2.5336 - regression_loss: 1.8805 - classification_loss 8401/10000 [========================>.....] - ETA: 12:38 - loss: 2.5335 - regression_loss: 1.8804 - classification_loss 8402/10000 [========================>.....] - ETA: 12:37 - loss: 2.5335 - regression_loss: 1.8804 - classification_loss 8403/10000 [========================>.....] - ETA: 12:37 - loss: 2.5334 - regression_loss: 1.8804 - classification_loss 8404/10000 [========================>.....] - ETA: 12:36 - loss: 2.5333 - regression_loss: 1.8803 - classification_loss 8405/10000 [========================>.....] - ETA: 12:36 - loss: 2.5333 - regression_loss: 1.8803 - classification_loss 8406/10000 [========================>.....] - ETA: 12:35 - loss: 2.5333 - regression_loss: 1.8803 - classification_loss 8407/10000 [========================>.....] - ETA: 12:35 - loss: 2.5333 - regression_loss: 1.8802 - classification_loss 8408/10000 [========================>.....] - ETA: 12:34 - loss: 2.5333 - regression_loss: 1.8802 - classification_loss 8409/10000 [========================>.....] - ETA: 12:34 - loss: 2.5333 - regression_loss: 1.8803 - classification_loss 8410/10000 [========================>.....] - ETA: 12:34 - loss: 2.5332 - regression_loss: 1.8802 - classification_loss 8411/10000 [========================>.....] - ETA: 12:33 - loss: 2.5330 - regression_loss: 1.8801 - classification_loss 8412/10000 [========================>.....] - ETA: 12:33 - loss: 2.5331 - regression_loss: 1.8801 - classification_loss 8413/10000 [========================>.....] - ETA: 12:32 - loss: 2.5329 - regression_loss: 1.8800 - classification_loss 8414/10000 [========================>.....] - ETA: 12:32 - loss: 2.5329 - regression_loss: 1.8800 - classification_loss 8415/10000 [========================>.....] - ETA: 12:31 - loss: 2.5328 - regression_loss: 1.8800 - classification_loss 8416/10000 [========================>.....] - ETA: 12:31 - loss: 2.5327 - regression_loss: 1.8799 - classification_loss 8417/10000 [========================>.....] - ETA: 12:30 - loss: 2.5326 - regression_loss: 1.8798 - classification_loss 8418/10000 [========================>.....] - ETA: 12:30 - loss: 2.5325 - regression_loss: 1.8798 - classification_loss 8419/10000 [========================>.....] - ETA: 12:29 - loss: 2.5324 - regression_loss: 1.8797 - classification_loss 8420/10000 [========================>.....] - ETA: 12:29 - loss: 2.5324 - regression_loss: 1.8797 - classification_loss 8421/10000 [========================>.....] - ETA: 12:28 - loss: 2.5323 - regression_loss: 1.8796 - classification_loss 8422/10000 [========================>.....] - ETA: 12:28 - loss: 2.5322 - regression_loss: 1.8796 - classification_loss 8423/10000 [========================>.....] - ETA: 12:27 - loss: 2.5323 - regression_loss: 1.8796 - classification_loss 8424/10000 [========================>.....] - ETA: 12:27 - loss: 2.5321 - regression_loss: 1.8795 - classification_loss 8425/10000 [========================>.....] - ETA: 12:26 - loss: 2.5321 - regression_loss: 1.8795 - classification_loss 8426/10000 [========================>.....] - ETA: 12:26 - loss: 2.5320 - regression_loss: 1.8795 - classification_loss 8427/10000 [========================>.....] - ETA: 12:25 - loss: 2.5319 - regression_loss: 1.8794 - classification_loss 8428/10000 [========================>.....] - ETA: 12:25 - loss: 2.5318 - regression_loss: 1.8794 - classification_loss 8429/10000 [========================>.....] - ETA: 12:24 - loss: 2.5318 - regression_loss: 1.8794 - classification_loss 8430/10000 [========================>.....] - ETA: 12:24 - loss: 2.5317 - regression_loss: 1.8792 - classification_loss 8431/10000 [========================>.....] - ETA: 12:24 - loss: 2.5316 - regression_loss: 1.8792 - classification_loss 8432/10000 [========================>.....] - ETA: 12:23 - loss: 2.5315 - regression_loss: 1.8791 - classification_loss 8433/10000 [========================>.....] - ETA: 12:23 - loss: 2.5315 - regression_loss: 1.8791 - classification_loss 8434/10000 [========================>.....] - ETA: 12:22 - loss: 2.5313 - regression_loss: 1.8789 - classification_loss 8435/10000 [========================>.....] - ETA: 12:22 - loss: 2.5311 - regression_loss: 1.8789 - classification_loss 8436/10000 [========================>.....] - ETA: 12:21 - loss: 2.5311 - regression_loss: 1.8788 - classification_loss 8437/10000 [========================>.....] - ETA: 12:21 - loss: 2.5311 - regression_loss: 1.8788 - classification_loss 8438/10000 [========================>.....] - ETA: 12:20 - loss: 2.5309 - regression_loss: 1.8787 - classification_loss 8439/10000 [========================>.....] - ETA: 12:20 - loss: 2.5309 - regression_loss: 1.8787 - classification_loss 8440/10000 [========================>.....] - ETA: 12:19 - loss: 2.5308 - regression_loss: 1.8786 - classification_loss 8441/10000 [========================>.....] - ETA: 12:19 - loss: 2.5307 - regression_loss: 1.8786 - classification_loss 8442/10000 [========================>.....] - ETA: 12:18 - loss: 2.5306 - regression_loss: 1.8785 - classification_loss 8443/10000 [========================>.....] - ETA: 12:18 - loss: 2.5306 - regression_loss: 1.8785 - classification_loss 8444/10000 [========================>.....] - ETA: 12:17 - loss: 2.5305 - regression_loss: 1.8783 - classification_loss 8445/10000 [========================>.....] - ETA: 12:17 - loss: 2.5304 - regression_loss: 1.8783 - classification_loss 8446/10000 [========================>.....] - ETA: 12:16 - loss: 2.5303 - regression_loss: 1.8782 - classification_loss 8447/10000 [========================>.....] - ETA: 12:16 - loss: 2.5303 - regression_loss: 1.8782 - classification_loss 8448/10000 [========================>.....] - ETA: 12:15 - loss: 2.5302 - regression_loss: 1.8780 - classification_loss 8449/10000 [========================>.....] - ETA: 12:15 - loss: 2.5301 - regression_loss: 1.8779 - classification_loss 8450/10000 [========================>.....] - ETA: 12:14 - loss: 2.5299 - regression_loss: 1.8778 - classification_loss 8451/10000 [========================>.....] - ETA: 12:14 - loss: 2.5299 - regression_loss: 1.8778 - classification_loss 8452/10000 [========================>.....] - ETA: 12:14 - loss: 2.5299 - regression_loss: 1.8778 - classification_loss 8453/10000 [========================>.....] - ETA: 12:13 - loss: 2.5298 - regression_loss: 1.8778 - classification_loss 8454/10000 [========================>.....] - ETA: 12:13 - loss: 2.5297 - regression_loss: 1.8776 - classification_loss 8455/10000 [========================>.....] - ETA: 12:12 - loss: 2.5297 - regression_loss: 1.8776 - classification_loss 8456/10000 [========================>.....] - ETA: 12:12 - loss: 2.5295 - regression_loss: 1.8775 - classification_loss 8457/10000 [========================>.....] - ETA: 12:11 - loss: 2.5294 - regression_loss: 1.8773 - classification_loss 8458/10000 [========================>.....] - ETA: 12:11 - loss: 2.5293 - regression_loss: 1.8773 - classification_loss 8459/10000 [========================>.....] - ETA: 12:10 - loss: 2.5293 - regression_loss: 1.8773 - classification_loss 8460/10000 [========================>.....] - ETA: 12:10 - loss: 2.5291 - regression_loss: 1.8771 - classification_loss 8461/10000 [========================>.....] - ETA: 12:09 - loss: 2.5290 - regression_loss: 1.8770 - classification_loss 8462/10000 [========================>.....] - ETA: 12:09 - loss: 2.5289 - regression_loss: 1.8770 - classification_loss 8463/10000 [========================>.....] - ETA: 12:08 - loss: 2.5287 - regression_loss: 1.8768 - classification_loss 8464/10000 [========================>.....] - ETA: 12:08 - loss: 2.5287 - regression_loss: 1.8769 - classification_loss 8465/10000 [========================>.....] - ETA: 12:07 - loss: 2.5287 - regression_loss: 1.8769 - classification_loss 8466/10000 [========================>.....] - ETA: 12:07 - loss: 2.5285 - regression_loss: 1.8767 - classification_loss 8467/10000 [========================>.....] - ETA: 12:06 - loss: 2.5284 - regression_loss: 1.8767 - classification_loss 8468/10000 [========================>.....] - ETA: 12:06 - loss: 2.5285 - regression_loss: 1.8767 - classification_loss 8469/10000 [========================>.....] - ETA: 12:05 - loss: 2.5283 - regression_loss: 1.8766 - classification_loss 8470/10000 [========================>.....] - ETA: 12:05 - loss: 2.5284 - regression_loss: 1.8766 - classification_loss 8471/10000 [========================>.....] - ETA: 12:04 - loss: 2.5284 - regression_loss: 1.8766 - classification_loss 8472/10000 [========================>.....] - ETA: 12:04 - loss: 2.5283 - regression_loss: 1.8766 - classification_loss 8473/10000 [========================>.....] - ETA: 12:04 - loss: 2.5282 - regression_loss: 1.8765 - classification_loss 8474/10000 [========================>.....] - ETA: 12:03 - loss: 2.5281 - regression_loss: 1.8764 - classification_loss 8475/10000 [========================>.....] - ETA: 12:03 - loss: 2.5280 - regression_loss: 1.8763 - classification_loss 8476/10000 [========================>.....] - ETA: 12:02 - loss: 2.5279 - regression_loss: 1.8762 - classification_loss 8477/10000 [========================>.....] - ETA: 12:02 - loss: 2.5279 - regression_loss: 1.8762 - classification_loss 8478/10000 [========================>.....] - ETA: 12:01 - loss: 2.5279 - regression_loss: 1.8762 - classification_loss 8479/10000 [========================>.....] - ETA: 12:01 - loss: 2.5277 - regression_loss: 1.8760 - classification_loss 8480/10000 [========================>.....] - ETA: 12:00 - loss: 2.5276 - regression_loss: 1.8759 - classification_loss 8481/10000 [========================>.....] - ETA: 12:00 - loss: 2.5275 - regression_loss: 1.8759 - classification_loss 8482/10000 [========================>.....] - ETA: 11:59 - loss: 2.5274 - regression_loss: 1.8758 - classification_loss 8483/10000 [========================>.....] - ETA: 11:59 - loss: 2.5274 - regression_loss: 1.8758 - classification_loss 8484/10000 [========================>.....] - ETA: 11:58 - loss: 2.5274 - regression_loss: 1.8758 - classification_loss 8485/10000 [========================>.....] - ETA: 11:58 - loss: 2.5273 - regression_loss: 1.8758 - classification_loss 8486/10000 [========================>.....] - ETA: 11:57 - loss: 2.5273 - regression_loss: 1.8758 - classification_loss 8487/10000 [========================>.....] - ETA: 11:57 - loss: 2.5273 - regression_loss: 1.8758 - classification_loss 8488/10000 [========================>.....] - ETA: 11:56 - loss: 2.5272 - regression_loss: 1.8757 - classification_loss 8489/10000 [========================>.....] - ETA: 11:56 - loss: 2.5272 - regression_loss: 1.8756 - classification_loss 8490/10000 [========================>.....] - ETA: 11:55 - loss: 2.5272 - regression_loss: 1.8757 - classification_loss 8491/10000 [========================>.....] - ETA: 11:55 - loss: 2.5272 - regression_loss: 1.8756 - classification_loss 8492/10000 [========================>.....] - ETA: 11:54 - loss: 2.5271 - regression_loss: 1.8756 - classification_loss 8493/10000 [========================>.....] - ETA: 11:54 - loss: 2.5270 - regression_loss: 1.8756 - classification_loss 8494/10000 [========================>.....] - ETA: 11:53 - loss: 2.5271 - regression_loss: 1.8756 - classification_loss 8495/10000 [========================>.....] - ETA: 11:53 - loss: 2.5271 - regression_loss: 1.8757 - classification_loss 8496/10000 [========================>.....] - ETA: 11:53 - loss: 2.5270 - regression_loss: 1.8757 - classification_loss 8497/10000 [========================>.....] - ETA: 11:52 - loss: 2.5270 - regression_loss: 1.8757 - classification_loss 8498/10000 [========================>.....] - ETA: 11:52 - loss: 2.5270 - regression_loss: 1.8756 - classification_loss 8499/10000 [========================>.....] - ETA: 11:51 - loss: 2.5269 - regression_loss: 1.8756 - classification_loss 8500/10000 [========================>.....] - ETA: 11:51 - loss: 2.5269 - regression_loss: 1.8756 - classification_loss 8501/10000 [========================>.....] - ETA: 11:50 - loss: 2.5268 - regression_loss: 1.8756 - classification_loss 8502/10000 [========================>.....] - ETA: 11:50 - loss: 2.5269 - regression_loss: 1.8756 - classification_loss 8503/10000 [========================>.....] - ETA: 11:49 - loss: 2.5267 - regression_loss: 1.8755 - classification_loss 8504/10000 [========================>.....] - ETA: 11:49 - loss: 2.5266 - regression_loss: 1.8755 - classification_loss 8505/10000 [========================>.....] - ETA: 11:48 - loss: 2.5266 - regression_loss: 1.8754 - classification_loss 8506/10000 [========================>.....] - ETA: 11:48 - loss: 2.5265 - regression_loss: 1.8754 - classification_loss 8507/10000 [========================>.....] - ETA: 11:47 - loss: 2.5264 - regression_loss: 1.8753 - classification_loss 8508/10000 [========================>.....] - ETA: 11:47 - loss: 2.5264 - regression_loss: 1.8753 - classification_loss 8509/10000 [========================>.....] - ETA: 11:46 - loss: 2.5263 - regression_loss: 1.8753 - classification_loss 8510/10000 [========================>.....] - ETA: 11:46 - loss: 2.5263 - regression_loss: 1.8753 - classification_loss 8511/10000 [========================>.....] - ETA: 11:45 - loss: 2.5263 - regression_loss: 1.8753 - classification_loss 8512/10000 [========================>.....] - ETA: 11:45 - loss: 2.5263 - regression_loss: 1.8753 - classification_loss 8513/10000 [========================>.....] - ETA: 11:44 - loss: 2.5262 - regression_loss: 1.8752 - classification_loss 8514/10000 [========================>.....] - ETA: 11:44 - loss: 2.5261 - regression_loss: 1.8752 - classification_loss 8515/10000 [========================>.....] - ETA: 11:43 - loss: 2.5261 - regression_loss: 1.8752 - classification_loss 8516/10000 [========================>.....] - ETA: 11:43 - loss: 2.5260 - regression_loss: 1.8751 - classification_loss 8517/10000 [========================>.....] - ETA: 11:42 - loss: 2.5260 - regression_loss: 1.8751 - classification_loss 8518/10000 [========================>.....] - ETA: 11:42 - loss: 2.5262 - regression_loss: 1.8752 - classification_loss 8519/10000 [========================>.....] - ETA: 11:42 - loss: 2.5263 - regression_loss: 1.8753 - classification_loss 8520/10000 [========================>.....] - ETA: 11:41 - loss: 2.5263 - regression_loss: 1.8753 - classification_loss 8521/10000 [========================>.....] - ETA: 11:41 - loss: 2.5263 - regression_loss: 1.8753 - classification_loss 8522/10000 [========================>.....] - ETA: 11:40 - loss: 2.5261 - regression_loss: 1.8752 - classification_loss 8523/10000 [========================>.....] - ETA: 11:40 - loss: 2.5261 - regression_loss: 1.8752 - classification_loss 8524/10000 [========================>.....] - ETA: 11:39 - loss: 2.5259 - regression_loss: 1.8750 - classification_loss 8525/10000 [========================>.....] - ETA: 11:39 - loss: 2.5258 - regression_loss: 1.8750 - classification_loss 8526/10000 [========================>.....] - ETA: 11:38 - loss: 2.5257 - regression_loss: 1.8749 - classification_loss 8527/10000 [========================>.....] - ETA: 11:38 - loss: 2.5256 - regression_loss: 1.8748 - classification_loss 8528/10000 [========================>.....] - ETA: 11:37 - loss: 2.5255 - regression_loss: 1.8747 - classification_loss 8529/10000 [========================>.....] - ETA: 11:37 - loss: 2.5254 - regression_loss: 1.8747 - classification_loss 8530/10000 [========================>.....] - ETA: 11:36 - loss: 2.5254 - regression_loss: 1.8747 - classification_loss 8531/10000 [========================>.....] - ETA: 11:36 - loss: 2.5253 - regression_loss: 1.8745 - classification_loss 8532/10000 [========================>.....] - ETA: 11:35 - loss: 2.5252 - regression_loss: 1.8745 - classification_loss 8533/10000 [========================>.....] - ETA: 11:35 - loss: 2.5251 - regression_loss: 1.8744 - classification_loss 8534/10000 [========================>.....] - ETA: 11:34 - loss: 2.5250 - regression_loss: 1.8744 - classification_loss 8535/10000 [========================>.....] - ETA: 11:34 - loss: 2.5249 - regression_loss: 1.8742 - classification_loss 8536/10000 [========================>.....] - ETA: 11:33 - loss: 2.5249 - regression_loss: 1.8743 - classification_loss 8537/10000 [========================>.....] - ETA: 11:33 - loss: 2.5248 - regression_loss: 1.8742 - classification_loss 8538/10000 [========================>.....] - ETA: 11:33 - loss: 2.5248 - regression_loss: 1.8742 - classification_loss 8539/10000 [========================>.....] - ETA: 11:32 - loss: 2.5249 - regression_loss: 1.8743 - classification_loss 8540/10000 [========================>.....] - ETA: 11:32 - loss: 2.5249 - regression_loss: 1.8743 - classification_loss 8541/10000 [========================>.....] - ETA: 11:31 - loss: 2.5248 - regression_loss: 1.8742 - classification_loss 8542/10000 [========================>.....] - ETA: 11:31 - loss: 2.5248 - regression_loss: 1.8743 - classification_loss 8543/10000 [========================>.....] - ETA: 11:30 - loss: 2.5248 - regression_loss: 1.8742 - classification_loss 8544/10000 [========================>.....] - ETA: 11:30 - loss: 2.5246 - regression_loss: 1.8741 - classification_loss 8545/10000 [========================>.....] - ETA: 11:29 - loss: 2.5245 - regression_loss: 1.8740 - classification_loss 8546/10000 [========================>.....] - ETA: 11:29 - loss: 2.5244 - regression_loss: 1.8740 - classification_loss 8547/10000 [========================>.....] - ETA: 11:28 - loss: 2.5243 - regression_loss: 1.8739 - classification_loss 8548/10000 [========================>.....] - ETA: 11:28 - loss: 2.5243 - regression_loss: 1.8739 - classification_loss 8549/10000 [========================>.....] - ETA: 11:27 - loss: 2.5242 - regression_loss: 1.8738 - classification_loss 8550/10000 [========================>.....] - ETA: 11:27 - loss: 2.5242 - regression_loss: 1.8739 - classification_loss 8551/10000 [========================>.....] - ETA: 11:26 - loss: 2.5241 - regression_loss: 1.8738 - classification_loss 8552/10000 [========================>.....] - ETA: 11:26 - loss: 2.5240 - regression_loss: 1.8737 - classification_loss 8553/10000 [========================>.....] - ETA: 11:25 - loss: 2.5239 - regression_loss: 1.8737 - classification_loss 8554/10000 [========================>.....] - ETA: 11:25 - loss: 2.5239 - regression_loss: 1.8736 - classification_loss 8555/10000 [========================>.....] - ETA: 11:24 - loss: 2.5238 - regression_loss: 1.8735 - classification_loss 8556/10000 [========================>.....] - ETA: 11:24 - loss: 2.5237 - regression_loss: 1.8734 - classification_loss 8557/10000 [========================>.....] - ETA: 11:23 - loss: 2.5237 - regression_loss: 1.8734 - classification_loss 8558/10000 [========================>.....] - ETA: 11:23 - loss: 2.5235 - regression_loss: 1.8734 - classification_loss 8559/10000 [========================>.....] - ETA: 11:23 - loss: 2.5235 - regression_loss: 1.8733 - classification_loss 8560/10000 [========================>.....] - ETA: 11:22 - loss: 2.5234 - regression_loss: 1.8733 - classification_loss 8561/10000 [========================>.....] - ETA: 11:22 - loss: 2.5234 - regression_loss: 1.8733 - classification_loss 8562/10000 [========================>.....] - ETA: 11:21 - loss: 2.5233 - regression_loss: 1.8732 - classification_loss 8563/10000 [========================>.....] - ETA: 11:21 - loss: 2.5234 - regression_loss: 1.8733 - classification_loss 8564/10000 [========================>.....] - ETA: 11:20 - loss: 2.5234 - regression_loss: 1.8733 - classification_loss 8565/10000 [========================>.....] - ETA: 11:20 - loss: 2.5234 - regression_loss: 1.8732 - classification_loss 8566/10000 [========================>.....] - ETA: 11:19 - loss: 2.5232 - regression_loss: 1.8731 - classification_loss 8567/10000 [========================>.....] - ETA: 11:19 - loss: 2.5232 - regression_loss: 1.8732 - classification_loss 8568/10000 [========================>.....] - ETA: 11:18 - loss: 2.5233 - regression_loss: 1.8732 - classification_loss 8569/10000 [========================>.....] - ETA: 11:18 - loss: 2.5233 - regression_loss: 1.8732 - classification_loss 8570/10000 [========================>.....] - ETA: 11:17 - loss: 2.5232 - regression_loss: 1.8732 - classification_loss 8571/10000 [========================>.....] - ETA: 11:17 - loss: 2.5231 - regression_loss: 1.8731 - classification_loss 8572/10000 [========================>.....] - ETA: 11:16 - loss: 2.5231 - regression_loss: 1.8731 - classification_loss 8573/10000 [========================>.....] - ETA: 11:16 - loss: 2.5230 - regression_loss: 1.8730 - classification_loss 8574/10000 [========================>.....] - ETA: 11:15 - loss: 2.5228 - regression_loss: 1.8729 - classification_loss 8575/10000 [========================>.....] - ETA: 11:15 - loss: 2.5228 - regression_loss: 1.8729 - classification_loss 8576/10000 [========================>.....] - ETA: 11:14 - loss: 2.5227 - regression_loss: 1.8728 - classification_loss 8577/10000 [========================>.....] - ETA: 11:14 - loss: 2.5227 - regression_loss: 1.8728 - classification_loss 8578/10000 [========================>.....] - ETA: 11:13 - loss: 2.5227 - regression_loss: 1.8729 - classification_loss 8579/10000 [========================>.....] - ETA: 11:13 - loss: 2.5227 - regression_loss: 1.8728 - classification_loss 8580/10000 [========================>.....] - ETA: 11:12 - loss: 2.5225 - regression_loss: 1.8727 - classification_loss 8581/10000 [========================>.....] - ETA: 11:12 - loss: 2.5225 - regression_loss: 1.8727 - classification_loss 8582/10000 [========================>.....] - ETA: 11:12 - loss: 2.5225 - regression_loss: 1.8727 - classification_loss 8583/10000 [========================>.....] - ETA: 11:11 - loss: 2.5225 - regression_loss: 1.8727 - classification_loss 8584/10000 [========================>.....] - ETA: 11:11 - loss: 2.5224 - regression_loss: 1.8727 - classification_loss 8585/10000 [========================>.....] - ETA: 11:10 - loss: 2.5224 - regression_loss: 1.8727 - classification_loss 8586/10000 [========================>.....] - ETA: 11:10 - loss: 2.5224 - regression_loss: 1.8727 - classification_loss 8587/10000 [========================>.....] - ETA: 11:09 - loss: 2.5223 - regression_loss: 1.8727 - classification_loss 8588/10000 [========================>.....] - ETA: 11:09 - loss: 2.5222 - regression_loss: 1.8726 - classification_loss 8589/10000 [========================>.....] - ETA: 11:08 - loss: 2.5221 - regression_loss: 1.8724 - classification_loss 8590/10000 [========================>.....] - ETA: 11:08 - loss: 2.5219 - regression_loss: 1.8723 - classification_loss 8591/10000 [========================>.....] - ETA: 11:07 - loss: 2.5219 - regression_loss: 1.8723 - classification_loss 8592/10000 [========================>.....] - ETA: 11:07 - loss: 2.5218 - regression_loss: 1.8722 - classification_loss 8593/10000 [========================>.....] - ETA: 11:06 - loss: 2.5217 - regression_loss: 1.8721 - classification_loss 8594/10000 [========================>.....] - ETA: 11:06 - loss: 2.5216 - regression_loss: 1.8721 - classification_loss 8595/10000 [========================>.....] - ETA: 11:05 - loss: 2.5215 - regression_loss: 1.8720 - classification_loss 8596/10000 [========================>.....] - ETA: 11:05 - loss: 2.5215 - regression_loss: 1.8719 - classification_loss 8597/10000 [========================>.....] - ETA: 11:04 - loss: 2.5214 - regression_loss: 1.8718 - classification_loss 8598/10000 [========================>.....] - ETA: 11:04 - loss: 2.5212 - regression_loss: 1.8718 - classification_loss 8599/10000 [========================>.....] - ETA: 11:03 - loss: 2.5213 - regression_loss: 1.8718 - classification_loss 8600/10000 [========================>.....] - ETA: 11:03 - loss: 2.5212 - regression_loss: 1.8718 - classification_loss 8601/10000 [========================>.....] - ETA: 11:03 - loss: 2.5211 - regression_loss: 1.8718 - classification_loss 8602/10000 [========================>.....] - ETA: 11:02 - loss: 2.5211 - regression_loss: 1.8718 - classification_loss 8603/10000 [========================>.....] - ETA: 11:02 - loss: 2.5210 - regression_loss: 1.8717 - classification_loss 8604/10000 [========================>.....] - ETA: 11:01 - loss: 2.5210 - regression_loss: 1.8717 - classification_loss 8605/10000 [========================>.....] - ETA: 11:01 - loss: 2.5210 - regression_loss: 1.8717 - classification_loss 8606/10000 [========================>.....] - ETA: 11:00 - loss: 2.5210 - regression_loss: 1.8717 - classification_loss 8607/10000 [========================>.....] - ETA: 11:00 - loss: 2.5209 - regression_loss: 1.8717 - classification_loss 8608/10000 [========================>.....] - ETA: 10:59 - loss: 2.5209 - regression_loss: 1.8717 - classification_loss 8609/10000 [========================>.....] - ETA: 10:59 - loss: 2.5209 - regression_loss: 1.8717 - classification_loss 8610/10000 [========================>.....] - ETA: 10:58 - loss: 2.5208 - regression_loss: 1.8716 - classification_loss 8611/10000 [========================>.....] - ETA: 10:58 - loss: 2.5208 - regression_loss: 1.8716 - classification_loss 8612/10000 [========================>.....] - ETA: 10:57 - loss: 2.5208 - regression_loss: 1.8716 - classification_loss 8613/10000 [========================>.....] - ETA: 10:57 - loss: 2.5208 - regression_loss: 1.8716 - classification_loss 8614/10000 [========================>.....] - ETA: 10:56 - loss: 2.5208 - regression_loss: 1.8716 - classification_loss 8615/10000 [========================>.....] - ETA: 10:56 - loss: 2.5207 - regression_loss: 1.8715 - classification_loss 8616/10000 [========================>.....] - ETA: 10:55 - loss: 2.5206 - regression_loss: 1.8715 - classification_loss 8617/10000 [========================>.....] - ETA: 10:55 - loss: 2.5206 - regression_loss: 1.8715 - classification_loss 8618/10000 [========================>.....] - ETA: 10:54 - loss: 2.5206 - regression_loss: 1.8715 - classification_loss 8619/10000 [========================>.....] - ETA: 10:54 - loss: 2.5206 - regression_loss: 1.8715 - classification_loss 8620/10000 [========================>.....] - ETA: 10:53 - loss: 2.5206 - regression_loss: 1.8715 - classification_loss 8621/10000 [========================>.....] - ETA: 10:53 - loss: 2.5205 - regression_loss: 1.8714 - classification_loss 8622/10000 [========================>.....] - ETA: 10:53 - loss: 2.5204 - regression_loss: 1.8713 - classification_loss 8623/10000 [========================>.....] - ETA: 10:52 - loss: 2.5203 - regression_loss: 1.8713 - classification_loss 8624/10000 [========================>.....] - ETA: 10:52 - loss: 2.5202 - regression_loss: 1.8712 - classification_loss 8625/10000 [========================>.....] - ETA: 10:51 - loss: 2.5201 - regression_loss: 1.8711 - classification_loss 8626/10000 [========================>.....] - ETA: 10:51 - loss: 2.5200 - regression_loss: 1.8711 - classification_loss 8627/10000 [========================>.....] - ETA: 10:50 - loss: 2.5200 - regression_loss: 1.8711 - classification_loss 8628/10000 [========================>.....] - ETA: 10:50 - loss: 2.5199 - regression_loss: 1.8710 - classification_loss 8629/10000 [========================>.....] - ETA: 10:49 - loss: 2.5198 - regression_loss: 1.8710 - classification_loss 8630/10000 [========================>.....] - ETA: 10:49 - loss: 2.5198 - regression_loss: 1.8709 - classification_loss 8631/10000 [========================>.....] - ETA: 10:48 - loss: 2.5197 - regression_loss: 1.8709 - classification_loss 8632/10000 [========================>.....] - ETA: 10:48 - loss: 2.5197 - regression_loss: 1.8709 - classification_loss 8633/10000 [========================>.....] - ETA: 10:47 - loss: 2.5196 - regression_loss: 1.8708 - classification_loss 8634/10000 [========================>.....] - ETA: 10:47 - loss: 2.5195 - regression_loss: 1.8708 - classification_loss 8635/10000 [========================>.....] - ETA: 10:46 - loss: 2.5195 - regression_loss: 1.8708 - classification_loss 8636/10000 [========================>.....] - ETA: 10:46 - loss: 2.5194 - regression_loss: 1.8707 - classification_loss 8637/10000 [========================>.....] - ETA: 10:45 - loss: 2.5194 - regression_loss: 1.8707 - classification_loss 8638/10000 [========================>.....] - ETA: 10:45 - loss: 2.5193 - regression_loss: 1.8707 - classification_loss 8639/10000 [========================>.....] - ETA: 10:44 - loss: 2.5193 - regression_loss: 1.8707 - classification_loss 8640/10000 [========================>.....] - ETA: 10:44 - loss: 2.5192 - regression_loss: 1.8706 - classification_loss 8641/10000 [========================>.....] - ETA: 10:43 - loss: 2.5190 - regression_loss: 1.8705 - classification_loss 8642/10000 [========================>.....] - ETA: 10:43 - loss: 2.5190 - regression_loss: 1.8705 - classification_loss 8643/10000 [========================>.....] - ETA: 10:43 - loss: 2.5190 - regression_loss: 1.8705 - classification_loss 8644/10000 [========================>.....] - ETA: 10:42 - loss: 2.5189 - regression_loss: 1.8705 - classification_loss 8645/10000 [========================>.....] - ETA: 10:42 - loss: 2.5189 - regression_loss: 1.8704 - classification_loss 8646/10000 [========================>.....] - ETA: 10:41 - loss: 2.5188 - regression_loss: 1.8704 - classification_loss 8647/10000 [========================>.....] - ETA: 10:41 - loss: 2.5189 - regression_loss: 1.8704 - classification_loss 8648/10000 [========================>.....] - ETA: 10:40 - loss: 2.5189 - regression_loss: 1.8705 - classification_loss 8649/10000 [========================>.....] - ETA: 10:40 - loss: 2.5189 - regression_loss: 1.8705 - classification_loss 8650/10000 [========================>.....] - ETA: 10:39 - loss: 2.5188 - regression_loss: 1.8704 - classification_loss 8651/10000 [========================>.....] - ETA: 10:39 - loss: 2.5189 - regression_loss: 1.8704 - classification_loss 8652/10000 [========================>.....] - ETA: 10:38 - loss: 2.5189 - regression_loss: 1.8704 - classification_loss 8653/10000 [========================>.....] - ETA: 10:38 - loss: 2.5188 - regression_loss: 1.8704 - classification_loss 8654/10000 [========================>.....] - ETA: 10:37 - loss: 2.5188 - regression_loss: 1.8703 - classification_loss 8655/10000 [========================>.....] - ETA: 10:37 - loss: 2.5187 - regression_loss: 1.8703 - classification_loss 8656/10000 [========================>.....] - ETA: 10:36 - loss: 2.5186 - regression_loss: 1.8703 - classification_loss 8657/10000 [========================>.....] - ETA: 10:36 - loss: 2.5186 - regression_loss: 1.8702 - classification_loss 8658/10000 [========================>.....] - ETA: 10:35 - loss: 2.5185 - regression_loss: 1.8702 - classification_loss 8659/10000 [========================>.....] - ETA: 10:35 - loss: 2.5185 - regression_loss: 1.8702 - classification_loss 8660/10000 [========================>.....] - ETA: 10:34 - loss: 2.5185 - regression_loss: 1.8702 - classification_loss 8661/10000 [========================>.....] - ETA: 10:34 - loss: 2.5184 - regression_loss: 1.8701 - classification_loss 8662/10000 [========================>.....] - ETA: 10:33 - loss: 2.5182 - regression_loss: 1.8700 - classification_loss 8663/10000 [========================>.....] - ETA: 10:33 - loss: 2.5181 - regression_loss: 1.8699 - classification_loss 8664/10000 [========================>.....] - ETA: 10:33 - loss: 2.5180 - regression_loss: 1.8698 - classification_loss 8665/10000 [========================>.....] - ETA: 10:32 - loss: 2.5178 - regression_loss: 1.8697 - classification_loss 8666/10000 [========================>.....] - ETA: 10:32 - loss: 2.5177 - regression_loss: 1.8696 - classification_loss 8667/10000 [=========================>....] - ETA: 10:31 - loss: 2.5177 - regression_loss: 1.8696 - classification_loss 8668/10000 [=========================>....] - ETA: 10:31 - loss: 2.5177 - regression_loss: 1.8696 - classification_loss 8669/10000 [=========================>....] - ETA: 10:30 - loss: 2.5175 - regression_loss: 1.8695 - classification_loss 8670/10000 [=========================>....] - ETA: 10:30 - loss: 2.5176 - regression_loss: 1.8696 - classification_loss 8671/10000 [=========================>....] - ETA: 10:29 - loss: 2.5175 - regression_loss: 1.8695 - classification_loss 8672/10000 [=========================>....] - ETA: 10:29 - loss: 2.5176 - regression_loss: 1.8696 - classification_loss 8673/10000 [=========================>....] - ETA: 10:28 - loss: 2.5175 - regression_loss: 1.8695 - classification_loss 8674/10000 [=========================>....] - ETA: 10:28 - loss: 2.5174 - regression_loss: 1.8695 - classification_loss 8675/10000 [=========================>....] - ETA: 10:27 - loss: 2.5172 - regression_loss: 1.8694 - classification_loss 8676/10000 [=========================>....] - ETA: 10:27 - loss: 2.5172 - regression_loss: 1.8693 - classification_loss 8677/10000 [=========================>....] - ETA: 10:26 - loss: 2.5173 - regression_loss: 1.8694 - classification_loss 8678/10000 [=========================>....] - ETA: 10:26 - loss: 2.5173 - regression_loss: 1.8694 - classification_loss 8679/10000 [=========================>....] - ETA: 10:25 - loss: 2.5174 - regression_loss: 1.8695 - classification_loss 8680/10000 [=========================>....] - ETA: 10:25 - loss: 2.5174 - regression_loss: 1.8695 - classification_loss 8681/10000 [=========================>....] - ETA: 10:24 - loss: 2.5174 - regression_loss: 1.8695 - classification_loss 8682/10000 [=========================>....] - ETA: 10:24 - loss: 2.5172 - regression_loss: 1.8694 - classification_loss 8683/10000 [=========================>....] - ETA: 10:24 - loss: 2.5172 - regression_loss: 1.8694 - classification_loss 8684/10000 [=========================>....] - ETA: 10:23 - loss: 2.5172 - regression_loss: 1.8694 - classification_loss 8685/10000 [=========================>....] - ETA: 10:23 - loss: 2.5171 - regression_loss: 1.8694 - classification_loss 8686/10000 [=========================>....] - ETA: 10:22 - loss: 2.5171 - regression_loss: 1.8693 - classification_loss 8687/10000 [=========================>....] - ETA: 10:22 - loss: 2.5171 - regression_loss: 1.8694 - classification_loss 8688/10000 [=========================>....] - ETA: 10:21 - loss: 2.5169 - regression_loss: 1.8693 - classification_loss 8689/10000 [=========================>....] - ETA: 10:21 - loss: 2.5169 - regression_loss: 1.8693 - classification_loss 8690/10000 [=========================>....] - ETA: 10:20 - loss: 2.5169 - regression_loss: 1.8692 - classification_loss 8691/10000 [=========================>....] - ETA: 10:20 - loss: 2.5169 - regression_loss: 1.8693 - classification_loss 8692/10000 [=========================>....] - ETA: 10:19 - loss: 2.5170 - regression_loss: 1.8693 - classification_loss 8693/10000 [=========================>....] - ETA: 10:19 - loss: 2.5170 - regression_loss: 1.8694 - classification_loss 8694/10000 [=========================>....] - ETA: 10:18 - loss: 2.5169 - regression_loss: 1.8694 - classification_loss 8695/10000 [=========================>....] - ETA: 10:18 - loss: 2.5169 - regression_loss: 1.8694 - classification_loss 8696/10000 [=========================>....] - ETA: 10:17 - loss: 2.5169 - regression_loss: 1.8694 - classification_loss 8697/10000 [=========================>....] - ETA: 10:17 - loss: 2.5169 - regression_loss: 1.8694 - classification_loss 8698/10000 [=========================>....] - ETA: 10:16 - loss: 2.5169 - regression_loss: 1.8694 - classification_loss 8699/10000 [=========================>....] - ETA: 10:16 - loss: 2.5168 - regression_loss: 1.8693 - classification_loss 8700/10000 [=========================>....] - ETA: 10:15 - loss: 2.5167 - regression_loss: 1.8693 - classification_loss 8701/10000 [=========================>....] - ETA: 10:15 - loss: 2.5167 - regression_loss: 1.8692 - classification_loss 8702/10000 [=========================>....] - ETA: 10:14 - loss: 2.5166 - regression_loss: 1.8693 - classification_loss 8703/10000 [=========================>....] - ETA: 10:14 - loss: 2.5166 - regression_loss: 1.8692 - classification_loss 8704/10000 [=========================>....] - ETA: 10:14 - loss: 2.5165 - regression_loss: 1.8691 - classification_loss 8705/10000 [=========================>....] - ETA: 10:13 - loss: 2.5164 - regression_loss: 1.8691 - classification_loss 8706/10000 [=========================>....] - ETA: 10:13 - loss: 2.5164 - regression_loss: 1.8691 - classification_loss 8707/10000 [=========================>....] - ETA: 10:12 - loss: 2.5162 - regression_loss: 1.8689 - classification_loss 8708/10000 [=========================>....] - ETA: 10:12 - loss: 2.5160 - regression_loss: 1.8688 - classification_loss 8709/10000 [=========================>....] - ETA: 10:11 - loss: 2.5159 - regression_loss: 1.8687 - classification_loss 8710/10000 [=========================>....] - ETA: 10:11 - loss: 2.5158 - regression_loss: 1.8686 - classification_loss 8711/10000 [=========================>....] - ETA: 10:10 - loss: 2.5157 - regression_loss: 1.8685 - classification_loss 8712/10000 [=========================>....] - ETA: 10:10 - loss: 2.5156 - regression_loss: 1.8684 - classification_loss 8713/10000 [=========================>....] - ETA: 10:09 - loss: 2.5155 - regression_loss: 1.8684 - classification_loss 8714/10000 [=========================>....] - ETA: 10:09 - loss: 2.5154 - regression_loss: 1.8683 - classification_loss 8715/10000 [=========================>....] - ETA: 10:08 - loss: 2.5155 - regression_loss: 1.8684 - classification_loss 8716/10000 [=========================>....] - ETA: 10:08 - loss: 2.5155 - regression_loss: 1.8684 - classification_loss 8717/10000 [=========================>....] - ETA: 10:07 - loss: 2.5155 - regression_loss: 1.8684 - classification_loss 8718/10000 [=========================>....] - ETA: 10:07 - loss: 2.5155 - regression_loss: 1.8684 - classification_loss 8719/10000 [=========================>....] - ETA: 10:06 - loss: 2.5155 - regression_loss: 1.8684 - classification_loss 8720/10000 [=========================>....] - ETA: 10:06 - loss: 2.5155 - regression_loss: 1.8684 - classification_loss 8721/10000 [=========================>....] - ETA: 10:05 - loss: 2.5154 - regression_loss: 1.8684 - classification_loss 8722/10000 [=========================>....] - ETA: 10:05 - loss: 2.5153 - regression_loss: 1.8683 - classification_loss 8723/10000 [=========================>....] - ETA: 10:05 - loss: 2.5153 - regression_loss: 1.8683 - classification_loss 8724/10000 [=========================>....] - ETA: 10:04 - loss: 2.5152 - regression_loss: 1.8682 - classification_loss 8725/10000 [=========================>....] - ETA: 10:04 - loss: 2.5151 - regression_loss: 1.8681 - classification_loss 8726/10000 [=========================>....] - ETA: 10:03 - loss: 2.5151 - regression_loss: 1.8681 - classification_loss 8727/10000 [=========================>....] - ETA: 10:03 - loss: 2.5152 - regression_loss: 1.8682 - classification_loss 8728/10000 [=========================>....] - ETA: 10:02 - loss: 2.5152 - regression_loss: 1.8682 - classification_loss 8729/10000 [=========================>....] - ETA: 10:02 - loss: 2.5152 - regression_loss: 1.8682 - classification_loss 8730/10000 [=========================>....] - ETA: 10:01 - loss: 2.5152 - regression_loss: 1.8682 - classification_loss 8731/10000 [=========================>....] - ETA: 10:01 - loss: 2.5151 - regression_loss: 1.8681 - classification_loss 8732/10000 [=========================>....] - ETA: 10:00 - loss: 2.5151 - regression_loss: 1.8681 - classification_loss 8733/10000 [=========================>....] - ETA: 10:00 - loss: 2.5150 - regression_loss: 1.8681 - classification_loss 8734/10000 [=========================>....] - ETA: 9:59 - loss: 2.5151 - regression_loss: 1.8681 - classification_loss: 8735/10000 [=========================>....] - ETA: 9:59 - loss: 2.5150 - regression_loss: 1.8680 - classification_loss: 8736/10000 [=========================>....] - ETA: 9:58 - loss: 2.5150 - regression_loss: 1.8680 - classification_loss: 8737/10000 [=========================>....] - ETA: 9:58 - loss: 2.5149 - regression_loss: 1.8679 - classification_loss: 8738/10000 [=========================>....] - ETA: 9:57 - loss: 2.5146 - regression_loss: 1.8677 - classification_loss: 8739/10000 [=========================>....] - ETA: 9:57 - loss: 2.5145 - regression_loss: 1.8676 - classification_loss: 8740/10000 [=========================>....] - ETA: 9:56 - loss: 2.5145 - regression_loss: 1.8676 - classification_loss: 8741/10000 [=========================>....] - ETA: 9:56 - loss: 2.5144 - regression_loss: 1.8676 - classification_loss: 8742/10000 [=========================>....] - ETA: 9:55 - loss: 2.5143 - regression_loss: 1.8675 - classification_loss: 8743/10000 [=========================>....] - ETA: 9:55 - loss: 2.5143 - regression_loss: 1.8675 - classification_loss: 8744/10000 [=========================>....] - ETA: 9:55 - loss: 2.5142 - regression_loss: 1.8674 - classification_loss: 8745/10000 [=========================>....] - ETA: 9:54 - loss: 2.5141 - regression_loss: 1.8673 - classification_loss: 8746/10000 [=========================>....] - ETA: 9:54 - loss: 2.5140 - regression_loss: 1.8672 - classification_loss: 8747/10000 [=========================>....] - ETA: 9:53 - loss: 2.5140 - regression_loss: 1.8672 - classification_loss: 8748/10000 [=========================>....] - ETA: 9:53 - loss: 2.5139 - regression_loss: 1.8671 - classification_loss: 8749/10000 [=========================>....] - ETA: 9:52 - loss: 2.5140 - regression_loss: 1.8672 - classification_loss: 8750/10000 [=========================>....] - ETA: 9:52 - loss: 2.5139 - regression_loss: 1.8671 - classification_loss: 8751/10000 [=========================>....] - ETA: 9:51 - loss: 2.5140 - regression_loss: 1.8671 - classification_loss: 8752/10000 [=========================>....] - ETA: 9:51 - loss: 2.5139 - regression_loss: 1.8671 - classification_loss: 8753/10000 [=========================>....] - ETA: 9:50 - loss: 2.5139 - regression_loss: 1.8671 - classification_loss: 8754/10000 [=========================>....] - ETA: 9:50 - loss: 2.5139 - regression_loss: 1.8671 - classification_loss: 8755/10000 [=========================>....] - ETA: 9:49 - loss: 2.5139 - regression_loss: 1.8672 - classification_loss: 8756/10000 [=========================>....] - ETA: 9:49 - loss: 2.5138 - regression_loss: 1.8671 - classification_loss: 8757/10000 [=========================>....] - ETA: 9:48 - loss: 2.5138 - regression_loss: 1.8670 - classification_loss: 8758/10000 [=========================>....] - ETA: 9:48 - loss: 2.5137 - regression_loss: 1.8670 - classification_loss: 8759/10000 [=========================>....] - ETA: 9:47 - loss: 2.5137 - regression_loss: 1.8670 - classification_loss: 8760/10000 [=========================>....] - ETA: 9:47 - loss: 2.5137 - regression_loss: 1.8670 - classification_loss: 8761/10000 [=========================>....] - ETA: 9:46 - loss: 2.5136 - regression_loss: 1.8669 - classification_loss: 8762/10000 [=========================>....] - ETA: 9:46 - loss: 2.5136 - regression_loss: 1.8669 - classification_loss: 8763/10000 [=========================>....] - ETA: 9:46 - loss: 2.5136 - regression_loss: 1.8669 - classification_loss: 8764/10000 [=========================>....] - ETA: 9:45 - loss: 2.5135 - regression_loss: 1.8668 - classification_loss: 8765/10000 [=========================>....] - ETA: 9:45 - loss: 2.5134 - regression_loss: 1.8668 - classification_loss: 8766/10000 [=========================>....] - ETA: 9:44 - loss: 2.5132 - regression_loss: 1.8667 - classification_loss: 8767/10000 [=========================>....] - ETA: 9:44 - loss: 2.5131 - regression_loss: 1.8666 - classification_loss: 8768/10000 [=========================>....] - ETA: 9:43 - loss: 2.5131 - regression_loss: 1.8666 - classification_loss: 8769/10000 [=========================>....] - ETA: 9:43 - loss: 2.5130 - regression_loss: 1.8665 - classification_loss: 8770/10000 [=========================>....] - ETA: 9:42 - loss: 2.5130 - regression_loss: 1.8665 - classification_loss: 8771/10000 [=========================>....] - ETA: 9:42 - loss: 2.5130 - regression_loss: 1.8665 - classification_loss: 8772/10000 [=========================>....] - ETA: 9:41 - loss: 2.5130 - regression_loss: 1.8666 - classification_loss: 8773/10000 [=========================>....] - ETA: 9:41 - loss: 2.5130 - regression_loss: 1.8666 - classification_loss: 8774/10000 [=========================>....] - ETA: 9:40 - loss: 2.5130 - regression_loss: 1.8666 - classification_loss: 8775/10000 [=========================>....] - ETA: 9:40 - loss: 2.5129 - regression_loss: 1.8665 - classification_loss: 8776/10000 [=========================>....] - ETA: 9:39 - loss: 2.5130 - regression_loss: 1.8666 - classification_loss: 8777/10000 [=========================>....] - ETA: 9:39 - loss: 2.5130 - regression_loss: 1.8665 - classification_loss: 8778/10000 [=========================>....] - ETA: 9:38 - loss: 2.5128 - regression_loss: 1.8663 - classification_loss: 8779/10000 [=========================>....] - ETA: 9:38 - loss: 2.5128 - regression_loss: 1.8663 - classification_loss: 8780/10000 [=========================>....] - ETA: 9:37 - loss: 2.5127 - regression_loss: 1.8662 - classification_loss: 8781/10000 [=========================>....] - ETA: 9:37 - loss: 2.5125 - regression_loss: 1.8661 - classification_loss: 8782/10000 [=========================>....] - ETA: 9:36 - loss: 2.5124 - regression_loss: 1.8661 - classification_loss: 8783/10000 [=========================>....] - ETA: 9:36 - loss: 2.5125 - regression_loss: 1.8661 - classification_loss: 8784/10000 [=========================>....] - ETA: 9:36 - loss: 2.5125 - regression_loss: 1.8662 - classification_loss: 8785/10000 [=========================>....] - ETA: 9:35 - loss: 2.5125 - regression_loss: 1.8662 - classification_loss: 8786/10000 [=========================>....] - ETA: 9:35 - loss: 2.5124 - regression_loss: 1.8661 - classification_loss: 8787/10000 [=========================>....] - ETA: 9:34 - loss: 2.5124 - regression_loss: 1.8661 - classification_loss: 8788/10000 [=========================>....] - ETA: 9:34 - loss: 2.5124 - regression_loss: 1.8661 - classification_loss: 8789/10000 [=========================>....] - ETA: 9:33 - loss: 2.5124 - regression_loss: 1.8661 - classification_loss: 8790/10000 [=========================>....] - ETA: 9:33 - loss: 2.5126 - regression_loss: 1.8662 - classification_loss: 8791/10000 [=========================>....] - ETA: 9:32 - loss: 2.5124 - regression_loss: 1.8661 - classification_loss: 8792/10000 [=========================>....] - ETA: 9:32 - loss: 2.5124 - regression_loss: 1.8661 - classification_loss: 8793/10000 [=========================>....] - ETA: 9:31 - loss: 2.5122 - regression_loss: 1.8659 - classification_loss: 8794/10000 [=========================>....] - ETA: 9:31 - loss: 2.5122 - regression_loss: 1.8659 - classification_loss: 8795/10000 [=========================>....] - ETA: 9:30 - loss: 2.5121 - regression_loss: 1.8659 - classification_loss: 8796/10000 [=========================>....] - ETA: 9:30 - loss: 2.5121 - regression_loss: 1.8658 - classification_loss: 8797/10000 [=========================>....] - ETA: 9:29 - loss: 2.5120 - regression_loss: 1.8658 - classification_loss: 8798/10000 [=========================>....] - ETA: 9:29 - loss: 2.5120 - regression_loss: 1.8658 - classification_loss: 8799/10000 [=========================>....] - ETA: 9:28 - loss: 2.5120 - regression_loss: 1.8658 - classification_loss: 8800/10000 [=========================>....] - ETA: 9:28 - loss: 2.5119 - regression_loss: 1.8657 - classification_loss: 8801/10000 [=========================>....] - ETA: 9:27 - loss: 2.5119 - regression_loss: 1.8657 - classification_loss: 8802/10000 [=========================>....] - ETA: 9:27 - loss: 2.5118 - regression_loss: 1.8657 - classification_loss: 8803/10000 [=========================>....] - ETA: 9:27 - loss: 2.5117 - regression_loss: 1.8656 - classification_loss: 8804/10000 [=========================>....] - ETA: 9:26 - loss: 2.5117 - regression_loss: 1.8655 - classification_loss: 8805/10000 [=========================>....] - ETA: 9:26 - loss: 2.5116 - regression_loss: 1.8655 - classification_loss: 8806/10000 [=========================>....] - ETA: 9:25 - loss: 2.5116 - regression_loss: 1.8655 - classification_loss: 8807/10000 [=========================>....] - ETA: 9:25 - loss: 2.5116 - regression_loss: 1.8655 - classification_loss: 8808/10000 [=========================>....] - ETA: 9:24 - loss: 2.5115 - regression_loss: 1.8654 - classification_loss: 8809/10000 [=========================>....] - ETA: 9:24 - loss: 2.5115 - regression_loss: 1.8654 - classification_loss: 8810/10000 [=========================>....] - ETA: 9:23 - loss: 2.5115 - regression_loss: 1.8654 - classification_loss: 8811/10000 [=========================>....] - ETA: 9:23 - loss: 2.5115 - regression_loss: 1.8654 - classification_loss: 8812/10000 [=========================>....] - ETA: 9:22 - loss: 2.5114 - regression_loss: 1.8654 - classification_loss: 8813/10000 [=========================>....] - ETA: 9:22 - loss: 2.5114 - regression_loss: 1.8654 - classification_loss: 8814/10000 [=========================>....] - ETA: 9:21 - loss: 2.5114 - regression_loss: 1.8653 - classification_loss: 8815/10000 [=========================>....] - ETA: 9:21 - loss: 2.5113 - regression_loss: 1.8653 - classification_loss: 8816/10000 [=========================>....] - ETA: 9:20 - loss: 2.5113 - regression_loss: 1.8653 - classification_loss: 8817/10000 [=========================>....] - ETA: 9:20 - loss: 2.5111 - regression_loss: 1.8651 - classification_loss: 8818/10000 [=========================>....] - ETA: 9:19 - loss: 2.5111 - regression_loss: 1.8651 - classification_loss: 8819/10000 [=========================>....] - ETA: 9:19 - loss: 2.5111 - regression_loss: 1.8652 - classification_loss: 8820/10000 [=========================>....] - ETA: 9:18 - loss: 2.5111 - regression_loss: 1.8651 - classification_loss: 8821/10000 [=========================>....] - ETA: 9:18 - loss: 2.5110 - regression_loss: 1.8650 - classification_loss: 8822/10000 [=========================>....] - ETA: 9:17 - loss: 2.5110 - regression_loss: 1.8650 - classification_loss: 8823/10000 [=========================>....] - ETA: 9:17 - loss: 2.5110 - regression_loss: 1.8651 - classification_loss: 8824/10000 [=========================>....] - ETA: 9:17 - loss: 2.5108 - regression_loss: 1.8649 - classification_loss: 8825/10000 [=========================>....] - ETA: 9:16 - loss: 2.5109 - regression_loss: 1.8650 - classification_loss: 8826/10000 [=========================>....] - ETA: 9:16 - loss: 2.5108 - regression_loss: 1.8649 - classification_loss: 8827/10000 [=========================>....] - ETA: 9:15 - loss: 2.5107 - regression_loss: 1.8648 - classification_loss: 8828/10000 [=========================>....] - ETA: 9:15 - loss: 2.5107 - regression_loss: 1.8648 - classification_loss: 8829/10000 [=========================>....] - ETA: 9:14 - loss: 2.5105 - regression_loss: 1.8647 - classification_loss: 8830/10000 [=========================>....] - ETA: 9:14 - loss: 2.5105 - regression_loss: 1.8647 - classification_loss: 8831/10000 [=========================>....] - ETA: 9:13 - loss: 2.5105 - regression_loss: 1.8647 - classification_loss: 8832/10000 [=========================>....] - ETA: 9:13 - loss: 2.5106 - regression_loss: 1.8648 - classification_loss: 8833/10000 [=========================>....] - ETA: 9:12 - loss: 2.5106 - regression_loss: 1.8648 - classification_loss: 8834/10000 [=========================>....] - ETA: 9:12 - loss: 2.5106 - regression_loss: 1.8648 - classification_loss: 8835/10000 [=========================>....] - ETA: 9:11 - loss: 2.5106 - regression_loss: 1.8648 - classification_loss: 8836/10000 [=========================>....] - ETA: 9:11 - loss: 2.5105 - regression_loss: 1.8647 - classification_loss: 8837/10000 [=========================>....] - ETA: 9:10 - loss: 2.5104 - regression_loss: 1.8647 - classification_loss: 8838/10000 [=========================>....] - ETA: 9:10 - loss: 2.5104 - regression_loss: 1.8647 - classification_loss: 8839/10000 [=========================>....] - ETA: 9:09 - loss: 2.5103 - regression_loss: 1.8646 - classification_loss: 8840/10000 [=========================>....] - ETA: 9:09 - loss: 2.5101 - regression_loss: 1.8644 - classification_loss: 8841/10000 [=========================>....] - ETA: 9:08 - loss: 2.5100 - regression_loss: 1.8644 - classification_loss: 8842/10000 [=========================>....] - ETA: 9:08 - loss: 2.5100 - regression_loss: 1.8644 - classification_loss: 8843/10000 [=========================>....] - ETA: 9:07 - loss: 2.5100 - regression_loss: 1.8643 - classification_loss: 8844/10000 [=========================>....] - ETA: 9:07 - loss: 2.5099 - regression_loss: 1.8643 - classification_loss: 8845/10000 [=========================>....] - ETA: 9:07 - loss: 2.5100 - regression_loss: 1.8643 - classification_loss: 8846/10000 [=========================>....] - ETA: 9:06 - loss: 2.5098 - regression_loss: 1.8642 - classification_loss: 8847/10000 [=========================>....] - ETA: 9:06 - loss: 2.5098 - regression_loss: 1.8642 - classification_loss: 8848/10000 [=========================>....] - ETA: 9:05 - loss: 2.5096 - regression_loss: 1.8640 - classification_loss: 8849/10000 [=========================>....] - ETA: 9:05 - loss: 2.5095 - regression_loss: 1.8640 - classification_loss: 8850/10000 [=========================>....] - ETA: 9:04 - loss: 2.5095 - regression_loss: 1.8640 - classification_loss: 8851/10000 [=========================>....] - ETA: 9:04 - loss: 2.5095 - regression_loss: 1.8640 - classification_loss: 8852/10000 [=========================>....] - ETA: 9:03 - loss: 2.5095 - regression_loss: 1.8639 - classification_loss: 8853/10000 [=========================>....] - ETA: 9:03 - loss: 2.5095 - regression_loss: 1.8640 - classification_loss: 8854/10000 [=========================>....] - ETA: 9:02 - loss: 2.5095 - regression_loss: 1.8641 - classification_loss: 8855/10000 [=========================>....] - ETA: 9:02 - loss: 2.5095 - regression_loss: 1.8640 - classification_loss: 8856/10000 [=========================>....] - ETA: 9:01 - loss: 2.5094 - regression_loss: 1.8640 - classification_loss: 8857/10000 [=========================>....] - ETA: 9:01 - loss: 2.5094 - regression_loss: 1.8640 - classification_loss: 8858/10000 [=========================>....] - ETA: 9:00 - loss: 2.5094 - regression_loss: 1.8640 - classification_loss: 8859/10000 [=========================>....] - ETA: 9:00 - loss: 2.5093 - regression_loss: 1.8639 - classification_loss: 8860/10000 [=========================>....] - ETA: 8:59 - loss: 2.5094 - regression_loss: 1.8639 - classification_loss: 8861/10000 [=========================>....] - ETA: 8:59 - loss: 2.5092 - regression_loss: 1.8638 - classification_loss: 8862/10000 [=========================>....] - ETA: 8:58 - loss: 2.5092 - regression_loss: 1.8638 - classification_loss: 8863/10000 [=========================>....] - ETA: 8:58 - loss: 2.5090 - regression_loss: 1.8637 - classification_loss: 8864/10000 [=========================>....] - ETA: 8:58 - loss: 2.5090 - regression_loss: 1.8637 - classification_loss: 8865/10000 [=========================>....] - ETA: 8:57 - loss: 2.5089 - regression_loss: 1.8636 - classification_loss: 8866/10000 [=========================>....] - ETA: 8:57 - loss: 2.5089 - regression_loss: 1.8636 - classification_loss: 8867/10000 [=========================>....] - ETA: 8:56 - loss: 2.5089 - regression_loss: 1.8636 - classification_loss: 8868/10000 [=========================>....] - ETA: 8:56 - loss: 2.5088 - regression_loss: 1.8635 - classification_loss: 8869/10000 [=========================>....] - ETA: 8:55 - loss: 2.5086 - regression_loss: 1.8634 - classification_loss: 8870/10000 [=========================>....] - ETA: 8:55 - loss: 2.5086 - regression_loss: 1.8634 - classification_loss: 8871/10000 [=========================>....] - ETA: 8:54 - loss: 2.5086 - regression_loss: 1.8634 - classification_loss: 8872/10000 [=========================>....] - ETA: 8:54 - loss: 2.5086 - regression_loss: 1.8634 - classification_loss: 8873/10000 [=========================>....] - ETA: 8:53 - loss: 2.5085 - regression_loss: 1.8633 - classification_loss: 8874/10000 [=========================>....] - ETA: 8:53 - loss: 2.5085 - regression_loss: 1.8634 - classification_loss: 8875/10000 [=========================>....] - ETA: 8:52 - loss: 2.5086 - regression_loss: 1.8634 - classification_loss: 8876/10000 [=========================>....] - ETA: 8:52 - loss: 2.5085 - regression_loss: 1.8634 - classification_loss: 8877/10000 [=========================>....] - ETA: 8:51 - loss: 2.5085 - regression_loss: 1.8633 - classification_loss: 8878/10000 [=========================>....] - ETA: 8:51 - loss: 2.5084 - regression_loss: 1.8633 - classification_loss: 8879/10000 [=========================>....] - ETA: 8:50 - loss: 2.5084 - regression_loss: 1.8633 - classification_loss: 8880/10000 [=========================>....] - ETA: 8:50 - loss: 2.5082 - regression_loss: 1.8632 - classification_loss: 8881/10000 [=========================>....] - ETA: 8:49 - loss: 2.5082 - regression_loss: 1.8631 - classification_loss: 8882/10000 [=========================>....] - ETA: 8:49 - loss: 2.5080 - regression_loss: 1.8630 - classification_loss: 8883/10000 [=========================>....] - ETA: 8:48 - loss: 2.5080 - regression_loss: 1.8630 - classification_loss: 8884/10000 [=========================>....] - ETA: 8:48 - loss: 2.5080 - regression_loss: 1.8630 - classification_loss: 8885/10000 [=========================>....] - ETA: 8:48 - loss: 2.5079 - regression_loss: 1.8629 - classification_loss: 8886/10000 [=========================>....] - ETA: 8:47 - loss: 2.5079 - regression_loss: 1.8630 - classification_loss: 8887/10000 [=========================>....] - ETA: 8:47 - loss: 2.5078 - regression_loss: 1.8629 - classification_loss: 8888/10000 [=========================>....] - ETA: 8:46 - loss: 2.5077 - regression_loss: 1.8628 - classification_loss: 8889/10000 [=========================>....] - ETA: 8:46 - loss: 2.5077 - regression_loss: 1.8628 - classification_loss: 8890/10000 [=========================>....] - ETA: 8:45 - loss: 2.5077 - regression_loss: 1.8628 - classification_loss: 8891/10000 [=========================>....] - ETA: 8:45 - loss: 2.5076 - regression_loss: 1.8627 - classification_loss: 8892/10000 [=========================>....] - ETA: 8:44 - loss: 2.5075 - regression_loss: 1.8626 - classification_loss: 8893/10000 [=========================>....] - ETA: 8:44 - loss: 2.5074 - regression_loss: 1.8626 - classification_loss: 8894/10000 [=========================>....] - ETA: 8:43 - loss: 2.5073 - regression_loss: 1.8625 - classification_loss: 8895/10000 [=========================>....] - ETA: 8:43 - loss: 2.5073 - regression_loss: 1.8625 - classification_loss: 8896/10000 [=========================>....] - ETA: 8:42 - loss: 2.5072 - regression_loss: 1.8624 - classification_loss: 8897/10000 [=========================>....] - ETA: 8:42 - loss: 2.5070 - regression_loss: 1.8623 - classification_loss: 8898/10000 [=========================>....] - ETA: 8:41 - loss: 2.5070 - regression_loss: 1.8623 - classification_loss: 8899/10000 [=========================>....] - ETA: 8:41 - loss: 2.5070 - regression_loss: 1.8623 - classification_loss: 8900/10000 [=========================>....] - ETA: 8:40 - loss: 2.5070 - regression_loss: 1.8623 - classification_loss: 8901/10000 [=========================>....] - ETA: 8:40 - loss: 2.5069 - regression_loss: 1.8622 - classification_loss: 8902/10000 [=========================>....] - ETA: 8:39 - loss: 2.5067 - regression_loss: 1.8621 - classification_loss: 8903/10000 [=========================>....] - ETA: 8:39 - loss: 2.5066 - regression_loss: 1.8620 - classification_loss: 8904/10000 [=========================>....] - ETA: 8:39 - loss: 2.5066 - regression_loss: 1.8620 - classification_loss: 8905/10000 [=========================>....] - ETA: 8:38 - loss: 2.5065 - regression_loss: 1.8619 - classification_loss: 8906/10000 [=========================>....] - ETA: 8:38 - loss: 2.5064 - regression_loss: 1.8618 - classification_loss: 8907/10000 [=========================>....] - ETA: 8:37 - loss: 2.5063 - regression_loss: 1.8618 - classification_loss: 8908/10000 [=========================>....] - ETA: 8:37 - loss: 2.5064 - regression_loss: 1.8618 - classification_loss: 8909/10000 [=========================>....] - ETA: 8:36 - loss: 2.5063 - regression_loss: 1.8618 - classification_loss: 8910/10000 [=========================>....] - ETA: 8:36 - loss: 2.5062 - regression_loss: 1.8617 - classification_loss: 8911/10000 [=========================>....] - ETA: 8:35 - loss: 2.5061 - regression_loss: 1.8617 - classification_loss: 8912/10000 [=========================>....] - ETA: 8:35 - loss: 2.5061 - regression_loss: 1.8617 - classification_loss: 8913/10000 [=========================>....] - ETA: 8:34 - loss: 2.5061 - regression_loss: 1.8616 - classification_loss: 8914/10000 [=========================>....] - ETA: 8:34 - loss: 2.5060 - regression_loss: 1.8616 - classification_loss: 8915/10000 [=========================>....] - ETA: 8:33 - loss: 2.5060 - regression_loss: 1.8616 - classification_loss: 8916/10000 [=========================>....] - ETA: 8:33 - loss: 2.5059 - regression_loss: 1.8616 - classification_loss: 8917/10000 [=========================>....] - ETA: 8:32 - loss: 2.5058 - regression_loss: 1.8614 - classification_loss: 8918/10000 [=========================>....] - ETA: 8:32 - loss: 2.5057 - regression_loss: 1.8614 - classification_loss: 8919/10000 [=========================>....] - ETA: 8:31 - loss: 2.5057 - regression_loss: 1.8614 - classification_loss: 8920/10000 [=========================>....] - ETA: 8:31 - loss: 2.5056 - regression_loss: 1.8613 - classification_loss: 8921/10000 [=========================>....] - ETA: 8:30 - loss: 2.5055 - regression_loss: 1.8613 - classification_loss: 8922/10000 [=========================>....] - ETA: 8:30 - loss: 2.5054 - regression_loss: 1.8612 - classification_loss: 8923/10000 [=========================>....] - ETA: 8:30 - loss: 2.5053 - regression_loss: 1.8611 - classification_loss: 8924/10000 [=========================>....] - ETA: 8:29 - loss: 2.5052 - regression_loss: 1.8611 - classification_loss: 8925/10000 [=========================>....] - ETA: 8:29 - loss: 2.5051 - regression_loss: 1.8609 - classification_loss: 8926/10000 [=========================>....] - ETA: 8:28 - loss: 2.5051 - regression_loss: 1.8609 - classification_loss: 8927/10000 [=========================>....] - ETA: 8:28 - loss: 2.5050 - regression_loss: 1.8609 - classification_loss: 8928/10000 [=========================>....] - ETA: 8:27 - loss: 2.5049 - regression_loss: 1.8608 - classification_loss: 8929/10000 [=========================>....] - ETA: 8:27 - loss: 2.5049 - regression_loss: 1.8608 - classification_loss: 8930/10000 [=========================>....] - ETA: 8:26 - loss: 2.5048 - regression_loss: 1.8608 - classification_loss: 8931/10000 [=========================>....] - ETA: 8:26 - loss: 2.5048 - regression_loss: 1.8607 - classification_loss: 8932/10000 [=========================>....] - ETA: 8:25 - loss: 2.5047 - regression_loss: 1.8606 - classification_loss: 8933/10000 [=========================>....] - ETA: 8:25 - loss: 2.5045 - regression_loss: 1.8605 - classification_loss: 8934/10000 [=========================>....] - ETA: 8:24 - loss: 2.5045 - regression_loss: 1.8605 - classification_loss: 8935/10000 [=========================>....] - ETA: 8:24 - loss: 2.5045 - regression_loss: 1.8605 - classification_loss: 8936/10000 [=========================>....] - ETA: 8:23 - loss: 2.5044 - regression_loss: 1.8604 - classification_loss: 8937/10000 [=========================>....] - ETA: 8:23 - loss: 2.5043 - regression_loss: 1.8603 - classification_loss: 8938/10000 [=========================>....] - ETA: 8:22 - loss: 2.5042 - regression_loss: 1.8603 - classification_loss: 8939/10000 [=========================>....] - ETA: 8:22 - loss: 2.5041 - regression_loss: 1.8602 - classification_loss: 8940/10000 [=========================>....] - ETA: 8:21 - loss: 2.5041 - regression_loss: 1.8602 - classification_loss: 8941/10000 [=========================>....] - ETA: 8:21 - loss: 2.5040 - regression_loss: 1.8601 - classification_loss: 8942/10000 [=========================>....] - ETA: 8:20 - loss: 2.5039 - regression_loss: 1.8601 - classification_loss: 8943/10000 [=========================>....] - ETA: 8:20 - loss: 2.5038 - regression_loss: 1.8600 - classification_loss: 8944/10000 [=========================>....] - ETA: 8:20 - loss: 2.5038 - regression_loss: 1.8600 - classification_loss: 8945/10000 [=========================>....] - ETA: 8:19 - loss: 2.5037 - regression_loss: 1.8600 - classification_loss: 8946/10000 [=========================>....] - ETA: 8:19 - loss: 2.5036 - regression_loss: 1.8599 - classification_loss: 8947/10000 [=========================>....] - ETA: 8:18 - loss: 2.5035 - regression_loss: 1.8598 - classification_loss: 8948/10000 [=========================>....] - ETA: 8:18 - loss: 2.5034 - regression_loss: 1.8598 - classification_loss: 8949/10000 [=========================>....] - ETA: 8:17 - loss: 2.5033 - regression_loss: 1.8597 - classification_loss: 8950/10000 [=========================>....] - ETA: 8:17 - loss: 2.5033 - regression_loss: 1.8597 - classification_loss: 8951/10000 [=========================>....] - ETA: 8:16 - loss: 2.5032 - regression_loss: 1.8596 - classification_loss: 8952/10000 [=========================>....] - ETA: 8:16 - loss: 2.5032 - regression_loss: 1.8597 - classification_loss: 8953/10000 [=========================>....] - ETA: 8:15 - loss: 2.5031 - regression_loss: 1.8596 - classification_loss: 8954/10000 [=========================>....] - ETA: 8:15 - loss: 2.5029 - regression_loss: 1.8595 - classification_loss: 8955/10000 [=========================>....] - ETA: 8:14 - loss: 2.5029 - regression_loss: 1.8594 - classification_loss: 8956/10000 [=========================>....] - ETA: 8:14 - loss: 2.5029 - regression_loss: 1.8594 - classification_loss: 8957/10000 [=========================>....] - ETA: 8:13 - loss: 2.5029 - regression_loss: 1.8594 - classification_loss: 8958/10000 [=========================>....] - ETA: 8:13 - loss: 2.5028 - regression_loss: 1.8594 - classification_loss: 8959/10000 [=========================>....] - ETA: 8:12 - loss: 2.5026 - regression_loss: 1.8592 - classification_loss: 8960/10000 [=========================>....] - ETA: 8:12 - loss: 2.5025 - regression_loss: 1.8592 - classification_loss: 8961/10000 [=========================>....] - ETA: 8:11 - loss: 2.5025 - regression_loss: 1.8591 - classification_loss: 8962/10000 [=========================>....] - ETA: 8:11 - loss: 2.5025 - regression_loss: 1.8591 - classification_loss: 8963/10000 [=========================>....] - ETA: 8:11 - loss: 2.5023 - regression_loss: 1.8590 - classification_loss: 8964/10000 [=========================>....] - ETA: 8:10 - loss: 2.5022 - regression_loss: 1.8589 - classification_loss: 8965/10000 [=========================>....] - ETA: 8:10 - loss: 2.5022 - regression_loss: 1.8589 - classification_loss: 8966/10000 [=========================>....] - ETA: 8:09 - loss: 2.5022 - regression_loss: 1.8589 - classification_loss: 8967/10000 [=========================>....] - ETA: 8:09 - loss: 2.5022 - regression_loss: 1.8589 - classification_loss: 8968/10000 [=========================>....] - ETA: 8:08 - loss: 2.5022 - regression_loss: 1.8589 - classification_loss: 8969/10000 [=========================>....] - ETA: 8:08 - loss: 2.5022 - regression_loss: 1.8589 - classification_loss: 8970/10000 [=========================>....] - ETA: 8:07 - loss: 2.5022 - regression_loss: 1.8590 - classification_loss: 8971/10000 [=========================>....] - ETA: 8:07 - loss: 2.5022 - regression_loss: 1.8590 - classification_loss: 8972/10000 [=========================>....] - ETA: 8:06 - loss: 2.5021 - regression_loss: 1.8589 - classification_loss: 8973/10000 [=========================>....] - ETA: 8:06 - loss: 2.5021 - regression_loss: 1.8589 - classification_loss: 8974/10000 [=========================>....] - ETA: 8:05 - loss: 2.5022 - regression_loss: 1.8590 - classification_loss: 8975/10000 [=========================>....] - ETA: 8:05 - loss: 2.5022 - regression_loss: 1.8590 - classification_loss: 8976/10000 [=========================>....] - ETA: 8:04 - loss: 2.5021 - regression_loss: 1.8590 - classification_loss: 8977/10000 [=========================>....] - ETA: 8:04 - loss: 2.5021 - regression_loss: 1.8590 - classification_loss: 8978/10000 [=========================>....] - ETA: 8:03 - loss: 2.5020 - regression_loss: 1.8589 - classification_loss: 8979/10000 [=========================>....] - ETA: 8:03 - loss: 2.5019 - regression_loss: 1.8588 - classification_loss: 8980/10000 [=========================>....] - ETA: 8:02 - loss: 2.5018 - regression_loss: 1.8588 - classification_loss: 8981/10000 [=========================>....] - ETA: 8:02 - loss: 2.5019 - regression_loss: 1.8588 - classification_loss: 8982/10000 [=========================>....] - ETA: 8:01 - loss: 2.5018 - regression_loss: 1.8588 - classification_loss: 8983/10000 [=========================>....] - ETA: 8:01 - loss: 2.5018 - regression_loss: 1.8588 - classification_loss: 8984/10000 [=========================>....] - ETA: 8:01 - loss: 2.5019 - regression_loss: 1.8588 - classification_loss: 8985/10000 [=========================>....] - ETA: 8:00 - loss: 2.5017 - regression_loss: 1.8587 - classification_loss: 8986/10000 [=========================>....] - ETA: 8:00 - loss: 2.5017 - regression_loss: 1.8587 - classification_loss: 8987/10000 [=========================>....] - ETA: 7:59 - loss: 2.5016 - regression_loss: 1.8586 - classification_loss: 8988/10000 [=========================>....] - ETA: 7:59 - loss: 2.5015 - regression_loss: 1.8585 - classification_loss: 8989/10000 [=========================>....] - ETA: 7:58 - loss: 2.5016 - regression_loss: 1.8586 - classification_loss: 8990/10000 [=========================>....] - ETA: 7:58 - loss: 2.5015 - regression_loss: 1.8585 - classification_loss: 8991/10000 [=========================>....] - ETA: 7:57 - loss: 2.5014 - regression_loss: 1.8584 - classification_loss: 8992/10000 [=========================>....] - ETA: 7:57 - loss: 2.5014 - regression_loss: 1.8585 - classification_loss: 8993/10000 [=========================>....] - ETA: 7:56 - loss: 2.5013 - regression_loss: 1.8584 - classification_loss: 8994/10000 [=========================>....] - ETA: 7:56 - loss: 2.5013 - regression_loss: 1.8584 - classification_loss: 8995/10000 [=========================>....] - ETA: 7:55 - loss: 2.5012 - regression_loss: 1.8584 - classification_loss: 8996/10000 [=========================>....] - ETA: 7:55 - loss: 2.5012 - regression_loss: 1.8584 - classification_loss: 8997/10000 [=========================>....] - ETA: 7:54 - loss: 2.5011 - regression_loss: 1.8583 - classification_loss: 8998/10000 [=========================>....] - ETA: 7:54 - loss: 2.5010 - regression_loss: 1.8583 - classification_loss: 8999/10000 [=========================>....] - ETA: 7:53 - loss: 2.5010 - regression_loss: 1.8582 - classification_loss: 9000/10000 [==========================>...] - ETA: 7:53 - loss: 2.5009 - regression_loss: 1.8582 - classification_loss: 9001/10000 [==========================>...] - ETA: 7:52 - loss: 2.5008 - regression_loss: 1.8581 - classification_loss: 9002/10000 [==========================>...] - ETA: 7:52 - loss: 2.5006 - regression_loss: 1.8580 - classification_loss: 9003/10000 [==========================>...] - ETA: 7:51 - loss: 2.5007 - regression_loss: 1.8580 - classification_loss: 9004/10000 [==========================>...] - ETA: 7:51 - loss: 2.5006 - regression_loss: 1.8580 - classification_loss: 9005/10000 [==========================>...] - ETA: 7:51 - loss: 2.5005 - regression_loss: 1.8579 - classification_loss: 9006/10000 [==========================>...] - ETA: 7:50 - loss: 2.5005 - regression_loss: 1.8580 - classification_loss: 9007/10000 [==========================>...] - ETA: 7:50 - loss: 2.5004 - regression_loss: 1.8579 - classification_loss: 9008/10000 [==========================>...] - ETA: 7:49 - loss: 2.5004 - regression_loss: 1.8579 - classification_loss: 9009/10000 [==========================>...] - ETA: 7:49 - loss: 2.5003 - regression_loss: 1.8579 - classification_loss: 9010/10000 [==========================>...] - ETA: 7:48 - loss: 2.5002 - regression_loss: 1.8578 - classification_loss: 9011/10000 [==========================>...] - ETA: 7:48 - loss: 2.5002 - regression_loss: 1.8577 - classification_loss: 9012/10000 [==========================>...] - ETA: 7:47 - loss: 2.5002 - regression_loss: 1.8577 - classification_loss: 9013/10000 [==========================>...] - ETA: 7:47 - loss: 2.5000 - regression_loss: 1.8576 - classification_loss: 9014/10000 [==========================>...] - ETA: 7:46 - loss: 2.5000 - regression_loss: 1.8576 - classification_loss: 9015/10000 [==========================>...] - ETA: 7:46 - loss: 2.5000 - regression_loss: 1.8576 - classification_loss: 9016/10000 [==========================>...] - ETA: 7:45 - loss: 2.4999 - regression_loss: 1.8576 - classification_loss: 9017/10000 [==========================>...] - ETA: 7:45 - loss: 2.4999 - regression_loss: 1.8576 - classification_loss: 9018/10000 [==========================>...] - ETA: 7:44 - loss: 2.4998 - regression_loss: 1.8575 - classification_loss: 9019/10000 [==========================>...] - ETA: 7:44 - loss: 2.4997 - regression_loss: 1.8575 - classification_loss: 9020/10000 [==========================>...] - ETA: 7:43 - loss: 2.4997 - regression_loss: 1.8575 - classification_loss: 9021/10000 [==========================>...] - ETA: 7:43 - loss: 2.4997 - regression_loss: 1.8575 - classification_loss: 9022/10000 [==========================>...] - ETA: 7:42 - loss: 2.4996 - regression_loss: 1.8574 - classification_loss: 9023/10000 [==========================>...] - ETA: 7:42 - loss: 2.4996 - regression_loss: 1.8574 - classification_loss: 9024/10000 [==========================>...] - ETA: 7:42 - loss: 2.4995 - regression_loss: 1.8574 - classification_loss: 9025/10000 [==========================>...] - ETA: 7:41 - loss: 2.4995 - regression_loss: 1.8574 - classification_loss: 9026/10000 [==========================>...] - ETA: 7:41 - loss: 2.4994 - regression_loss: 1.8573 - classification_loss: 9027/10000 [==========================>...] - ETA: 7:40 - loss: 2.4994 - regression_loss: 1.8573 - classification_loss: 9028/10000 [==========================>...] - ETA: 7:40 - loss: 2.4994 - regression_loss: 1.8574 - classification_loss: 9029/10000 [==========================>...] - ETA: 7:39 - loss: 2.4993 - regression_loss: 1.8573 - classification_loss: 9030/10000 [==========================>...] - ETA: 7:39 - loss: 2.4993 - regression_loss: 1.8573 - classification_loss: 9031/10000 [==========================>...] - ETA: 7:38 - loss: 2.4992 - regression_loss: 1.8572 - classification_loss: 9032/10000 [==========================>...] - ETA: 7:38 - loss: 2.4991 - regression_loss: 1.8571 - classification_loss: 9033/10000 [==========================>...] - ETA: 7:37 - loss: 2.4990 - regression_loss: 1.8571 - classification_loss: 9034/10000 [==========================>...] - ETA: 7:37 - loss: 2.4989 - regression_loss: 1.8570 - classification_loss: 9035/10000 [==========================>...] - ETA: 7:36 - loss: 2.4988 - regression_loss: 1.8570 - classification_loss: 9036/10000 [==========================>...] - ETA: 7:36 - loss: 2.4988 - regression_loss: 1.8569 - classification_loss: 9037/10000 [==========================>...] - ETA: 7:35 - loss: 2.4988 - regression_loss: 1.8569 - classification_loss: 9038/10000 [==========================>...] - ETA: 7:35 - loss: 2.4987 - regression_loss: 1.8569 - classification_loss: 9039/10000 [==========================>...] - ETA: 7:34 - loss: 2.4987 - regression_loss: 1.8569 - classification_loss: 9040/10000 [==========================>...] - ETA: 7:34 - loss: 2.4987 - regression_loss: 1.8569 - classification_loss: 9041/10000 [==========================>...] - ETA: 7:33 - loss: 2.4987 - regression_loss: 1.8569 - classification_loss: 9042/10000 [==========================>...] - ETA: 7:33 - loss: 2.4986 - regression_loss: 1.8569 - classification_loss: 9043/10000 [==========================>...] - ETA: 7:32 - loss: 2.4986 - regression_loss: 1.8569 - classification_loss: 9044/10000 [==========================>...] - ETA: 7:32 - loss: 2.4985 - regression_loss: 1.8568 - classification_loss: 9045/10000 [==========================>...] - ETA: 7:32 - loss: 2.4986 - regression_loss: 1.8569 - classification_loss: 9046/10000 [==========================>...] - ETA: 7:31 - loss: 2.4985 - regression_loss: 1.8569 - classification_loss: 9047/10000 [==========================>...] - ETA: 7:31 - loss: 2.4985 - regression_loss: 1.8568 - classification_loss: 9048/10000 [==========================>...] - ETA: 7:30 - loss: 2.4985 - regression_loss: 1.8569 - classification_loss: 9049/10000 [==========================>...] - ETA: 7:30 - loss: 2.4984 - regression_loss: 1.8568 - classification_loss: 9050/10000 [==========================>...] - ETA: 7:29 - loss: 2.4984 - regression_loss: 1.8568 - classification_loss: 9051/10000 [==========================>...] - ETA: 7:29 - loss: 2.4984 - regression_loss: 1.8568 - classification_loss: 9052/10000 [==========================>...] - ETA: 7:28 - loss: 2.4983 - regression_loss: 1.8567 - classification_loss: 9053/10000 [==========================>...] - ETA: 7:28 - loss: 2.4982 - regression_loss: 1.8567 - classification_loss: 9054/10000 [==========================>...] - ETA: 7:27 - loss: 2.4982 - regression_loss: 1.8567 - classification_loss: 9055/10000 [==========================>...] - ETA: 7:27 - loss: 2.4981 - regression_loss: 1.8566 - classification_loss: 9056/10000 [==========================>...] - ETA: 7:26 - loss: 2.4982 - regression_loss: 1.8567 - classification_loss: 9057/10000 [==========================>...] - ETA: 7:26 - loss: 2.4982 - regression_loss: 1.8567 - classification_loss: 9058/10000 [==========================>...] - ETA: 7:25 - loss: 2.4982 - regression_loss: 1.8567 - classification_loss: 9059/10000 [==========================>...] - ETA: 7:25 - loss: 2.4980 - regression_loss: 1.8566 - classification_loss: 9060/10000 [==========================>...] - ETA: 7:24 - loss: 2.4980 - regression_loss: 1.8566 - classification_loss: 9061/10000 [==========================>...] - ETA: 7:24 - loss: 2.4980 - regression_loss: 1.8566 - classification_loss: 9062/10000 [==========================>...] - ETA: 7:23 - loss: 2.4980 - regression_loss: 1.8565 - classification_loss: 9063/10000 [==========================>...] - ETA: 7:23 - loss: 2.4979 - regression_loss: 1.8565 - classification_loss: 9064/10000 [==========================>...] - ETA: 7:23 - loss: 2.4978 - regression_loss: 1.8565 - classification_loss: 9065/10000 [==========================>...] - ETA: 7:22 - loss: 2.4978 - regression_loss: 1.8564 - classification_loss: 9066/10000 [==========================>...] - ETA: 7:22 - loss: 2.4977 - regression_loss: 1.8564 - classification_loss: 9067/10000 [==========================>...] - ETA: 7:21 - loss: 2.4976 - regression_loss: 1.8563 - classification_loss: 9068/10000 [==========================>...] - ETA: 7:21 - loss: 2.4975 - regression_loss: 1.8562 - classification_loss: 9069/10000 [==========================>...] - ETA: 7:20 - loss: 2.4973 - regression_loss: 1.8561 - classification_loss: 9070/10000 [==========================>...] - ETA: 7:20 - loss: 2.4973 - regression_loss: 1.8560 - classification_loss: 9071/10000 [==========================>...] - ETA: 7:19 - loss: 2.4972 - regression_loss: 1.8559 - classification_loss: 9072/10000 [==========================>...] - ETA: 7:19 - loss: 2.4970 - regression_loss: 1.8558 - classification_loss: 9073/10000 [==========================>...] - ETA: 7:18 - loss: 2.4970 - regression_loss: 1.8558 - classification_loss: 9074/10000 [==========================>...] - ETA: 7:18 - loss: 2.4969 - regression_loss: 1.8557 - classification_loss: 9075/10000 [==========================>...] - ETA: 7:17 - loss: 2.4968 - regression_loss: 1.8557 - classification_loss: 9076/10000 [==========================>...] - ETA: 7:17 - loss: 2.4968 - regression_loss: 1.8556 - classification_loss: 9077/10000 [==========================>...] - ETA: 7:16 - loss: 2.4968 - regression_loss: 1.8556 - classification_loss: 9078/10000 [==========================>...] - ETA: 7:16 - loss: 2.4967 - regression_loss: 1.8556 - classification_loss: 9079/10000 [==========================>...] - ETA: 7:15 - loss: 2.4968 - regression_loss: 1.8557 - classification_loss: 9080/10000 [==========================>...] - ETA: 7:15 - loss: 2.4967 - regression_loss: 1.8556 - classification_loss: 9081/10000 [==========================>...] - ETA: 7:14 - loss: 2.4967 - regression_loss: 1.8556 - classification_loss: 9082/10000 [==========================>...] - ETA: 7:14 - loss: 2.4966 - regression_loss: 1.8555 - classification_loss: 9083/10000 [==========================>...] - ETA: 7:14 - loss: 2.4965 - regression_loss: 1.8554 - classification_loss: 9084/10000 [==========================>...] - ETA: 7:13 - loss: 2.4965 - regression_loss: 1.8554 - classification_loss: 9085/10000 [==========================>...] - ETA: 7:13 - loss: 2.4965 - regression_loss: 1.8554 - classification_loss: 9086/10000 [==========================>...] - ETA: 7:12 - loss: 2.4963 - regression_loss: 1.8553 - classification_loss: 9087/10000 [==========================>...] - ETA: 7:12 - loss: 2.4964 - regression_loss: 1.8554 - classification_loss: 9088/10000 [==========================>...] - ETA: 7:11 - loss: 2.4962 - regression_loss: 1.8553 - classification_loss: 9089/10000 [==========================>...] - ETA: 7:11 - loss: 2.4962 - regression_loss: 1.8553 - classification_loss: 9090/10000 [==========================>...] - ETA: 7:10 - loss: 2.4962 - regression_loss: 1.8553 - classification_loss: 9091/10000 [==========================>...] - ETA: 7:10 - loss: 2.4962 - regression_loss: 1.8553 - classification_loss: 9092/10000 [==========================>...] - ETA: 7:09 - loss: 2.4960 - regression_loss: 1.8552 - classification_loss: 9093/10000 [==========================>...] - ETA: 7:09 - loss: 2.4961 - regression_loss: 1.8552 - classification_loss: 9094/10000 [==========================>...] - ETA: 7:08 - loss: 2.4960 - regression_loss: 1.8552 - classification_loss: 9095/10000 [==========================>...] - ETA: 7:08 - loss: 2.4960 - regression_loss: 1.8552 - classification_loss: 9096/10000 [==========================>...] - ETA: 7:07 - loss: 2.4959 - regression_loss: 1.8551 - classification_loss: 9097/10000 [==========================>...] - ETA: 7:07 - loss: 2.4958 - regression_loss: 1.8551 - classification_loss: 9098/10000 [==========================>...] - ETA: 7:06 - loss: 2.4958 - regression_loss: 1.8551 - classification_loss: 9099/10000 [==========================>...] - ETA: 7:06 - loss: 2.4958 - regression_loss: 1.8550 - classification_loss: 9100/10000 [==========================>...] - ETA: 7:05 - loss: 2.4956 - regression_loss: 1.8549 - classification_loss: 9101/10000 [==========================>...] - ETA: 7:05 - loss: 2.4956 - regression_loss: 1.8549 - classification_loss: 9102/10000 [==========================>...] - ETA: 7:05 - loss: 2.4955 - regression_loss: 1.8549 - classification_loss: 9103/10000 [==========================>...] - ETA: 7:04 - loss: 2.4955 - regression_loss: 1.8549 - classification_loss: 9104/10000 [==========================>...] - ETA: 7:04 - loss: 2.4955 - regression_loss: 1.8547 - classification_loss: 9105/10000 [==========================>...] - ETA: 7:03 - loss: 2.4953 - regression_loss: 1.8545 - classification_loss: 9106/10000 [==========================>...] - ETA: 7:03 - loss: 2.4952 - regression_loss: 1.8545 - classification_loss: 9107/10000 [==========================>...] - ETA: 7:02 - loss: 2.4952 - regression_loss: 1.8545 - classification_loss: 9108/10000 [==========================>...] - ETA: 7:02 - loss: 2.4950 - regression_loss: 1.8543 - classification_loss: 9109/10000 [==========================>...] - ETA: 7:01 - loss: 2.4949 - regression_loss: 1.8543 - classification_loss: 9110/10000 [==========================>...] - ETA: 7:01 - loss: 2.4949 - regression_loss: 1.8543 - classification_loss: 9111/10000 [==========================>...] - ETA: 7:00 - loss: 2.4948 - regression_loss: 1.8542 - classification_loss: 9112/10000 [==========================>...] - ETA: 7:00 - loss: 2.4948 - regression_loss: 1.8542 - classification_loss: 9113/10000 [==========================>...] - ETA: 6:59 - loss: 2.4946 - regression_loss: 1.8540 - classification_loss: 9114/10000 [==========================>...] - ETA: 6:59 - loss: 2.4945 - regression_loss: 1.8539 - classification_loss: 9115/10000 [==========================>...] - ETA: 6:58 - loss: 2.4946 - regression_loss: 1.8540 - classification_loss: 9116/10000 [==========================>...] - ETA: 6:58 - loss: 2.4945 - regression_loss: 1.8540 - classification_loss: 9117/10000 [==========================>...] - ETA: 6:57 - loss: 2.4944 - regression_loss: 1.8539 - classification_loss: 9118/10000 [==========================>...] - ETA: 6:57 - loss: 2.4943 - regression_loss: 1.8538 - classification_loss: 9119/10000 [==========================>...] - ETA: 6:56 - loss: 2.4943 - regression_loss: 1.8538 - classification_loss: 9120/10000 [==========================>...] - ETA: 6:56 - loss: 2.4942 - regression_loss: 1.8537 - classification_loss: 9121/10000 [==========================>...] - ETA: 6:55 - loss: 2.4940 - regression_loss: 1.8536 - classification_loss: 9122/10000 [==========================>...] - ETA: 6:55 - loss: 2.4939 - regression_loss: 1.8535 - classification_loss: 9123/10000 [==========================>...] - ETA: 6:55 - loss: 2.4938 - regression_loss: 1.8534 - classification_loss: 9124/10000 [==========================>...] - ETA: 6:54 - loss: 2.4938 - regression_loss: 1.8534 - classification_loss: 9125/10000 [==========================>...] - ETA: 6:54 - loss: 2.4937 - regression_loss: 1.8533 - classification_loss: 9126/10000 [==========================>...] - ETA: 6:53 - loss: 2.4937 - regression_loss: 1.8533 - classification_loss: 9127/10000 [==========================>...] - ETA: 6:53 - loss: 2.4937 - regression_loss: 1.8534 - classification_loss: 9128/10000 [==========================>...] - ETA: 6:52 - loss: 2.4936 - regression_loss: 1.8533 - classification_loss: 9129/10000 [==========================>...] - ETA: 6:52 - loss: 2.4936 - regression_loss: 1.8533 - classification_loss: 9130/10000 [==========================>...] - ETA: 6:51 - loss: 2.4937 - regression_loss: 1.8533 - classification_loss: 9131/10000 [==========================>...] - ETA: 6:51 - loss: 2.4936 - regression_loss: 1.8532 - classification_loss: 9132/10000 [==========================>...] - ETA: 6:50 - loss: 2.4935 - regression_loss: 1.8532 - classification_loss: 9133/10000 [==========================>...] - ETA: 6:50 - loss: 2.4934 - regression_loss: 1.8531 - classification_loss: 9134/10000 [==========================>...] - ETA: 6:49 - loss: 2.4933 - regression_loss: 1.8530 - classification_loss: 9135/10000 [==========================>...] - ETA: 6:49 - loss: 2.4933 - regression_loss: 1.8530 - classification_loss: 9136/10000 [==========================>...] - ETA: 6:48 - loss: 2.4933 - regression_loss: 1.8530 - classification_loss: 9137/10000 [==========================>...] - ETA: 6:48 - loss: 2.4932 - regression_loss: 1.8530 - classification_loss: 9138/10000 [==========================>...] - ETA: 6:47 - loss: 2.4932 - regression_loss: 1.8529 - classification_loss: 9139/10000 [==========================>...] - ETA: 6:47 - loss: 2.4932 - regression_loss: 1.8529 - classification_loss: 9140/10000 [==========================>...] - ETA: 6:46 - loss: 2.4932 - regression_loss: 1.8530 - classification_loss: 9141/10000 [==========================>...] - ETA: 6:46 - loss: 2.4931 - regression_loss: 1.8529 - classification_loss: 9142/10000 [==========================>...] - ETA: 6:45 - loss: 2.4931 - regression_loss: 1.8529 - classification_loss: 9143/10000 [==========================>...] - ETA: 6:45 - loss: 2.4930 - regression_loss: 1.8528 - classification_loss: 9144/10000 [==========================>...] - ETA: 6:45 - loss: 2.4932 - regression_loss: 1.8529 - classification_loss: 9145/10000 [==========================>...] - ETA: 6:44 - loss: 2.4930 - regression_loss: 1.8528 - classification_loss: 9146/10000 [==========================>...] - ETA: 6:44 - loss: 2.4931 - regression_loss: 1.8529 - classification_loss: 9147/10000 [==========================>...] - ETA: 6:43 - loss: 2.4930 - regression_loss: 1.8528 - classification_loss: 9148/10000 [==========================>...] - ETA: 6:43 - loss: 2.4930 - regression_loss: 1.8528 - classification_loss: 9149/10000 [==========================>...] - ETA: 6:42 - loss: 2.4928 - regression_loss: 1.8527 - classification_loss: 9150/10000 [==========================>...] - ETA: 6:42 - loss: 2.4928 - regression_loss: 1.8527 - classification_loss: 9151/10000 [==========================>...] - ETA: 6:41 - loss: 2.4928 - regression_loss: 1.8527 - classification_loss: 9152/10000 [==========================>...] - ETA: 6:41 - loss: 2.4928 - regression_loss: 1.8527 - classification_loss: 9153/10000 [==========================>...] - ETA: 6:40 - loss: 2.4928 - regression_loss: 1.8527 - classification_loss: 9154/10000 [==========================>...] - ETA: 6:40 - loss: 2.4928 - regression_loss: 1.8526 - classification_loss: 9155/10000 [==========================>...] - ETA: 6:39 - loss: 2.4927 - regression_loss: 1.8524 - classification_loss: 9156/10000 [==========================>...] - ETA: 6:39 - loss: 2.4927 - regression_loss: 1.8524 - classification_loss: 9157/10000 [==========================>...] - ETA: 6:38 - loss: 2.4927 - regression_loss: 1.8525 - classification_loss: 9158/10000 [==========================>...] - ETA: 6:38 - loss: 2.4927 - regression_loss: 1.8524 - classification_loss: 9159/10000 [==========================>...] - ETA: 6:37 - loss: 2.4926 - regression_loss: 1.8524 - classification_loss: 9160/10000 [==========================>...] - ETA: 6:37 - loss: 2.4926 - regression_loss: 1.8524 - classification_loss: 9161/10000 [==========================>...] - ETA: 6:36 - loss: 2.4927 - regression_loss: 1.8525 - classification_loss: 9162/10000 [==========================>...] - ETA: 6:36 - loss: 2.4926 - regression_loss: 1.8524 - classification_loss: 9163/10000 [==========================>...] - ETA: 6:36 - loss: 2.4925 - regression_loss: 1.8524 - classification_loss: 9164/10000 [==========================>...] - ETA: 6:35 - loss: 2.4925 - regression_loss: 1.8524 - classification_loss: 9165/10000 [==========================>...] - ETA: 6:35 - loss: 2.4926 - regression_loss: 1.8524 - classification_loss: 9166/10000 [==========================>...] - ETA: 6:34 - loss: 2.4925 - regression_loss: 1.8523 - classification_loss: 9167/10000 [==========================>...] - ETA: 6:34 - loss: 2.4923 - regression_loss: 1.8522 - classification_loss: 9168/10000 [==========================>...] - ETA: 6:33 - loss: 2.4922 - regression_loss: 1.8522 - classification_loss: 9169/10000 [==========================>...] - ETA: 6:33 - loss: 2.4922 - regression_loss: 1.8521 - classification_loss: 9170/10000 [==========================>...] - ETA: 6:32 - loss: 2.4921 - regression_loss: 1.8521 - classification_loss: 9171/10000 [==========================>...] - ETA: 6:32 - loss: 2.4920 - regression_loss: 1.8520 - classification_loss: 9172/10000 [==========================>...] - ETA: 6:31 - loss: 2.4920 - regression_loss: 1.8520 - classification_loss: 9173/10000 [==========================>...] - ETA: 6:31 - loss: 2.4920 - regression_loss: 1.8520 - classification_loss: 9174/10000 [==========================>...] - ETA: 6:30 - loss: 2.4919 - regression_loss: 1.8520 - classification_loss: 9175/10000 [==========================>...] - ETA: 6:30 - loss: 2.4919 - regression_loss: 1.8519 - classification_loss: 9176/10000 [==========================>...] - ETA: 6:29 - loss: 2.4918 - regression_loss: 1.8518 - classification_loss: 9177/10000 [==========================>...] - ETA: 6:29 - loss: 2.4918 - regression_loss: 1.8518 - classification_loss: 9178/10000 [==========================>...] - ETA: 6:28 - loss: 2.4916 - regression_loss: 1.8517 - classification_loss: 9179/10000 [==========================>...] - ETA: 6:28 - loss: 2.4916 - regression_loss: 1.8517 - classification_loss: 9180/10000 [==========================>...] - ETA: 6:27 - loss: 2.4916 - regression_loss: 1.8517 - classification_loss: 9181/10000 [==========================>...] - ETA: 6:27 - loss: 2.4916 - regression_loss: 1.8517 - classification_loss: 9182/10000 [==========================>...] - ETA: 6:26 - loss: 2.4915 - regression_loss: 1.8517 - classification_loss: 9183/10000 [==========================>...] - ETA: 6:26 - loss: 2.4915 - regression_loss: 1.8517 - classification_loss: 9184/10000 [==========================>...] - ETA: 6:26 - loss: 2.4915 - regression_loss: 1.8516 - classification_loss: 9185/10000 [==========================>...] - ETA: 6:25 - loss: 2.4914 - regression_loss: 1.8516 - classification_loss: 9186/10000 [==========================>...] - ETA: 6:25 - loss: 2.4914 - regression_loss: 1.8516 - classification_loss: 9187/10000 [==========================>...] - ETA: 6:24 - loss: 2.4914 - regression_loss: 1.8517 - classification_loss: 9188/10000 [==========================>...] - ETA: 6:24 - loss: 2.4914 - regression_loss: 1.8516 - classification_loss: 9189/10000 [==========================>...] - ETA: 6:23 - loss: 2.4913 - regression_loss: 1.8516 - classification_loss: 9190/10000 [==========================>...] - ETA: 6:23 - loss: 2.4912 - regression_loss: 1.8515 - classification_loss: 9191/10000 [==========================>...] - ETA: 6:22 - loss: 2.4911 - regression_loss: 1.8514 - classification_loss: 9192/10000 [==========================>...] - ETA: 6:22 - loss: 2.4912 - regression_loss: 1.8515 - classification_loss: 9193/10000 [==========================>...] - ETA: 6:21 - loss: 2.4912 - regression_loss: 1.8515 - classification_loss: 9194/10000 [==========================>...] - ETA: 6:21 - loss: 2.4911 - regression_loss: 1.8514 - classification_loss: 9195/10000 [==========================>...] - ETA: 6:20 - loss: 2.4911 - regression_loss: 1.8514 - classification_loss: 9196/10000 [==========================>...] - ETA: 6:20 - loss: 2.4910 - regression_loss: 1.8513 - classification_loss: 9197/10000 [==========================>...] - ETA: 6:19 - loss: 2.4910 - regression_loss: 1.8513 - classification_loss: 9198/10000 [==========================>...] - ETA: 6:19 - loss: 2.4908 - regression_loss: 1.8512 - classification_loss: 9199/10000 [==========================>...] - ETA: 6:18 - loss: 2.4907 - regression_loss: 1.8511 - classification_loss: 9200/10000 [==========================>...] - ETA: 6:18 - loss: 2.4906 - regression_loss: 1.8510 - classification_loss: 9201/10000 [==========================>...] - ETA: 6:17 - loss: 2.4906 - regression_loss: 1.8511 - classification_loss: 9202/10000 [==========================>...] - ETA: 6:17 - loss: 2.4906 - regression_loss: 1.8510 - classification_loss: 9203/10000 [==========================>...] - ETA: 6:17 - loss: 2.4905 - regression_loss: 1.8509 - classification_loss: 9204/10000 [==========================>...] - ETA: 6:16 - loss: 2.4905 - regression_loss: 1.8509 - classification_loss: 9205/10000 [==========================>...] - ETA: 6:16 - loss: 2.4904 - regression_loss: 1.8509 - classification_loss: 9206/10000 [==========================>...] - ETA: 6:15 - loss: 2.4902 - regression_loss: 1.8507 - classification_loss: 9207/10000 [==========================>...] - ETA: 6:15 - loss: 2.4901 - regression_loss: 1.8507 - classification_loss: 9208/10000 [==========================>...] - ETA: 6:14 - loss: 2.4901 - regression_loss: 1.8506 - classification_loss: 9209/10000 [==========================>...] - ETA: 6:14 - loss: 2.4900 - regression_loss: 1.8506 - classification_loss: 9210/10000 [==========================>...] - ETA: 6:13 - loss: 2.4899 - regression_loss: 1.8505 - classification_loss: 9211/10000 [==========================>...] - ETA: 6:13 - loss: 2.4899 - regression_loss: 1.8505 - classification_loss: 9212/10000 [==========================>...] - ETA: 6:12 - loss: 2.4899 - regression_loss: 1.8505 - classification_loss: 9213/10000 [==========================>...] - ETA: 6:12 - loss: 2.4898 - regression_loss: 1.8504 - classification_loss: 9214/10000 [==========================>...] - ETA: 6:11 - loss: 2.4897 - regression_loss: 1.8504 - classification_loss: 9215/10000 [==========================>...] - ETA: 6:11 - loss: 2.4897 - regression_loss: 1.8504 - classification_loss: 9216/10000 [==========================>...] - ETA: 6:10 - loss: 2.4897 - regression_loss: 1.8504 - classification_loss: 9217/10000 [==========================>...] - ETA: 6:10 - loss: 2.4896 - regression_loss: 1.8503 - classification_loss: 9218/10000 [==========================>...] - ETA: 6:09 - loss: 2.4896 - regression_loss: 1.8503 - classification_loss: 9219/10000 [==========================>...] - ETA: 6:09 - loss: 2.4895 - regression_loss: 1.8503 - classification_loss: 9220/10000 [==========================>...] - ETA: 6:08 - loss: 2.4894 - regression_loss: 1.8502 - classification_loss: 9221/10000 [==========================>...] - ETA: 6:08 - loss: 2.4894 - regression_loss: 1.8502 - classification_loss: 9222/10000 [==========================>...] - ETA: 6:08 - loss: 2.4893 - regression_loss: 1.8502 - classification_loss: 9223/10000 [==========================>...] - ETA: 6:07 - loss: 2.4893 - regression_loss: 1.8502 - classification_loss: 9224/10000 [==========================>...] - ETA: 6:07 - loss: 2.4892 - regression_loss: 1.8501 - classification_loss: 9225/10000 [==========================>...] - ETA: 6:06 - loss: 2.4892 - regression_loss: 1.8501 - classification_loss: 9226/10000 [==========================>...] - ETA: 6:06 - loss: 2.4891 - regression_loss: 1.8501 - classification_loss: 9227/10000 [==========================>...] - ETA: 6:05 - loss: 2.4889 - regression_loss: 1.8499 - classification_loss: 9228/10000 [==========================>...] - ETA: 6:05 - loss: 2.4890 - regression_loss: 1.8499 - classification_loss: 9229/10000 [==========================>...] - ETA: 6:04 - loss: 2.4889 - regression_loss: 1.8499 - classification_loss: 9230/10000 [==========================>...] - ETA: 6:04 - loss: 2.4887 - regression_loss: 1.8498 - classification_loss: 9231/10000 [==========================>...] - ETA: 6:03 - loss: 2.4887 - regression_loss: 1.8498 - classification_loss: 9232/10000 [==========================>...] - ETA: 6:03 - loss: 2.4888 - regression_loss: 1.8498 - classification_loss: 9233/10000 [==========================>...] - ETA: 6:02 - loss: 2.4887 - regression_loss: 1.8497 - classification_loss: 9234/10000 [==========================>...] - ETA: 6:02 - loss: 2.4886 - regression_loss: 1.8497 - classification_loss: 9235/10000 [==========================>...] - ETA: 6:01 - loss: 2.4886 - regression_loss: 1.8496 - classification_loss: 9236/10000 [==========================>...] - ETA: 6:01 - loss: 2.4884 - regression_loss: 1.8495 - classification_loss: 9237/10000 [==========================>...] - ETA: 6:00 - loss: 2.4883 - regression_loss: 1.8494 - classification_loss: 9238/10000 [==========================>...] - ETA: 6:00 - loss: 2.4884 - regression_loss: 1.8494 - classification_loss: 9239/10000 [==========================>...] - ETA: 5:59 - loss: 2.4883 - regression_loss: 1.8494 - classification_loss: 9240/10000 [==========================>...] - ETA: 5:59 - loss: 2.4884 - regression_loss: 1.8495 - classification_loss: 9241/10000 [==========================>...] - ETA: 5:59 - loss: 2.4883 - regression_loss: 1.8495 - classification_loss: 9242/10000 [==========================>...] - ETA: 5:58 - loss: 2.4881 - regression_loss: 1.8493 - classification_loss: 9243/10000 [==========================>...] - ETA: 5:58 - loss: 2.4880 - regression_loss: 1.8493 - classification_loss: 9244/10000 [==========================>...] - ETA: 5:57 - loss: 2.4879 - regression_loss: 1.8492 - classification_loss: 9245/10000 [==========================>...] - ETA: 5:57 - loss: 2.4878 - regression_loss: 1.8491 - classification_loss: 9246/10000 [==========================>...] - ETA: 5:56 - loss: 2.4877 - regression_loss: 1.8490 - classification_loss: 9247/10000 [==========================>...] - ETA: 5:56 - loss: 2.4876 - regression_loss: 1.8490 - classification_loss: 9248/10000 [==========================>...] - ETA: 5:55 - loss: 2.4875 - regression_loss: 1.8489 - classification_loss: 9249/10000 [==========================>...] - ETA: 5:55 - loss: 2.4874 - regression_loss: 1.8488 - classification_loss: 9250/10000 [==========================>...] - ETA: 5:54 - loss: 2.4874 - regression_loss: 1.8488 - classification_loss: 9251/10000 [==========================>...] - ETA: 5:54 - loss: 2.4873 - regression_loss: 1.8488 - classification_loss: 9252/10000 [==========================>...] - ETA: 5:53 - loss: 2.4872 - regression_loss: 1.8487 - classification_loss: 9253/10000 [==========================>...] - ETA: 5:53 - loss: 2.4872 - regression_loss: 1.8487 - classification_loss: 9254/10000 [==========================>...] - ETA: 5:52 - loss: 2.4872 - regression_loss: 1.8487 - classification_loss: 9255/10000 [==========================>...] - ETA: 5:52 - loss: 2.4873 - regression_loss: 1.8488 - classification_loss: 9256/10000 [==========================>...] - ETA: 5:51 - loss: 2.4872 - regression_loss: 1.8487 - classification_loss: 9257/10000 [==========================>...] - ETA: 5:51 - loss: 2.4871 - regression_loss: 1.8487 - classification_loss: 9258/10000 [==========================>...] - ETA: 5:50 - loss: 2.4871 - regression_loss: 1.8487 - classification_loss: 9259/10000 [==========================>...] - ETA: 5:50 - loss: 2.4870 - regression_loss: 1.8486 - classification_loss: 9260/10000 [==========================>...] - ETA: 5:50 - loss: 2.4868 - regression_loss: 1.8485 - classification_loss: 9261/10000 [==========================>...] - ETA: 5:49 - loss: 2.4869 - regression_loss: 1.8485 - classification_loss: 9262/10000 [==========================>...] - ETA: 5:49 - loss: 2.4868 - regression_loss: 1.8484 - classification_loss: 9263/10000 [==========================>...] - ETA: 5:48 - loss: 2.4867 - regression_loss: 1.8484 - classification_loss: 9264/10000 [==========================>...] - ETA: 5:48 - loss: 2.4867 - regression_loss: 1.8484 - classification_loss: 9265/10000 [==========================>...] - ETA: 5:47 - loss: 2.4866 - regression_loss: 1.8483 - classification_loss: 9266/10000 [==========================>...] - ETA: 5:47 - loss: 2.4865 - regression_loss: 1.8482 - classification_loss: 9267/10000 [==========================>...] - ETA: 5:46 - loss: 2.4863 - regression_loss: 1.8481 - classification_loss: 9268/10000 [==========================>...] - ETA: 5:46 - loss: 2.4863 - regression_loss: 1.8481 - classification_loss: 9269/10000 [==========================>...] - ETA: 5:45 - loss: 2.4863 - regression_loss: 1.8481 - classification_loss: 9270/10000 [==========================>...] - ETA: 5:45 - loss: 2.4863 - regression_loss: 1.8481 - classification_loss: 9271/10000 [==========================>...] - ETA: 5:44 - loss: 2.4863 - regression_loss: 1.8481 - classification_loss: 9272/10000 [==========================>...] - ETA: 5:44 - loss: 2.4862 - regression_loss: 1.8480 - classification_loss: 9273/10000 [==========================>...] - ETA: 5:43 - loss: 2.4862 - regression_loss: 1.8480 - classification_loss: 9274/10000 [==========================>...] - ETA: 5:43 - loss: 2.4861 - regression_loss: 1.8480 - classification_loss: 9275/10000 [==========================>...] - ETA: 5:42 - loss: 2.4861 - regression_loss: 1.8480 - classification_loss: 9276/10000 [==========================>...] - ETA: 5:42 - loss: 2.4860 - regression_loss: 1.8479 - classification_loss: 9277/10000 [==========================>...] - ETA: 5:41 - loss: 2.4860 - regression_loss: 1.8479 - classification_loss: 9278/10000 [==========================>...] - ETA: 5:41 - loss: 2.4859 - regression_loss: 1.8478 - classification_loss: 9279/10000 [==========================>...] - ETA: 5:41 - loss: 2.4858 - regression_loss: 1.8478 - classification_loss: 9280/10000 [==========================>...] - ETA: 5:40 - loss: 2.4861 - regression_loss: 1.8478 - classification_loss: 9281/10000 [==========================>...] - ETA: 5:40 - loss: 2.4861 - regression_loss: 1.8478 - classification_loss: 9282/10000 [==========================>...] - ETA: 5:39 - loss: 2.4860 - regression_loss: 1.8478 - classification_loss: 9283/10000 [==========================>...] - ETA: 5:39 - loss: 2.4860 - regression_loss: 1.8478 - classification_loss: 9284/10000 [==========================>...] - ETA: 5:38 - loss: 2.4860 - regression_loss: 1.8478 - classification_loss: 9285/10000 [==========================>...] - ETA: 5:38 - loss: 2.4860 - regression_loss: 1.8478 - classification_loss: 9286/10000 [==========================>...] - ETA: 5:37 - loss: 2.4858 - regression_loss: 1.8476 - classification_loss: 9287/10000 [==========================>...] - ETA: 5:37 - loss: 2.4858 - regression_loss: 1.8477 - classification_loss: 9288/10000 [==========================>...] - ETA: 5:36 - loss: 2.4858 - regression_loss: 1.8477 - classification_loss: 9289/10000 [==========================>...] - ETA: 5:36 - loss: 2.4858 - regression_loss: 1.8476 - classification_loss: 9290/10000 [==========================>...] - ETA: 5:35 - loss: 2.4857 - regression_loss: 1.8475 - classification_loss: 9291/10000 [==========================>...] - ETA: 5:35 - loss: 2.4855 - regression_loss: 1.8474 - classification_loss: 9292/10000 [==========================>...] - ETA: 5:34 - loss: 2.4855 - regression_loss: 1.8474 - classification_loss: 9293/10000 [==========================>...] - ETA: 5:34 - loss: 2.4856 - regression_loss: 1.8475 - classification_loss: 9294/10000 [==========================>...] - ETA: 5:33 - loss: 2.4855 - regression_loss: 1.8474 - classification_loss: 9295/10000 [==========================>...] - ETA: 5:33 - loss: 2.4855 - regression_loss: 1.8474 - classification_loss: 9296/10000 [==========================>...] - ETA: 5:32 - loss: 2.4855 - regression_loss: 1.8474 - classification_loss: 9297/10000 [==========================>...] - ETA: 5:32 - loss: 2.4856 - regression_loss: 1.8475 - classification_loss: 9298/10000 [==========================>...] - ETA: 5:32 - loss: 2.4856 - regression_loss: 1.8475 - classification_loss: 9299/10000 [==========================>...] - ETA: 5:31 - loss: 2.4856 - regression_loss: 1.8475 - classification_loss: 9300/10000 [==========================>...] - ETA: 5:31 - loss: 2.4855 - regression_loss: 1.8474 - classification_loss: 9301/10000 [==========================>...] - ETA: 5:30 - loss: 2.4854 - regression_loss: 1.8473 - classification_loss: 9302/10000 [==========================>...] - ETA: 5:30 - loss: 2.4854 - regression_loss: 1.8473 - classification_loss: 9303/10000 [==========================>...] - ETA: 5:29 - loss: 2.4854 - regression_loss: 1.8474 - classification_loss: 9304/10000 [==========================>...] - ETA: 5:29 - loss: 2.4853 - regression_loss: 1.8473 - classification_loss: 9305/10000 [==========================>...] - ETA: 5:28 - loss: 2.4853 - regression_loss: 1.8473 - classification_loss: 9306/10000 [==========================>...] - ETA: 5:28 - loss: 2.4853 - regression_loss: 1.8474 - classification_loss: 9307/10000 [==========================>...] - ETA: 5:27 - loss: 2.4852 - regression_loss: 1.8473 - classification_loss: 9308/10000 [==========================>...] - ETA: 5:27 - loss: 2.4852 - regression_loss: 1.8473 - classification_loss: 9309/10000 [==========================>...] - ETA: 5:26 - loss: 2.4850 - regression_loss: 1.8471 - classification_loss: 9310/10000 [==========================>...] - ETA: 5:26 - loss: 2.4850 - regression_loss: 1.8471 - classification_loss: 9311/10000 [==========================>...] - ETA: 5:25 - loss: 2.4848 - regression_loss: 1.8469 - classification_loss: 9312/10000 [==========================>...] - ETA: 5:25 - loss: 2.4847 - regression_loss: 1.8469 - classification_loss: 9313/10000 [==========================>...] - ETA: 5:24 - loss: 2.4846 - regression_loss: 1.8467 - classification_loss: 9314/10000 [==========================>...] - ETA: 5:24 - loss: 2.4846 - regression_loss: 1.8468 - classification_loss: 9315/10000 [==========================>...] - ETA: 5:23 - loss: 2.4846 - regression_loss: 1.8467 - classification_loss: 9316/10000 [==========================>...] - ETA: 5:23 - loss: 2.4846 - regression_loss: 1.8467 - classification_loss: 9317/10000 [==========================>...] - ETA: 5:22 - loss: 2.4847 - regression_loss: 1.8468 - classification_loss: 9318/10000 [==========================>...] - ETA: 5:22 - loss: 2.4845 - regression_loss: 1.8467 - classification_loss: 9319/10000 [==========================>...] - ETA: 5:22 - loss: 2.4843 - regression_loss: 1.8465 - classification_loss: 9320/10000 [==========================>...] - ETA: 5:21 - loss: 2.4843 - regression_loss: 1.8465 - classification_loss: 9321/10000 [==========================>...] - ETA: 5:21 - loss: 2.4844 - regression_loss: 1.8465 - classification_loss: 9322/10000 [==========================>...] - ETA: 5:20 - loss: 2.4842 - regression_loss: 1.8465 - classification_loss: 9323/10000 [==========================>...] - ETA: 5:20 - loss: 2.4842 - regression_loss: 1.8465 - classification_loss: 9324/10000 [==========================>...] - ETA: 5:19 - loss: 2.4841 - regression_loss: 1.8464 - classification_loss: 9325/10000 [==========================>...] - ETA: 5:19 - loss: 2.4841 - regression_loss: 1.8463 - classification_loss: 9326/10000 [==========================>...] - ETA: 5:18 - loss: 2.4839 - regression_loss: 1.8462 - classification_loss: 9327/10000 [==========================>...] - ETA: 5:18 - loss: 2.4840 - regression_loss: 1.8463 - classification_loss: 9328/10000 [==========================>...] - ETA: 5:17 - loss: 2.4839 - regression_loss: 1.8462 - classification_loss: 9329/10000 [==========================>...] - ETA: 5:17 - loss: 2.4838 - regression_loss: 1.8462 - classification_loss: 9330/10000 [==========================>...] - ETA: 5:16 - loss: 2.4838 - regression_loss: 1.8461 - classification_loss: 9331/10000 [==========================>...] - ETA: 5:16 - loss: 2.4838 - regression_loss: 1.8461 - classification_loss: 9332/10000 [==========================>...] - ETA: 5:15 - loss: 2.4838 - regression_loss: 1.8461 - classification_loss: 9333/10000 [==========================>...] - ETA: 5:15 - loss: 2.4838 - regression_loss: 1.8461 - classification_loss: 9334/10000 [===========================>..] - ETA: 5:14 - loss: 2.4837 - regression_loss: 1.8460 - classification_loss: 9335/10000 [===========================>..] - ETA: 5:14 - loss: 2.4836 - regression_loss: 1.8460 - classification_loss: 9336/10000 [===========================>..] - ETA: 5:13 - loss: 2.4835 - regression_loss: 1.8459 - classification_loss: 9337/10000 [===========================>..] - ETA: 5:13 - loss: 2.4834 - regression_loss: 1.8458 - classification_loss: 9338/10000 [===========================>..] - ETA: 5:13 - loss: 2.4833 - regression_loss: 1.8457 - classification_loss: 9339/10000 [===========================>..] - ETA: 5:12 - loss: 2.4833 - regression_loss: 1.8458 - classification_loss: 9340/10000 [===========================>..] - ETA: 5:12 - loss: 2.4833 - regression_loss: 1.8458 - classification_loss: 9341/10000 [===========================>..] - ETA: 5:11 - loss: 2.4832 - regression_loss: 1.8457 - classification_loss: 9342/10000 [===========================>..] - ETA: 5:11 - loss: 2.4832 - regression_loss: 1.8457 - classification_loss: 9343/10000 [===========================>..] - ETA: 5:10 - loss: 2.4831 - regression_loss: 1.8456 - classification_loss: 9344/10000 [===========================>..] - ETA: 5:10 - loss: 2.4830 - regression_loss: 1.8455 - classification_loss: 9345/10000 [===========================>..] - ETA: 5:09 - loss: 2.4829 - regression_loss: 1.8454 - classification_loss: 9346/10000 [===========================>..] - ETA: 5:09 - loss: 2.4828 - regression_loss: 1.8454 - classification_loss: 9347/10000 [===========================>..] - ETA: 5:08 - loss: 2.4827 - regression_loss: 1.8453 - classification_loss: 9348/10000 [===========================>..] - ETA: 5:08 - loss: 2.4826 - regression_loss: 1.8452 - classification_loss: 9349/10000 [===========================>..] - ETA: 5:07 - loss: 2.4825 - regression_loss: 1.8451 - classification_loss: 9350/10000 [===========================>..] - ETA: 5:07 - loss: 2.4825 - regression_loss: 1.8451 - classification_loss: 9351/10000 [===========================>..] - ETA: 5:06 - loss: 2.4825 - regression_loss: 1.8451 - classification_loss: 9352/10000 [===========================>..] - ETA: 5:06 - loss: 2.4825 - regression_loss: 1.8451 - classification_loss: 9353/10000 [===========================>..] - ETA: 5:05 - loss: 2.4825 - regression_loss: 1.8451 - classification_loss: 9354/10000 [===========================>..] - ETA: 5:05 - loss: 2.4823 - regression_loss: 1.8450 - classification_loss: 9355/10000 [===========================>..] - ETA: 5:04 - loss: 2.4823 - regression_loss: 1.8450 - classification_loss: 9356/10000 [===========================>..] - ETA: 5:04 - loss: 2.4823 - regression_loss: 1.8450 - classification_loss: 9357/10000 [===========================>..] - ETA: 5:04 - loss: 2.4822 - regression_loss: 1.8450 - classification_loss: 9358/10000 [===========================>..] - ETA: 5:03 - loss: 2.4824 - regression_loss: 1.8451 - classification_loss: 9359/10000 [===========================>..] - ETA: 5:03 - loss: 2.4824 - regression_loss: 1.8451 - classification_loss: 9360/10000 [===========================>..] - ETA: 5:02 - loss: 2.4823 - regression_loss: 1.8450 - classification_loss: 9361/10000 [===========================>..] - ETA: 5:02 - loss: 2.4823 - regression_loss: 1.8450 - classification_loss: 9362/10000 [===========================>..] - ETA: 5:01 - loss: 2.4821 - regression_loss: 1.8449 - classification_loss: 9363/10000 [===========================>..] - ETA: 5:01 - loss: 2.4819 - regression_loss: 1.8447 - classification_loss: 9364/10000 [===========================>..] - ETA: 5:00 - loss: 2.4819 - regression_loss: 1.8448 - classification_loss: 9365/10000 [===========================>..] - ETA: 5:00 - loss: 2.4819 - regression_loss: 1.8448 - classification_loss: 9366/10000 [===========================>..] - ETA: 4:59 - loss: 2.4818 - regression_loss: 1.8447 - classification_loss: 9367/10000 [===========================>..] - ETA: 4:59 - loss: 2.4819 - regression_loss: 1.8447 - classification_loss: 9368/10000 [===========================>..] - ETA: 4:58 - loss: 2.4818 - regression_loss: 1.8447 - classification_loss: 9369/10000 [===========================>..] - ETA: 4:58 - loss: 2.4818 - regression_loss: 1.8447 - classification_loss: 9370/10000 [===========================>..] - ETA: 4:57 - loss: 2.4817 - regression_loss: 1.8447 - classification_loss: 9371/10000 [===========================>..] - ETA: 4:57 - loss: 2.4817 - regression_loss: 1.8447 - classification_loss: 9372/10000 [===========================>..] - ETA: 4:56 - loss: 2.4817 - regression_loss: 1.8446 - classification_loss: 9373/10000 [===========================>..] - ETA: 4:56 - loss: 2.4817 - regression_loss: 1.8446 - classification_loss: 9374/10000 [===========================>..] - ETA: 4:55 - loss: 2.4816 - regression_loss: 1.8446 - classification_loss: 9375/10000 [===========================>..] - ETA: 4:55 - loss: 2.4815 - regression_loss: 1.8445 - classification_loss: 9376/10000 [===========================>..] - ETA: 4:55 - loss: 2.4814 - regression_loss: 1.8443 - classification_loss: 9377/10000 [===========================>..] - ETA: 4:54 - loss: 2.4812 - regression_loss: 1.8442 - classification_loss: 9378/10000 [===========================>..] - ETA: 4:54 - loss: 2.4812 - regression_loss: 1.8442 - classification_loss: 9379/10000 [===========================>..] - ETA: 4:53 - loss: 2.4811 - regression_loss: 1.8442 - classification_loss: 9380/10000 [===========================>..] - ETA: 4:53 - loss: 2.4811 - regression_loss: 1.8442 - classification_loss: 9381/10000 [===========================>..] - ETA: 4:52 - loss: 2.4810 - regression_loss: 1.8441 - classification_loss: 9382/10000 [===========================>..] - ETA: 4:52 - loss: 2.4809 - regression_loss: 1.8440 - classification_loss: 9383/10000 [===========================>..] - ETA: 4:51 - loss: 2.4811 - regression_loss: 1.8441 - classification_loss: 9384/10000 [===========================>..] - ETA: 4:51 - loss: 2.4810 - regression_loss: 1.8441 - classification_loss: 9385/10000 [===========================>..] - ETA: 4:50 - loss: 2.4810 - regression_loss: 1.8441 - classification_loss: 9386/10000 [===========================>..] - ETA: 4:50 - loss: 2.4809 - regression_loss: 1.8440 - classification_loss: 9387/10000 [===========================>..] - ETA: 4:49 - loss: 2.4809 - regression_loss: 1.8440 - classification_loss: 9388/10000 [===========================>..] - ETA: 4:49 - loss: 2.4809 - regression_loss: 1.8440 - classification_loss: 9389/10000 [===========================>..] - ETA: 4:48 - loss: 2.4809 - regression_loss: 1.8440 - classification_loss: 9390/10000 [===========================>..] - ETA: 4:48 - loss: 2.4808 - regression_loss: 1.8439 - classification_loss: 9391/10000 [===========================>..] - ETA: 4:47 - loss: 2.4808 - regression_loss: 1.8440 - classification_loss: 9392/10000 [===========================>..] - ETA: 4:47 - loss: 2.4807 - regression_loss: 1.8438 - classification_loss: 9393/10000 [===========================>..] - ETA: 4:47 - loss: 2.4806 - regression_loss: 1.8438 - classification_loss: 9394/10000 [===========================>..] - ETA: 4:46 - loss: 2.4806 - regression_loss: 1.8438 - classification_loss: 9395/10000 [===========================>..] - ETA: 4:46 - loss: 2.4805 - regression_loss: 1.8437 - classification_loss: 9396/10000 [===========================>..] - ETA: 4:45 - loss: 2.4804 - regression_loss: 1.8437 - classification_loss: 9397/10000 [===========================>..] - ETA: 4:45 - loss: 2.4803 - regression_loss: 1.8436 - classification_loss: 9398/10000 [===========================>..] - ETA: 4:44 - loss: 2.4803 - regression_loss: 1.8436 - classification_loss: 9399/10000 [===========================>..] - ETA: 4:44 - loss: 2.4804 - regression_loss: 1.8436 - classification_loss: 9400/10000 [===========================>..] - ETA: 4:43 - loss: 2.4805 - regression_loss: 1.8437 - classification_loss: 9401/10000 [===========================>..] - ETA: 4:43 - loss: 2.4804 - regression_loss: 1.8436 - classification_loss: 9402/10000 [===========================>..] - ETA: 4:42 - loss: 2.4804 - regression_loss: 1.8436 - classification_loss: 9403/10000 [===========================>..] - ETA: 4:42 - loss: 2.4802 - regression_loss: 1.8435 - classification_loss: 9404/10000 [===========================>..] - ETA: 4:41 - loss: 2.4801 - regression_loss: 1.8434 - classification_loss: 9405/10000 [===========================>..] - ETA: 4:41 - loss: 2.4801 - regression_loss: 1.8434 - classification_loss: 9406/10000 [===========================>..] - ETA: 4:40 - loss: 2.4801 - regression_loss: 1.8434 - classification_loss: 9407/10000 [===========================>..] - ETA: 4:40 - loss: 2.4801 - regression_loss: 1.8434 - classification_loss: 9408/10000 [===========================>..] - ETA: 4:39 - loss: 2.4801 - regression_loss: 1.8435 - classification_loss: 9409/10000 [===========================>..] - ETA: 4:39 - loss: 2.4800 - regression_loss: 1.8434 - classification_loss: 9410/10000 [===========================>..] - ETA: 4:38 - loss: 2.4800 - regression_loss: 1.8434 - classification_loss: 9411/10000 [===========================>..] - ETA: 4:38 - loss: 2.4800 - regression_loss: 1.8435 - classification_loss: 9412/10000 [===========================>..] - ETA: 4:38 - loss: 2.4800 - regression_loss: 1.8435 - classification_loss: 9413/10000 [===========================>..] - ETA: 4:37 - loss: 2.4801 - regression_loss: 1.8435 - classification_loss: 9414/10000 [===========================>..] - ETA: 4:37 - loss: 2.4800 - regression_loss: 1.8435 - classification_loss: 9415/10000 [===========================>..] - ETA: 4:36 - loss: 2.4800 - regression_loss: 1.8434 - classification_loss: 9416/10000 [===========================>..] - ETA: 4:36 - loss: 2.4799 - regression_loss: 1.8434 - classification_loss: 9417/10000 [===========================>..] - ETA: 4:35 - loss: 2.4800 - regression_loss: 1.8434 - classification_loss: 9418/10000 [===========================>..] - ETA: 4:35 - loss: 2.4799 - regression_loss: 1.8434 - classification_loss: 9419/10000 [===========================>..] - ETA: 4:34 - loss: 2.4799 - regression_loss: 1.8434 - classification_loss: 9420/10000 [===========================>..] - ETA: 4:34 - loss: 2.4799 - regression_loss: 1.8434 - classification_loss: 9421/10000 [===========================>..] - ETA: 4:33 - loss: 2.4797 - regression_loss: 1.8433 - classification_loss: 9422/10000 [===========================>..] - ETA: 4:33 - loss: 2.4796 - regression_loss: 1.8432 - classification_loss: 9423/10000 [===========================>..] - ETA: 4:32 - loss: 2.4796 - regression_loss: 1.8432 - classification_loss: 9424/10000 [===========================>..] - ETA: 4:32 - loss: 2.4795 - regression_loss: 1.8430 - classification_loss: 9425/10000 [===========================>..] - ETA: 4:31 - loss: 2.4795 - regression_loss: 1.8430 - classification_loss: 9426/10000 [===========================>..] - ETA: 4:31 - loss: 2.4793 - regression_loss: 1.8430 - classification_loss: 9427/10000 [===========================>..] - ETA: 4:30 - loss: 2.4793 - regression_loss: 1.8429 - classification_loss: 9428/10000 [===========================>..] - ETA: 4:30 - loss: 2.4792 - regression_loss: 1.8428 - classification_loss: 9429/10000 [===========================>..] - ETA: 4:29 - loss: 2.4792 - regression_loss: 1.8428 - classification_loss: 9430/10000 [===========================>..] - ETA: 4:29 - loss: 2.4791 - regression_loss: 1.8427 - classification_loss: 9431/10000 [===========================>..] - ETA: 4:29 - loss: 2.4791 - regression_loss: 1.8427 - classification_loss: 9432/10000 [===========================>..] - ETA: 4:28 - loss: 2.4791 - regression_loss: 1.8427 - classification_loss: 9433/10000 [===========================>..] - ETA: 4:28 - loss: 2.4789 - regression_loss: 1.8426 - classification_loss: 9434/10000 [===========================>..] - ETA: 4:27 - loss: 2.4789 - regression_loss: 1.8426 - classification_loss: 9435/10000 [===========================>..] - ETA: 4:27 - loss: 2.4787 - regression_loss: 1.8425 - classification_loss: 9436/10000 [===========================>..] - ETA: 4:26 - loss: 2.4787 - regression_loss: 1.8425 - classification_loss: 9437/10000 [===========================>..] - ETA: 4:26 - loss: 2.4785 - regression_loss: 1.8423 - classification_loss: 9438/10000 [===========================>..] - ETA: 4:25 - loss: 2.4785 - regression_loss: 1.8423 - classification_loss: 9439/10000 [===========================>..] - ETA: 4:25 - loss: 2.4785 - regression_loss: 1.8423 - classification_loss: 9440/10000 [===========================>..] - ETA: 4:24 - loss: 2.4784 - regression_loss: 1.8423 - classification_loss: 9441/10000 [===========================>..] - ETA: 4:24 - loss: 2.4783 - regression_loss: 1.8421 - classification_loss: 9442/10000 [===========================>..] - ETA: 4:23 - loss: 2.4783 - regression_loss: 1.8422 - classification_loss: 9443/10000 [===========================>..] - ETA: 4:23 - loss: 2.4783 - regression_loss: 1.8422 - classification_loss: 9444/10000 [===========================>..] - ETA: 4:22 - loss: 2.4783 - regression_loss: 1.8422 - classification_loss: 9445/10000 [===========================>..] - ETA: 4:22 - loss: 2.4783 - regression_loss: 1.8422 - classification_loss: 9446/10000 [===========================>..] - ETA: 4:21 - loss: 2.4782 - regression_loss: 1.8421 - classification_loss: 9447/10000 [===========================>..] - ETA: 4:21 - loss: 2.4781 - regression_loss: 1.8421 - classification_loss: 9448/10000 [===========================>..] - ETA: 4:20 - loss: 2.4780 - regression_loss: 1.8420 - classification_loss: 9449/10000 [===========================>..] - ETA: 4:20 - loss: 2.4780 - regression_loss: 1.8420 - classification_loss: 9450/10000 [===========================>..] - ETA: 4:20 - loss: 2.4780 - regression_loss: 1.8420 - classification_loss: 9451/10000 [===========================>..] - ETA: 4:19 - loss: 2.4779 - regression_loss: 1.8420 - classification_loss: 9452/10000 [===========================>..] - ETA: 4:19 - loss: 2.4778 - regression_loss: 1.8419 - classification_loss: 9453/10000 [===========================>..] - ETA: 4:18 - loss: 2.4777 - regression_loss: 1.8418 - classification_loss: 9454/10000 [===========================>..] - ETA: 4:18 - loss: 2.4777 - regression_loss: 1.8418 - classification_loss: 9455/10000 [===========================>..] - ETA: 4:17 - loss: 2.4777 - regression_loss: 1.8418 - classification_loss: 9456/10000 [===========================>..] - ETA: 4:17 - loss: 2.4777 - regression_loss: 1.8418 - classification_loss: 9457/10000 [===========================>..] - ETA: 4:16 - loss: 2.4778 - regression_loss: 1.8419 - classification_loss: 9458/10000 [===========================>..] - ETA: 4:16 - loss: 2.4777 - regression_loss: 1.8418 - classification_loss: 9459/10000 [===========================>..] - ETA: 4:15 - loss: 2.4778 - regression_loss: 1.8419 - classification_loss: 9460/10000 [===========================>..] - ETA: 4:15 - loss: 2.4778 - regression_loss: 1.8418 - classification_loss: 9461/10000 [===========================>..] - ETA: 4:14 - loss: 2.4777 - regression_loss: 1.8418 - classification_loss: 9462/10000 [===========================>..] - ETA: 4:14 - loss: 2.4776 - regression_loss: 1.8417 - classification_loss: 9463/10000 [===========================>..] - ETA: 4:13 - loss: 2.4776 - regression_loss: 1.8417 - classification_loss: 9464/10000 [===========================>..] - ETA: 4:13 - loss: 2.4775 - regression_loss: 1.8417 - classification_loss: 9465/10000 [===========================>..] - ETA: 4:12 - loss: 2.4774 - regression_loss: 1.8416 - classification_loss: 9466/10000 [===========================>..] - ETA: 4:12 - loss: 2.4774 - regression_loss: 1.8416 - classification_loss: 9467/10000 [===========================>..] - ETA: 4:11 - loss: 2.4774 - regression_loss: 1.8416 - classification_loss: 9468/10000 [===========================>..] - ETA: 4:11 - loss: 2.4774 - regression_loss: 1.8416 - classification_loss: 9469/10000 [===========================>..] - ETA: 4:11 - loss: 2.4773 - regression_loss: 1.8415 - classification_loss: 9470/10000 [===========================>..] - ETA: 4:10 - loss: 2.4772 - regression_loss: 1.8414 - classification_loss: 9471/10000 [===========================>..] - ETA: 4:10 - loss: 2.4771 - regression_loss: 1.8414 - classification_loss: 9472/10000 [===========================>..] - ETA: 4:09 - loss: 2.4771 - regression_loss: 1.8414 - classification_loss: 9473/10000 [===========================>..] - ETA: 4:09 - loss: 2.4771 - regression_loss: 1.8414 - classification_loss: 9474/10000 [===========================>..] - ETA: 4:08 - loss: 2.4771 - regression_loss: 1.8415 - classification_loss: 9475/10000 [===========================>..] - ETA: 4:08 - loss: 2.4770 - regression_loss: 1.8414 - classification_loss: 9476/10000 [===========================>..] - ETA: 4:07 - loss: 2.4769 - regression_loss: 1.8413 - classification_loss: 9477/10000 [===========================>..] - ETA: 4:07 - loss: 2.4768 - regression_loss: 1.8412 - classification_loss: 9478/10000 [===========================>..] - ETA: 4:06 - loss: 2.4768 - regression_loss: 1.8413 - classification_loss: 9479/10000 [===========================>..] - ETA: 4:06 - loss: 2.4768 - regression_loss: 1.8413 - classification_loss: 9480/10000 [===========================>..] - ETA: 4:05 - loss: 2.4768 - regression_loss: 1.8413 - classification_loss: 9481/10000 [===========================>..] - ETA: 4:05 - loss: 2.4768 - regression_loss: 1.8413 - classification_loss: 9482/10000 [===========================>..] - ETA: 4:04 - loss: 2.4768 - regression_loss: 1.8413 - classification_loss: 9483/10000 [===========================>..] - ETA: 4:04 - loss: 2.4767 - regression_loss: 1.8412 - classification_loss: 9484/10000 [===========================>..] - ETA: 4:03 - loss: 2.4766 - regression_loss: 1.8411 - classification_loss: 9485/10000 [===========================>..] - ETA: 4:03 - loss: 2.4765 - regression_loss: 1.8411 - classification_loss: 9486/10000 [===========================>..] - ETA: 4:02 - loss: 2.4765 - regression_loss: 1.8411 - classification_loss: 9487/10000 [===========================>..] - ETA: 4:02 - loss: 2.4765 - regression_loss: 1.8411 - classification_loss: 9488/10000 [===========================>..] - ETA: 4:02 - loss: 2.4765 - regression_loss: 1.8411 - classification_loss: 9489/10000 [===========================>..] - ETA: 4:01 - loss: 2.4763 - regression_loss: 1.8410 - classification_loss: 9490/10000 [===========================>..] - ETA: 4:01 - loss: 2.4763 - regression_loss: 1.8409 - classification_loss: 9491/10000 [===========================>..] - ETA: 4:00 - loss: 2.4763 - regression_loss: 1.8409 - classification_loss: 9492/10000 [===========================>..] - ETA: 4:00 - loss: 2.4762 - regression_loss: 1.8408 - classification_loss: 9493/10000 [===========================>..] - ETA: 3:59 - loss: 2.4760 - regression_loss: 1.8407 - classification_loss: 9494/10000 [===========================>..] - ETA: 3:59 - loss: 2.4760 - regression_loss: 1.8407 - classification_loss: 9495/10000 [===========================>..] - ETA: 3:58 - loss: 2.4760 - regression_loss: 1.8407 - classification_loss: 9496/10000 [===========================>..] - ETA: 3:58 - loss: 2.4759 - regression_loss: 1.8407 - classification_loss: 9497/10000 [===========================>..] - ETA: 3:57 - loss: 2.4759 - regression_loss: 1.8407 - classification_loss: 9498/10000 [===========================>..] - ETA: 3:57 - loss: 2.4758 - regression_loss: 1.8406 - classification_loss: 9499/10000 [===========================>..] - ETA: 3:56 - loss: 2.4758 - regression_loss: 1.8406 - classification_loss: 9500/10000 [===========================>..] - ETA: 3:56 - loss: 2.4757 - regression_loss: 1.8405 - classification_loss: 9501/10000 [===========================>..] - ETA: 3:55 - loss: 2.4756 - regression_loss: 1.8405 - classification_loss: 9502/10000 [===========================>..] - ETA: 3:55 - loss: 2.4755 - regression_loss: 1.8403 - classification_loss: 9503/10000 [===========================>..] - ETA: 3:54 - loss: 2.4754 - regression_loss: 1.8402 - classification_loss: 9504/10000 [===========================>..] - ETA: 3:54 - loss: 2.4753 - regression_loss: 1.8402 - classification_loss: 9505/10000 [===========================>..] - ETA: 3:53 - loss: 2.4753 - regression_loss: 1.8402 - classification_loss: 9506/10000 [===========================>..] - ETA: 3:53 - loss: 2.4753 - regression_loss: 1.8402 - classification_loss: 9507/10000 [===========================>..] - ETA: 3:53 - loss: 2.4752 - regression_loss: 1.8401 - classification_loss: 9508/10000 [===========================>..] - ETA: 3:52 - loss: 2.4751 - regression_loss: 1.8401 - classification_loss: 9509/10000 [===========================>..] - ETA: 3:52 - loss: 2.4750 - regression_loss: 1.8400 - classification_loss: 9510/10000 [===========================>..] - ETA: 3:51 - loss: 2.4751 - regression_loss: 1.8400 - classification_loss: 9511/10000 [===========================>..] - ETA: 3:51 - loss: 2.4749 - regression_loss: 1.8399 - classification_loss: 9512/10000 [===========================>..] - ETA: 3:50 - loss: 2.4748 - regression_loss: 1.8399 - classification_loss: 9513/10000 [===========================>..] - ETA: 3:50 - loss: 2.4748 - regression_loss: 1.8398 - classification_loss: 9514/10000 [===========================>..] - ETA: 3:49 - loss: 2.4747 - regression_loss: 1.8398 - classification_loss: 9515/10000 [===========================>..] - ETA: 3:49 - loss: 2.4747 - regression_loss: 1.8398 - classification_loss: 9516/10000 [===========================>..] - ETA: 3:48 - loss: 2.4746 - regression_loss: 1.8397 - classification_loss: 9517/10000 [===========================>..] - ETA: 3:48 - loss: 2.4746 - regression_loss: 1.8397 - classification_loss: 9518/10000 [===========================>..] - ETA: 3:47 - loss: 2.4745 - regression_loss: 1.8397 - classification_loss: 9519/10000 [===========================>..] - ETA: 3:47 - loss: 2.4744 - regression_loss: 1.8397 - classification_loss: 9520/10000 [===========================>..] - ETA: 3:46 - loss: 2.4744 - regression_loss: 1.8397 - classification_loss: 9521/10000 [===========================>..] - ETA: 3:46 - loss: 2.4744 - regression_loss: 1.8396 - classification_loss: 9522/10000 [===========================>..] - ETA: 3:45 - loss: 2.4744 - regression_loss: 1.8396 - classification_loss: 9523/10000 [===========================>..] - ETA: 3:45 - loss: 2.4743 - regression_loss: 1.8396 - classification_loss: 9524/10000 [===========================>..] - ETA: 3:44 - loss: 2.4743 - regression_loss: 1.8396 - classification_loss: 9525/10000 [===========================>..] - ETA: 3:44 - loss: 2.4743 - regression_loss: 1.8395 - classification_loss: 9526/10000 [===========================>..] - ETA: 3:44 - loss: 2.4742 - regression_loss: 1.8395 - classification_loss: 9527/10000 [===========================>..] - ETA: 3:43 - loss: 2.4741 - regression_loss: 1.8394 - classification_loss: 9528/10000 [===========================>..] - ETA: 3:43 - loss: 2.4740 - regression_loss: 1.8393 - classification_loss: 9529/10000 [===========================>..] - ETA: 3:42 - loss: 2.4739 - regression_loss: 1.8393 - classification_loss: 9530/10000 [===========================>..] - ETA: 3:42 - loss: 2.4741 - regression_loss: 1.8394 - classification_loss: 9531/10000 [===========================>..] - ETA: 3:41 - loss: 2.4741 - regression_loss: 1.8394 - classification_loss: 9532/10000 [===========================>..] - ETA: 3:41 - loss: 2.4740 - regression_loss: 1.8394 - classification_loss: 9533/10000 [===========================>..] - ETA: 3:40 - loss: 2.4741 - regression_loss: 1.8394 - classification_loss: 9534/10000 [===========================>..] - ETA: 3:40 - loss: 2.4741 - regression_loss: 1.8394 - classification_loss: 9535/10000 [===========================>..] - ETA: 3:39 - loss: 2.4742 - regression_loss: 1.8395 - classification_loss: 9536/10000 [===========================>..] - ETA: 3:39 - loss: 2.4741 - regression_loss: 1.8395 - classification_loss: 9537/10000 [===========================>..] - ETA: 3:38 - loss: 2.4742 - regression_loss: 1.8395 - classification_loss: 9538/10000 [===========================>..] - ETA: 3:38 - loss: 2.4741 - regression_loss: 1.8395 - classification_loss: 9539/10000 [===========================>..] - ETA: 3:37 - loss: 2.4741 - regression_loss: 1.8394 - classification_loss: 9540/10000 [===========================>..] - ETA: 3:37 - loss: 2.4740 - regression_loss: 1.8394 - classification_loss: 9541/10000 [===========================>..] - ETA: 3:36 - loss: 2.4738 - regression_loss: 1.8393 - classification_loss: 9542/10000 [===========================>..] - ETA: 3:36 - loss: 2.4739 - regression_loss: 1.8393 - classification_loss: 9543/10000 [===========================>..] - ETA: 3:35 - loss: 2.4738 - regression_loss: 1.8392 - classification_loss: 9544/10000 [===========================>..] - ETA: 3:35 - loss: 2.4738 - regression_loss: 1.8392 - classification_loss: 9545/10000 [===========================>..] - ETA: 3:35 - loss: 2.4737 - regression_loss: 1.8392 - classification_loss: 9546/10000 [===========================>..] - ETA: 3:34 - loss: 2.4736 - regression_loss: 1.8391 - classification_loss: 9547/10000 [===========================>..] - ETA: 3:34 - loss: 2.4736 - regression_loss: 1.8391 - classification_loss: 9548/10000 [===========================>..] - ETA: 3:33 - loss: 2.4734 - regression_loss: 1.8390 - classification_loss: 9549/10000 [===========================>..] - ETA: 3:33 - loss: 2.4734 - regression_loss: 1.8390 - classification_loss: 9550/10000 [===========================>..] - ETA: 3:32 - loss: 2.4733 - regression_loss: 1.8389 - classification_loss: 9551/10000 [===========================>..] - ETA: 3:32 - loss: 2.4732 - regression_loss: 1.8388 - classification_loss: 9552/10000 [===========================>..] - ETA: 3:31 - loss: 2.4731 - regression_loss: 1.8388 - classification_loss: 9553/10000 [===========================>..] - ETA: 3:31 - loss: 2.4732 - regression_loss: 1.8388 - classification_loss: 9554/10000 [===========================>..] - ETA: 3:30 - loss: 2.4730 - regression_loss: 1.8387 - classification_loss: 9555/10000 [===========================>..] - ETA: 3:30 - loss: 2.4731 - regression_loss: 1.8388 - classification_loss: 9556/10000 [===========================>..] - ETA: 3:29 - loss: 2.4730 - regression_loss: 1.8387 - classification_loss: 9557/10000 [===========================>..] - ETA: 3:29 - loss: 2.4729 - regression_loss: 1.8386 - classification_loss: 9558/10000 [===========================>..] - ETA: 3:28 - loss: 2.4728 - regression_loss: 1.8385 - classification_loss: 9559/10000 [===========================>..] - ETA: 3:28 - loss: 2.4727 - regression_loss: 1.8385 - classification_loss: 9560/10000 [===========================>..] - ETA: 3:27 - loss: 2.4726 - regression_loss: 1.8384 - classification_loss: 9561/10000 [===========================>..] - ETA: 3:27 - loss: 2.4727 - regression_loss: 1.8384 - classification_loss: 9562/10000 [===========================>..] - ETA: 3:27 - loss: 2.4726 - regression_loss: 1.8383 - classification_loss: 9563/10000 [===========================>..] - ETA: 3:26 - loss: 2.4725 - regression_loss: 1.8383 - classification_loss: 9564/10000 [===========================>..] - ETA: 3:26 - loss: 2.4724 - regression_loss: 1.8382 - classification_loss: 9565/10000 [===========================>..] - ETA: 3:25 - loss: 2.4724 - regression_loss: 1.8382 - classification_loss: 9566/10000 [===========================>..] - ETA: 3:25 - loss: 2.4722 - regression_loss: 1.8381 - classification_loss: 9567/10000 [===========================>..] - ETA: 3:24 - loss: 2.4721 - regression_loss: 1.8380 - classification_loss: 9568/10000 [===========================>..] - ETA: 3:24 - loss: 2.4721 - regression_loss: 1.8380 - classification_loss: 9569/10000 [===========================>..] - ETA: 3:23 - loss: 2.4721 - regression_loss: 1.8380 - classification_loss: 9570/10000 [===========================>..] - ETA: 3:23 - loss: 2.4722 - regression_loss: 1.8381 - classification_loss: 9571/10000 [===========================>..] - ETA: 3:22 - loss: 2.4722 - regression_loss: 1.8381 - classification_loss: 9572/10000 [===========================>..] - ETA: 3:22 - loss: 2.4722 - regression_loss: 1.8381 - classification_loss: 9573/10000 [===========================>..] - ETA: 3:21 - loss: 2.4721 - regression_loss: 1.8381 - classification_loss: 9574/10000 [===========================>..] - ETA: 3:21 - loss: 2.4721 - regression_loss: 1.8381 - classification_loss: 9575/10000 [===========================>..] - ETA: 3:20 - loss: 2.4720 - regression_loss: 1.8380 - classification_loss: 9576/10000 [===========================>..] - ETA: 3:20 - loss: 2.4719 - regression_loss: 1.8380 - classification_loss: 9577/10000 [===========================>..] - ETA: 3:19 - loss: 2.4719 - regression_loss: 1.8379 - classification_loss: 9578/10000 [===========================>..] - ETA: 3:19 - loss: 2.4718 - regression_loss: 1.8379 - classification_loss: 9579/10000 [===========================>..] - ETA: 3:18 - loss: 2.4718 - regression_loss: 1.8379 - classification_loss: 9580/10000 [===========================>..] - ETA: 3:18 - loss: 2.4717 - regression_loss: 1.8379 - classification_loss: 9581/10000 [===========================>..] - ETA: 3:18 - loss: 2.4716 - regression_loss: 1.8378 - classification_loss: 9582/10000 [===========================>..] - ETA: 3:17 - loss: 2.4715 - regression_loss: 1.8377 - classification_loss: 9583/10000 [===========================>..] - ETA: 3:17 - loss: 2.4715 - regression_loss: 1.8377 - classification_loss: 9584/10000 [===========================>..] - ETA: 3:16 - loss: 2.4714 - regression_loss: 1.8376 - classification_loss: 9585/10000 [===========================>..] - ETA: 3:16 - loss: 2.4714 - regression_loss: 1.8377 - classification_loss: 9586/10000 [===========================>..] - ETA: 3:15 - loss: 2.4714 - regression_loss: 1.8377 - classification_loss: 9587/10000 [===========================>..] - ETA: 3:15 - loss: 2.4714 - regression_loss: 1.8376 - classification_loss: 9588/10000 [===========================>..] - ETA: 3:14 - loss: 2.4715 - regression_loss: 1.8377 - classification_loss: 9589/10000 [===========================>..] - ETA: 3:14 - loss: 2.4714 - regression_loss: 1.8376 - classification_loss: 9590/10000 [===========================>..] - ETA: 3:13 - loss: 2.4713 - regression_loss: 1.8376 - classification_loss: 9591/10000 [===========================>..] - ETA: 3:13 - loss: 2.4713 - regression_loss: 1.8376 - classification_loss: 9592/10000 [===========================>..] - ETA: 3:12 - loss: 2.4712 - regression_loss: 1.8375 - classification_loss: 9593/10000 [===========================>..] - ETA: 3:12 - loss: 2.4711 - regression_loss: 1.8374 - classification_loss: 9594/10000 [===========================>..] - ETA: 3:11 - loss: 2.4710 - regression_loss: 1.8374 - classification_loss: 9595/10000 [===========================>..] - ETA: 3:11 - loss: 2.4710 - regression_loss: 1.8373 - classification_loss: 9596/10000 [===========================>..] - ETA: 3:10 - loss: 2.4709 - regression_loss: 1.8372 - classification_loss: 9597/10000 [===========================>..] - ETA: 3:10 - loss: 2.4707 - regression_loss: 1.8371 - classification_loss: 9598/10000 [===========================>..] - ETA: 3:09 - loss: 2.4708 - regression_loss: 1.8372 - classification_loss: 9599/10000 [===========================>..] - ETA: 3:09 - loss: 2.4707 - regression_loss: 1.8371 - classification_loss: 9600/10000 [===========================>..] - ETA: 3:09 - loss: 2.4706 - regression_loss: 1.8371 - classification_loss: 9601/10000 [===========================>..] - ETA: 3:08 - loss: 2.4705 - regression_loss: 1.8369 - classification_loss: 9602/10000 [===========================>..] - ETA: 3:08 - loss: 2.4704 - regression_loss: 1.8369 - classification_loss: 9603/10000 [===========================>..] - ETA: 3:07 - loss: 2.4704 - regression_loss: 1.8368 - classification_loss: 9604/10000 [===========================>..] - ETA: 3:07 - loss: 2.4703 - regression_loss: 1.8368 - classification_loss: 9605/10000 [===========================>..] - ETA: 3:06 - loss: 2.4702 - regression_loss: 1.8367 - classification_loss: 9606/10000 [===========================>..] - ETA: 3:06 - loss: 2.4702 - regression_loss: 1.8367 - classification_loss: 9607/10000 [===========================>..] - ETA: 3:05 - loss: 2.4700 - regression_loss: 1.8366 - classification_loss: 9608/10000 [===========================>..] - ETA: 3:05 - loss: 2.4700 - regression_loss: 1.8365 - classification_loss: 9609/10000 [===========================>..] - ETA: 3:04 - loss: 2.4699 - regression_loss: 1.8365 - classification_loss: 9610/10000 [===========================>..] - ETA: 3:04 - loss: 2.4699 - regression_loss: 1.8365 - classification_loss: 9611/10000 [===========================>..] - ETA: 3:03 - loss: 2.4698 - regression_loss: 1.8364 - classification_loss: 9612/10000 [===========================>..] - ETA: 3:03 - loss: 2.4697 - regression_loss: 1.8364 - classification_loss: 9613/10000 [===========================>..] - ETA: 3:02 - loss: 2.4696 - regression_loss: 1.8363 - classification_loss: 9614/10000 [===========================>..] - ETA: 3:02 - loss: 2.4696 - regression_loss: 1.8363 - classification_loss: 9615/10000 [===========================>..] - ETA: 3:01 - loss: 2.4694 - regression_loss: 1.8362 - classification_loss: 9616/10000 [===========================>..] - ETA: 3:01 - loss: 2.4695 - regression_loss: 1.8362 - classification_loss: 9617/10000 [===========================>..] - ETA: 3:00 - loss: 2.4694 - regression_loss: 1.8361 - classification_loss: 9618/10000 [===========================>..] - ETA: 3:00 - loss: 2.4693 - regression_loss: 1.8361 - classification_loss: 9619/10000 [===========================>..] - ETA: 3:00 - loss: 2.4693 - regression_loss: 1.8361 - classification_loss: 9620/10000 [===========================>..] - ETA: 2:59 - loss: 2.4692 - regression_loss: 1.8360 - classification_loss: 9621/10000 [===========================>..] - ETA: 2:59 - loss: 2.4692 - regression_loss: 1.8360 - classification_loss: 9622/10000 [===========================>..] - ETA: 2:58 - loss: 2.4691 - regression_loss: 1.8359 - classification_loss: 9623/10000 [===========================>..] - ETA: 2:58 - loss: 2.4690 - regression_loss: 1.8358 - classification_loss: 9624/10000 [===========================>..] - ETA: 2:57 - loss: 2.4690 - regression_loss: 1.8359 - classification_loss: 9625/10000 [===========================>..] - ETA: 2:57 - loss: 2.4690 - regression_loss: 1.8359 - classification_loss: 9626/10000 [===========================>..] - ETA: 2:56 - loss: 2.4691 - regression_loss: 1.8359 - classification_loss: 9627/10000 [===========================>..] - ETA: 2:56 - loss: 2.4690 - regression_loss: 1.8359 - classification_loss: 9628/10000 [===========================>..] - ETA: 2:55 - loss: 2.4689 - regression_loss: 1.8359 - classification_loss: 9629/10000 [===========================>..] - ETA: 2:55 - loss: 2.4688 - regression_loss: 1.8357 - classification_loss: 9630/10000 [===========================>..] - ETA: 2:54 - loss: 2.4687 - regression_loss: 1.8356 - classification_loss: 9631/10000 [===========================>..] - ETA: 2:54 - loss: 2.4686 - regression_loss: 1.8356 - classification_loss: 9632/10000 [===========================>..] - ETA: 2:53 - loss: 2.4686 - regression_loss: 1.8356 - classification_loss: 9633/10000 [===========================>..] - ETA: 2:53 - loss: 2.4685 - regression_loss: 1.8355 - classification_loss: 9634/10000 [===========================>..] - ETA: 2:52 - loss: 2.4686 - regression_loss: 1.8356 - classification_loss: 9635/10000 [===========================>..] - ETA: 2:52 - loss: 2.4685 - regression_loss: 1.8356 - classification_loss: 9636/10000 [===========================>..] - ETA: 2:51 - loss: 2.4685 - regression_loss: 1.8355 - classification_loss: 9637/10000 [===========================>..] - ETA: 2:51 - loss: 2.4685 - regression_loss: 1.8356 - classification_loss: 9638/10000 [===========================>..] - ETA: 2:51 - loss: 2.4684 - regression_loss: 1.8355 - classification_loss: 9639/10000 [===========================>..] - ETA: 2:50 - loss: 2.4685 - regression_loss: 1.8355 - classification_loss: 9640/10000 [===========================>..] - ETA: 2:50 - loss: 2.4685 - regression_loss: 1.8356 - classification_loss: 9641/10000 [===========================>..] - ETA: 2:49 - loss: 2.4686 - regression_loss: 1.8357 - classification_loss: 9642/10000 [===========================>..] - ETA: 2:49 - loss: 2.4684 - regression_loss: 1.8356 - classification_loss: 9643/10000 [===========================>..] - ETA: 2:48 - loss: 2.4683 - regression_loss: 1.8355 - classification_loss: 9644/10000 [===========================>..] - ETA: 2:48 - loss: 2.4683 - regression_loss: 1.8355 - classification_loss: 9645/10000 [===========================>..] - ETA: 2:47 - loss: 2.4682 - regression_loss: 1.8355 - classification_loss: 9646/10000 [===========================>..] - ETA: 2:47 - loss: 2.4684 - regression_loss: 1.8356 - classification_loss: 9647/10000 [===========================>..] - ETA: 2:46 - loss: 2.4684 - regression_loss: 1.8356 - classification_loss: 9648/10000 [===========================>..] - ETA: 2:46 - loss: 2.4684 - regression_loss: 1.8356 - classification_loss: 9649/10000 [===========================>..] - ETA: 2:45 - loss: 2.4685 - regression_loss: 1.8357 - classification_loss: 9650/10000 [===========================>..] - ETA: 2:45 - loss: 2.4685 - regression_loss: 1.8357 - classification_loss: 9651/10000 [===========================>..] - ETA: 2:44 - loss: 2.4685 - regression_loss: 1.8357 - classification_loss: 9652/10000 [===========================>..] - ETA: 2:44 - loss: 2.4684 - regression_loss: 1.8357 - classification_loss: 9653/10000 [===========================>..] - ETA: 2:43 - loss: 2.4682 - regression_loss: 1.8356 - classification_loss: 9654/10000 [===========================>..] - ETA: 2:43 - loss: 2.4682 - regression_loss: 1.8355 - classification_loss: 9655/10000 [===========================>..] - ETA: 2:43 - loss: 2.4682 - regression_loss: 1.8355 - classification_loss: 9656/10000 [===========================>..] - ETA: 2:42 - loss: 2.4682 - regression_loss: 1.8355 - classification_loss: 9657/10000 [===========================>..] - ETA: 2:42 - loss: 2.4680 - regression_loss: 1.8354 - classification_loss: 9658/10000 [===========================>..] - ETA: 2:41 - loss: 2.4680 - regression_loss: 1.8354 - classification_loss: 9659/10000 [===========================>..] - ETA: 2:41 - loss: 2.4679 - regression_loss: 1.8354 - classification_loss: 9660/10000 [===========================>..] - ETA: 2:40 - loss: 2.4678 - regression_loss: 1.8353 - classification_loss: 9661/10000 [===========================>..] - ETA: 2:40 - loss: 2.4677 - regression_loss: 1.8352 - classification_loss: 9662/10000 [===========================>..] - ETA: 2:39 - loss: 2.4676 - regression_loss: 1.8352 - classification_loss: 9663/10000 [===========================>..] - ETA: 2:39 - loss: 2.4676 - regression_loss: 1.8351 - classification_loss: 9664/10000 [===========================>..] - ETA: 2:38 - loss: 2.4674 - regression_loss: 1.8350 - classification_loss: 9665/10000 [===========================>..] - ETA: 2:38 - loss: 2.4674 - regression_loss: 1.8350 - classification_loss: 9666/10000 [===========================>..] - ETA: 2:37 - loss: 2.4673 - regression_loss: 1.8349 - classification_loss: 9667/10000 [============================>.] - ETA: 2:37 - loss: 2.4673 - regression_loss: 1.8349 - classification_loss: 9668/10000 [============================>.] - ETA: 2:36 - loss: 2.4673 - regression_loss: 1.8349 - classification_loss: 9669/10000 [============================>.] - ETA: 2:36 - loss: 2.4672 - regression_loss: 1.8348 - classification_loss: 9670/10000 [============================>.] - ETA: 2:35 - loss: 2.4670 - regression_loss: 1.8347 - classification_loss: 9671/10000 [============================>.] - ETA: 2:35 - loss: 2.4670 - regression_loss: 1.8347 - classification_loss: 9672/10000 [============================>.] - ETA: 2:34 - loss: 2.4670 - regression_loss: 1.8346 - classification_loss: 9673/10000 [============================>.] - ETA: 2:34 - loss: 2.4668 - regression_loss: 1.8345 - classification_loss: 9674/10000 [============================>.] - ETA: 2:34 - loss: 2.4669 - regression_loss: 1.8346 - classification_loss: 9675/10000 [============================>.] - ETA: 2:33 - loss: 2.4669 - regression_loss: 1.8345 - classification_loss: 9676/10000 [============================>.] - ETA: 2:33 - loss: 2.4668 - regression_loss: 1.8345 - classification_loss: 9677/10000 [============================>.] - ETA: 2:32 - loss: 2.4668 - regression_loss: 1.8345 - classification_loss: 9678/10000 [============================>.] - ETA: 2:32 - loss: 2.4667 - regression_loss: 1.8344 - classification_loss: 9679/10000 [============================>.] - ETA: 2:31 - loss: 2.4667 - regression_loss: 1.8344 - classification_loss: 9680/10000 [============================>.] - ETA: 2:31 - loss: 2.4667 - regression_loss: 1.8344 - classification_loss: 9681/10000 [============================>.] - ETA: 2:30 - loss: 2.4667 - regression_loss: 1.8344 - classification_loss: 9682/10000 [============================>.] - ETA: 2:30 - loss: 2.4667 - regression_loss: 1.8344 - classification_loss: 9683/10000 [============================>.] - ETA: 2:29 - loss: 2.4667 - regression_loss: 1.8344 - classification_loss: 9684/10000 [============================>.] - ETA: 2:29 - loss: 2.4665 - regression_loss: 1.8343 - classification_loss: 9685/10000 [============================>.] - ETA: 2:28 - loss: 2.4664 - regression_loss: 1.8342 - classification_loss: 9686/10000 [============================>.] - ETA: 2:28 - loss: 2.4664 - regression_loss: 1.8342 - classification_loss: 9687/10000 [============================>.] - ETA: 2:27 - loss: 2.4664 - regression_loss: 1.8342 - classification_loss: 9688/10000 [============================>.] - ETA: 2:27 - loss: 2.4663 - regression_loss: 1.8342 - classification_loss: 9689/10000 [============================>.] - ETA: 2:26 - loss: 2.4663 - regression_loss: 1.8342 - classification_loss: 9690/10000 [============================>.] - ETA: 2:26 - loss: 2.4662 - regression_loss: 1.8341 - classification_loss: 9691/10000 [============================>.] - ETA: 2:25 - loss: 2.4661 - regression_loss: 1.8340 - classification_loss: 9692/10000 [============================>.] - ETA: 2:25 - loss: 2.4660 - regression_loss: 1.8339 - classification_loss: 9693/10000 [============================>.] - ETA: 2:25 - loss: 2.4658 - regression_loss: 1.8338 - classification_loss: 9694/10000 [============================>.] - ETA: 2:24 - loss: 2.4658 - regression_loss: 1.8337 - classification_loss: 9695/10000 [============================>.] - ETA: 2:24 - loss: 2.4657 - regression_loss: 1.8337 - classification_loss: 9696/10000 [============================>.] - ETA: 2:23 - loss: 2.4657 - regression_loss: 1.8337 - classification_loss: 9697/10000 [============================>.] - ETA: 2:23 - loss: 2.4656 - regression_loss: 1.8336 - classification_loss: 9698/10000 [============================>.] - ETA: 2:22 - loss: 2.4656 - regression_loss: 1.8336 - classification_loss: 9699/10000 [============================>.] - ETA: 2:22 - loss: 2.4656 - regression_loss: 1.8336 - classification_loss: 9700/10000 [============================>.] - ETA: 2:21 - loss: 2.4655 - regression_loss: 1.8336 - classification_loss: 9701/10000 [============================>.] - ETA: 2:21 - loss: 2.4654 - regression_loss: 1.8335 - classification_loss: 9702/10000 [============================>.] - ETA: 2:20 - loss: 2.4653 - regression_loss: 1.8334 - classification_loss: 9703/10000 [============================>.] - ETA: 2:20 - loss: 2.4653 - regression_loss: 1.8334 - classification_loss: 9704/10000 [============================>.] - ETA: 2:19 - loss: 2.4652 - regression_loss: 1.8334 - classification_loss: 9705/10000 [============================>.] - ETA: 2:19 - loss: 2.4651 - regression_loss: 1.8333 - classification_loss: 9706/10000 [============================>.] - ETA: 2:18 - loss: 2.4651 - regression_loss: 1.8333 - classification_loss: 9707/10000 [============================>.] - ETA: 2:18 - loss: 2.4651 - regression_loss: 1.8333 - classification_loss: 9708/10000 [============================>.] - ETA: 2:17 - loss: 2.4649 - regression_loss: 1.8332 - classification_loss: 9709/10000 [============================>.] - ETA: 2:17 - loss: 2.4649 - regression_loss: 1.8331 - classification_loss: 9710/10000 [============================>.] - ETA: 2:17 - loss: 2.4647 - regression_loss: 1.8331 - classification_loss: 9711/10000 [============================>.] - ETA: 2:16 - loss: 2.4646 - regression_loss: 1.8330 - classification_loss: 9712/10000 [============================>.] - ETA: 2:16 - loss: 2.4646 - regression_loss: 1.8329 - classification_loss: 9713/10000 [============================>.] - ETA: 2:15 - loss: 2.4645 - regression_loss: 1.8329 - classification_loss: 9714/10000 [============================>.] - ETA: 2:15 - loss: 2.4644 - regression_loss: 1.8328 - classification_loss: 9715/10000 [============================>.] - ETA: 2:14 - loss: 2.4645 - regression_loss: 1.8329 - classification_loss: 9716/10000 [============================>.] - ETA: 2:14 - loss: 2.4645 - regression_loss: 1.8329 - classification_loss: 9717/10000 [============================>.] - ETA: 2:13 - loss: 2.4644 - regression_loss: 1.8328 - classification_loss: 9718/10000 [============================>.] - ETA: 2:13 - loss: 2.4644 - regression_loss: 1.8328 - classification_loss: 9719/10000 [============================>.] - ETA: 2:12 - loss: 2.4642 - regression_loss: 1.8327 - classification_loss: 9720/10000 [============================>.] - ETA: 2:12 - loss: 2.4642 - regression_loss: 1.8327 - classification_loss: 9721/10000 [============================>.] - ETA: 2:11 - loss: 2.4642 - regression_loss: 1.8327 - classification_loss: 9722/10000 [============================>.] - ETA: 2:11 - loss: 2.4642 - regression_loss: 1.8328 - classification_loss: 9723/10000 [============================>.] - ETA: 2:10 - loss: 2.4641 - regression_loss: 1.8327 - classification_loss: 9724/10000 [============================>.] - ETA: 2:10 - loss: 2.4640 - regression_loss: 1.8327 - classification_loss: 9725/10000 [============================>.] - ETA: 2:09 - loss: 2.4639 - regression_loss: 1.8326 - classification_loss: 9726/10000 [============================>.] - ETA: 2:09 - loss: 2.4638 - regression_loss: 1.8324 - classification_loss: 9727/10000 [============================>.] - ETA: 2:08 - loss: 2.4637 - regression_loss: 1.8324 - classification_loss: 9728/10000 [============================>.] - ETA: 2:08 - loss: 2.4636 - regression_loss: 1.8323 - classification_loss: 9729/10000 [============================>.] - ETA: 2:08 - loss: 2.4635 - regression_loss: 1.8323 - classification_loss: 9730/10000 [============================>.] - ETA: 2:07 - loss: 2.4635 - regression_loss: 1.8322 - classification_loss: 9731/10000 [============================>.] - ETA: 2:07 - loss: 2.4634 - regression_loss: 1.8322 - classification_loss: 9732/10000 [============================>.] - ETA: 2:06 - loss: 2.4633 - regression_loss: 1.8321 - classification_loss: 9733/10000 [============================>.] - ETA: 2:06 - loss: 2.4633 - regression_loss: 1.8321 - classification_loss: 9734/10000 [============================>.] - ETA: 2:05 - loss: 2.4632 - regression_loss: 1.8321 - classification_loss: 9735/10000 [============================>.] - ETA: 2:05 - loss: 2.4632 - regression_loss: 1.8321 - classification_loss: 9736/10000 [============================>.] - ETA: 2:04 - loss: 2.4632 - regression_loss: 1.8321 - classification_loss: 9737/10000 [============================>.] - ETA: 2:04 - loss: 2.4631 - regression_loss: 1.8320 - classification_loss: 9738/10000 [============================>.] - ETA: 2:03 - loss: 2.4629 - regression_loss: 1.8319 - classification_loss: 9739/10000 [============================>.] - ETA: 2:03 - loss: 2.4630 - regression_loss: 1.8319 - classification_loss: 9740/10000 [============================>.] - ETA: 2:02 - loss: 2.4629 - regression_loss: 1.8319 - classification_loss: 9741/10000 [============================>.] - ETA: 2:02 - loss: 2.4629 - regression_loss: 1.8318 - classification_loss: 9742/10000 [============================>.] - ETA: 2:01 - loss: 2.4628 - regression_loss: 1.8318 - classification_loss: 9743/10000 [============================>.] - ETA: 2:01 - loss: 2.4627 - regression_loss: 1.8318 - classification_loss: 9744/10000 [============================>.] - ETA: 2:00 - loss: 2.4626 - regression_loss: 1.8317 - classification_loss: 9745/10000 [============================>.] - ETA: 2:00 - loss: 2.4625 - regression_loss: 1.8316 - classification_loss: 9746/10000 [============================>.] - ETA: 1:59 - loss: 2.4623 - regression_loss: 1.8315 - classification_loss: 9747/10000 [============================>.] - ETA: 1:59 - loss: 2.4624 - regression_loss: 1.8316 - classification_loss: 9748/10000 [============================>.] - ETA: 1:59 - loss: 2.4623 - regression_loss: 1.8314 - classification_loss: 9749/10000 [============================>.] - ETA: 1:58 - loss: 2.4622 - regression_loss: 1.8314 - classification_loss: 9750/10000 [============================>.] - ETA: 1:58 - loss: 2.4621 - regression_loss: 1.8314 - classification_loss: 9751/10000 [============================>.] - ETA: 1:57 - loss: 2.4620 - regression_loss: 1.8313 - classification_loss: 9752/10000 [============================>.] - ETA: 1:57 - loss: 2.4619 - regression_loss: 1.8312 - classification_loss: 9753/10000 [============================>.] - ETA: 1:56 - loss: 2.4619 - regression_loss: 1.8312 - classification_loss: 9754/10000 [============================>.] - ETA: 1:56 - loss: 2.4619 - regression_loss: 1.8312 - classification_loss: 9755/10000 [============================>.] - ETA: 1:55 - loss: 2.4619 - regression_loss: 1.8312 - classification_loss: 9756/10000 [============================>.] - ETA: 1:55 - loss: 2.4619 - regression_loss: 1.8312 - classification_loss: 9757/10000 [============================>.] - ETA: 1:54 - loss: 2.4619 - regression_loss: 1.8312 - classification_loss: 9758/10000 [============================>.] - ETA: 1:54 - loss: 2.4620 - regression_loss: 1.8313 - classification_loss: 9759/10000 [============================>.] - ETA: 1:53 - loss: 2.4619 - regression_loss: 1.8313 - classification_loss: 9760/10000 [============================>.] - ETA: 1:53 - loss: 2.4619 - regression_loss: 1.8313 - classification_loss: 9761/10000 [============================>.] - ETA: 1:52 - loss: 2.4617 - regression_loss: 1.8311 - classification_loss: 9762/10000 [============================>.] - ETA: 1:52 - loss: 2.4617 - regression_loss: 1.8311 - classification_loss: 9763/10000 [============================>.] - ETA: 1:51 - loss: 2.4617 - regression_loss: 1.8311 - classification_loss: 9764/10000 [============================>.] - ETA: 1:51 - loss: 2.4618 - regression_loss: 1.8312 - classification_loss: 9765/10000 [============================>.] - ETA: 1:51 - loss: 2.4619 - regression_loss: 1.8312 - classification_loss: 9766/10000 [============================>.] - ETA: 1:50 - loss: 2.4618 - regression_loss: 1.8312 - classification_loss: 9767/10000 [============================>.] - ETA: 1:50 - loss: 2.4618 - regression_loss: 1.8312 - classification_loss: 9768/10000 [============================>.] - ETA: 1:49 - loss: 2.4618 - regression_loss: 1.8311 - classification_loss: 9769/10000 [============================>.] - ETA: 1:49 - loss: 2.4617 - regression_loss: 1.8310 - classification_loss: 9770/10000 [============================>.] - ETA: 1:48 - loss: 2.4618 - regression_loss: 1.8311 - classification_loss: 9771/10000 [============================>.] - ETA: 1:48 - loss: 2.4616 - regression_loss: 1.8310 - classification_loss: 9772/10000 [============================>.] - ETA: 1:47 - loss: 2.4616 - regression_loss: 1.8310 - classification_loss: 9773/10000 [============================>.] - ETA: 1:47 - loss: 2.4616 - regression_loss: 1.8310 - classification_loss: 9774/10000 [============================>.] - ETA: 1:46 - loss: 2.4615 - regression_loss: 1.8310 - classification_loss: 9775/10000 [============================>.] - ETA: 1:46 - loss: 2.4614 - regression_loss: 1.8309 - classification_loss: 9776/10000 [============================>.] - ETA: 1:45 - loss: 2.4613 - regression_loss: 1.8308 - classification_loss: 9777/10000 [============================>.] - ETA: 1:45 - loss: 2.4614 - regression_loss: 1.8309 - classification_loss: 9778/10000 [============================>.] - ETA: 1:44 - loss: 2.4613 - regression_loss: 1.8309 - classification_loss: 9779/10000 [============================>.] - ETA: 1:44 - loss: 2.4612 - regression_loss: 1.8307 - classification_loss: 9780/10000 [============================>.] - ETA: 1:43 - loss: 2.4611 - regression_loss: 1.8306 - classification_loss: 9781/10000 [============================>.] - ETA: 1:43 - loss: 2.4612 - regression_loss: 1.8307 - classification_loss: 9782/10000 [============================>.] - ETA: 1:42 - loss: 2.4611 - regression_loss: 1.8307 - classification_loss: 9783/10000 [============================>.] - ETA: 1:42 - loss: 2.4610 - regression_loss: 1.8306 - classification_loss: 9784/10000 [============================>.] - ETA: 1:42 - loss: 2.4609 - regression_loss: 1.8306 - classification_loss: 9785/10000 [============================>.] - ETA: 1:41 - loss: 2.4608 - regression_loss: 1.8305 - classification_loss: 9786/10000 [============================>.] - ETA: 1:41 - loss: 2.4608 - regression_loss: 1.8304 - classification_loss: 9787/10000 [============================>.] - ETA: 1:40 - loss: 2.4606 - regression_loss: 1.8304 - classification_loss: 9788/10000 [============================>.] - ETA: 1:40 - loss: 2.4606 - regression_loss: 1.8303 - classification_loss: 9789/10000 [============================>.] - ETA: 1:39 - loss: 2.4606 - regression_loss: 1.8303 - classification_loss: 9790/10000 [============================>.] - ETA: 1:39 - loss: 2.4605 - regression_loss: 1.8302 - classification_loss: 9791/10000 [============================>.] - ETA: 1:38 - loss: 2.4605 - regression_loss: 1.8302 - classification_loss: 9792/10000 [============================>.] - ETA: 1:38 - loss: 2.4604 - regression_loss: 1.8302 - classification_loss: 9793/10000 [============================>.] - ETA: 1:37 - loss: 2.4604 - regression_loss: 1.8302 - classification_loss: 9794/10000 [============================>.] - ETA: 1:37 - loss: 2.4603 - regression_loss: 1.8302 - classification_loss: 9795/10000 [============================>.] - ETA: 1:36 - loss: 2.4602 - regression_loss: 1.8301 - classification_loss: 9796/10000 [============================>.] - ETA: 1:36 - loss: 2.4602 - regression_loss: 1.8301 - classification_loss: 9797/10000 [============================>.] - ETA: 1:35 - loss: 2.4602 - regression_loss: 1.8301 - classification_loss: 9798/10000 [============================>.] - ETA: 1:35 - loss: 2.4602 - regression_loss: 1.8301 - classification_loss: 9799/10000 [============================>.] - ETA: 1:34 - loss: 2.4601 - regression_loss: 1.8300 - classification_loss: 9800/10000 [============================>.] - ETA: 1:34 - loss: 2.4601 - regression_loss: 1.8300 - classification_loss: 9801/10000 [============================>.] - ETA: 1:33 - loss: 2.4601 - regression_loss: 1.8300 - classification_loss: 9802/10000 [============================>.] - ETA: 1:33 - loss: 2.4600 - regression_loss: 1.8300 - classification_loss: 9803/10000 [============================>.] - ETA: 1:33 - loss: 2.4600 - regression_loss: 1.8299 - classification_loss: 9804/10000 [============================>.] - ETA: 1:32 - loss: 2.4599 - regression_loss: 1.8299 - classification_loss: 9805/10000 [============================>.] - ETA: 1:32 - loss: 2.4599 - regression_loss: 1.8298 - classification_loss: 9806/10000 [============================>.] - ETA: 1:31 - loss: 2.4598 - regression_loss: 1.8298 - classification_loss: 9807/10000 [============================>.] - ETA: 1:31 - loss: 2.4598 - regression_loss: 1.8297 - classification_loss: 9808/10000 [============================>.] - ETA: 1:30 - loss: 2.4598 - regression_loss: 1.8298 - classification_loss: 9809/10000 [============================>.] - ETA: 1:30 - loss: 2.4597 - regression_loss: 1.8297 - classification_loss: 9810/10000 [============================>.] - ETA: 1:29 - loss: 2.4596 - regression_loss: 1.8296 - classification_loss: 9811/10000 [============================>.] - ETA: 1:29 - loss: 2.4596 - regression_loss: 1.8296 - classification_loss: 9812/10000 [============================>.] - ETA: 1:28 - loss: 2.4596 - regression_loss: 1.8297 - classification_loss: 9813/10000 [============================>.] - ETA: 1:28 - loss: 2.4595 - regression_loss: 1.8296 - classification_loss: 9814/10000 [============================>.] - ETA: 1:27 - loss: 2.4596 - regression_loss: 1.8297 - classification_loss: 9815/10000 [============================>.] - ETA: 1:27 - loss: 2.4596 - regression_loss: 1.8297 - classification_loss: 9816/10000 [============================>.] - ETA: 1:26 - loss: 2.4595 - regression_loss: 1.8296 - classification_loss: 9817/10000 [============================>.] - ETA: 1:26 - loss: 2.4595 - regression_loss: 1.8296 - classification_loss: 9818/10000 [============================>.] - ETA: 1:25 - loss: 2.4595 - regression_loss: 1.8296 - classification_loss: 9819/10000 [============================>.] - ETA: 1:25 - loss: 2.4595 - regression_loss: 1.8296 - classification_loss: 9820/10000 [============================>.] - ETA: 1:25 - loss: 2.4594 - regression_loss: 1.8296 - classification_loss: 9821/10000 [============================>.] - ETA: 1:24 - loss: 2.4594 - regression_loss: 1.8296 - classification_loss: 9822/10000 [============================>.] - ETA: 1:24 - loss: 2.4593 - regression_loss: 1.8295 - classification_loss: 9823/10000 [============================>.] - ETA: 1:23 - loss: 2.4592 - regression_loss: 1.8294 - classification_loss: 9824/10000 [============================>.] - ETA: 1:23 - loss: 2.4591 - regression_loss: 1.8294 - classification_loss: 9825/10000 [============================>.] - ETA: 1:22 - loss: 2.4590 - regression_loss: 1.8293 - classification_loss: 9826/10000 [============================>.] - ETA: 1:22 - loss: 2.4590 - regression_loss: 1.8293 - classification_loss: 9827/10000 [============================>.] - ETA: 1:21 - loss: 2.4590 - regression_loss: 1.8293 - classification_loss: 9828/10000 [============================>.] - ETA: 1:21 - loss: 2.4590 - regression_loss: 1.8294 - classification_loss: 9829/10000 [============================>.] - ETA: 1:20 - loss: 2.4590 - regression_loss: 1.8293 - classification_loss: 9830/10000 [============================>.] - ETA: 1:20 - loss: 2.4588 - regression_loss: 1.8293 - classification_loss: 9831/10000 [============================>.] - ETA: 1:19 - loss: 2.4588 - regression_loss: 1.8292 - classification_loss: 9832/10000 [============================>.] - ETA: 1:19 - loss: 2.4587 - regression_loss: 1.8291 - classification_loss: 9833/10000 [============================>.] - ETA: 1:18 - loss: 2.4587 - regression_loss: 1.8292 - classification_loss: 9834/10000 [============================>.] - ETA: 1:18 - loss: 2.4587 - regression_loss: 1.8292 - classification_loss: 9835/10000 [============================>.] - ETA: 1:17 - loss: 2.4587 - regression_loss: 1.8292 - classification_loss: 9836/10000 [============================>.] - ETA: 1:17 - loss: 2.4588 - regression_loss: 1.8293 - classification_loss: 9837/10000 [============================>.] - ETA: 1:16 - loss: 2.4587 - regression_loss: 1.8292 - classification_loss: 9838/10000 [============================>.] - ETA: 1:16 - loss: 2.4587 - regression_loss: 1.8292 - classification_loss: 9839/10000 [============================>.] - ETA: 1:16 - loss: 2.4585 - regression_loss: 1.8291 - classification_loss: 9840/10000 [============================>.] - ETA: 1:15 - loss: 2.4585 - regression_loss: 1.8291 - classification_loss: 9841/10000 [============================>.] - ETA: 1:15 - loss: 2.4584 - regression_loss: 1.8290 - classification_loss: 9842/10000 [============================>.] - ETA: 1:14 - loss: 2.4584 - regression_loss: 1.8291 - classification_loss: 9843/10000 [============================>.] - ETA: 1:14 - loss: 2.4585 - regression_loss: 1.8291 - classification_loss: 9844/10000 [============================>.] - ETA: 1:13 - loss: 2.4584 - regression_loss: 1.8290 - classification_loss: 9845/10000 [============================>.] - ETA: 1:13 - loss: 2.4584 - regression_loss: 1.8290 - classification_loss: 9846/10000 [============================>.] - ETA: 1:12 - loss: 2.4584 - regression_loss: 1.8290 - classification_loss: 9847/10000 [============================>.] - ETA: 1:12 - loss: 2.4584 - regression_loss: 1.8290 - classification_loss: 9848/10000 [============================>.] - ETA: 1:11 - loss: 2.4583 - regression_loss: 1.8289 - classification_loss: 9849/10000 [============================>.] - ETA: 1:11 - loss: 2.4583 - regression_loss: 1.8290 - classification_loss: 9850/10000 [============================>.] - ETA: 1:10 - loss: 2.4583 - regression_loss: 1.8289 - classification_loss: 9851/10000 [============================>.] - ETA: 1:10 - loss: 2.4581 - regression_loss: 1.8288 - classification_loss: 9852/10000 [============================>.] - ETA: 1:09 - loss: 2.4581 - regression_loss: 1.8288 - classification_loss: 9853/10000 [============================>.] - ETA: 1:09 - loss: 2.4580 - regression_loss: 1.8288 - classification_loss: 9854/10000 [============================>.] - ETA: 1:08 - loss: 2.4581 - regression_loss: 1.8288 - classification_loss: 9855/10000 [============================>.] - ETA: 1:08 - loss: 2.4580 - regression_loss: 1.8288 - classification_loss: 9856/10000 [============================>.] - ETA: 1:08 - loss: 2.4579 - regression_loss: 1.8287 - classification_loss: 9857/10000 [============================>.] - ETA: 1:07 - loss: 2.4578 - regression_loss: 1.8287 - classification_loss: 9858/10000 [============================>.] - ETA: 1:07 - loss: 2.4578 - regression_loss: 1.8286 - classification_loss: 9859/10000 [============================>.] - ETA: 1:06 - loss: 2.4577 - regression_loss: 1.8286 - classification_loss: 9860/10000 [============================>.] - ETA: 1:06 - loss: 2.4575 - regression_loss: 1.8285 - classification_loss: 9861/10000 [============================>.] - ETA: 1:05 - loss: 2.4575 - regression_loss: 1.8284 - classification_loss: 9862/10000 [============================>.] - ETA: 1:05 - loss: 2.4575 - regression_loss: 1.8284 - classification_loss: 9863/10000 [============================>.] - ETA: 1:04 - loss: 2.4576 - regression_loss: 1.8285 - classification_loss: 9864/10000 [============================>.] - ETA: 1:04 - loss: 2.4574 - regression_loss: 1.8284 - classification_loss: 9865/10000 [============================>.] - ETA: 1:03 - loss: 2.4574 - regression_loss: 1.8283 - classification_loss: 9866/10000 [============================>.] - ETA: 1:03 - loss: 2.4573 - regression_loss: 1.8283 - classification_loss: 9867/10000 [============================>.] - ETA: 1:02 - loss: 2.4572 - regression_loss: 1.8282 - classification_loss: 9868/10000 [============================>.] - ETA: 1:02 - loss: 2.4572 - regression_loss: 1.8282 - classification_loss: 9869/10000 [============================>.] - ETA: 1:01 - loss: 2.4572 - regression_loss: 1.8282 - classification_loss: 9870/10000 [============================>.] - ETA: 1:01 - loss: 2.4572 - regression_loss: 1.8282 - classification_loss: 9871/10000 [============================>.] - ETA: 1:00 - loss: 2.4573 - regression_loss: 1.8282 - classification_loss: 9872/10000 [============================>.] - ETA: 1:00 - loss: 2.4573 - regression_loss: 1.8283 - classification_loss: 9873/10000 [============================>.] - ETA: 59s - loss: 2.4572 - regression_loss: 1.8282 - classification_loss:  9874/10000 [============================>.] - ETA: 59s - loss: 2.4572 - regression_loss: 1.8282 - classification_loss:  9875/10000 [============================>.] - ETA: 59s - loss: 2.4572 - regression_loss: 1.8282 - classification_loss:  9876/10000 [============================>.] - ETA: 58s - loss: 2.4571 - regression_loss: 1.8282 - classification_loss:  9877/10000 [============================>.] - ETA: 58s - loss: 2.4570 - regression_loss: 1.8281 - classification_loss:  9878/10000 [============================>.] - ETA: 57s - loss: 2.4570 - regression_loss: 1.8281 - classification_loss:  9879/10000 [============================>.] - ETA: 57s - loss: 2.4570 - regression_loss: 1.8281 - classification_loss:  9880/10000 [============================>.] - ETA: 56s - loss: 2.4570 - regression_loss: 1.8281 - classification_loss:  9881/10000 [============================>.] - ETA: 56s - loss: 2.4569 - regression_loss: 1.8281 - classification_loss:  9882/10000 [============================>.] - ETA: 55s - loss: 2.4568 - regression_loss: 1.8280 - classification_loss:  9883/10000 [============================>.] - ETA: 55s - loss: 2.4567 - regression_loss: 1.8279 - classification_loss:  9884/10000 [============================>.] - ETA: 54s - loss: 2.4567 - regression_loss: 1.8279 - classification_loss:  9885/10000 [============================>.] - ETA: 54s - loss: 2.4567 - regression_loss: 1.8279 - classification_loss:  9886/10000 [============================>.] - ETA: 53s - loss: 2.4567 - regression_loss: 1.8279 - classification_loss:  9887/10000 [============================>.] - ETA: 53s - loss: 2.4567 - regression_loss: 1.8280 - classification_loss:  9888/10000 [============================>.] - ETA: 52s - loss: 2.4567 - regression_loss: 1.8279 - classification_loss:  9889/10000 [============================>.] - ETA: 52s - loss: 2.4565 - regression_loss: 1.8278 - classification_loss:  9890/10000 [============================>.] - ETA: 51s - loss: 2.4565 - regression_loss: 1.8278 - classification_loss:  9891/10000 [============================>.] - ETA: 51s - loss: 2.4564 - regression_loss: 1.8277 - classification_loss:  9892/10000 [============================>.] - ETA: 50s - loss: 2.4562 - regression_loss: 1.8276 - classification_loss:  9893/10000 [============================>.] - ETA: 50s - loss: 2.4561 - regression_loss: 1.8275 - classification_loss:  9894/10000 [============================>.] - ETA: 50s - loss: 2.4560 - regression_loss: 1.8275 - classification_loss:  9895/10000 [============================>.] - ETA: 49s - loss: 2.4560 - regression_loss: 1.8274 - classification_loss:  9896/10000 [============================>.] - ETA: 49s - loss: 2.4560 - regression_loss: 1.8275 - classification_loss:  9897/10000 [============================>.] - ETA: 48s - loss: 2.4560 - regression_loss: 1.8275 - classification_loss:  9898/10000 [============================>.] - ETA: 48s - loss: 2.4560 - regression_loss: 1.8275 - classification_loss:  9899/10000 [============================>.] - ETA: 47s - loss: 2.4559 - regression_loss: 1.8274 - classification_loss:  9900/10000 [============================>.] - ETA: 47s - loss: 2.4557 - regression_loss: 1.8273 - classification_loss:  9901/10000 [============================>.] - ETA: 46s - loss: 2.4557 - regression_loss: 1.8273 - classification_loss:  9902/10000 [============================>.] - ETA: 46s - loss: 2.4556 - regression_loss: 1.8271 - classification_loss:  9903/10000 [============================>.] - ETA: 45s - loss: 2.4555 - regression_loss: 1.8271 - classification_loss:  9904/10000 [============================>.] - ETA: 45s - loss: 2.4554 - regression_loss: 1.8270 - classification_loss:  9905/10000 [============================>.] - ETA: 44s - loss: 2.4554 - regression_loss: 1.8270 - classification_loss:  9906/10000 [============================>.] - ETA: 44s - loss: 2.4553 - regression_loss: 1.8270 - classification_loss:  9907/10000 [============================>.] - ETA: 43s - loss: 2.4553 - regression_loss: 1.8269 - classification_loss:  9908/10000 [============================>.] - ETA: 43s - loss: 2.4552 - regression_loss: 1.8269 - classification_loss:  9909/10000 [============================>.] - ETA: 42s - loss: 2.4552 - regression_loss: 1.8269 - classification_loss:  9910/10000 [============================>.] - ETA: 42s - loss: 2.4552 - regression_loss: 1.8269 - classification_loss:  9911/10000 [============================>.] - ETA: 42s - loss: 2.4551 - regression_loss: 1.8268 - classification_loss:  9912/10000 [============================>.] - ETA: 41s - loss: 2.4551 - regression_loss: 1.8268 - classification_loss:  9913/10000 [============================>.] - ETA: 41s - loss: 2.4551 - regression_loss: 1.8268 - classification_loss:  9914/10000 [============================>.] - ETA: 40s - loss: 2.4550 - regression_loss: 1.8268 - classification_loss:  9915/10000 [============================>.] - ETA: 40s - loss: 2.4549 - regression_loss: 1.8267 - classification_loss:  9916/10000 [============================>.] - ETA: 39s - loss: 2.4548 - regression_loss: 1.8266 - classification_loss:  9917/10000 [============================>.] - ETA: 39s - loss: 2.4549 - regression_loss: 1.8267 - classification_loss:  9918/10000 [============================>.] - ETA: 38s - loss: 2.4548 - regression_loss: 1.8267 - classification_loss:  9919/10000 [============================>.] - ETA: 38s - loss: 2.4548 - regression_loss: 1.8266 - classification_loss:  9920/10000 [============================>.] - ETA: 37s - loss: 2.4547 - regression_loss: 1.8265 - classification_loss:  9921/10000 [============================>.] - ETA: 37s - loss: 2.4547 - regression_loss: 1.8266 - classification_loss:  9922/10000 [============================>.] - ETA: 36s - loss: 2.4547 - regression_loss: 1.8265 - classification_loss:  9923/10000 [============================>.] - ETA: 36s - loss: 2.4546 - regression_loss: 1.8265 - classification_loss:  9924/10000 [============================>.] - ETA: 35s - loss: 2.4545 - regression_loss: 1.8264 - classification_loss:  9925/10000 [============================>.] - ETA: 35s - loss: 2.4544 - regression_loss: 1.8263 - classification_loss:  9926/10000 [============================>.] - ETA: 34s - loss: 2.4542 - regression_loss: 1.8262 - classification_loss:  9927/10000 [============================>.] - ETA: 34s - loss: 2.4542 - regression_loss: 1.8262 - classification_loss:  9928/10000 [============================>.] - ETA: 33s - loss: 2.4542 - regression_loss: 1.8262 - classification_loss:  9929/10000 [============================>.] - ETA: 33s - loss: 2.4541 - regression_loss: 1.8261 - classification_loss:  9930/10000 [============================>.] - ETA: 33s - loss: 2.4541 - regression_loss: 1.8262 - classification_loss:  9931/10000 [============================>.] - ETA: 32s - loss: 2.4541 - regression_loss: 1.8262 - classification_loss:  9932/10000 [============================>.] - ETA: 32s - loss: 2.4542 - regression_loss: 1.8263 - classification_loss:  9933/10000 [============================>.] - ETA: 31s - loss: 2.4541 - regression_loss: 1.8262 - classification_loss:  9934/10000 [============================>.] - ETA: 31s - loss: 2.4541 - regression_loss: 1.8262 - classification_loss:  9935/10000 [============================>.] - ETA: 30s - loss: 2.4541 - regression_loss: 1.8262 - classification_loss:  9936/10000 [============================>.] - ETA: 30s - loss: 2.4541 - regression_loss: 1.8262 - classification_loss:  9937/10000 [============================>.] - ETA: 29s - loss: 2.4541 - regression_loss: 1.8262 - classification_loss:  9938/10000 [============================>.] - ETA: 29s - loss: 2.4541 - regression_loss: 1.8262 - classification_loss:  9939/10000 [============================>.] - ETA: 28s - loss: 2.4540 - regression_loss: 1.8262 - classification_loss:  9940/10000 [============================>.] - ETA: 28s - loss: 2.4539 - regression_loss: 1.8261 - classification_loss:  9941/10000 [============================>.] - ETA: 27s - loss: 2.4539 - regression_loss: 1.8261 - classification_loss:  9942/10000 [============================>.] - ETA: 27s - loss: 2.4538 - regression_loss: 1.8261 - classification_loss:  9943/10000 [============================>.] - ETA: 26s - loss: 2.4538 - regression_loss: 1.8260 - classification_loss:  9944/10000 [============================>.] - ETA: 26s - loss: 2.4538 - regression_loss: 1.8260 - classification_loss:  9945/10000 [============================>.] - ETA: 25s - loss: 2.4537 - regression_loss: 1.8260 - classification_loss:  9946/10000 [============================>.] - ETA: 25s - loss: 2.4536 - regression_loss: 1.8259 - classification_loss:  9947/10000 [============================>.] - ETA: 25s - loss: 2.4535 - regression_loss: 1.8259 - classification_loss:  9948/10000 [============================>.] - ETA: 24s - loss: 2.4534 - regression_loss: 1.8258 - classification_loss:  9949/10000 [============================>.] - ETA: 24s - loss: 2.4534 - regression_loss: 1.8257 - classification_loss:  9950/10000 [============================>.] - ETA: 23s - loss: 2.4533 - regression_loss: 1.8257 - classification_loss:  9951/10000 [============================>.] - ETA: 23s - loss: 2.4533 - regression_loss: 1.8257 - classification_loss:  9952/10000 [============================>.] - ETA: 22s - loss: 2.4533 - regression_loss: 1.8257 - classification_loss:  9953/10000 [============================>.] - ETA: 22s - loss: 2.4534 - regression_loss: 1.8258 - classification_loss:  9954/10000 [============================>.] - ETA: 21s - loss: 2.4533 - regression_loss: 1.8257 - classification_loss:  9955/10000 [============================>.] - ETA: 21s - loss: 2.4533 - regression_loss: 1.8257 - classification_loss:  9956/10000 [============================>.] - ETA: 20s - loss: 2.4534 - regression_loss: 1.8259 - classification_loss:  9957/10000 [============================>.] - ETA: 20s - loss: 2.4534 - regression_loss: 1.8258 - classification_loss:  9958/10000 [============================>.] - ETA: 19s - loss: 2.4533 - regression_loss: 1.8258 - classification_loss:  9959/10000 [============================>.] - ETA: 19s - loss: 2.4532 - regression_loss: 1.8257 - classification_loss:  9960/10000 [============================>.] - ETA: 18s - loss: 2.4532 - regression_loss: 1.8258 - classification_loss:  9961/10000 [============================>.] - ETA: 18s - loss: 2.4532 - regression_loss: 1.8257 - classification_loss:  9962/10000 [============================>.] - ETA: 17s - loss: 2.4532 - regression_loss: 1.8257 - classification_loss:  9963/10000 [============================>.] - ETA: 17s - loss: 2.4531 - regression_loss: 1.8257 - classification_loss:  9964/10000 [============================>.] - ETA: 16s - loss: 2.4530 - regression_loss: 1.8256 - classification_loss:  9965/10000 [============================>.] - ETA: 16s - loss: 2.4529 - regression_loss: 1.8255 - classification_loss:  9966/10000 [============================>.] - ETA: 16s - loss: 2.4530 - regression_loss: 1.8256 - classification_loss:  9967/10000 [============================>.] - ETA: 15s - loss: 2.4529 - regression_loss: 1.8255 - classification_loss:  9968/10000 [============================>.] - ETA: 15s - loss: 2.4528 - regression_loss: 1.8255 - classification_loss:  9969/10000 [============================>.] - ETA: 14s - loss: 2.4528 - regression_loss: 1.8255 - classification_loss:  9970/10000 [============================>.] - ETA: 14s - loss: 2.4527 - regression_loss: 1.8254 - classification_loss:  9971/10000 [============================>.] - ETA: 13s - loss: 2.4527 - regression_loss: 1.8254 - classification_loss:  9972/10000 [============================>.] - ETA: 13s - loss: 2.4527 - regression_loss: 1.8254 - classification_loss:  9973/10000 [============================>.] - ETA: 12s - loss: 2.4526 - regression_loss: 1.8254 - classification_loss:  9974/10000 [============================>.] - ETA: 12s - loss: 2.4526 - regression_loss: 1.8253 - classification_loss:  9975/10000 [============================>.] - ETA: 11s - loss: 2.4525 - regression_loss: 1.8253 - classification_loss:  9976/10000 [============================>.] - ETA: 11s - loss: 2.4526 - regression_loss: 1.8254 - classification_loss:  9977/10000 [============================>.] - ETA: 10s - loss: 2.4526 - regression_loss: 1.8254 - classification_loss:  9978/10000 [============================>.] - ETA: 10s - loss: 2.4527 - regression_loss: 1.8254 - classification_loss:  9979/10000 [============================>.] - ETA: 9s - loss: 2.4526 - regression_loss: 1.8254 - classification_loss: 0 9980/10000 [============================>.] - ETA: 9s - loss: 2.4525 - regression_loss: 1.8253 - classification_loss: 0 9981/10000 [============================>.] - ETA: 8s - loss: 2.4524 - regression_loss: 1.8253 - classification_loss: 0 9982/10000 [============================>.] - ETA: 8s - loss: 2.4524 - regression_loss: 1.8253 - classification_loss: 0 9983/10000 [============================>.] - ETA: 8s - loss: 2.4524 - regression_loss: 1.8253 - classification_loss: 0 9984/10000 [============================>.] - ETA: 7s - loss: 2.4523 - regression_loss: 1.8252 - classification_loss: 0 9985/10000 [============================>.] - ETA: 7s - loss: 2.4523 - regression_loss: 1.8252 - classification_loss: 0 9986/10000 [============================>.] - ETA: 6s - loss: 2.4522 - regression_loss: 1.8251 - classification_loss: 0 9987/10000 [============================>.] - ETA: 6s - loss: 2.4521 - regression_loss: 1.8251 - classification_loss: 0 9988/10000 [============================>.] - ETA: 5s - loss: 2.4521 - regression_loss: 1.8251 - classification_loss: 0 9989/10000 [============================>.] - ETA: 5s - loss: 2.4520 - regression_loss: 1.8250 - classification_loss: 0 9990/10000 [============================>.] - ETA: 4s - loss: 2.4520 - regression_loss: 1.8250 - classification_loss: 0 9991/10000 [============================>.] - ETA: 4s - loss: 2.4519 - regression_loss: 1.8250 - classification_loss: 0 9992/10000 [============================>.] - ETA: 3s - loss: 2.4518 - regression_loss: 1.8249 - classification_loss: 0 9993/10000 [============================>.] - ETA: 3s - loss: 2.4518 - regression_loss: 1.8248 - classification_loss: 0 9994/10000 [============================>.] - ETA: 2s - loss: 2.4517 - regression_loss: 1.8247 - classification_loss: 0 9995/10000 [============================>.] - ETA: 2s - loss: 2.4517 - regression_loss: 1.8247 - classification_loss: 0 9996/10000 [============================>.] - ETA: 1s - loss: 2.4517 - regression_loss: 1.8247 - classification_loss: 0 9997/10000 [============================>.] - ETA: 1s - loss: 2.4516 - regression_loss: 1.8247 - classification_loss: 0 9998/10000 [============================>.] - ETA: 0s - loss: 2.4515 - regression_loss: 1.8246 - classification_loss: 0 9999/10000 [============================>.] - ETA: 0s - loss: 2.4514 - regression_loss: 1.8246 - classification_loss: 010000/10000 [==============================] - 4720s 472ms/step - loss: 2.4513 - regression_loss: 1.8245 - n_loss: 0.6268
aeroplane 0.0686
bicycle 0.1302
bird 0.0612
boat 0.0150
bottle 0.1257
bus 0.1008
car 0.4783
cat 0.2215
chair 0.1700
cow 0.0698
diningtable 0.0868
dog 0.1651
horse 0.2102
motorbike 0.1440
person 0.5364
pottedplant 0.0083
sheep 0.0507
sofa 0.0743
train 0.0943
tvmonitor 0.2722
mAP: 0.1542

Epoch 00001: saving model to ./snapshots\resnet50_pascal_01.h5









 6362/10000 [==================>...........] - ETA: 29:01 - loss: 1.8228 - regression_loss: 1.3884 - classification_loss 6363/10000 [==================>...........] - ETA: 29:01 - loss: 1.8227 - regression_loss: 1.3884 - classification_loss 6364/10000 [==================>...........] - ETA: 29:00 - loss: 1.8228 - regression_loss: 1.3884 - classification_loss 6365/10000 [==================>...........] - ETA: 29:00 - loss: 1.8228 - regression_loss: 1.3884 - classification_loss 6366/10000 [==================>...........] - ETA: 28:59 - loss: 1.8228 - regression_loss: 1.3884 - classification_loss 6367/10000 [==================>...........] - ETA: 28:59 - loss: 1.8229 - regression_loss: 1.3885 - classification_loss 6368/10000 [==================>...........] - ETA: 28:58 - loss: 1.8230 - regression_loss: 1.3886 - classification_loss 6369/10000 [==================>...........] - ETA: 28:58 - loss: 1.8230 - regression_loss: 1.3886 - classification_loss 6370/10000 [==================>...........] - ETA: 28:57 - loss: 1.8230 - regression_loss: 1.3886 - classification_loss 6371/10000 [==================>...........] - ETA: 28:57 - loss: 1.8230 - regression_loss: 1.3886 - classification_loss 6372/10000 [==================>...........] - ETA: 28:56 - loss: 1.8233 - regression_loss: 1.3888 - classification_loss 6373/10000 [==================>...........] - ETA: 28:56 - loss: 1.8233 - regression_loss: 1.3889 - classification_loss 6374/10000 [==================>...........] - ETA: 28:55 - loss: 1.8234 - regression_loss: 1.3889 - classification_loss 6375/10000 [==================>...........] - ETA: 28:55 - loss: 1.8233 - regression_loss: 1.3888 - classification_loss 6376/10000 [==================>...........] - ETA: 28:54 - loss: 1.8232 - regression_loss: 1.3888 - classification_loss 6377/10000 [==================>...........] - ETA: 28:54 - loss: 1.8233 - regression_loss: 1.3889 - classification_loss 6378/10000 [==================>...........] - ETA: 28:53 - loss: 1.8234 - regression_loss: 1.3889 - classification_loss 6379/10000 [==================>...........] - ETA: 28:53 - loss: 1.8235 - regression_loss: 1.3890 - classification_loss 6380/10000 [==================>...........] - ETA: 28:52 - loss: 1.8235 - regression_loss: 1.3890 - classification_loss 6381/10000 [==================>...........] - ETA: 28:52 - loss: 1.8235 - regression_loss: 1.3890 - classification_loss 6382/10000 [==================>...........] - ETA: 28:51 - loss: 1.8234 - regression_loss: 1.3890 - classification_loss 6383/10000 [==================>...........] - ETA: 28:51 - loss: 1.8234 - regression_loss: 1.3890 - classification_loss 6384/10000 [==================>...........] - ETA: 28:50 - loss: 1.8233 - regression_loss: 1.3889 - classification_loss 6385/10000 [==================>...........] - ETA: 28:50 - loss: 1.8233 - regression_loss: 1.3889 - classification_loss 6386/10000 [==================>...........] - ETA: 28:49 - loss: 1.8233 - regression_loss: 1.3889 - classification_loss 6387/10000 [==================>...........] - ETA: 28:49 - loss: 1.8234 - regression_loss: 1.3889 - classification_loss 6388/10000 [==================>...........] - ETA: 28:48 - loss: 1.8233 - regression_loss: 1.3889 - classification_loss 6389/10000 [==================>...........] - ETA: 28:48 - loss: 1.8232 - regression_loss: 1.3888 - classification_loss 6390/10000 [==================>...........] - ETA: 28:47 - loss: 1.8232 - regression_loss: 1.3888 - classification_loss 6391/10000 [==================>...........] - ETA: 28:47 - loss: 1.8233 - regression_loss: 1.3889 - classification_loss 6392/10000 [==================>...........] - ETA: 28:46 - loss: 1.8234 - regression_loss: 1.3889 - classification_loss 6393/10000 [==================>...........] - ETA: 28:46 - loss: 1.8233 - regression_loss: 1.3889 - classification_loss 6394/10000 [==================>...........] - ETA: 28:45 - loss: 1.8233 - regression_loss: 1.3889 - classification_loss 6395/10000 [==================>...........] - ETA: 28:45 - loss: 1.8233 - regression_loss: 1.3889 - classification_loss 6396/10000 [==================>...........] - ETA: 28:44 - loss: 1.8232 - regression_loss: 1.3888 - classification_loss 6397/10000 [==================>...........] - ETA: 28:44 - loss: 1.8230 - regression_loss: 1.3887 - classification_loss 6398/10000 [==================>...........] - ETA: 28:44 - loss: 1.8231 - regression_loss: 1.3887 - classification_loss 6399/10000 [==================>...........] - ETA: 28:43 - loss: 1.8230 - regression_loss: 1.3887 - classification_loss 6400/10000 [==================>...........] - ETA: 28:43 - loss: 1.8230 - regression_loss: 1.3886 - classification_loss 6401/10000 [==================>...........] - ETA: 28:42 - loss: 1.8229 - regression_loss: 1.3885 - classification_loss 6402/10000 [==================>...........] - ETA: 28:42 - loss: 1.8227 - regression_loss: 1.3885 - classification_loss 6403/10000 [==================>...........] - ETA: 28:41 - loss: 1.8227 - regression_loss: 1.3884 - classification_loss 6404/10000 [==================>...........] - ETA: 28:41 - loss: 1.8225 - regression_loss: 1.3883 - classification_loss 6405/10000 [==================>...........] - ETA: 28:40 - loss: 1.8224 - regression_loss: 1.3882 - classification_loss 6406/10000 [==================>...........] - ETA: 28:40 - loss: 1.8222 - regression_loss: 1.3881 - classification_loss 6407/10000 [==================>...........] - ETA: 28:39 - loss: 1.8223 - regression_loss: 1.3881 - classification_loss 6408/10000 [==================>...........] - ETA: 28:39 - loss: 1.8223 - regression_loss: 1.3882 - classification_loss 6409/10000 [==================>...........] - ETA: 28:38 - loss: 1.8223 - regression_loss: 1.3881 - classification_loss 6410/10000 [==================>...........] - ETA: 28:38 - loss: 1.8222 - regression_loss: 1.3880 - classification_loss 6411/10000 [==================>...........] - ETA: 28:37 - loss: 1.8220 - regression_loss: 1.3879 - classification_loss 6412/10000 [==================>...........] - ETA: 28:37 - loss: 1.8221 - regression_loss: 1.3880 - classification_loss 6413/10000 [==================>...........] - ETA: 28:36 - loss: 1.8219 - regression_loss: 1.3878 - classification_loss 6414/10000 [==================>...........] - ETA: 28:36 - loss: 1.8218 - regression_loss: 1.3877 - classification_loss 6415/10000 [==================>...........] - ETA: 28:35 - loss: 1.8218 - regression_loss: 1.3877 - classification_loss 6416/10000 [==================>...........] - ETA: 28:35 - loss: 1.8218 - regression_loss: 1.3877 - classification_loss 6417/10000 [==================>...........] - ETA: 28:34 - loss: 1.8217 - regression_loss: 1.3876 - classification_loss 6418/10000 [==================>...........] - ETA: 28:34 - loss: 1.8216 - regression_loss: 1.3875 - classification_loss 6419/10000 [==================>...........] - ETA: 28:33 - loss: 1.8215 - regression_loss: 1.3875 - classification_loss 6420/10000 [==================>...........] - ETA: 28:33 - loss: 1.8215 - regression_loss: 1.3875 - classification_loss 6421/10000 [==================>...........] - ETA: 28:32 - loss: 1.8216 - regression_loss: 1.3876 - classification_loss 6422/10000 [==================>...........] - ETA: 28:32 - loss: 1.8217 - regression_loss: 1.3876 - classification_loss 6423/10000 [==================>...........] - ETA: 28:31 - loss: 1.8216 - regression_loss: 1.3875 - classification_loss 6424/10000 [==================>...........] - ETA: 28:31 - loss: 1.8217 - regression_loss: 1.3876 - classification_loss 6425/10000 [==================>...........] - ETA: 28:30 - loss: 1.8215 - regression_loss: 1.3874 - classification_loss 6426/10000 [==================>...........] - ETA: 28:30 - loss: 1.8216 - regression_loss: 1.3876 - classification_loss 6427/10000 [==================>...........] - ETA: 28:29 - loss: 1.8214 - regression_loss: 1.3873 - classification_loss 6428/10000 [==================>...........] - ETA: 28:29 - loss: 1.8212 - regression_loss: 1.3872 - classification_loss 6429/10000 [==================>...........] - ETA: 28:28 - loss: 1.8211 - regression_loss: 1.3871 - classification_loss 6430/10000 [==================>...........] - ETA: 28:28 - loss: 1.8210 - regression_loss: 1.3871 - classification_loss 6431/10000 [==================>...........] - ETA: 28:27 - loss: 1.8210 - regression_loss: 1.3871 - classification_loss 6432/10000 [==================>...........] - ETA: 28:27 - loss: 1.8209 - regression_loss: 1.3870 - classification_loss 6433/10000 [==================>...........] - ETA: 28:26 - loss: 1.8209 - regression_loss: 1.3870 - classification_loss 6434/10000 [==================>...........] - ETA: 28:26 - loss: 1.8208 - regression_loss: 1.3869 - classification_loss 6435/10000 [==================>...........] - ETA: 28:25 - loss: 1.8209 - regression_loss: 1.3870 - classification_loss 6436/10000 [==================>...........] - ETA: 28:25 - loss: 1.8208 - regression_loss: 1.3870 - classification_loss 6437/10000 [==================>...........] - ETA: 28:24 - loss: 1.8208 - regression_loss: 1.3870 - classification_loss 6438/10000 [==================>...........] - ETA: 28:24 - loss: 1.8208 - regression_loss: 1.3870 - classification_loss 6439/10000 [==================>...........] - ETA: 28:23 - loss: 1.8207 - regression_loss: 1.3869 - classification_loss 6440/10000 [==================>...........] - ETA: 28:23 - loss: 1.8206 - regression_loss: 1.3868 - classification_loss 6441/10000 [==================>...........] - ETA: 28:22 - loss: 1.8206 - regression_loss: 1.3868 - classification_loss 6442/10000 [==================>...........] - ETA: 28:22 - loss: 1.8206 - regression_loss: 1.3868 - classification_loss 6443/10000 [==================>...........] - ETA: 28:21 - loss: 1.8204 - regression_loss: 1.3867 - classification_loss 6444/10000 [==================>...........] - ETA: 28:21 - loss: 1.8205 - regression_loss: 1.3867 - classification_loss 6445/10000 [==================>...........] - ETA: 28:20 - loss: 1.8203 - regression_loss: 1.3866 - classification_loss 6446/10000 [==================>...........] - ETA: 28:20 - loss: 1.8203 - regression_loss: 1.3865 - classification_loss 6447/10000 [==================>...........] - ETA: 28:19 - loss: 1.8202 - regression_loss: 1.3865 - classification_loss 6448/10000 [==================>...........] - ETA: 28:19 - loss: 1.8201 - regression_loss: 1.3865 - classification_loss 6449/10000 [==================>...........] - ETA: 28:18 - loss: 1.8200 - regression_loss: 1.3863 - classification_loss 6450/10000 [==================>...........] - ETA: 28:18 - loss: 1.8200 - regression_loss: 1.3863 - classification_loss 6451/10000 [==================>...........] - ETA: 28:17 - loss: 1.8201 - regression_loss: 1.3864 - classification_loss 6452/10000 [==================>...........] - ETA: 28:17 - loss: 1.8201 - regression_loss: 1.3864 - classification_loss 6453/10000 [==================>...........] - ETA: 28:16 - loss: 1.8200 - regression_loss: 1.3863 - classification_loss 6454/10000 [==================>...........] - ETA: 28:16 - loss: 1.8200 - regression_loss: 1.3864 - classification_loss 6455/10000 [==================>...........] - ETA: 28:15 - loss: 1.8199 - regression_loss: 1.3863 - classification_loss 6456/10000 [==================>...........] - ETA: 28:15 - loss: 1.8199 - regression_loss: 1.3862 - classification_loss 6457/10000 [==================>...........] - ETA: 28:14 - loss: 1.8199 - regression_loss: 1.3863 - classification_loss 6458/10000 [==================>...........] - ETA: 28:14 - loss: 1.8199 - regression_loss: 1.3863 - classification_loss 6459/10000 [==================>...........] - ETA: 28:13 - loss: 1.8200 - regression_loss: 1.3864 - classification_loss 6460/10000 [==================>...........] - ETA: 28:13 - loss: 1.8201 - regression_loss: 1.3864 - classification_loss 6461/10000 [==================>...........] - ETA: 28:13 - loss: 1.8201 - regression_loss: 1.3864 - classification_loss 6462/10000 [==================>...........] - ETA: 28:12 - loss: 1.8200 - regression_loss: 1.3864 - classification_loss 6463/10000 [==================>...........] - ETA: 28:12 - loss: 1.8199 - regression_loss: 1.3863 - classification_loss 6464/10000 [==================>...........] - ETA: 28:11 - loss: 1.8198 - regression_loss: 1.3862 - classification_loss 6465/10000 [==================>...........] - ETA: 28:11 - loss: 1.8196 - regression_loss: 1.3861 - classification_loss 6466/10000 [==================>...........] - ETA: 28:10 - loss: 1.8197 - regression_loss: 1.3862 - classification_loss 6467/10000 [==================>...........] - ETA: 28:10 - loss: 1.8197 - regression_loss: 1.3861 - classification_loss 6468/10000 [==================>...........] - ETA: 28:09 - loss: 1.8195 - regression_loss: 1.3860 - classification_loss 6469/10000 [==================>...........] - ETA: 28:09 - loss: 1.8196 - regression_loss: 1.3861 - classification_loss 6470/10000 [==================>...........] - ETA: 28:08 - loss: 1.8196 - regression_loss: 1.3861 - classification_loss 6471/10000 [==================>...........] - ETA: 28:08 - loss: 1.8195 - regression_loss: 1.3860 - classification_loss 6472/10000 [==================>...........] - ETA: 28:07 - loss: 1.8195 - regression_loss: 1.3860 - classification_loss 6473/10000 [==================>...........] - ETA: 28:07 - loss: 1.8197 - regression_loss: 1.3861 - classification_loss 6474/10000 [==================>...........] - ETA: 28:06 - loss: 1.8196 - regression_loss: 1.3860 - classification_loss 6475/10000 [==================>...........] - ETA: 28:06 - loss: 1.8195 - regression_loss: 1.3859 - classification_loss 6476/10000 [==================>...........] - ETA: 28:05 - loss: 1.8194 - regression_loss: 1.3858 - classification_loss 6477/10000 [==================>...........] - ETA: 28:05 - loss: 1.8194 - regression_loss: 1.3858 - classification_loss 6478/10000 [==================>...........] - ETA: 28:04 - loss: 1.8193 - regression_loss: 1.3858 - classification_loss 6479/10000 [==================>...........] - ETA: 28:04 - loss: 1.8194 - regression_loss: 1.3859 - classification_loss 6480/10000 [==================>...........] - ETA: 28:03 - loss: 1.8194 - regression_loss: 1.3859 - classification_loss 6481/10000 [==================>...........] - ETA: 28:03 - loss: 1.8193 - regression_loss: 1.3858 - classification_loss 6482/10000 [==================>...........] - ETA: 28:02 - loss: 1.8192 - regression_loss: 1.3857 - classification_loss 6483/10000 [==================>...........] - ETA: 28:02 - loss: 1.8192 - regression_loss: 1.3857 - classification_loss 6484/10000 [==================>...........] - ETA: 28:01 - loss: 1.8192 - regression_loss: 1.3857 - classification_loss 6485/10000 [==================>...........] - ETA: 28:01 - loss: 1.8191 - regression_loss: 1.3856 - classification_loss 6486/10000 [==================>...........] - ETA: 28:00 - loss: 1.8192 - regression_loss: 1.3857 - classification_loss 6487/10000 [==================>...........] - ETA: 28:00 - loss: 1.8191 - regression_loss: 1.3856 - classification_loss 6488/10000 [==================>...........] - ETA: 27:59 - loss: 1.8191 - regression_loss: 1.3856 - classification_loss 6489/10000 [==================>...........] - ETA: 27:59 - loss: 1.8191 - regression_loss: 1.3856 - classification_loss 6490/10000 [==================>...........] - ETA: 27:58 - loss: 1.8191 - regression_loss: 1.3855 - classification_loss 6491/10000 [==================>...........] - ETA: 27:58 - loss: 1.8190 - regression_loss: 1.3855 - classification_loss 6492/10000 [==================>...........] - ETA: 27:57 - loss: 1.8189 - regression_loss: 1.3854 - classification_loss 6493/10000 [==================>...........] - ETA: 27:57 - loss: 1.8190 - regression_loss: 1.3855 - classification_loss 6494/10000 [==================>...........] - ETA: 27:56 - loss: 1.8191 - regression_loss: 1.3855 - classification_loss 6495/10000 [==================>...........] - ETA: 27:56 - loss: 1.8191 - regression_loss: 1.3856 - classification_loss 6496/10000 [==================>...........] - ETA: 27:56 - loss: 1.8192 - regression_loss: 1.3857 - classification_loss 6497/10000 [==================>...........] - ETA: 27:55 - loss: 1.8191 - regression_loss: 1.3856 - classification_loss 6498/10000 [==================>...........] - ETA: 27:55 - loss: 1.8191 - regression_loss: 1.3856 - classification_loss 6499/10000 [==================>...........] - ETA: 27:54 - loss: 1.8192 - regression_loss: 1.3857 - classification_loss 6500/10000 [==================>...........] - ETA: 27:54 - loss: 1.8193 - regression_loss: 1.3858 - classification_loss 6501/10000 [==================>...........] - ETA: 27:53 - loss: 1.8191 - regression_loss: 1.3856 - classification_loss 6502/10000 [==================>...........] - ETA: 27:53 - loss: 1.8191 - regression_loss: 1.3856 - classification_loss 6503/10000 [==================>...........] - ETA: 27:52 - loss: 1.8190 - regression_loss: 1.3854 - classification_loss 6504/10000 [==================>...........] - ETA: 27:52 - loss: 1.8190 - regression_loss: 1.3855 - classification_loss 6505/10000 [==================>...........] - ETA: 27:51 - loss: 1.8188 - regression_loss: 1.3853 - classification_loss 6506/10000 [==================>...........] - ETA: 27:51 - loss: 1.8187 - regression_loss: 1.3853 - classification_loss 6507/10000 [==================>...........] - ETA: 27:50 - loss: 1.8187 - regression_loss: 1.3853 - classification_loss 6508/10000 [==================>...........] - ETA: 27:50 - loss: 1.8187 - regression_loss: 1.3853 - classification_loss 6509/10000 [==================>...........] - ETA: 27:49 - loss: 1.8187 - regression_loss: 1.3853 - classification_loss 6510/10000 [==================>...........] - ETA: 27:49 - loss: 1.8187 - regression_loss: 1.3853 - classification_loss 6511/10000 [==================>...........] - ETA: 27:48 - loss: 1.8187 - regression_loss: 1.3853 - classification_loss 6512/10000 [==================>...........] - ETA: 27:48 - loss: 1.8186 - regression_loss: 1.3852 - classification_loss 6513/10000 [==================>...........] - ETA: 27:47 - loss: 1.8186 - regression_loss: 1.3852 - classification_loss 6514/10000 [==================>...........] - ETA: 27:47 - loss: 1.8186 - regression_loss: 1.3852 - classification_loss 6515/10000 [==================>...........] - ETA: 27:46 - loss: 1.8186 - regression_loss: 1.3853 - classification_loss 6516/10000 [==================>...........] - ETA: 27:46 - loss: 1.8185 - regression_loss: 1.3851 - classification_loss 6517/10000 [==================>...........] - ETA: 27:45 - loss: 1.8185 - regression_loss: 1.3852 - classification_loss 6518/10000 [==================>...........] - ETA: 27:45 - loss: 1.8185 - regression_loss: 1.3851 - classification_loss 6519/10000 [==================>...........] - ETA: 27:44 - loss: 1.8186 - regression_loss: 1.3852 - classification_loss 6520/10000 [==================>...........] - ETA: 27:44 - loss: 1.8184 - regression_loss: 1.3851 - classification_loss 6521/10000 [==================>...........] - ETA: 27:43 - loss: 1.8182 - regression_loss: 1.3850 - classification_loss 6522/10000 [==================>...........] - ETA: 27:43 - loss: 1.8182 - regression_loss: 1.3849 - classification_loss 6523/10000 [==================>...........] - ETA: 27:42 - loss: 1.8181 - regression_loss: 1.3849 - classification_loss 6524/10000 [==================>...........] - ETA: 27:42 - loss: 1.8181 - regression_loss: 1.3848 - classification_loss 6525/10000 [==================>...........] - ETA: 27:41 - loss: 1.8181 - regression_loss: 1.3848 - classification_loss 6526/10000 [==================>...........] - ETA: 27:41 - loss: 1.8181 - regression_loss: 1.3848 - classification_loss 6527/10000 [==================>...........] - ETA: 27:40 - loss: 1.8181 - regression_loss: 1.3849 - classification_loss 6528/10000 [==================>...........] - ETA: 27:40 - loss: 1.8183 - regression_loss: 1.3850 - classification_loss 6529/10000 [==================>...........] - ETA: 27:39 - loss: 1.8181 - regression_loss: 1.3849 - classification_loss 6530/10000 [==================>...........] - ETA: 27:39 - loss: 1.8180 - regression_loss: 1.3848 - classification_loss 6531/10000 [==================>...........] - ETA: 27:38 - loss: 1.8179 - regression_loss: 1.3847 - classification_loss 6532/10000 [==================>...........] - ETA: 27:38 - loss: 1.8181 - regression_loss: 1.3848 - classification_loss 6533/10000 [==================>...........] - ETA: 27:37 - loss: 1.8182 - regression_loss: 1.3849 - classification_loss 6534/10000 [==================>...........] - ETA: 27:37 - loss: 1.8183 - regression_loss: 1.3850 - classification_loss 6535/10000 [==================>...........] - ETA: 27:36 - loss: 1.8183 - regression_loss: 1.3850 - classification_loss 6536/10000 [==================>...........] - ETA: 27:36 - loss: 1.8182 - regression_loss: 1.3849 - classification_loss 6537/10000 [==================>...........] - ETA: 27:35 - loss: 1.8182 - regression_loss: 1.3849 - classification_loss 6538/10000 [==================>...........] - ETA: 27:35 - loss: 1.8182 - regression_loss: 1.3849 - classification_loss 6539/10000 [==================>...........] - ETA: 27:35 - loss: 1.8181 - regression_loss: 1.3848 - classification_loss 6540/10000 [==================>...........] - ETA: 27:34 - loss: 1.8181 - regression_loss: 1.3848 - classification_loss 6541/10000 [==================>...........] - ETA: 27:34 - loss: 1.8181 - regression_loss: 1.3848 - classification_loss 6542/10000 [==================>...........] - ETA: 27:33 - loss: 1.8180 - regression_loss: 1.3848 - classification_loss 6543/10000 [==================>...........] - ETA: 27:33 - loss: 1.8181 - regression_loss: 1.3848 - classification_loss 6544/10000 [==================>...........] - ETA: 27:32 - loss: 1.8181 - regression_loss: 1.3848 - classification_loss 6545/10000 [==================>...........] - ETA: 27:32 - loss: 1.8180 - regression_loss: 1.3848 - classification_loss 6546/10000 [==================>...........] - ETA: 27:31 - loss: 1.8180 - regression_loss: 1.3848 - classification_loss 6547/10000 [==================>...........] - ETA: 27:31 - loss: 1.8181 - regression_loss: 1.3848 - classification_loss 6548/10000 [==================>...........] - ETA: 27:30 - loss: 1.8181 - regression_loss: 1.3849 - classification_loss 6549/10000 [==================>...........] - ETA: 27:30 - loss: 1.8180 - regression_loss: 1.3848 - classification_loss 6550/10000 [==================>...........] - ETA: 27:29 - loss: 1.8180 - regression_loss: 1.3848 - classification_loss 6551/10000 [==================>...........] - ETA: 27:29 - loss: 1.8181 - regression_loss: 1.3848 - classification_loss 6552/10000 [==================>...........] - ETA: 27:28 - loss: 1.8182 - regression_loss: 1.3849 - classification_loss 6553/10000 [==================>...........] - ETA: 27:28 - loss: 1.8181 - regression_loss: 1.3848 - classification_loss 6554/10000 [==================>...........] - ETA: 27:27 - loss: 1.8181 - regression_loss: 1.3848 - classification_loss 6555/10000 [==================>...........] - ETA: 27:27 - loss: 1.8179 - regression_loss: 1.3846 - classification_loss 6556/10000 [==================>...........] - ETA: 27:26 - loss: 1.8180 - regression_loss: 1.3847 - classification_loss 6557/10000 [==================>...........] - ETA: 27:26 - loss: 1.8181 - regression_loss: 1.3848 - classification_loss 6558/10000 [==================>...........] - ETA: 27:25 - loss: 1.8182 - regression_loss: 1.3848 - classification_loss 6559/10000 [==================>...........] - ETA: 27:25 - loss: 1.8182 - regression_loss: 1.3849 - classification_loss 6560/10000 [==================>...........] - ETA: 27:24 - loss: 1.8182 - regression_loss: 1.3849 - classification_loss 6561/10000 [==================>...........] - ETA: 27:24 - loss: 1.8182 - regression_loss: 1.3849 - classification_loss 6562/10000 [==================>...........] - ETA: 27:23 - loss: 1.8182 - regression_loss: 1.3849 - classification_loss 6563/10000 [==================>...........] - ETA: 27:23 - loss: 1.8181 - regression_loss: 1.3848 - classification_loss 6564/10000 [==================>...........] - ETA: 27:22 - loss: 1.8182 - regression_loss: 1.3849 - classification_loss 6565/10000 [==================>...........] - ETA: 27:22 - loss: 1.8182 - regression_loss: 1.3849 - classification_loss 6566/10000 [==================>...........] - ETA: 27:21 - loss: 1.8183 - regression_loss: 1.3850 - classification_loss 6567/10000 [==================>...........] - ETA: 27:21 - loss: 1.8184 - regression_loss: 1.3850 - classification_loss 6568/10000 [==================>...........] - ETA: 27:20 - loss: 1.8183 - regression_loss: 1.3849 - classification_loss 6569/10000 [==================>...........] - ETA: 27:20 - loss: 1.8185 - regression_loss: 1.3851 - classification_loss 6570/10000 [==================>...........] - ETA: 27:19 - loss: 1.8184 - regression_loss: 1.3850 - classification_loss 6571/10000 [==================>...........] - ETA: 27:19 - loss: 1.8183 - regression_loss: 1.3849 - classification_loss 6572/10000 [==================>...........] - ETA: 27:18 - loss: 1.8184 - regression_loss: 1.3849 - classification_loss 6573/10000 [==================>...........] - ETA: 27:18 - loss: 1.8182 - regression_loss: 1.3848 - classification_loss 6574/10000 [==================>...........] - ETA: 27:17 - loss: 1.8181 - regression_loss: 1.3847 - classification_loss 6575/10000 [==================>...........] - ETA: 27:17 - loss: 1.8181 - regression_loss: 1.3847 - classification_loss 6576/10000 [==================>...........] - ETA: 27:17 - loss: 1.8180 - regression_loss: 1.3846 - classification_loss 6577/10000 [==================>...........] - ETA: 27:16 - loss: 1.8179 - regression_loss: 1.3846 - classification_loss 6578/10000 [==================>...........] - ETA: 27:16 - loss: 1.8180 - regression_loss: 1.3846 - classification_loss 6579/10000 [==================>...........] - ETA: 27:15 - loss: 1.8182 - regression_loss: 1.3847 - classification_loss 6580/10000 [==================>...........] - ETA: 27:14 - loss: 1.8182 - regression_loss: 1.3847 - classification_loss 6581/10000 [==================>...........] - ETA: 27:14 - loss: 1.8183 - regression_loss: 1.3848 - classification_loss 6582/10000 [==================>...........] - ETA: 27:14 - loss: 1.8182 - regression_loss: 1.3847 - classification_loss 6583/10000 [==================>...........] - ETA: 27:13 - loss: 1.8181 - regression_loss: 1.3846 - classification_loss 6584/10000 [==================>...........] - ETA: 27:13 - loss: 1.8180 - regression_loss: 1.3846 - classification_loss 6585/10000 [==================>...........] - ETA: 27:12 - loss: 1.8180 - regression_loss: 1.3845 - classification_loss 6586/10000 [==================>...........] - ETA: 27:12 - loss: 1.8180 - regression_loss: 1.3846 - classification_loss 6587/10000 [==================>...........] - ETA: 27:11 - loss: 1.8180 - regression_loss: 1.3846 - classification_loss 6588/10000 [==================>...........] - ETA: 27:11 - loss: 1.8179 - regression_loss: 1.3845 - classification_loss 6589/10000 [==================>...........] - ETA: 27:10 - loss: 1.8178 - regression_loss: 1.3844 - classification_loss 6590/10000 [==================>...........] - ETA: 27:10 - loss: 1.8179 - regression_loss: 1.3844 - classification_loss 6591/10000 [==================>...........] - ETA: 27:09 - loss: 1.8178 - regression_loss: 1.3843 - classification_loss 6592/10000 [==================>...........] - ETA: 27:09 - loss: 1.8178 - regression_loss: 1.3844 - classification_loss 6593/10000 [==================>...........] - ETA: 27:08 - loss: 1.8179 - regression_loss: 1.3844 - classification_loss 6594/10000 [==================>...........] - ETA: 27:08 - loss: 1.8179 - regression_loss: 1.3844 - classification_loss 6595/10000 [==================>...........] - ETA: 27:07 - loss: 1.8179 - regression_loss: 1.3845 - classification_loss 6596/10000 [==================>...........] - ETA: 27:07 - loss: 1.8179 - regression_loss: 1.3845 - classification_loss 6597/10000 [==================>...........] - ETA: 27:06 - loss: 1.8180 - regression_loss: 1.3846 - classification_loss 6598/10000 [==================>...........] - ETA: 27:06 - loss: 1.8180 - regression_loss: 1.3846 - classification_loss 6599/10000 [==================>...........] - ETA: 27:05 - loss: 1.8180 - regression_loss: 1.3845 - classification_loss 6600/10000 [==================>...........] - ETA: 27:05 - loss: 1.8179 - regression_loss: 1.3845 - classification_loss 6601/10000 [==================>...........] - ETA: 27:04 - loss: 1.8180 - regression_loss: 1.3846 - classification_loss 6602/10000 [==================>...........] - ETA: 27:04 - loss: 1.8180 - regression_loss: 1.3846 - classification_loss 6603/10000 [==================>...........] - ETA: 27:03 - loss: 1.8180 - regression_loss: 1.3846 - classification_loss 6604/10000 [==================>...........] - ETA: 27:03 - loss: 1.8180 - regression_loss: 1.3846 - classification_loss 6605/10000 [==================>...........] - ETA: 27:02 - loss: 1.8179 - regression_loss: 1.3846 - classification_loss 6606/10000 [==================>...........] - ETA: 27:02 - loss: 1.8179 - regression_loss: 1.3845 - classification_loss 6607/10000 [==================>...........] - ETA: 27:01 - loss: 1.8178 - regression_loss: 1.3845 - classification_loss 6608/10000 [==================>...........] - ETA: 27:01 - loss: 1.8179 - regression_loss: 1.3846 - classification_loss 6609/10000 [==================>...........] - ETA: 27:00 - loss: 1.8179 - regression_loss: 1.3846 - classification_loss 6610/10000 [==================>...........] - ETA: 27:00 - loss: 1.8178 - regression_loss: 1.3845 - classification_loss 6611/10000 [==================>...........] - ETA: 26:59 - loss: 1.8177 - regression_loss: 1.3844 - classification_loss 6612/10000 [==================>...........] - ETA: 26:59 - loss: 1.8176 - regression_loss: 1.3843 - classification_loss 6613/10000 [==================>...........] - ETA: 26:58 - loss: 1.8176 - regression_loss: 1.3843 - classification_loss 6614/10000 [==================>...........] - ETA: 26:58 - loss: 1.8175 - regression_loss: 1.3842 - classification_loss 6615/10000 [==================>...........] - ETA: 26:57 - loss: 1.8177 - regression_loss: 1.3843 - classification_loss 6616/10000 [==================>...........] - ETA: 26:57 - loss: 1.8175 - regression_loss: 1.3842 - classification_loss 6617/10000 [==================>...........] - ETA: 26:56 - loss: 1.8174 - regression_loss: 1.3841 - classification_loss 6618/10000 [==================>...........] - ETA: 26:56 - loss: 1.8174 - regression_loss: 1.3841 - classification_loss 6619/10000 [==================>...........] - ETA: 26:55 - loss: 1.8174 - regression_loss: 1.3840 - classification_loss 6620/10000 [==================>...........] - ETA: 26:55 - loss: 1.8173 - regression_loss: 1.3840 - classification_loss 6621/10000 [==================>...........] - ETA: 26:54 - loss: 1.8173 - regression_loss: 1.3840 - classification_loss 6622/10000 [==================>...........] - ETA: 26:54 - loss: 1.8172 - regression_loss: 1.3839 - classification_loss 6623/10000 [==================>...........] - ETA: 26:53 - loss: 1.8173 - regression_loss: 1.3840 - classification_loss 6624/10000 [==================>...........] - ETA: 26:53 - loss: 1.8172 - regression_loss: 1.3839 - classification_loss 6625/10000 [==================>...........] - ETA: 26:52 - loss: 1.8173 - regression_loss: 1.3840 - classification_loss 6626/10000 [==================>...........] - ETA: 26:52 - loss: 1.8171 - regression_loss: 1.3838 - classification_loss 6627/10000 [==================>...........] - ETA: 26:51 - loss: 1.8170 - regression_loss: 1.3837 - classification_loss 6628/10000 [==================>...........] - ETA: 26:51 - loss: 1.8171 - regression_loss: 1.3838 - classification_loss 6629/10000 [==================>...........] - ETA: 26:50 - loss: 1.8170 - regression_loss: 1.3838 - classification_loss 6630/10000 [==================>...........] - ETA: 26:50 - loss: 1.8170 - regression_loss: 1.3837 - classification_loss 6631/10000 [==================>...........] - ETA: 26:50 - loss: 1.8170 - regression_loss: 1.3838 - classification_loss 6632/10000 [==================>...........] - ETA: 26:49 - loss: 1.8170 - regression_loss: 1.3838 - classification_loss 6633/10000 [==================>...........] - ETA: 26:49 - loss: 1.8170 - regression_loss: 1.3837 - classification_loss 6634/10000 [==================>...........] - ETA: 26:48 - loss: 1.8169 - regression_loss: 1.3837 - classification_loss 6635/10000 [==================>...........] - ETA: 26:48 - loss: 1.8169 - regression_loss: 1.3836 - classification_loss 6636/10000 [==================>...........] - ETA: 26:47 - loss: 1.8169 - regression_loss: 1.3836 - classification_loss 6637/10000 [==================>...........] - ETA: 26:47 - loss: 1.8169 - regression_loss: 1.3837 - classification_loss 6638/10000 [==================>...........] - ETA: 26:46 - loss: 1.8169 - regression_loss: 1.3836 - classification_loss 6639/10000 [==================>...........] - ETA: 26:46 - loss: 1.8168 - regression_loss: 1.3836 - classification_loss 6640/10000 [==================>...........] - ETA: 26:45 - loss: 1.8168 - regression_loss: 1.3836 - classification_loss 6641/10000 [==================>...........] - ETA: 26:45 - loss: 1.8168 - regression_loss: 1.3836 - classification_loss 6642/10000 [==================>...........] - ETA: 26:44 - loss: 1.8169 - regression_loss: 1.3837 - classification_loss 6643/10000 [==================>...........] - ETA: 26:44 - loss: 1.8170 - regression_loss: 1.3838 - classification_loss 6644/10000 [==================>...........] - ETA: 26:43 - loss: 1.8171 - regression_loss: 1.3838 - classification_loss 6645/10000 [==================>...........] - ETA: 26:43 - loss: 1.8170 - regression_loss: 1.3838 - classification_loss 6646/10000 [==================>...........] - ETA: 26:42 - loss: 1.8170 - regression_loss: 1.3838 - classification_loss 6647/10000 [==================>...........] - ETA: 26:42 - loss: 1.8170 - regression_loss: 1.3837 - classification_loss 6648/10000 [==================>...........] - ETA: 26:41 - loss: 1.8170 - regression_loss: 1.3837 - classification_loss 6649/10000 [==================>...........] - ETA: 26:41 - loss: 1.8169 - regression_loss: 1.3837 - classification_loss 6650/10000 [==================>...........] - ETA: 26:40 - loss: 1.8169 - regression_loss: 1.3837 - classification_loss 6651/10000 [==================>...........] - ETA: 26:40 - loss: 1.8169 - regression_loss: 1.3836 - classification_loss 6652/10000 [==================>...........] - ETA: 26:39 - loss: 1.8169 - regression_loss: 1.3837 - classification_loss 6653/10000 [==================>...........] - ETA: 26:39 - loss: 1.8169 - regression_loss: 1.3837 - classification_loss 6654/10000 [==================>...........] - ETA: 26:38 - loss: 1.8168 - regression_loss: 1.3836 - classification_loss 6655/10000 [==================>...........] - ETA: 26:38 - loss: 1.8169 - regression_loss: 1.3837 - classification_loss 6656/10000 [==================>...........] - ETA: 26:37 - loss: 1.8169 - regression_loss: 1.3837 - classification_loss 6657/10000 [==================>...........] - ETA: 26:37 - loss: 1.8167 - regression_loss: 1.3835 - classification_loss 6658/10000 [==================>...........] - ETA: 26:36 - loss: 1.8167 - regression_loss: 1.3835 - classification_loss 6659/10000 [==================>...........] - ETA: 26:36 - loss: 1.8167 - regression_loss: 1.3836 - classification_loss 6660/10000 [==================>...........] - ETA: 26:35 - loss: 1.8168 - regression_loss: 1.3836 - classification_loss 6661/10000 [==================>...........] - ETA: 26:35 - loss: 1.8168 - regression_loss: 1.3837 - classification_loss 6662/10000 [==================>...........] - ETA: 26:34 - loss: 1.8168 - regression_loss: 1.3836 - classification_loss 6663/10000 [==================>...........] - ETA: 26:34 - loss: 1.8166 - regression_loss: 1.3835 - classification_loss 6664/10000 [==================>...........] - ETA: 26:34 - loss: 1.8167 - regression_loss: 1.3836 - classification_loss 6665/10000 [==================>...........] - ETA: 26:33 - loss: 1.8167 - regression_loss: 1.3836 - classification_loss 6666/10000 [==================>...........] - ETA: 26:33 - loss: 1.8168 - regression_loss: 1.3836 - classification_loss 6667/10000 [===================>..........] - ETA: 26:32 - loss: 1.8166 - regression_loss: 1.3835 - classification_loss 6668/10000 [===================>..........] - ETA: 26:32 - loss: 1.8165 - regression_loss: 1.3835 - classification_loss 6669/10000 [===================>..........] - ETA: 26:31 - loss: 1.8166 - regression_loss: 1.3835 - classification_loss 6670/10000 [===================>..........] - ETA: 26:31 - loss: 1.8168 - regression_loss: 1.3837 - classification_loss 6671/10000 [===================>..........] - ETA: 26:30 - loss: 1.8166 - regression_loss: 1.3836 - classification_loss 6672/10000 [===================>..........] - ETA: 26:30 - loss: 1.8165 - regression_loss: 1.3835 - classification_loss 6673/10000 [===================>..........] - ETA: 26:29 - loss: 1.8166 - regression_loss: 1.3835 - classification_loss 6674/10000 [===================>..........] - ETA: 26:29 - loss: 1.8166 - regression_loss: 1.3836 - classification_loss 6675/10000 [===================>..........] - ETA: 26:28 - loss: 1.8165 - regression_loss: 1.3835 - classification_loss 6676/10000 [===================>..........] - ETA: 26:28 - loss: 1.8164 - regression_loss: 1.3835 - classification_loss 6677/10000 [===================>..........] - ETA: 26:27 - loss: 1.8163 - regression_loss: 1.3833 - classification_loss 6678/10000 [===================>..........] - ETA: 26:27 - loss: 1.8162 - regression_loss: 1.3832 - classification_loss 6679/10000 [===================>..........] - ETA: 26:26 - loss: 1.8161 - regression_loss: 1.3832 - classification_loss 6680/10000 [===================>..........] - ETA: 26:26 - loss: 1.8161 - regression_loss: 1.3832 - classification_loss 6681/10000 [===================>..........] - ETA: 26:25 - loss: 1.8160 - regression_loss: 1.3832 - classification_loss 6682/10000 [===================>..........] - ETA: 26:25 - loss: 1.8161 - regression_loss: 1.3832 - classification_loss 6683/10000 [===================>..........] - ETA: 26:24 - loss: 1.8160 - regression_loss: 1.3832 - classification_loss 6684/10000 [===================>..........] - ETA: 26:24 - loss: 1.8160 - regression_loss: 1.3832 - classification_loss 6685/10000 [===================>..........] - ETA: 26:23 - loss: 1.8159 - regression_loss: 1.3831 - classification_loss 6686/10000 [===================>..........] - ETA: 26:23 - loss: 1.8158 - regression_loss: 1.3830 - classification_loss 6687/10000 [===================>..........] - ETA: 26:22 - loss: 1.8157 - regression_loss: 1.3829 - classification_loss 6688/10000 [===================>..........] - ETA: 26:22 - loss: 1.8156 - regression_loss: 1.3829 - classification_loss 6689/10000 [===================>..........] - ETA: 26:21 - loss: 1.8158 - regression_loss: 1.3830 - classification_loss 6690/10000 [===================>..........] - ETA: 26:21 - loss: 1.8157 - regression_loss: 1.3830 - classification_loss 6691/10000 [===================>..........] - ETA: 26:20 - loss: 1.8157 - regression_loss: 1.3830 - classification_loss 6692/10000 [===================>..........] - ETA: 26:20 - loss: 1.8157 - regression_loss: 1.3829 - classification_loss 6693/10000 [===================>..........] - ETA: 26:19 - loss: 1.8157 - regression_loss: 1.3829 - classification_loss 6694/10000 [===================>..........] - ETA: 26:19 - loss: 1.8158 - regression_loss: 1.3830 - classification_loss 6695/10000 [===================>..........] - ETA: 26:18 - loss: 1.8158 - regression_loss: 1.3830 - classification_loss 6696/10000 [===================>..........] - ETA: 26:18 - loss: 1.8158 - regression_loss: 1.3830 - classification_loss 6697/10000 [===================>..........] - ETA: 26:17 - loss: 1.8158 - regression_loss: 1.3830 - classification_loss 6698/10000 [===================>..........] - ETA: 26:17 - loss: 1.8158 - regression_loss: 1.3830 - classification_loss 6699/10000 [===================>..........] - ETA: 26:16 - loss: 1.8157 - regression_loss: 1.3829 - classification_loss 6700/10000 [===================>..........] - ETA: 26:16 - loss: 1.8156 - regression_loss: 1.3829 - classification_loss 6701/10000 [===================>..........] - ETA: 26:15 - loss: 1.8156 - regression_loss: 1.3829 - classification_loss 6702/10000 [===================>..........] - ETA: 26:15 - loss: 1.8157 - regression_loss: 1.3830 - classification_loss 6703/10000 [===================>..........] - ETA: 26:14 - loss: 1.8157 - regression_loss: 1.3830 - classification_loss 6704/10000 [===================>..........] - ETA: 26:14 - loss: 1.8157 - regression_loss: 1.3830 - classification_loss 6705/10000 [===================>..........] - ETA: 26:14 - loss: 1.8159 - regression_loss: 1.3832 - classification_loss 6706/10000 [===================>..........] - ETA: 26:13 - loss: 1.8158 - regression_loss: 1.3831 - classification_loss 6707/10000 [===================>..........] - ETA: 26:13 - loss: 1.8158 - regression_loss: 1.3831 - classification_loss 6708/10000 [===================>..........] - ETA: 26:12 - loss: 1.8157 - regression_loss: 1.3831 - classification_loss 6709/10000 [===================>..........] - ETA: 26:12 - loss: 1.8156 - regression_loss: 1.3830 - classification_loss 6710/10000 [===================>..........] - ETA: 26:11 - loss: 1.8158 - regression_loss: 1.3831 - classification_loss 6711/10000 [===================>..........] - ETA: 26:11 - loss: 1.8157 - regression_loss: 1.3830 - classification_loss 6712/10000 [===================>..........] - ETA: 26:10 - loss: 1.8159 - regression_loss: 1.3832 - classification_loss 6713/10000 [===================>..........] - ETA: 26:10 - loss: 1.8158 - regression_loss: 1.3831 - classification_loss 6714/10000 [===================>..........] - ETA: 26:09 - loss: 1.8158 - regression_loss: 1.3831 - classification_loss 6715/10000 [===================>..........] - ETA: 26:09 - loss: 1.8158 - regression_loss: 1.3831 - classification_loss 6716/10000 [===================>..........] - ETA: 26:08 - loss: 1.8157 - regression_loss: 1.3830 - classification_loss 6717/10000 [===================>..........] - ETA: 26:08 - loss: 1.8157 - regression_loss: 1.3830 - classification_loss 6718/10000 [===================>..........] - ETA: 26:07 - loss: 1.8156 - regression_loss: 1.3830 - classification_loss 6719/10000 [===================>..........] - ETA: 26:07 - loss: 1.8154 - regression_loss: 1.3828 - classification_loss 6720/10000 [===================>..........] - ETA: 26:06 - loss: 1.8154 - regression_loss: 1.3828 - classification_loss 6721/10000 [===================>..........] - ETA: 26:06 - loss: 1.8154 - regression_loss: 1.3828 - classification_loss 6722/10000 [===================>..........] - ETA: 26:05 - loss: 1.8153 - regression_loss: 1.3828 - classification_loss 6723/10000 [===================>..........] - ETA: 26:05 - loss: 1.8153 - regression_loss: 1.3828 - classification_loss 6724/10000 [===================>..........] - ETA: 26:04 - loss: 1.8153 - regression_loss: 1.3827 - classification_loss 6725/10000 [===================>..........] - ETA: 26:04 - loss: 1.8153 - regression_loss: 1.3827 - classification_loss 6726/10000 [===================>..........] - ETA: 26:03 - loss: 1.8153 - regression_loss: 1.3827 - classification_loss 6727/10000 [===================>..........] - ETA: 26:03 - loss: 1.8151 - regression_loss: 1.3826 - classification_loss 6728/10000 [===================>..........] - ETA: 26:02 - loss: 1.8151 - regression_loss: 1.3826 - classification_loss 6729/10000 [===================>..........] - ETA: 26:02 - loss: 1.8151 - regression_loss: 1.3826 - classification_loss 6730/10000 [===================>..........] - ETA: 26:01 - loss: 1.8150 - regression_loss: 1.3825 - classification_loss 6731/10000 [===================>..........] - ETA: 26:01 - loss: 1.8150 - regression_loss: 1.3825 - classification_loss 6732/10000 [===================>..........] - ETA: 26:00 - loss: 1.8151 - regression_loss: 1.3826 - classification_loss 6733/10000 [===================>..........] - ETA: 26:00 - loss: 1.8151 - regression_loss: 1.3826 - classification_loss 6734/10000 [===================>..........] - ETA: 25:59 - loss: 1.8152 - regression_loss: 1.3827 - classification_loss 6735/10000 [===================>..........] - ETA: 25:59 - loss: 1.8152 - regression_loss: 1.3827 - classification_loss 6736/10000 [===================>..........] - ETA: 25:58 - loss: 1.8153 - regression_loss: 1.3828 - classification_loss 6737/10000 [===================>..........] - ETA: 25:58 - loss: 1.8154 - regression_loss: 1.3829 - classification_loss 6738/10000 [===================>..........] - ETA: 25:58 - loss: 1.8153 - regression_loss: 1.3829 - classification_loss 6739/10000 [===================>..........] - ETA: 25:57 - loss: 1.8152 - regression_loss: 1.3828 - classification_loss 6740/10000 [===================>..........] - ETA: 25:57 - loss: 1.8152 - regression_loss: 1.3827 - classification_loss 6741/10000 [===================>..........] - ETA: 25:56 - loss: 1.8152 - regression_loss: 1.3827 - classification_loss 6742/10000 [===================>..........] - ETA: 25:56 - loss: 1.8153 - regression_loss: 1.3829 - classification_loss 6743/10000 [===================>..........] - ETA: 25:55 - loss: 1.8153 - regression_loss: 1.3829 - classification_loss 6744/10000 [===================>..........] - ETA: 25:55 - loss: 1.8153 - regression_loss: 1.3829 - classification_loss 6745/10000 [===================>..........] - ETA: 25:54 - loss: 1.8154 - regression_loss: 1.3830 - classification_loss 6746/10000 [===================>..........] - ETA: 25:54 - loss: 1.8152 - regression_loss: 1.3828 - classification_loss 6747/10000 [===================>..........] - ETA: 25:53 - loss: 1.8152 - regression_loss: 1.3828 - classification_loss 6748/10000 [===================>..........] - ETA: 25:53 - loss: 1.8152 - regression_loss: 1.3829 - classification_loss 6749/10000 [===================>..........] - ETA: 25:52 - loss: 1.8152 - regression_loss: 1.3829 - classification_loss 6750/10000 [===================>..........] - ETA: 25:52 - loss: 1.8154 - regression_loss: 1.3829 - classification_loss 6751/10000 [===================>..........] - ETA: 25:51 - loss: 1.8154 - regression_loss: 1.3830 - classification_loss 6752/10000 [===================>..........] - ETA: 25:51 - loss: 1.8154 - regression_loss: 1.3829 - classification_loss 6753/10000 [===================>..........] - ETA: 25:50 - loss: 1.8154 - regression_loss: 1.3829 - classification_loss 6754/10000 [===================>..........] - ETA: 25:50 - loss: 1.8153 - regression_loss: 1.3828 - classification_loss 6755/10000 [===================>..........] - ETA: 25:49 - loss: 1.8156 - regression_loss: 1.3829 - classification_loss 6756/10000 [===================>..........] - ETA: 25:49 - loss: 1.8156 - regression_loss: 1.3829 - classification_loss 6757/10000 [===================>..........] - ETA: 25:48 - loss: 1.8155 - regression_loss: 1.3828 - classification_loss 6758/10000 [===================>..........] - ETA: 25:48 - loss: 1.8154 - regression_loss: 1.3828 - classification_loss 6759/10000 [===================>..........] - ETA: 25:47 - loss: 1.8153 - regression_loss: 1.3827 - classification_loss 6760/10000 [===================>..........] - ETA: 25:47 - loss: 1.8154 - regression_loss: 1.3828 - classification_loss 6761/10000 [===================>..........] - ETA: 25:46 - loss: 1.8155 - regression_loss: 1.3829 - classification_loss 6762/10000 [===================>..........] - ETA: 25:46 - loss: 1.8156 - regression_loss: 1.3830 - classification_loss 6763/10000 [===================>..........] - ETA: 25:45 - loss: 1.8157 - regression_loss: 1.3831 - classification_loss 6764/10000 [===================>..........] - ETA: 25:45 - loss: 1.8158 - regression_loss: 1.3832 - classification_loss 6765/10000 [===================>..........] - ETA: 25:44 - loss: 1.8158 - regression_loss: 1.3832 - classification_loss 6766/10000 [===================>..........] - ETA: 25:44 - loss: 1.8157 - regression_loss: 1.3832 - classification_loss 6767/10000 [===================>..........] - ETA: 25:44 - loss: 1.8156 - regression_loss: 1.3831 - classification_loss 6768/10000 [===================>..........] - ETA: 25:43 - loss: 1.8155 - regression_loss: 1.3830 - classification_loss 6769/10000 [===================>..........] - ETA: 25:43 - loss: 1.8155 - regression_loss: 1.3830 - classification_loss 6770/10000 [===================>..........] - ETA: 25:42 - loss: 1.8154 - regression_loss: 1.3829 - classification_loss 6771/10000 [===================>..........] - ETA: 25:42 - loss: 1.8155 - regression_loss: 1.3830 - classification_loss 6772/10000 [===================>..........] - ETA: 25:41 - loss: 1.8155 - regression_loss: 1.3830 - classification_loss 6773/10000 [===================>..........] - ETA: 25:41 - loss: 1.8155 - regression_loss: 1.3830 - classification_loss 6774/10000 [===================>..........] - ETA: 25:40 - loss: 1.8154 - regression_loss: 1.3830 - classification_loss 6775/10000 [===================>..........] - ETA: 25:40 - loss: 1.8153 - regression_loss: 1.3829 - classification_loss 6776/10000 [===================>..........] - ETA: 25:39 - loss: 1.8153 - regression_loss: 1.3829 - classification_loss 6777/10000 [===================>..........] - ETA: 25:39 - loss: 1.8153 - regression_loss: 1.3829 - classification_loss 6778/10000 [===================>..........] - ETA: 25:38 - loss: 1.8153 - regression_loss: 1.3829 - classification_loss 6779/10000 [===================>..........] - ETA: 25:38 - loss: 1.8153 - regression_loss: 1.3829 - classification_loss 6780/10000 [===================>..........] - ETA: 25:37 - loss: 1.8155 - regression_loss: 1.3831 - classification_loss 6781/10000 [===================>..........] - ETA: 25:37 - loss: 1.8155 - regression_loss: 1.3831 - classification_loss 6782/10000 [===================>..........] - ETA: 25:36 - loss: 1.8156 - regression_loss: 1.3831 - classification_loss 6783/10000 [===================>..........] - ETA: 25:36 - loss: 1.8155 - regression_loss: 1.3830 - classification_loss 6784/10000 [===================>..........] - ETA: 25:35 - loss: 1.8154 - regression_loss: 1.3830 - classification_loss 6785/10000 [===================>..........] - ETA: 25:35 - loss: 1.8154 - regression_loss: 1.3829 - classification_loss 6786/10000 [===================>..........] - ETA: 25:34 - loss: 1.8154 - regression_loss: 1.3830 - classification_loss 6787/10000 [===================>..........] - ETA: 25:34 - loss: 1.8153 - regression_loss: 1.3829 - classification_loss 6788/10000 [===================>..........] - ETA: 25:33 - loss: 1.8153 - regression_loss: 1.3829 - classification_loss 6789/10000 [===================>..........] - ETA: 25:33 - loss: 1.8151 - regression_loss: 1.3828 - classification_loss 6790/10000 [===================>..........] - ETA: 25:32 - loss: 1.8151 - regression_loss: 1.3828 - classification_loss 6791/10000 [===================>..........] - ETA: 25:32 - loss: 1.8151 - regression_loss: 1.3827 - classification_loss 6792/10000 [===================>..........] - ETA: 25:31 - loss: 1.8152 - regression_loss: 1.3828 - classification_loss 6793/10000 [===================>..........] - ETA: 25:31 - loss: 1.8152 - regression_loss: 1.3829 - classification_loss 6794/10000 [===================>..........] - ETA: 25:30 - loss: 1.8152 - regression_loss: 1.3829 - classification_loss 6795/10000 [===================>..........] - ETA: 25:30 - loss: 1.8153 - regression_loss: 1.3830 - classification_loss 6796/10000 [===================>..........] - ETA: 25:29 - loss: 1.8152 - regression_loss: 1.3829 - classification_loss 6797/10000 [===================>..........] - ETA: 25:29 - loss: 1.8152 - regression_loss: 1.3829 - classification_loss 6798/10000 [===================>..........] - ETA: 25:28 - loss: 1.8152 - regression_loss: 1.3828 - classification_loss 6799/10000 [===================>..........] - ETA: 25:28 - loss: 1.8151 - regression_loss: 1.3828 - classification_loss 6800/10000 [===================>..........] - ETA: 25:27 - loss: 1.8150 - regression_loss: 1.3827 - classification_loss 6801/10000 [===================>..........] - ETA: 25:27 - loss: 1.8150 - regression_loss: 1.3827 - classification_loss 6802/10000 [===================>..........] - ETA: 25:26 - loss: 1.8152 - regression_loss: 1.3829 - classification_loss 6803/10000 [===================>..........] - ETA: 25:26 - loss: 1.8151 - regression_loss: 1.3828 - classification_loss 6804/10000 [===================>..........] - ETA: 25:25 - loss: 1.8150 - regression_loss: 1.3827 - classification_loss 6805/10000 [===================>..........] - ETA: 25:25 - loss: 1.8149 - regression_loss: 1.3827 - classification_loss 6806/10000 [===================>..........] - ETA: 25:24 - loss: 1.8152 - regression_loss: 1.3828 - classification_loss 6807/10000 [===================>..........] - ETA: 25:24 - loss: 1.8153 - regression_loss: 1.3828 - classification_loss 6808/10000 [===================>..........] - ETA: 25:23 - loss: 1.8153 - regression_loss: 1.3828 - classification_loss 6809/10000 [===================>..........] - ETA: 25:23 - loss: 1.8152 - regression_loss: 1.3828 - classification_loss 6810/10000 [===================>..........] - ETA: 25:23 - loss: 1.8152 - regression_loss: 1.3827 - classification_loss 6811/10000 [===================>..........] - ETA: 25:22 - loss: 1.8151 - regression_loss: 1.3826 - classification_loss 6812/10000 [===================>..........] - ETA: 25:22 - loss: 1.8151 - regression_loss: 1.3826 - classification_loss 6813/10000 [===================>..........] - ETA: 25:21 - loss: 1.8151 - regression_loss: 1.3827 - classification_loss 6814/10000 [===================>..........] - ETA: 25:21 - loss: 1.8151 - regression_loss: 1.3827 - classification_loss 6815/10000 [===================>..........] - ETA: 25:20 - loss: 1.8149 - regression_loss: 1.3826 - classification_loss 6816/10000 [===================>..........] - ETA: 25:20 - loss: 1.8149 - regression_loss: 1.3825 - classification_loss 6817/10000 [===================>..........] - ETA: 25:19 - loss: 1.8147 - regression_loss: 1.3824 - classification_loss 6818/10000 [===================>..........] - ETA: 25:19 - loss: 1.8147 - regression_loss: 1.3824 - classification_loss 6819/10000 [===================>..........] - ETA: 25:18 - loss: 1.8147 - regression_loss: 1.3824 - classification_loss 6820/10000 [===================>..........] - ETA: 25:18 - loss: 1.8146 - regression_loss: 1.3824 - classification_loss 6821/10000 [===================>..........] - ETA: 25:17 - loss: 1.8147 - regression_loss: 1.3824 - classification_loss 6822/10000 [===================>..........] - ETA: 25:17 - loss: 1.8147 - regression_loss: 1.3824 - classification_loss 6823/10000 [===================>..........] - ETA: 25:16 - loss: 1.8146 - regression_loss: 1.3824 - classification_loss 6824/10000 [===================>..........] - ETA: 25:16 - loss: 1.8145 - regression_loss: 1.3822 - classification_loss 6825/10000 [===================>..........] - ETA: 25:15 - loss: 1.8145 - regression_loss: 1.3823 - classification_loss 6826/10000 [===================>..........] - ETA: 25:15 - loss: 1.8145 - regression_loss: 1.3823 - classification_loss 6827/10000 [===================>..........] - ETA: 25:14 - loss: 1.8144 - regression_loss: 1.3822 - classification_loss 6828/10000 [===================>..........] - ETA: 25:14 - loss: 1.8143 - regression_loss: 1.3821 - classification_loss 6829/10000 [===================>..........] - ETA: 25:13 - loss: 1.8143 - regression_loss: 1.3821 - classification_loss 6830/10000 [===================>..........] - ETA: 25:13 - loss: 1.8142 - regression_loss: 1.3820 - classification_loss 6831/10000 [===================>..........] - ETA: 25:12 - loss: 1.8142 - regression_loss: 1.3821 - classification_loss 6832/10000 [===================>..........] - ETA: 25:12 - loss: 1.8142 - regression_loss: 1.3821 - classification_loss 6833/10000 [===================>..........] - ETA: 25:11 - loss: 1.8142 - regression_loss: 1.3821 - classification_loss 6834/10000 [===================>..........] - ETA: 25:11 - loss: 1.8141 - regression_loss: 1.3820 - classification_loss 6835/10000 [===================>..........] - ETA: 25:10 - loss: 1.8140 - regression_loss: 1.3820 - classification_loss 6836/10000 [===================>..........] - ETA: 25:10 - loss: 1.8140 - regression_loss: 1.3820 - classification_loss 6837/10000 [===================>..........] - ETA: 25:09 - loss: 1.8140 - regression_loss: 1.3820 - classification_loss 6838/10000 [===================>..........] - ETA: 25:09 - loss: 1.8140 - regression_loss: 1.3819 - classification_loss 6839/10000 [===================>..........] - ETA: 25:08 - loss: 1.8139 - regression_loss: 1.3819 - classification_loss 6840/10000 [===================>..........] - ETA: 25:08 - loss: 1.8139 - regression_loss: 1.3819 - classification_loss 6841/10000 [===================>..........] - ETA: 25:07 - loss: 1.8139 - regression_loss: 1.3819 - classification_loss 6842/10000 [===================>..........] - ETA: 25:07 - loss: 1.8140 - regression_loss: 1.3820 - classification_loss 6843/10000 [===================>..........] - ETA: 25:06 - loss: 1.8141 - regression_loss: 1.3821 - classification_loss 6844/10000 [===================>..........] - ETA: 25:06 - loss: 1.8140 - regression_loss: 1.3821 - classification_loss 6845/10000 [===================>..........] - ETA: 25:05 - loss: 1.8140 - regression_loss: 1.3821 - classification_loss 6846/10000 [===================>..........] - ETA: 25:05 - loss: 1.8140 - regression_loss: 1.3820 - classification_loss 6847/10000 [===================>..........] - ETA: 25:04 - loss: 1.8141 - regression_loss: 1.3821 - classification_loss 6848/10000 [===================>..........] - ETA: 25:04 - loss: 1.8140 - regression_loss: 1.3820 - classification_loss 6849/10000 [===================>..........] - ETA: 25:04 - loss: 1.8139 - regression_loss: 1.3820 - classification_loss 6850/10000 [===================>..........] - ETA: 25:03 - loss: 1.8139 - regression_loss: 1.3819 - classification_loss 6851/10000 [===================>..........] - ETA: 25:03 - loss: 1.8139 - regression_loss: 1.3820 - classification_loss 6852/10000 [===================>..........] - ETA: 25:02 - loss: 1.8139 - regression_loss: 1.3820 - classification_loss 6853/10000 [===================>..........] - ETA: 25:02 - loss: 1.8138 - regression_loss: 1.3819 - classification_loss 6854/10000 [===================>..........] - ETA: 25:01 - loss: 1.8139 - regression_loss: 1.3820 - classification_loss 6855/10000 [===================>..........] - ETA: 25:01 - loss: 1.8138 - regression_loss: 1.3819 - classification_loss 6856/10000 [===================>..........] - ETA: 25:00 - loss: 1.8138 - regression_loss: 1.3819 - classification_loss 6857/10000 [===================>..........] - ETA: 25:00 - loss: 1.8136 - regression_loss: 1.3818 - classification_loss 6858/10000 [===================>..........] - ETA: 24:59 - loss: 1.8137 - regression_loss: 1.3818 - classification_loss 6859/10000 [===================>..........] - ETA: 24:59 - loss: 1.8136 - regression_loss: 1.3817 - classification_loss 6860/10000 [===================>..........] - ETA: 24:58 - loss: 1.8135 - regression_loss: 1.3816 - classification_loss 6861/10000 [===================>..........] - ETA: 24:58 - loss: 1.8135 - regression_loss: 1.3816 - classification_loss 6862/10000 [===================>..........] - ETA: 24:57 - loss: 1.8136 - regression_loss: 1.3818 - classification_loss 6863/10000 [===================>..........] - ETA: 24:57 - loss: 1.8134 - regression_loss: 1.3816 - classification_loss 6864/10000 [===================>..........] - ETA: 24:56 - loss: 1.8135 - regression_loss: 1.3817 - classification_loss 6865/10000 [===================>..........] - ETA: 24:56 - loss: 1.8135 - regression_loss: 1.3817 - classification_loss 6866/10000 [===================>..........] - ETA: 24:55 - loss: 1.8135 - regression_loss: 1.3818 - classification_loss 6867/10000 [===================>..........] - ETA: 24:55 - loss: 1.8135 - regression_loss: 1.3818 - classification_loss 6868/10000 [===================>..........] - ETA: 24:54 - loss: 1.8135 - regression_loss: 1.3818 - classification_loss 6869/10000 [===================>..........] - ETA: 24:54 - loss: 1.8135 - regression_loss: 1.3817 - classification_loss 6870/10000 [===================>..........] - ETA: 24:53 - loss: 1.8134 - regression_loss: 1.3817 - classification_loss 6871/10000 [===================>..........] - ETA: 24:53 - loss: 1.8135 - regression_loss: 1.3817 - classification_loss 6872/10000 [===================>..........] - ETA: 24:52 - loss: 1.8134 - regression_loss: 1.3817 - classification_loss 6873/10000 [===================>..........] - ETA: 24:52 - loss: 1.8135 - regression_loss: 1.3818 - classification_loss 6874/10000 [===================>..........] - ETA: 24:51 - loss: 1.8135 - regression_loss: 1.3818 - classification_loss 6875/10000 [===================>..........] - ETA: 24:51 - loss: 1.8137 - regression_loss: 1.3820 - classification_loss 6876/10000 [===================>..........] - ETA: 24:50 - loss: 1.8137 - regression_loss: 1.3820 - classification_loss 6877/10000 [===================>..........] - ETA: 24:50 - loss: 1.8136 - regression_loss: 1.3819 - classification_loss 6878/10000 [===================>..........] - ETA: 24:49 - loss: 1.8135 - regression_loss: 1.3819 - classification_loss 6879/10000 [===================>..........] - ETA: 24:49 - loss: 1.8134 - regression_loss: 1.3817 - classification_loss 6880/10000 [===================>..........] - ETA: 24:49 - loss: 1.8134 - regression_loss: 1.3817 - classification_loss 6881/10000 [===================>..........] - ETA: 24:48 - loss: 1.8134 - regression_loss: 1.3817 - classification_loss 6882/10000 [===================>..........] - ETA: 24:48 - loss: 1.8133 - regression_loss: 1.3817 - classification_loss 6883/10000 [===================>..........] - ETA: 24:47 - loss: 1.8132 - regression_loss: 1.3816 - classification_loss 6884/10000 [===================>..........] - ETA: 24:47 - loss: 1.8133 - regression_loss: 1.3817 - classification_loss 6885/10000 [===================>..........] - ETA: 24:46 - loss: 1.8134 - regression_loss: 1.3817 - classification_loss 6886/10000 [===================>..........] - ETA: 24:46 - loss: 1.8134 - regression_loss: 1.3817 - classification_loss 6887/10000 [===================>..........] - ETA: 24:45 - loss: 1.8134 - regression_loss: 1.3817 - classification_loss 6888/10000 [===================>..........] - ETA: 24:45 - loss: 1.8133 - regression_loss: 1.3816 - classification_loss 6889/10000 [===================>..........] - ETA: 24:44 - loss: 1.8131 - regression_loss: 1.3815 - classification_loss 6890/10000 [===================>..........] - ETA: 24:44 - loss: 1.8131 - regression_loss: 1.3814 - classification_loss 6891/10000 [===================>..........] - ETA: 24:43 - loss: 1.8130 - regression_loss: 1.3814 - classification_loss 6892/10000 [===================>..........] - ETA: 24:43 - loss: 1.8130 - regression_loss: 1.3814 - classification_loss 6893/10000 [===================>..........] - ETA: 24:42 - loss: 1.8130 - regression_loss: 1.3814 - classification_loss 6894/10000 [===================>..........] - ETA: 24:42 - loss: 1.8129 - regression_loss: 1.3813 - classification_loss 6895/10000 [===================>..........] - ETA: 24:41 - loss: 1.8130 - regression_loss: 1.3813 - classification_loss 6896/10000 [===================>..........] - ETA: 24:41 - loss: 1.8128 - regression_loss: 1.3812 - classification_loss 6897/10000 [===================>..........] - ETA: 24:40 - loss: 1.8128 - regression_loss: 1.3812 - classification_loss 6898/10000 [===================>..........] - ETA: 24:40 - loss: 1.8128 - regression_loss: 1.3812 - classification_loss 6899/10000 [===================>..........] - ETA: 24:39 - loss: 1.8129 - regression_loss: 1.3812 - classification_loss 6900/10000 [===================>..........] - ETA: 24:39 - loss: 1.8129 - regression_loss: 1.3812 - classification_loss 6901/10000 [===================>..........] - ETA: 24:38 - loss: 1.8129 - regression_loss: 1.3813 - classification_loss 6902/10000 [===================>..........] - ETA: 24:38 - loss: 1.8130 - regression_loss: 1.3813 - classification_loss 6903/10000 [===================>..........] - ETA: 24:37 - loss: 1.8130 - regression_loss: 1.3813 - classification_loss 6904/10000 [===================>..........] - ETA: 24:37 - loss: 1.8128 - regression_loss: 1.3812 - classification_loss 6905/10000 [===================>..........] - ETA: 24:36 - loss: 1.8128 - regression_loss: 1.3812 - classification_loss 6906/10000 [===================>..........] - ETA: 24:36 - loss: 1.8128 - regression_loss: 1.3812 - classification_loss 6907/10000 [===================>..........] - ETA: 24:35 - loss: 1.8127 - regression_loss: 1.3811 - classification_loss 6908/10000 [===================>..........] - ETA: 24:35 - loss: 1.8128 - regression_loss: 1.3812 - classification_loss 6909/10000 [===================>..........] - ETA: 24:34 - loss: 1.8127 - regression_loss: 1.3812 - classification_loss 6910/10000 [===================>..........] - ETA: 24:34 - loss: 1.8127 - regression_loss: 1.3811 - classification_loss 6911/10000 [===================>..........] - ETA: 24:33 - loss: 1.8127 - regression_loss: 1.3812 - classification_loss 6912/10000 [===================>..........] - ETA: 24:33 - loss: 1.8127 - regression_loss: 1.3812 - classification_loss 6913/10000 [===================>..........] - ETA: 24:32 - loss: 1.8125 - regression_loss: 1.3811 - classification_loss 6914/10000 [===================>..........] - ETA: 24:32 - loss: 1.8124 - regression_loss: 1.3810 - classification_loss 6915/10000 [===================>..........] - ETA: 24:32 - loss: 1.8125 - regression_loss: 1.3810 - classification_loss 6916/10000 [===================>..........] - ETA: 24:31 - loss: 1.8125 - regression_loss: 1.3810 - classification_loss 6917/10000 [===================>..........] - ETA: 24:31 - loss: 1.8125 - regression_loss: 1.3810 - classification_loss 6918/10000 [===================>..........] - ETA: 24:30 - loss: 1.8125 - regression_loss: 1.3811 - classification_loss 6919/10000 [===================>..........] - ETA: 24:30 - loss: 1.8125 - regression_loss: 1.3811 - classification_loss 6920/10000 [===================>..........] - ETA: 24:29 - loss: 1.8126 - regression_loss: 1.3811 - classification_loss 6921/10000 [===================>..........] - ETA: 24:29 - loss: 1.8126 - regression_loss: 1.3811 - classification_loss 6922/10000 [===================>..........] - ETA: 24:28 - loss: 1.8126 - regression_loss: 1.3811 - classification_loss 6923/10000 [===================>..........] - ETA: 24:28 - loss: 1.8126 - regression_loss: 1.3811 - classification_loss 6924/10000 [===================>..........] - ETA: 24:27 - loss: 1.8127 - regression_loss: 1.3812 - classification_loss 6925/10000 [===================>..........] - ETA: 24:27 - loss: 1.8126 - regression_loss: 1.3811 - classification_loss 6926/10000 [===================>..........] - ETA: 24:26 - loss: 1.8127 - regression_loss: 1.3812 - classification_loss 6927/10000 [===================>..........] - ETA: 24:26 - loss: 1.8128 - regression_loss: 1.3813 - classification_loss 6928/10000 [===================>..........] - ETA: 24:25 - loss: 1.8129 - regression_loss: 1.3814 - classification_loss 6929/10000 [===================>..........] - ETA: 24:25 - loss: 1.8129 - regression_loss: 1.3813 - classification_loss 6930/10000 [===================>..........] - ETA: 24:24 - loss: 1.8129 - regression_loss: 1.3814 - classification_loss 6931/10000 [===================>..........] - ETA: 24:24 - loss: 1.8128 - regression_loss: 1.3813 - classification_loss 6932/10000 [===================>..........] - ETA: 24:23 - loss: 1.8128 - regression_loss: 1.3811 - classification_loss 6933/10000 [===================>..........] - ETA: 24:23 - loss: 1.8127 - regression_loss: 1.3810 - classification_loss 6934/10000 [===================>..........] - ETA: 24:22 - loss: 1.8128 - regression_loss: 1.3811 - classification_loss 6935/10000 [===================>..........] - ETA: 24:22 - loss: 1.8126 - regression_loss: 1.3809 - classification_loss 6936/10000 [===================>..........] - ETA: 24:21 - loss: 1.8127 - regression_loss: 1.3810 - classification_loss 6937/10000 [===================>..........] - ETA: 24:21 - loss: 1.8127 - regression_loss: 1.3810 - classification_loss 6938/10000 [===================>..........] - ETA: 24:20 - loss: 1.8126 - regression_loss: 1.3809 - classification_loss 6939/10000 [===================>..........] - ETA: 24:20 - loss: 1.8125 - regression_loss: 1.3808 - classification_loss 6940/10000 [===================>..........] - ETA: 24:19 - loss: 1.8124 - regression_loss: 1.3807 - classification_loss 6941/10000 [===================>..........] - ETA: 24:19 - loss: 1.8124 - regression_loss: 1.3807 - classification_loss 6942/10000 [===================>..........] - ETA: 24:18 - loss: 1.8124 - regression_loss: 1.3807 - classification_loss 6943/10000 [===================>..........] - ETA: 24:18 - loss: 1.8122 - regression_loss: 1.3806 - classification_loss 6944/10000 [===================>..........] - ETA: 24:17 - loss: 1.8121 - regression_loss: 1.3804 - classification_loss 6945/10000 [===================>..........] - ETA: 24:17 - loss: 1.8121 - regression_loss: 1.3804 - classification_loss 6946/10000 [===================>..........] - ETA: 24:16 - loss: 1.8120 - regression_loss: 1.3803 - classification_loss 6947/10000 [===================>..........] - ETA: 24:16 - loss: 1.8121 - regression_loss: 1.3804 - classification_loss 6948/10000 [===================>..........] - ETA: 24:16 - loss: 1.8120 - regression_loss: 1.3803 - classification_loss 6949/10000 [===================>..........] - ETA: 24:15 - loss: 1.8121 - regression_loss: 1.3804 - classification_loss 6950/10000 [===================>..........] - ETA: 24:15 - loss: 1.8122 - regression_loss: 1.3805 - classification_loss 6951/10000 [===================>..........] - ETA: 24:14 - loss: 1.8123 - regression_loss: 1.3806 - classification_loss 6952/10000 [===================>..........] - ETA: 24:14 - loss: 1.8123 - regression_loss: 1.3806 - classification_loss 6953/10000 [===================>..........] - ETA: 24:13 - loss: 1.8122 - regression_loss: 1.3805 - classification_loss 6954/10000 [===================>..........] - ETA: 24:13 - loss: 1.8123 - regression_loss: 1.3806 - classification_loss 6955/10000 [===================>..........] - ETA: 24:12 - loss: 1.8123 - regression_loss: 1.3806 - classification_loss 6956/10000 [===================>..........] - ETA: 24:12 - loss: 1.8122 - regression_loss: 1.3805 - classification_loss 6957/10000 [===================>..........] - ETA: 24:11 - loss: 1.8122 - regression_loss: 1.3805 - classification_loss 6958/10000 [===================>..........] - ETA: 24:11 - loss: 1.8122 - regression_loss: 1.3806 - classification_loss 6959/10000 [===================>..........] - ETA: 24:10 - loss: 1.8122 - regression_loss: 1.3805 - classification_loss 6960/10000 [===================>..........] - ETA: 24:10 - loss: 1.8122 - regression_loss: 1.3805 - classification_loss 6961/10000 [===================>..........] - ETA: 24:09 - loss: 1.8121 - regression_loss: 1.3805 - classification_loss 6962/10000 [===================>..........] - ETA: 24:09 - loss: 1.8121 - regression_loss: 1.3804 - classification_loss 6963/10000 [===================>..........] - ETA: 24:08 - loss: 1.8120 - regression_loss: 1.3804 - classification_loss 6964/10000 [===================>..........] - ETA: 24:08 - loss: 1.8119 - regression_loss: 1.3803 - classification_loss 6965/10000 [===================>..........] - ETA: 24:07 - loss: 1.8120 - regression_loss: 1.3804 - classification_loss 6966/10000 [===================>..........] - ETA: 24:07 - loss: 1.8120 - regression_loss: 1.3803 - classification_loss 6967/10000 [===================>..........] - ETA: 24:06 - loss: 1.8118 - regression_loss: 1.3802 - classification_loss 6968/10000 [===================>..........] - ETA: 24:06 - loss: 1.8117 - regression_loss: 1.3801 - classification_loss 6969/10000 [===================>..........] - ETA: 24:05 - loss: 1.8117 - regression_loss: 1.3801 - classification_loss 6970/10000 [===================>..........] - ETA: 24:05 - loss: 1.8118 - regression_loss: 1.3801 - classification_loss 6971/10000 [===================>..........] - ETA: 24:04 - loss: 1.8118 - regression_loss: 1.3802 - classification_loss 6972/10000 [===================>..........] - ETA: 24:04 - loss: 1.8117 - regression_loss: 1.3801 - classification_loss 6973/10000 [===================>..........] - ETA: 24:03 - loss: 1.8117 - regression_loss: 1.3800 - classification_loss 6974/10000 [===================>..........] - ETA: 24:03 - loss: 1.8116 - regression_loss: 1.3800 - classification_loss 6975/10000 [===================>..........] - ETA: 24:02 - loss: 1.8115 - regression_loss: 1.3800 - classification_loss 6976/10000 [===================>..........] - ETA: 24:02 - loss: 1.8114 - regression_loss: 1.3799 - classification_loss 6977/10000 [===================>..........] - ETA: 24:01 - loss: 1.8113 - regression_loss: 1.3798 - classification_loss 6978/10000 [===================>..........] - ETA: 24:01 - loss: 1.8111 - regression_loss: 1.3796 - classification_loss 6979/10000 [===================>..........] - ETA: 24:00 - loss: 1.8111 - regression_loss: 1.3796 - classification_loss 6980/10000 [===================>..........] - ETA: 24:00 - loss: 1.8111 - regression_loss: 1.3796 - classification_loss 6981/10000 [===================>..........] - ETA: 24:00 - loss: 1.8112 - regression_loss: 1.3797 - classification_loss 6982/10000 [===================>..........] - ETA: 23:59 - loss: 1.8114 - regression_loss: 1.3798 - classification_loss 6983/10000 [===================>..........] - ETA: 23:59 - loss: 1.8113 - regression_loss: 1.3798 - classification_loss 6984/10000 [===================>..........] - ETA: 23:58 - loss: 1.8113 - regression_loss: 1.3798 - classification_loss 6985/10000 [===================>..........] - ETA: 23:58 - loss: 1.8113 - regression_loss: 1.3798 - classification_loss 6986/10000 [===================>..........] - ETA: 23:57 - loss: 1.8113 - regression_loss: 1.3798 - classification_loss 6987/10000 [===================>..........] - ETA: 23:57 - loss: 1.8113 - regression_loss: 1.3798 - classification_loss 6988/10000 [===================>..........] - ETA: 23:56 - loss: 1.8113 - regression_loss: 1.3797 - classification_loss 6989/10000 [===================>..........] - ETA: 23:56 - loss: 1.8112 - regression_loss: 1.3797 - classification_loss 6990/10000 [===================>..........] - ETA: 23:55 - loss: 1.8112 - regression_loss: 1.3797 - classification_loss 6991/10000 [===================>..........] - ETA: 23:55 - loss: 1.8112 - regression_loss: 1.3796 - classification_loss 6992/10000 [===================>..........] - ETA: 23:54 - loss: 1.8111 - regression_loss: 1.3796 - classification_loss 6993/10000 [===================>..........] - ETA: 23:54 - loss: 1.8111 - regression_loss: 1.3795 - classification_loss 6994/10000 [===================>..........] - ETA: 23:53 - loss: 1.8111 - regression_loss: 1.3795 - classification_loss 6995/10000 [===================>..........] - ETA: 23:53 - loss: 1.8110 - regression_loss: 1.3794 - classification_loss 6996/10000 [===================>..........] - ETA: 23:52 - loss: 1.8109 - regression_loss: 1.3794 - classification_loss 6997/10000 [===================>..........] - ETA: 23:52 - loss: 1.8110 - regression_loss: 1.3794 - classification_loss 6998/10000 [===================>..........] - ETA: 23:51 - loss: 1.8109 - regression_loss: 1.3794 - classification_loss 6999/10000 [===================>..........] - ETA: 23:51 - loss: 1.8107 - regression_loss: 1.3792 - classification_loss 7000/10000 [====================>.........] - ETA: 23:50 - loss: 1.8107 - regression_loss: 1.3792 - classification_loss 7001/10000 [====================>.........] - ETA: 23:50 - loss: 1.8106 - regression_loss: 1.3791 - classification_loss 7002/10000 [====================>.........] - ETA: 23:49 - loss: 1.8107 - regression_loss: 1.3792 - classification_loss 7003/10000 [====================>.........] - ETA: 23:49 - loss: 1.8107 - regression_loss: 1.3792 - classification_loss 7004/10000 [====================>.........] - ETA: 23:48 - loss: 1.8107 - regression_loss: 1.3792 - classification_loss 7005/10000 [====================>.........] - ETA: 23:48 - loss: 1.8106 - regression_loss: 1.3791 - classification_loss 7006/10000 [====================>.........] - ETA: 23:48 - loss: 1.8106 - regression_loss: 1.3791 - classification_loss 7007/10000 [====================>.........] - ETA: 23:47 - loss: 1.8105 - regression_loss: 1.3791 - classification_loss 7008/10000 [====================>.........] - ETA: 23:47 - loss: 1.8107 - regression_loss: 1.3792 - classification_loss 7009/10000 [====================>.........] - ETA: 23:46 - loss: 1.8107 - regression_loss: 1.3792 - classification_loss 7010/10000 [====================>.........] - ETA: 23:46 - loss: 1.8108 - regression_loss: 1.3792 - classification_loss 7011/10000 [====================>.........] - ETA: 23:45 - loss: 1.8108 - regression_loss: 1.3792 - classification_loss 7012/10000 [====================>.........] - ETA: 23:45 - loss: 1.8108 - regression_loss: 1.3793 - classification_loss 7013/10000 [====================>.........] - ETA: 23:44 - loss: 1.8108 - regression_loss: 1.3793 - classification_loss 7014/10000 [====================>.........] - ETA: 23:44 - loss: 1.8108 - regression_loss: 1.3793 - classification_loss 7015/10000 [====================>.........] - ETA: 23:43 - loss: 1.8109 - regression_loss: 1.3793 - classification_loss 7016/10000 [====================>.........] - ETA: 23:43 - loss: 1.8110 - regression_loss: 1.3795 - classification_loss 7017/10000 [====================>.........] - ETA: 23:42 - loss: 1.8109 - regression_loss: 1.3794 - classification_loss 7018/10000 [====================>.........] - ETA: 23:42 - loss: 1.8109 - regression_loss: 1.3794 - classification_loss 7019/10000 [====================>.........] - ETA: 23:41 - loss: 1.8109 - regression_loss: 1.3794 - classification_loss 7020/10000 [====================>.........] - ETA: 23:41 - loss: 1.8109 - regression_loss: 1.3794 - classification_loss 7021/10000 [====================>.........] - ETA: 23:40 - loss: 1.8109 - regression_loss: 1.3794 - classification_loss 7022/10000 [====================>.........] - ETA: 23:40 - loss: 1.8110 - regression_loss: 1.3795 - classification_loss 7023/10000 [====================>.........] - ETA: 23:39 - loss: 1.8109 - regression_loss: 1.3794 - classification_loss 7024/10000 [====================>.........] - ETA: 23:39 - loss: 1.8110 - regression_loss: 1.3795 - classification_loss 7025/10000 [====================>.........] - ETA: 23:38 - loss: 1.8109 - regression_loss: 1.3794 - classification_loss 7026/10000 [====================>.........] - ETA: 23:38 - loss: 1.8109 - regression_loss: 1.3794 - classification_loss 7027/10000 [====================>.........] - ETA: 23:37 - loss: 1.8108 - regression_loss: 1.3794 - classification_loss 7028/10000 [====================>.........] - ETA: 23:37 - loss: 1.8107 - regression_loss: 1.3793 - classification_loss 7029/10000 [====================>.........] - ETA: 23:36 - loss: 1.8107 - regression_loss: 1.3793 - classification_loss 7030/10000 [====================>.........] - ETA: 23:36 - loss: 1.8106 - regression_loss: 1.3792 - classification_loss 7031/10000 [====================>.........] - ETA: 23:35 - loss: 1.8107 - regression_loss: 1.3793 - classification_loss 7032/10000 [====================>.........] - ETA: 23:35 - loss: 1.8106 - regression_loss: 1.3793 - classification_loss 7033/10000 [====================>.........] - ETA: 23:34 - loss: 1.8105 - regression_loss: 1.3792 - classification_loss 7034/10000 [====================>.........] - ETA: 23:34 - loss: 1.8105 - regression_loss: 1.3791 - classification_loss 7035/10000 [====================>.........] - ETA: 23:34 - loss: 1.8105 - regression_loss: 1.3791 - classification_loss 7036/10000 [====================>.........] - ETA: 23:33 - loss: 1.8104 - regression_loss: 1.3790 - classification_loss 7037/10000 [====================>.........] - ETA: 23:33 - loss: 1.8104 - regression_loss: 1.3791 - classification_loss 7038/10000 [====================>.........] - ETA: 23:32 - loss: 1.8103 - regression_loss: 1.3791 - classification_loss 7039/10000 [====================>.........] - ETA: 23:32 - loss: 1.8102 - regression_loss: 1.3789 - classification_loss 7040/10000 [====================>.........] - ETA: 23:31 - loss: 1.8101 - regression_loss: 1.3789 - classification_loss 7041/10000 [====================>.........] - ETA: 23:31 - loss: 1.8103 - regression_loss: 1.3790 - classification_loss 7042/10000 [====================>.........] - ETA: 23:30 - loss: 1.8102 - regression_loss: 1.3790 - classification_loss 7043/10000 [====================>.........] - ETA: 23:30 - loss: 1.8102 - regression_loss: 1.3789 - classification_loss 7044/10000 [====================>.........] - ETA: 23:29 - loss: 1.8102 - regression_loss: 1.3790 - classification_loss 7045/10000 [====================>.........] - ETA: 23:29 - loss: 1.8100 - regression_loss: 1.3789 - classification_loss 7046/10000 [====================>.........] - ETA: 23:28 - loss: 1.8100 - regression_loss: 1.3789 - classification_loss 7047/10000 [====================>.........] - ETA: 23:28 - loss: 1.8100 - regression_loss: 1.3789 - classification_loss 7048/10000 [====================>.........] - ETA: 23:27 - loss: 1.8099 - regression_loss: 1.3788 - classification_loss 7049/10000 [====================>.........] - ETA: 23:27 - loss: 1.8099 - regression_loss: 1.3788 - classification_loss 7050/10000 [====================>.........] - ETA: 23:26 - loss: 1.8097 - regression_loss: 1.3787 - classification_loss 7051/10000 [====================>.........] - ETA: 23:26 - loss: 1.8098 - regression_loss: 1.3787 - classification_loss 7052/10000 [====================>.........] - ETA: 23:25 - loss: 1.8097 - regression_loss: 1.3787 - classification_loss 7053/10000 [====================>.........] - ETA: 23:25 - loss: 1.8097 - regression_loss: 1.3787 - classification_loss 7054/10000 [====================>.........] - ETA: 23:24 - loss: 1.8096 - regression_loss: 1.3786 - classification_loss 7055/10000 [====================>.........] - ETA: 23:24 - loss: 1.8095 - regression_loss: 1.3785 - classification_loss 7056/10000 [====================>.........] - ETA: 23:23 - loss: 1.8095 - regression_loss: 1.3785 - classification_loss 7057/10000 [====================>.........] - ETA: 23:23 - loss: 1.8094 - regression_loss: 1.3785 - classification_loss 7058/10000 [====================>.........] - ETA: 23:22 - loss: 1.8093 - regression_loss: 1.3784 - classification_loss 7059/10000 [====================>.........] - ETA: 23:22 - loss: 1.8092 - regression_loss: 1.3783 - classification_loss 7060/10000 [====================>.........] - ETA: 23:21 - loss: 1.8092 - regression_loss: 1.3782 - classification_loss 7061/10000 [====================>.........] - ETA: 23:21 - loss: 1.8091 - regression_loss: 1.3782 - classification_loss 7062/10000 [====================>.........] - ETA: 23:20 - loss: 1.8090 - regression_loss: 1.3781 - classification_loss 7063/10000 [====================>.........] - ETA: 23:20 - loss: 1.8089 - regression_loss: 1.3780 - classification_loss 7064/10000 [====================>.........] - ETA: 23:19 - loss: 1.8090 - regression_loss: 1.3781 - classification_loss 7065/10000 [====================>.........] - ETA: 23:19 - loss: 1.8089 - regression_loss: 1.3780 - classification_loss 7066/10000 [====================>.........] - ETA: 23:18 - loss: 1.8088 - regression_loss: 1.3780 - classification_loss 7067/10000 [====================>.........] - ETA: 23:18 - loss: 1.8089 - regression_loss: 1.3780 - classification_loss 7068/10000 [====================>.........] - ETA: 23:17 - loss: 1.8087 - regression_loss: 1.3779 - classification_loss 7069/10000 [====================>.........] - ETA: 23:17 - loss: 1.8087 - regression_loss: 1.3779 - classification_loss 7070/10000 [====================>.........] - ETA: 23:16 - loss: 1.8087 - regression_loss: 1.3779 - classification_loss 7071/10000 [====================>.........] - ETA: 23:16 - loss: 1.8087 - regression_loss: 1.3779 - classification_loss 7072/10000 [====================>.........] - ETA: 23:16 - loss: 1.8087 - regression_loss: 1.3779 - classification_loss 7073/10000 [====================>.........] - ETA: 23:15 - loss: 1.8088 - regression_loss: 1.3780 - classification_loss 7074/10000 [====================>.........] - ETA: 23:15 - loss: 1.8087 - regression_loss: 1.3779 - classification_loss 7075/10000 [====================>.........] - ETA: 23:14 - loss: 1.8088 - regression_loss: 1.3780 - classification_loss 7076/10000 [====================>.........] - ETA: 23:14 - loss: 1.8089 - regression_loss: 1.3781 - classification_loss 7077/10000 [====================>.........] - ETA: 23:13 - loss: 1.8089 - regression_loss: 1.3781 - classification_loss 7078/10000 [====================>.........] - ETA: 23:13 - loss: 1.8090 - regression_loss: 1.3781 - classification_loss 7079/10000 [====================>.........] - ETA: 23:12 - loss: 1.8088 - regression_loss: 1.3780 - classification_loss 7080/10000 [====================>.........] - ETA: 23:12 - loss: 1.8088 - regression_loss: 1.3780 - classification_loss 7081/10000 [====================>.........] - ETA: 23:11 - loss: 1.8088 - regression_loss: 1.3780 - classification_loss 7082/10000 [====================>.........] - ETA: 23:11 - loss: 1.8087 - regression_loss: 1.3780 - classification_loss 7083/10000 [====================>.........] - ETA: 23:10 - loss: 1.8086 - regression_loss: 1.3779 - classification_loss 7084/10000 [====================>.........] - ETA: 23:10 - loss: 1.8085 - regression_loss: 1.3778 - classification_loss 7085/10000 [====================>.........] - ETA: 23:09 - loss: 1.8085 - regression_loss: 1.3778 - classification_loss 7086/10000 [====================>.........] - ETA: 23:09 - loss: 1.8085 - regression_loss: 1.3778 - classification_loss 7087/10000 [====================>.........] - ETA: 23:08 - loss: 1.8084 - regression_loss: 1.3777 - classification_loss 7088/10000 [====================>.........] - ETA: 23:08 - loss: 1.8085 - regression_loss: 1.3778 - classification_loss 7089/10000 [====================>.........] - ETA: 23:07 - loss: 1.8084 - regression_loss: 1.3778 - classification_loss 7090/10000 [====================>.........] - ETA: 23:07 - loss: 1.8083 - regression_loss: 1.3777 - classification_loss 7091/10000 [====================>.........] - ETA: 23:06 - loss: 1.8082 - regression_loss: 1.3775 - classification_loss 7092/10000 [====================>.........] - ETA: 23:06 - loss: 1.8081 - regression_loss: 1.3775 - classification_loss 7093/10000 [====================>.........] - ETA: 23:05 - loss: 1.8080 - regression_loss: 1.3774 - classification_loss 7094/10000 [====================>.........] - ETA: 23:05 - loss: 1.8081 - regression_loss: 1.3775 - classification_loss 7095/10000 [====================>.........] - ETA: 23:04 - loss: 1.8080 - regression_loss: 1.3774 - classification_loss 7096/10000 [====================>.........] - ETA: 23:04 - loss: 1.8080 - regression_loss: 1.3774 - classification_loss 7097/10000 [====================>.........] - ETA: 23:03 - loss: 1.8080 - regression_loss: 1.3775 - classification_loss 7098/10000 [====================>.........] - ETA: 23:03 - loss: 1.8083 - regression_loss: 1.3777 - classification_loss 7099/10000 [====================>.........] - ETA: 23:02 - loss: 1.8084 - regression_loss: 1.3778 - classification_loss 7100/10000 [====================>.........] - ETA: 23:02 - loss: 1.8084 - regression_loss: 1.3778 - classification_loss 7101/10000 [====================>.........] - ETA: 23:01 - loss: 1.8083 - regression_loss: 1.3777 - classification_loss 7102/10000 [====================>.........] - ETA: 23:01 - loss: 1.8083 - regression_loss: 1.3777 - classification_loss 7103/10000 [====================>.........] - ETA: 23:00 - loss: 1.8082 - regression_loss: 1.3777 - classification_loss 7104/10000 [====================>.........] - ETA: 23:00 - loss: 1.8084 - regression_loss: 1.3778 - classification_loss 7105/10000 [====================>.........] - ETA: 23:00 - loss: 1.8083 - regression_loss: 1.3777 - classification_loss 7106/10000 [====================>.........] - ETA: 22:59 - loss: 1.8082 - regression_loss: 1.3776 - classification_loss 7107/10000 [====================>.........] - ETA: 22:59 - loss: 1.8081 - regression_loss: 1.3776 - classification_loss 7108/10000 [====================>.........] - ETA: 22:58 - loss: 1.8081 - regression_loss: 1.3775 - classification_loss 7109/10000 [====================>.........] - ETA: 22:58 - loss: 1.8080 - regression_loss: 1.3775 - classification_loss 7110/10000 [====================>.........] - ETA: 22:57 - loss: 1.8081 - regression_loss: 1.3776 - classification_loss 7111/10000 [====================>.........] - ETA: 22:57 - loss: 1.8081 - regression_loss: 1.3776 - classification_loss 7112/10000 [====================>.........] - ETA: 22:56 - loss: 1.8080 - regression_loss: 1.3775 - classification_loss 7113/10000 [====================>.........] - ETA: 22:56 - loss: 1.8079 - regression_loss: 1.3774 - classification_loss 7114/10000 [====================>.........] - ETA: 22:55 - loss: 1.8079 - regression_loss: 1.3774 - classification_loss 7115/10000 [====================>.........] - ETA: 22:55 - loss: 1.8078 - regression_loss: 1.3773 - classification_loss 7116/10000 [====================>.........] - ETA: 22:54 - loss: 1.8079 - regression_loss: 1.3773 - classification_loss 7117/10000 [====================>.........] - ETA: 22:54 - loss: 1.8079 - regression_loss: 1.3774 - classification_loss 7118/10000 [====================>.........] - ETA: 22:53 - loss: 1.8078 - regression_loss: 1.3773 - classification_loss 7119/10000 [====================>.........] - ETA: 22:53 - loss: 1.8078 - regression_loss: 1.3773 - classification_loss 7120/10000 [====================>.........] - ETA: 22:52 - loss: 1.8078 - regression_loss: 1.3772 - classification_loss 7121/10000 [====================>.........] - ETA: 22:52 - loss: 1.8079 - regression_loss: 1.3773 - classification_loss 7122/10000 [====================>.........] - ETA: 22:51 - loss: 1.8079 - regression_loss: 1.3774 - classification_loss 7123/10000 [====================>.........] - ETA: 22:51 - loss: 1.8079 - regression_loss: 1.3774 - classification_loss 7124/10000 [====================>.........] - ETA: 22:50 - loss: 1.8079 - regression_loss: 1.3774 - classification_loss 7125/10000 [====================>.........] - ETA: 22:50 - loss: 1.8079 - regression_loss: 1.3774 - classification_loss 7126/10000 [====================>.........] - ETA: 22:49 - loss: 1.8077 - regression_loss: 1.3772 - classification_loss 7127/10000 [====================>.........] - ETA: 22:49 - loss: 1.8077 - regression_loss: 1.3772 - classification_loss 7128/10000 [====================>.........] - ETA: 22:48 - loss: 1.8076 - regression_loss: 1.3771 - classification_loss 7129/10000 [====================>.........] - ETA: 22:48 - loss: 1.8076 - regression_loss: 1.3772 - classification_loss 7130/10000 [====================>.........] - ETA: 22:47 - loss: 1.8076 - regression_loss: 1.3772 - classification_loss 7131/10000 [====================>.........] - ETA: 22:47 - loss: 1.8076 - regression_loss: 1.3772 - classification_loss 7132/10000 [====================>.........] - ETA: 22:46 - loss: 1.8076 - regression_loss: 1.3772 - classification_loss 7133/10000 [====================>.........] - ETA: 22:46 - loss: 1.8076 - regression_loss: 1.3772 - classification_loss 7134/10000 [====================>.........] - ETA: 22:45 - loss: 1.8079 - regression_loss: 1.3774 - classification_loss 7135/10000 [====================>.........] - ETA: 22:45 - loss: 1.8079 - regression_loss: 1.3775 - classification_loss 7136/10000 [====================>.........] - ETA: 22:44 - loss: 1.8078 - regression_loss: 1.3774 - classification_loss 7137/10000 [====================>.........] - ETA: 22:44 - loss: 1.8078 - regression_loss: 1.3774 - classification_loss 7138/10000 [====================>.........] - ETA: 22:44 - loss: 1.8078 - regression_loss: 1.3774 - classification_loss 7139/10000 [====================>.........] - ETA: 22:43 - loss: 1.8078 - regression_loss: 1.3774 - classification_loss 7140/10000 [====================>.........] - ETA: 22:43 - loss: 1.8077 - regression_loss: 1.3772 - classification_loss 7141/10000 [====================>.........] - ETA: 22:42 - loss: 1.8079 - regression_loss: 1.3774 - classification_loss 7142/10000 [====================>.........] - ETA: 22:42 - loss: 1.8079 - regression_loss: 1.3774 - classification_loss 7143/10000 [====================>.........] - ETA: 22:41 - loss: 1.8078 - regression_loss: 1.3773 - classification_loss 7144/10000 [====================>.........] - ETA: 22:41 - loss: 1.8078 - regression_loss: 1.3773 - classification_loss 7145/10000 [====================>.........] - ETA: 22:40 - loss: 1.8076 - regression_loss: 1.3772 - classification_loss 7146/10000 [====================>.........] - ETA: 22:40 - loss: 1.8078 - regression_loss: 1.3773 - classification_loss 7147/10000 [====================>.........] - ETA: 22:39 - loss: 1.8078 - regression_loss: 1.3773 - classification_loss 7148/10000 [====================>.........] - ETA: 22:39 - loss: 1.8077 - regression_loss: 1.3772 - classification_loss 7149/10000 [====================>.........] - ETA: 22:38 - loss: 1.8076 - regression_loss: 1.3772 - classification_loss 7150/10000 [====================>.........] - ETA: 22:38 - loss: 1.8076 - regression_loss: 1.3772 - classification_loss 7151/10000 [====================>.........] - ETA: 22:37 - loss: 1.8078 - regression_loss: 1.3774 - classification_loss 7152/10000 [====================>.........] - ETA: 22:37 - loss: 1.8077 - regression_loss: 1.3773 - classification_loss 7153/10000 [====================>.........] - ETA: 22:36 - loss: 1.8078 - regression_loss: 1.3774 - classification_loss 7154/10000 [====================>.........] - ETA: 22:36 - loss: 1.8078 - regression_loss: 1.3774 - classification_loss 7155/10000 [====================>.........] - ETA: 22:35 - loss: 1.8078 - regression_loss: 1.3774 - classification_loss 7156/10000 [====================>.........] - ETA: 22:35 - loss: 1.8078 - regression_loss: 1.3773 - classification_loss 7157/10000 [====================>.........] - ETA: 22:34 - loss: 1.8076 - regression_loss: 1.3772 - classification_loss 7158/10000 [====================>.........] - ETA: 22:34 - loss: 1.8075 - regression_loss: 1.3771 - classification_loss 7159/10000 [====================>.........] - ETA: 22:33 - loss: 1.8075 - regression_loss: 1.3771 - classification_loss 7160/10000 [====================>.........] - ETA: 22:33 - loss: 1.8075 - regression_loss: 1.3771 - classification_loss 7161/10000 [====================>.........] - ETA: 22:32 - loss: 1.8074 - regression_loss: 1.3771 - classification_loss 7162/10000 [====================>.........] - ETA: 22:32 - loss: 1.8073 - regression_loss: 1.3770 - classification_loss 7163/10000 [====================>.........] - ETA: 22:31 - loss: 1.8073 - regression_loss: 1.3770 - classification_loss 7164/10000 [====================>.........] - ETA: 22:31 - loss: 1.8071 - regression_loss: 1.3769 - classification_loss 7165/10000 [====================>.........] - ETA: 22:31 - loss: 1.8071 - regression_loss: 1.3769 - classification_loss 7166/10000 [====================>.........] - ETA: 22:30 - loss: 1.8071 - regression_loss: 1.3769 - classification_loss 7167/10000 [====================>.........] - ETA: 22:30 - loss: 1.8072 - regression_loss: 1.3770 - classification_loss 7168/10000 [====================>.........] - ETA: 22:29 - loss: 1.8073 - regression_loss: 1.3770 - classification_loss 7169/10000 [====================>.........] - ETA: 22:29 - loss: 1.8073 - regression_loss: 1.3770 - classification_loss 7170/10000 [====================>.........] - ETA: 22:28 - loss: 1.8073 - regression_loss: 1.3770 - classification_loss 7171/10000 [====================>.........] - ETA: 22:28 - loss: 1.8074 - regression_loss: 1.3771 - classification_loss 7172/10000 [====================>.........] - ETA: 22:27 - loss: 1.8073 - regression_loss: 1.3770 - classification_loss 7173/10000 [====================>.........] - ETA: 22:27 - loss: 1.8073 - regression_loss: 1.3770 - classification_loss 7174/10000 [====================>.........] - ETA: 22:26 - loss: 1.8073 - regression_loss: 1.3769 - classification_loss 7175/10000 [====================>.........] - ETA: 22:26 - loss: 1.8075 - regression_loss: 1.3770 - classification_loss 7176/10000 [====================>.........] - ETA: 22:25 - loss: 1.8074 - regression_loss: 1.3770 - classification_loss 7177/10000 [====================>.........] - ETA: 22:25 - loss: 1.8075 - regression_loss: 1.3771 - classification_loss 7178/10000 [====================>.........] - ETA: 22:24 - loss: 1.8075 - regression_loss: 1.3771 - classification_loss 7179/10000 [====================>.........] - ETA: 22:24 - loss: 1.8076 - regression_loss: 1.3771 - classification_loss 7180/10000 [====================>.........] - ETA: 22:23 - loss: 1.8075 - regression_loss: 1.3771 - classification_loss 7181/10000 [====================>.........] - ETA: 22:23 - loss: 1.8074 - regression_loss: 1.3770 - classification_loss 7182/10000 [====================>.........] - ETA: 22:22 - loss: 1.8074 - regression_loss: 1.3770 - classification_loss 7183/10000 [====================>.........] - ETA: 22:22 - loss: 1.8075 - regression_loss: 1.3771 - classification_loss 7184/10000 [====================>.........] - ETA: 22:21 - loss: 1.8074 - regression_loss: 1.3770 - classification_loss 7185/10000 [====================>.........] - ETA: 22:21 - loss: 1.8073 - regression_loss: 1.3770 - classification_loss 7186/10000 [====================>.........] - ETA: 22:20 - loss: 1.8074 - regression_loss: 1.3770 - classification_loss 7187/10000 [====================>.........] - ETA: 22:20 - loss: 1.8074 - regression_loss: 1.3770 - classification_loss 7188/10000 [====================>.........] - ETA: 22:19 - loss: 1.8073 - regression_loss: 1.3770 - classification_loss 7189/10000 [====================>.........] - ETA: 22:19 - loss: 1.8074 - regression_loss: 1.3771 - classification_loss 7190/10000 [====================>.........] - ETA: 22:18 - loss: 1.8073 - regression_loss: 1.3770 - classification_loss 7191/10000 [====================>.........] - ETA: 22:18 - loss: 1.8074 - regression_loss: 1.3771 - classification_loss 7192/10000 [====================>.........] - ETA: 22:17 - loss: 1.8074 - regression_loss: 1.3770 - classification_loss 7193/10000 [====================>.........] - ETA: 22:17 - loss: 1.8073 - regression_loss: 1.3770 - classification_loss 7194/10000 [====================>.........] - ETA: 22:16 - loss: 1.8073 - regression_loss: 1.3770 - classification_loss 7195/10000 [====================>.........] - ETA: 22:16 - loss: 1.8071 - regression_loss: 1.3769 - classification_loss 7196/10000 [====================>.........] - ETA: 22:15 - loss: 1.8072 - regression_loss: 1.3769 - classification_loss 7197/10000 [====================>.........] - ETA: 22:15 - loss: 1.8071 - regression_loss: 1.3769 - classification_loss 7198/10000 [====================>.........] - ETA: 22:15 - loss: 1.8071 - regression_loss: 1.3769 - classification_loss 7199/10000 [====================>.........] - ETA: 22:14 - loss: 1.8072 - regression_loss: 1.3769 - classification_loss 7200/10000 [====================>.........] - ETA: 22:14 - loss: 1.8071 - regression_loss: 1.3769 - classification_loss 7201/10000 [====================>.........] - ETA: 22:13 - loss: 1.8071 - regression_loss: 1.3769 - classification_loss 7202/10000 [====================>.........] - ETA: 22:13 - loss: 1.8070 - regression_loss: 1.3768 - classification_loss 7203/10000 [====================>.........] - ETA: 22:12 - loss: 1.8070 - regression_loss: 1.3768 - classification_loss 7204/10000 [====================>.........] - ETA: 22:12 - loss: 1.8070 - regression_loss: 1.3768 - classification_loss 7205/10000 [====================>.........] - ETA: 22:11 - loss: 1.8068 - regression_loss: 1.3767 - classification_loss 7206/10000 [====================>.........] - ETA: 22:11 - loss: 1.8067 - regression_loss: 1.3765 - classification_loss 7207/10000 [====================>.........] - ETA: 22:10 - loss: 1.8067 - regression_loss: 1.3766 - classification_loss 7208/10000 [====================>.........] - ETA: 22:10 - loss: 1.8065 - regression_loss: 1.3764 - classification_loss 7209/10000 [====================>.........] - ETA: 22:09 - loss: 1.8064 - regression_loss: 1.3764 - classification_loss 7210/10000 [====================>.........] - ETA: 22:09 - loss: 1.8064 - regression_loss: 1.3764 - classification_loss 7211/10000 [====================>.........] - ETA: 22:08 - loss: 1.8064 - regression_loss: 1.3763 - classification_loss 7212/10000 [====================>.........] - ETA: 22:08 - loss: 1.8064 - regression_loss: 1.3764 - classification_loss 7213/10000 [====================>.........] - ETA: 22:07 - loss: 1.8064 - regression_loss: 1.3763 - classification_loss 7214/10000 [====================>.........] - ETA: 22:07 - loss: 1.8063 - regression_loss: 1.3762 - classification_loss 7215/10000 [====================>.........] - ETA: 22:06 - loss: 1.8063 - regression_loss: 1.3762 - classification_loss 7216/10000 [====================>.........] - ETA: 22:06 - loss: 1.8063 - regression_loss: 1.3762 - classification_loss 7217/10000 [====================>.........] - ETA: 22:05 - loss: 1.8064 - regression_loss: 1.3762 - classification_loss 7218/10000 [====================>.........] - ETA: 22:05 - loss: 1.8063 - regression_loss: 1.3762 - classification_loss 7219/10000 [====================>.........] - ETA: 22:04 - loss: 1.8061 - regression_loss: 1.3761 - classification_loss 7220/10000 [====================>.........] - ETA: 22:04 - loss: 1.8061 - regression_loss: 1.3760 - classification_loss 7221/10000 [====================>.........] - ETA: 22:03 - loss: 1.8061 - regression_loss: 1.3760 - classification_loss 7222/10000 [====================>.........] - ETA: 22:03 - loss: 1.8061 - regression_loss: 1.3760 - classification_loss 7223/10000 [====================>.........] - ETA: 22:03 - loss: 1.8061 - regression_loss: 1.3761 - classification_loss 7224/10000 [====================>.........] - ETA: 22:02 - loss: 1.8061 - regression_loss: 1.3760 - classification_loss 7225/10000 [====================>.........] - ETA: 22:02 - loss: 1.8061 - regression_loss: 1.3760 - classification_loss 7226/10000 [====================>.........] - ETA: 22:01 - loss: 1.8061 - regression_loss: 1.3760 - classification_loss 7227/10000 [====================>.........] - ETA: 22:01 - loss: 1.8060 - regression_loss: 1.3760 - classification_loss 7228/10000 [====================>.........] - ETA: 22:00 - loss: 1.8060 - regression_loss: 1.3760 - classification_loss 7229/10000 [====================>.........] - ETA: 22:00 - loss: 1.8061 - regression_loss: 1.3761 - classification_loss 7230/10000 [====================>.........] - ETA: 21:59 - loss: 1.8060 - regression_loss: 1.3760 - classification_loss 7231/10000 [====================>.........] - ETA: 21:59 - loss: 1.8060 - regression_loss: 1.3760 - classification_loss 7232/10000 [====================>.........] - ETA: 21:58 - loss: 1.8060 - regression_loss: 1.3760 - classification_loss 7233/10000 [====================>.........] - ETA: 21:58 - loss: 1.8060 - regression_loss: 1.3760 - classification_loss 7234/10000 [====================>.........] - ETA: 21:57 - loss: 1.8059 - regression_loss: 1.3760 - classification_loss 7235/10000 [====================>.........] - ETA: 21:57 - loss: 1.8059 - regression_loss: 1.3759 - classification_loss 7236/10000 [====================>.........] - ETA: 21:56 - loss: 1.8059 - regression_loss: 1.3759 - classification_loss 7237/10000 [====================>.........] - ETA: 21:56 - loss: 1.8059 - regression_loss: 1.3760 - classification_loss 7238/10000 [====================>.........] - ETA: 21:55 - loss: 1.8061 - regression_loss: 1.3761 - classification_loss 7239/10000 [====================>.........] - ETA: 21:55 - loss: 1.8059 - regression_loss: 1.3760 - classification_loss 7240/10000 [====================>.........] - ETA: 21:54 - loss: 1.8058 - regression_loss: 1.3759 - classification_loss 7241/10000 [====================>.........] - ETA: 21:54 - loss: 1.8059 - regression_loss: 1.3760 - classification_loss 7242/10000 [====================>.........] - ETA: 21:53 - loss: 1.8059 - regression_loss: 1.3759 - classification_loss 7243/10000 [====================>.........] - ETA: 21:53 - loss: 1.8060 - regression_loss: 1.3760 - classification_loss 7244/10000 [====================>.........] - ETA: 21:52 - loss: 1.8060 - regression_loss: 1.3761 - classification_loss 7245/10000 [====================>.........] - ETA: 21:52 - loss: 1.8060 - regression_loss: 1.3761 - classification_loss 7246/10000 [====================>.........] - ETA: 21:51 - loss: 1.8061 - regression_loss: 1.3762 - classification_loss 7247/10000 [====================>.........] - ETA: 21:51 - loss: 1.8061 - regression_loss: 1.3762 - classification_loss 7248/10000 [====================>.........] - ETA: 21:50 - loss: 1.8064 - regression_loss: 1.3764 - classification_loss 7249/10000 [====================>.........] - ETA: 21:50 - loss: 1.8063 - regression_loss: 1.3764 - classification_loss 7250/10000 [====================>.........] - ETA: 21:49 - loss: 1.8064 - regression_loss: 1.3764 - classification_loss 7251/10000 [====================>.........] - ETA: 21:49 - loss: 1.8065 - regression_loss: 1.3765 - classification_loss 7252/10000 [====================>.........] - ETA: 21:49 - loss: 1.8064 - regression_loss: 1.3764 - classification_loss 7253/10000 [====================>.........] - ETA: 21:48 - loss: 1.8064 - regression_loss: 1.3764 - classification_loss 7254/10000 [====================>.........] - ETA: 21:48 - loss: 1.8062 - regression_loss: 1.3763 - classification_loss 7255/10000 [====================>.........] - ETA: 21:47 - loss: 1.8062 - regression_loss: 1.3763 - classification_loss 7256/10000 [====================>.........] - ETA: 21:47 - loss: 1.8062 - regression_loss: 1.3763 - classification_loss 7257/10000 [====================>.........] - ETA: 21:46 - loss: 1.8063 - regression_loss: 1.3763 - classification_loss 7258/10000 [====================>.........] - ETA: 21:46 - loss: 1.8062 - regression_loss: 1.3762 - classification_loss 7259/10000 [====================>.........] - ETA: 21:45 - loss: 1.8061 - regression_loss: 1.3762 - classification_loss 7260/10000 [====================>.........] - ETA: 21:45 - loss: 1.8061 - regression_loss: 1.3762 - classification_loss 7261/10000 [====================>.........] - ETA: 21:44 - loss: 1.8061 - regression_loss: 1.3762 - classification_loss 7262/10000 [====================>.........] - ETA: 21:44 - loss: 1.8060 - regression_loss: 1.3761 - classification_loss 7263/10000 [====================>.........] - ETA: 21:43 - loss: 1.8060 - regression_loss: 1.3761 - classification_loss 7264/10000 [====================>.........] - ETA: 21:43 - loss: 1.8060 - regression_loss: 1.3761 - classification_loss 7265/10000 [====================>.........] - ETA: 21:42 - loss: 1.8060 - regression_loss: 1.3761 - classification_loss 7266/10000 [====================>.........] - ETA: 21:42 - loss: 1.8059 - regression_loss: 1.3760 - classification_loss 7267/10000 [====================>.........] - ETA: 21:41 - loss: 1.8058 - regression_loss: 1.3759 - classification_loss 7268/10000 [====================>.........] - ETA: 21:41 - loss: 1.8057 - regression_loss: 1.3759 - classification_loss 7269/10000 [====================>.........] - ETA: 21:40 - loss: 1.8057 - regression_loss: 1.3759 - classification_loss 7270/10000 [====================>.........] - ETA: 21:40 - loss: 1.8056 - regression_loss: 1.3757 - classification_loss 7271/10000 [====================>.........] - ETA: 21:39 - loss: 1.8054 - regression_loss: 1.3756 - classification_loss 7272/10000 [====================>.........] - ETA: 21:39 - loss: 1.8053 - regression_loss: 1.3755 - classification_loss 7273/10000 [====================>.........] - ETA: 21:38 - loss: 1.8054 - regression_loss: 1.3756 - classification_loss 7274/10000 [====================>.........] - ETA: 21:38 - loss: 1.8054 - regression_loss: 1.3756 - classification_loss 7275/10000 [====================>.........] - ETA: 21:37 - loss: 1.8055 - regression_loss: 1.3757 - classification_loss 7276/10000 [====================>.........] - ETA: 21:37 - loss: 1.8057 - regression_loss: 1.3757 - classification_loss 7277/10000 [====================>.........] - ETA: 21:36 - loss: 1.8055 - regression_loss: 1.3756 - classification_loss 7278/10000 [====================>.........] - ETA: 21:36 - loss: 1.8055 - regression_loss: 1.3756 - classification_loss 7279/10000 [====================>.........] - ETA: 21:35 - loss: 1.8054 - regression_loss: 1.3756 - classification_loss 7280/10000 [====================>.........] - ETA: 21:35 - loss: 1.8055 - regression_loss: 1.3756 - classification_loss 7281/10000 [====================>.........] - ETA: 21:35 - loss: 1.8055 - regression_loss: 1.3757 - classification_loss 7282/10000 [====================>.........] - ETA: 21:34 - loss: 1.8054 - regression_loss: 1.3755 - classification_loss 7283/10000 [====================>.........] - ETA: 21:34 - loss: 1.8054 - regression_loss: 1.3756 - classification_loss 7284/10000 [====================>.........] - ETA: 21:33 - loss: 1.8054 - regression_loss: 1.3756 - classification_loss 7285/10000 [====================>.........] - ETA: 21:33 - loss: 1.8054 - regression_loss: 1.3756 - classification_loss 7286/10000 [====================>.........] - ETA: 21:32 - loss: 1.8055 - regression_loss: 1.3756 - classification_loss 7287/10000 [====================>.........] - ETA: 21:32 - loss: 1.8055 - regression_loss: 1.3757 - classification_loss 7288/10000 [====================>.........] - ETA: 21:31 - loss: 1.8056 - regression_loss: 1.3758 - classification_loss 7289/10000 [====================>.........] - ETA: 21:31 - loss: 1.8056 - regression_loss: 1.3758 - classification_loss 7290/10000 [====================>.........] - ETA: 21:30 - loss: 1.8055 - regression_loss: 1.3757 - classification_loss 7291/10000 [====================>.........] - ETA: 21:30 - loss: 1.8055 - regression_loss: 1.3757 - classification_loss 7292/10000 [====================>.........] - ETA: 21:29 - loss: 1.8054 - regression_loss: 1.3756 - classification_loss 7293/10000 [====================>.........] - ETA: 21:29 - loss: 1.8055 - regression_loss: 1.3757 - classification_loss 7294/10000 [====================>.........] - ETA: 21:28 - loss: 1.8055 - regression_loss: 1.3756 - classification_loss 7295/10000 [====================>.........] - ETA: 21:28 - loss: 1.8055 - regression_loss: 1.3757 - classification_loss 7296/10000 [====================>.........] - ETA: 21:27 - loss: 1.8056 - regression_loss: 1.3758 - classification_loss 7297/10000 [====================>.........] - ETA: 21:27 - loss: 1.8054 - regression_loss: 1.3757 - classification_loss 7298/10000 [====================>.........] - ETA: 21:26 - loss: 1.8054 - regression_loss: 1.3756 - classification_loss 7299/10000 [====================>.........] - ETA: 21:26 - loss: 1.8054 - regression_loss: 1.3757 - classification_loss 7300/10000 [====================>.........] - ETA: 21:25 - loss: 1.8055 - regression_loss: 1.3757 - classification_loss 7301/10000 [====================>.........] - ETA: 21:25 - loss: 1.8054 - regression_loss: 1.3757 - classification_loss 7302/10000 [====================>.........] - ETA: 21:24 - loss: 1.8053 - regression_loss: 1.3756 - classification_loss 7303/10000 [====================>.........] - ETA: 21:24 - loss: 1.8053 - regression_loss: 1.3756 - classification_loss 7304/10000 [====================>.........] - ETA: 21:23 - loss: 1.8054 - regression_loss: 1.3756 - classification_loss 7305/10000 [====================>.........] - ETA: 21:23 - loss: 1.8054 - regression_loss: 1.3756 - classification_loss 7306/10000 [====================>.........] - ETA: 21:22 - loss: 1.8053 - regression_loss: 1.3756 - classification_loss 7307/10000 [====================>.........] - ETA: 21:22 - loss: 1.8054 - regression_loss: 1.3756 - classification_loss 7308/10000 [====================>.........] - ETA: 21:21 - loss: 1.8053 - regression_loss: 1.3756 - classification_loss 7309/10000 [====================>.........] - ETA: 21:21 - loss: 1.8054 - regression_loss: 1.3756 - classification_loss 7310/10000 [====================>.........] - ETA: 21:20 - loss: 1.8054 - regression_loss: 1.3756 - classification_loss 7311/10000 [====================>.........] - ETA: 21:20 - loss: 1.8054 - regression_loss: 1.3757 - classification_loss 7312/10000 [====================>.........] - ETA: 21:20 - loss: 1.8054 - regression_loss: 1.3757 - classification_loss 7313/10000 [====================>.........] - ETA: 21:19 - loss: 1.8053 - regression_loss: 1.3756 - classification_loss 7314/10000 [====================>.........] - ETA: 21:19 - loss: 1.8052 - regression_loss: 1.3755 - classification_loss 7315/10000 [====================>.........] - ETA: 21:18 - loss: 1.8052 - regression_loss: 1.3755 - classification_loss 7316/10000 [====================>.........] - ETA: 21:18 - loss: 1.8052 - regression_loss: 1.3755 - classification_loss 7317/10000 [====================>.........] - ETA: 21:17 - loss: 1.8051 - regression_loss: 1.3754 - classification_loss 7318/10000 [====================>.........] - ETA: 21:17 - loss: 1.8051 - regression_loss: 1.3754 - classification_loss 7319/10000 [====================>.........] - ETA: 21:16 - loss: 1.8051 - regression_loss: 1.3755 - classification_loss 7320/10000 [====================>.........] - ETA: 21:16 - loss: 1.8050 - regression_loss: 1.3754 - classification_loss 7321/10000 [====================>.........] - ETA: 21:15 - loss: 1.8049 - regression_loss: 1.3753 - classification_loss 7322/10000 [====================>.........] - ETA: 21:15 - loss: 1.8048 - regression_loss: 1.3752 - classification_loss 7323/10000 [====================>.........] - ETA: 21:14 - loss: 1.8047 - regression_loss: 1.3752 - classification_loss 7324/10000 [====================>.........] - ETA: 21:14 - loss: 1.8048 - regression_loss: 1.3752 - classification_loss 7325/10000 [====================>.........] - ETA: 21:13 - loss: 1.8048 - regression_loss: 1.3753 - classification_loss 7326/10000 [====================>.........] - ETA: 21:13 - loss: 1.8048 - regression_loss: 1.3752 - classification_loss 7327/10000 [====================>.........] - ETA: 21:12 - loss: 1.8048 - regression_loss: 1.3752 - classification_loss 7328/10000 [====================>.........] - ETA: 21:12 - loss: 1.8048 - regression_loss: 1.3752 - classification_loss 7329/10000 [====================>.........] - ETA: 21:11 - loss: 1.8048 - regression_loss: 1.3753 - classification_loss 7330/10000 [====================>.........] - ETA: 21:11 - loss: 1.8047 - regression_loss: 1.3752 - classification_loss 7331/10000 [====================>.........] - ETA: 21:10 - loss: 1.8046 - regression_loss: 1.3751 - classification_loss 7332/10000 [====================>.........] - ETA: 21:10 - loss: 1.8048 - regression_loss: 1.3752 - classification_loss 7333/10000 [====================>.........] - ETA: 21:09 - loss: 1.8047 - regression_loss: 1.3752 - classification_loss 7334/10000 [=====================>........] - ETA: 21:09 - loss: 1.8047 - regression_loss: 1.3751 - classification_loss 7335/10000 [=====================>........] - ETA: 21:08 - loss: 1.8047 - regression_loss: 1.3751 - classification_loss 7336/10000 [=====================>........] - ETA: 21:08 - loss: 1.8047 - regression_loss: 1.3752 - classification_loss 7337/10000 [=====================>........] - ETA: 21:07 - loss: 1.8046 - regression_loss: 1.3751 - classification_loss 7338/10000 [=====================>........] - ETA: 21:07 - loss: 1.8047 - regression_loss: 1.3752 - classification_loss 7339/10000 [=====================>........] - ETA: 21:06 - loss: 1.8045 - regression_loss: 1.3751 - classification_loss 7340/10000 [=====================>........] - ETA: 21:06 - loss: 1.8046 - regression_loss: 1.3751 - classification_loss 7341/10000 [=====================>........] - ETA: 21:06 - loss: 1.8045 - regression_loss: 1.3750 - classification_loss 7342/10000 [=====================>........] - ETA: 21:05 - loss: 1.8045 - regression_loss: 1.3750 - classification_loss 7343/10000 [=====================>........] - ETA: 21:05 - loss: 1.8044 - regression_loss: 1.3749 - classification_loss 7344/10000 [=====================>........] - ETA: 21:04 - loss: 1.8045 - regression_loss: 1.3750 - classification_loss 7345/10000 [=====================>........] - ETA: 21:04 - loss: 1.8045 - regression_loss: 1.3751 - classification_loss 7346/10000 [=====================>........] - ETA: 21:03 - loss: 1.8045 - regression_loss: 1.3750 - classification_loss 7347/10000 [=====================>........] - ETA: 21:03 - loss: 1.8046 - regression_loss: 1.3751 - classification_loss 7348/10000 [=====================>........] - ETA: 21:02 - loss: 1.8046 - regression_loss: 1.3751 - classification_loss 7349/10000 [=====================>........] - ETA: 21:02 - loss: 1.8045 - regression_loss: 1.3751 - classification_loss 7350/10000 [=====================>........] - ETA: 21:01 - loss: 1.8044 - regression_loss: 1.3750 - classification_loss 7351/10000 [=====================>........] - ETA: 21:01 - loss: 1.8044 - regression_loss: 1.3750 - classification_loss 7352/10000 [=====================>........] - ETA: 21:00 - loss: 1.8044 - regression_loss: 1.3750 - classification_loss 7353/10000 [=====================>........] - ETA: 21:00 - loss: 1.8045 - regression_loss: 1.3750 - classification_loss 7354/10000 [=====================>........] - ETA: 20:59 - loss: 1.8045 - regression_loss: 1.3750 - classification_loss 7355/10000 [=====================>........] - ETA: 20:59 - loss: 1.8048 - regression_loss: 1.3748 - classification_loss 7356/10000 [=====================>........] - ETA: 20:58 - loss: 1.8049 - regression_loss: 1.3749 - classification_loss 7357/10000 [=====================>........] - ETA: 20:58 - loss: 1.8047 - regression_loss: 1.3748 - classification_loss 7358/10000 [=====================>........] - ETA: 20:57 - loss: 1.8047 - regression_loss: 1.3748 - classification_loss 7359/10000 [=====================>........] - ETA: 20:57 - loss: 1.8047 - regression_loss: 1.3748 - classification_loss 7360/10000 [=====================>........] - ETA: 20:56 - loss: 1.8046 - regression_loss: 1.3748 - classification_loss 7361/10000 [=====================>........] - ETA: 20:56 - loss: 1.8047 - regression_loss: 1.3748 - classification_loss 7362/10000 [=====================>........] - ETA: 20:55 - loss: 1.8045 - regression_loss: 1.3747 - classification_loss 7363/10000 [=====================>........] - ETA: 20:55 - loss: 1.8045 - regression_loss: 1.3747 - classification_loss 7364/10000 [=====================>........] - ETA: 20:54 - loss: 1.8045 - regression_loss: 1.3746 - classification_loss 7365/10000 [=====================>........] - ETA: 20:54 - loss: 1.8045 - regression_loss: 1.3746 - classification_loss 7366/10000 [=====================>........] - ETA: 20:53 - loss: 1.8045 - regression_loss: 1.3747 - classification_loss 7367/10000 [=====================>........] - ETA: 20:53 - loss: 1.8045 - regression_loss: 1.3747 - classification_loss 7368/10000 [=====================>........] - ETA: 20:53 - loss: 1.8045 - regression_loss: 1.3747 - classification_loss 7369/10000 [=====================>........] - ETA: 20:52 - loss: 1.8044 - regression_loss: 1.3746 - classification_loss 7370/10000 [=====================>........] - ETA: 20:52 - loss: 1.8045 - regression_loss: 1.3747 - classification_loss 7371/10000 [=====================>........] - ETA: 20:51 - loss: 1.8043 - regression_loss: 1.3746 - classification_loss 7372/10000 [=====================>........] - ETA: 20:51 - loss: 1.8043 - regression_loss: 1.3745 - classification_loss 7373/10000 [=====================>........] - ETA: 20:50 - loss: 1.8043 - regression_loss: 1.3745 - classification_loss 7374/10000 [=====================>........] - ETA: 20:50 - loss: 1.8043 - regression_loss: 1.3745 - classification_loss 7375/10000 [=====================>........] - ETA: 20:49 - loss: 1.8042 - regression_loss: 1.3745 - classification_loss 7376/10000 [=====================>........] - ETA: 20:49 - loss: 1.8041 - regression_loss: 1.3744 - classification_loss 7377/10000 [=====================>........] - ETA: 20:48 - loss: 1.8040 - regression_loss: 1.3743 - classification_loss 7378/10000 [=====================>........] - ETA: 20:48 - loss: 1.8041 - regression_loss: 1.3743 - classification_loss 7379/10000 [=====================>........] - ETA: 20:47 - loss: 1.8042 - regression_loss: 1.3745 - classification_loss 7380/10000 [=====================>........] - ETA: 20:47 - loss: 1.8042 - regression_loss: 1.3745 - classification_loss 7381/10000 [=====================>........] - ETA: 20:46 - loss: 1.8041 - regression_loss: 1.3744 - classification_loss 7382/10000 [=====================>........] - ETA: 20:46 - loss: 1.8040 - regression_loss: 1.3743 - classification_loss 7383/10000 [=====================>........] - ETA: 20:45 - loss: 1.8039 - regression_loss: 1.3742 - classification_loss 7384/10000 [=====================>........] - ETA: 20:45 - loss: 1.8039 - regression_loss: 1.3742 - classification_loss 7385/10000 [=====================>........] - ETA: 20:44 - loss: 1.8038 - regression_loss: 1.3741 - classification_loss 7386/10000 [=====================>........] - ETA: 20:44 - loss: 1.8038 - regression_loss: 1.3741 - classification_loss 7387/10000 [=====================>........] - ETA: 20:43 - loss: 1.8037 - regression_loss: 1.3740 - classification_loss 7388/10000 [=====================>........] - ETA: 20:43 - loss: 1.8037 - regression_loss: 1.3740 - classification_loss 7389/10000 [=====================>........] - ETA: 20:42 - loss: 1.8036 - regression_loss: 1.3740 - classification_loss 7390/10000 [=====================>........] - ETA: 20:42 - loss: 1.8035 - regression_loss: 1.3739 - classification_loss 7391/10000 [=====================>........] - ETA: 20:42 - loss: 1.8035 - regression_loss: 1.3739 - classification_loss 7392/10000 [=====================>........] - ETA: 20:41 - loss: 1.8035 - regression_loss: 1.3739 - classification_loss 7393/10000 [=====================>........] - ETA: 20:41 - loss: 1.8035 - regression_loss: 1.3739 - classification_loss 7394/10000 [=====================>........] - ETA: 20:40 - loss: 1.8035 - regression_loss: 1.3739 - classification_loss 7395/10000 [=====================>........] - ETA: 20:40 - loss: 1.8034 - regression_loss: 1.3738 - classification_loss 7396/10000 [=====================>........] - ETA: 20:39 - loss: 1.8034 - regression_loss: 1.3738 - classification_loss 7397/10000 [=====================>........] - ETA: 20:39 - loss: 1.8034 - regression_loss: 1.3738 - classification_loss 7398/10000 [=====================>........] - ETA: 20:38 - loss: 1.8033 - regression_loss: 1.3737 - classification_loss 7399/10000 [=====================>........] - ETA: 20:38 - loss: 1.8034 - regression_loss: 1.3738 - classification_loss 7400/10000 [=====================>........] - ETA: 20:37 - loss: 1.8034 - regression_loss: 1.3738 - classification_loss 7401/10000 [=====================>........] - ETA: 20:37 - loss: 1.8033 - regression_loss: 1.3737 - classification_loss 7402/10000 [=====================>........] - ETA: 20:36 - loss: 1.8032 - regression_loss: 1.3737 - classification_loss 7403/10000 [=====================>........] - ETA: 20:36 - loss: 1.8033 - regression_loss: 1.3737 - classification_loss 7404/10000 [=====================>........] - ETA: 20:35 - loss: 1.8033 - regression_loss: 1.3738 - classification_loss 7405/10000 [=====================>........] - ETA: 20:35 - loss: 1.8032 - regression_loss: 1.3737 - classification_loss 7406/10000 [=====================>........] - ETA: 20:34 - loss: 1.8033 - regression_loss: 1.3737 - classification_loss 7407/10000 [=====================>........] - ETA: 20:34 - loss: 1.8033 - regression_loss: 1.3737 - classification_loss 7408/10000 [=====================>........] - ETA: 20:33 - loss: 1.8033 - regression_loss: 1.3738 - classification_loss 7409/10000 [=====================>........] - ETA: 20:33 - loss: 1.8033 - regression_loss: 1.3738 - classification_loss 7410/10000 [=====================>........] - ETA: 20:32 - loss: 1.8033 - regression_loss: 1.3738 - classification_loss 7411/10000 [=====================>........] - ETA: 20:32 - loss: 1.8032 - regression_loss: 1.3737 - classification_loss 7412/10000 [=====================>........] - ETA: 20:31 - loss: 1.8031 - regression_loss: 1.3736 - classification_loss 7413/10000 [=====================>........] - ETA: 20:31 - loss: 1.8032 - regression_loss: 1.3736 - classification_loss 7414/10000 [=====================>........] - ETA: 20:30 - loss: 1.8031 - regression_loss: 1.3736 - classification_loss 7415/10000 [=====================>........] - ETA: 20:30 - loss: 1.8030 - regression_loss: 1.3735 - classification_loss 7416/10000 [=====================>........] - ETA: 20:29 - loss: 1.8029 - regression_loss: 1.3734 - classification_loss 7417/10000 [=====================>........] - ETA: 20:29 - loss: 1.8029 - regression_loss: 1.3735 - classification_loss 7418/10000 [=====================>........] - ETA: 20:28 - loss: 1.8029 - regression_loss: 1.3734 - classification_loss 7419/10000 [=====================>........] - ETA: 20:28 - loss: 1.8028 - regression_loss: 1.3733 - classification_loss 7420/10000 [=====================>........] - ETA: 20:28 - loss: 1.8029 - regression_loss: 1.3734 - classification_loss 7421/10000 [=====================>........] - ETA: 20:27 - loss: 1.8029 - regression_loss: 1.3735 - classification_loss 7422/10000 [=====================>........] - ETA: 20:27 - loss: 1.8030 - regression_loss: 1.3735 - classification_loss 7423/10000 [=====================>........] - ETA: 20:26 - loss: 1.8030 - regression_loss: 1.3735 - classification_loss 7424/10000 [=====================>........] - ETA: 20:26 - loss: 1.8029 - regression_loss: 1.3735 - classification_loss 7425/10000 [=====================>........] - ETA: 20:25 - loss: 1.8028 - regression_loss: 1.3734 - classification_loss 7426/10000 [=====================>........] - ETA: 20:25 - loss: 1.8028 - regression_loss: 1.3734 - classification_loss 7427/10000 [=====================>........] - ETA: 20:24 - loss: 1.8028 - regression_loss: 1.3734 - classification_loss 7428/10000 [=====================>........] - ETA: 20:24 - loss: 1.8026 - regression_loss: 1.3732 - classification_loss 7429/10000 [=====================>........] - ETA: 20:23 - loss: 1.8027 - regression_loss: 1.3733 - classification_loss 7430/10000 [=====================>........] - ETA: 20:23 - loss: 1.8028 - regression_loss: 1.3734 - classification_loss 7431/10000 [=====================>........] - ETA: 20:22 - loss: 1.8026 - regression_loss: 1.3732 - classification_loss 7432/10000 [=====================>........] - ETA: 20:22 - loss: 1.8025 - regression_loss: 1.3732 - classification_loss 7433/10000 [=====================>........] - ETA: 20:21 - loss: 1.8023 - regression_loss: 1.3730 - classification_loss 7434/10000 [=====================>........] - ETA: 20:21 - loss: 1.8022 - regression_loss: 1.3729 - classification_loss 7435/10000 [=====================>........] - ETA: 20:20 - loss: 1.8023 - regression_loss: 1.3729 - classification_loss 7436/10000 [=====================>........] - ETA: 20:20 - loss: 1.8023 - regression_loss: 1.3729 - classification_loss 7437/10000 [=====================>........] - ETA: 20:19 - loss: 1.8023 - regression_loss: 1.3729 - classification_loss 7438/10000 [=====================>........] - ETA: 20:19 - loss: 1.8023 - regression_loss: 1.3729 - classification_loss 7439/10000 [=====================>........] - ETA: 20:18 - loss: 1.8023 - regression_loss: 1.3729 - classification_loss 7440/10000 [=====================>........] - ETA: 20:18 - loss: 1.8022 - regression_loss: 1.3728 - classification_loss 7441/10000 [=====================>........] - ETA: 20:17 - loss: 1.8021 - regression_loss: 1.3727 - classification_loss 7442/10000 [=====================>........] - ETA: 20:17 - loss: 1.8021 - regression_loss: 1.3727 - classification_loss 7443/10000 [=====================>........] - ETA: 20:16 - loss: 1.8019 - regression_loss: 1.3726 - classification_loss 7444/10000 [=====================>........] - ETA: 20:16 - loss: 1.8021 - regression_loss: 1.3727 - classification_loss 7445/10000 [=====================>........] - ETA: 20:15 - loss: 1.8021 - regression_loss: 1.3727 - classification_loss 7446/10000 [=====================>........] - ETA: 20:15 - loss: 1.8019 - regression_loss: 1.3725 - classification_loss 7447/10000 [=====================>........] - ETA: 20:14 - loss: 1.8019 - regression_loss: 1.3726 - classification_loss 7448/10000 [=====================>........] - ETA: 20:14 - loss: 1.8019 - regression_loss: 1.3726 - classification_loss 7449/10000 [=====================>........] - ETA: 20:14 - loss: 1.8018 - regression_loss: 1.3725 - classification_loss 7450/10000 [=====================>........] - ETA: 20:13 - loss: 1.8017 - regression_loss: 1.3724 - classification_loss 7451/10000 [=====================>........] - ETA: 20:13 - loss: 1.8017 - regression_loss: 1.3723 - classification_loss 7452/10000 [=====================>........] - ETA: 20:12 - loss: 1.8017 - regression_loss: 1.3724 - classification_loss 7453/10000 [=====================>........] - ETA: 20:12 - loss: 1.8017 - regression_loss: 1.3724 - classification_loss 7454/10000 [=====================>........] - ETA: 20:11 - loss: 1.8015 - regression_loss: 1.3722 - classification_loss 7455/10000 [=====================>........] - ETA: 20:11 - loss: 1.8015 - regression_loss: 1.3722 - classification_loss 7456/10000 [=====================>........] - ETA: 20:10 - loss: 1.8015 - regression_loss: 1.3722 - classification_loss 7457/10000 [=====================>........] - ETA: 20:10 - loss: 1.8016 - regression_loss: 1.3723 - classification_loss 7458/10000 [=====================>........] - ETA: 20:09 - loss: 1.8017 - regression_loss: 1.3724 - classification_loss 7459/10000 [=====================>........] - ETA: 20:09 - loss: 1.8018 - regression_loss: 1.3724 - classification_loss 7460/10000 [=====================>........] - ETA: 20:08 - loss: 1.8017 - regression_loss: 1.3723 - classification_loss 7461/10000 [=====================>........] - ETA: 20:08 - loss: 1.8015 - regression_loss: 1.3722 - classification_loss 7462/10000 [=====================>........] - ETA: 20:07 - loss: 1.8016 - regression_loss: 1.3722 - classification_loss 7463/10000 [=====================>........] - ETA: 20:07 - loss: 1.8016 - regression_loss: 1.3723 - classification_loss 7464/10000 [=====================>........] - ETA: 20:06 - loss: 1.8017 - regression_loss: 1.3724 - classification_loss 7465/10000 [=====================>........] - ETA: 20:06 - loss: 1.8017 - regression_loss: 1.3724 - classification_loss 7466/10000 [=====================>........] - ETA: 20:05 - loss: 1.8017 - regression_loss: 1.3724 - classification_loss 7467/10000 [=====================>........] - ETA: 20:05 - loss: 1.8016 - regression_loss: 1.3723 - classification_loss 7468/10000 [=====================>........] - ETA: 20:04 - loss: 1.8015 - regression_loss: 1.3722 - classification_loss 7469/10000 [=====================>........] - ETA: 20:04 - loss: 1.8015 - regression_loss: 1.3722 - classification_loss 7470/10000 [=====================>........] - ETA: 20:03 - loss: 1.8014 - regression_loss: 1.3722 - classification_loss 7471/10000 [=====================>........] - ETA: 20:03 - loss: 1.8013 - regression_loss: 1.3721 - classification_loss 7472/10000 [=====================>........] - ETA: 20:02 - loss: 1.8013 - regression_loss: 1.3722 - classification_loss 7473/10000 [=====================>........] - ETA: 20:02 - loss: 1.8014 - regression_loss: 1.3722 - classification_loss 7474/10000 [=====================>........] - ETA: 20:01 - loss: 1.8015 - regression_loss: 1.3723 - classification_loss 7475/10000 [=====================>........] - ETA: 20:01 - loss: 1.8016 - regression_loss: 1.3724 - classification_loss 7476/10000 [=====================>........] - ETA: 20:00 - loss: 1.8014 - regression_loss: 1.3723 - classification_loss 7477/10000 [=====================>........] - ETA: 20:00 - loss: 1.8015 - regression_loss: 1.3724 - classification_loss 7478/10000 [=====================>........] - ETA: 20:00 - loss: 1.8015 - regression_loss: 1.3724 - classification_loss 7479/10000 [=====================>........] - ETA: 19:59 - loss: 1.8014 - regression_loss: 1.3723 - classification_loss 7480/10000 [=====================>........] - ETA: 19:59 - loss: 1.8014 - regression_loss: 1.3723 - classification_loss 7481/10000 [=====================>........] - ETA: 19:58 - loss: 1.8014 - regression_loss: 1.3723 - classification_loss 7482/10000 [=====================>........] - ETA: 19:58 - loss: 1.8014 - regression_loss: 1.3723 - classification_loss 7483/10000 [=====================>........] - ETA: 19:57 - loss: 1.8014 - regression_loss: 1.3723 - classification_loss 7484/10000 [=====================>........] - ETA: 19:57 - loss: 1.8015 - regression_loss: 1.3723 - classification_loss 7485/10000 [=====================>........] - ETA: 19:56 - loss: 1.8015 - regression_loss: 1.3724 - classification_loss 7486/10000 [=====================>........] - ETA: 19:56 - loss: 1.8015 - regression_loss: 1.3724 - classification_loss 7487/10000 [=====================>........] - ETA: 19:55 - loss: 1.8014 - regression_loss: 1.3723 - classification_loss 7488/10000 [=====================>........] - ETA: 19:55 - loss: 1.8013 - regression_loss: 1.3722 - classification_loss 7489/10000 [=====================>........] - ETA: 19:54 - loss: 1.8012 - regression_loss: 1.3722 - classification_loss 7490/10000 [=====================>........] - ETA: 19:54 - loss: 1.8013 - regression_loss: 1.3722 - classification_loss 7491/10000 [=====================>........] - ETA: 19:53 - loss: 1.8011 - regression_loss: 1.3721 - classification_loss 7492/10000 [=====================>........] - ETA: 19:53 - loss: 1.8010 - regression_loss: 1.3720 - classification_loss 7493/10000 [=====================>........] - ETA: 19:52 - loss: 1.8010 - regression_loss: 1.3720 - classification_loss 7494/10000 [=====================>........] - ETA: 19:52 - loss: 1.8009 - regression_loss: 1.3719 - classification_loss 7495/10000 [=====================>........] - ETA: 19:51 - loss: 1.8009 - regression_loss: 1.3718 - classification_loss 7496/10000 [=====================>........] - ETA: 19:51 - loss: 1.8009 - regression_loss: 1.3718 - classification_loss 7497/10000 [=====================>........] - ETA: 19:50 - loss: 1.8009 - regression_loss: 1.3718 - classification_loss 7498/10000 [=====================>........] - ETA: 19:50 - loss: 1.8007 - regression_loss: 1.3718 - classification_loss 7499/10000 [=====================>........] - ETA: 19:49 - loss: 1.8006 - regression_loss: 1.3716 - classification_loss 7500/10000 [=====================>........] - ETA: 19:49 - loss: 1.8005 - regression_loss: 1.3715 - classification_loss 7501/10000 [=====================>........] - ETA: 19:49 - loss: 1.8005 - regression_loss: 1.3715 - classification_loss 7502/10000 [=====================>........] - ETA: 19:48 - loss: 1.8005 - regression_loss: 1.3716 - classification_loss 7503/10000 [=====================>........] - ETA: 19:48 - loss: 1.8005 - regression_loss: 1.3716 - classification_loss 7504/10000 [=====================>........] - ETA: 19:47 - loss: 1.8005 - regression_loss: 1.3716 - classification_loss 7505/10000 [=====================>........] - ETA: 19:47 - loss: 1.8003 - regression_loss: 1.3714 - classification_loss 7506/10000 [=====================>........] - ETA: 19:46 - loss: 1.8003 - regression_loss: 1.3714 - classification_loss 7507/10000 [=====================>........] - ETA: 19:46 - loss: 1.8003 - regression_loss: 1.3714 - classification_loss 7508/10000 [=====================>........] - ETA: 19:45 - loss: 1.8003 - regression_loss: 1.3714 - classification_loss 7509/10000 [=====================>........] - ETA: 19:45 - loss: 1.8002 - regression_loss: 1.3714 - classification_loss 7510/10000 [=====================>........] - ETA: 19:44 - loss: 1.8002 - regression_loss: 1.3714 - classification_loss 7511/10000 [=====================>........] - ETA: 19:44 - loss: 1.8000 - regression_loss: 1.3713 - classification_loss 7512/10000 [=====================>........] - ETA: 19:43 - loss: 1.8001 - regression_loss: 1.3713 - classification_loss 7513/10000 [=====================>........] - ETA: 19:43 - loss: 1.8001 - regression_loss: 1.3713 - classification_loss 7514/10000 [=====================>........] - ETA: 19:42 - loss: 1.8000 - regression_loss: 1.3712 - classification_loss 7515/10000 [=====================>........] - ETA: 19:42 - loss: 1.8000 - regression_loss: 1.3712 - classification_loss 7516/10000 [=====================>........] - ETA: 19:41 - loss: 1.7999 - regression_loss: 1.3711 - classification_loss 7517/10000 [=====================>........] - ETA: 19:41 - loss: 1.7998 - regression_loss: 1.3710 - classification_loss 7518/10000 [=====================>........] - ETA: 19:40 - loss: 1.7998 - regression_loss: 1.3710 - classification_loss 7519/10000 [=====================>........] - ETA: 19:40 - loss: 1.7998 - regression_loss: 1.3710 - classification_loss 7520/10000 [=====================>........] - ETA: 19:39 - loss: 1.7999 - regression_loss: 1.3711 - classification_loss 7521/10000 [=====================>........] - ETA: 19:39 - loss: 1.7999 - regression_loss: 1.3711 - classification_loss 7522/10000 [=====================>........] - ETA: 19:38 - loss: 1.7999 - regression_loss: 1.3712 - classification_loss 7523/10000 [=====================>........] - ETA: 19:38 - loss: 1.7999 - regression_loss: 1.3712 - classification_loss 7524/10000 [=====================>........] - ETA: 19:37 - loss: 1.7999 - regression_loss: 1.3712 - classification_loss 7525/10000 [=====================>........] - ETA: 19:37 - loss: 1.7999 - regression_loss: 1.3711 - classification_loss 7526/10000 [=====================>........] - ETA: 19:36 - loss: 1.7998 - regression_loss: 1.3711 - classification_loss 7527/10000 [=====================>........] - ETA: 19:36 - loss: 1.7998 - regression_loss: 1.3711 - classification_loss 7528/10000 [=====================>........] - ETA: 19:35 - loss: 1.8000 - regression_loss: 1.3713 - classification_loss 7529/10000 [=====================>........] - ETA: 19:35 - loss: 1.8000 - regression_loss: 1.3712 - classification_loss 7530/10000 [=====================>........] - ETA: 19:35 - loss: 1.7999 - regression_loss: 1.3712 - classification_loss 7531/10000 [=====================>........] - ETA: 19:34 - loss: 1.7999 - regression_loss: 1.3711 - classification_loss 7532/10000 [=====================>........] - ETA: 19:34 - loss: 1.7999 - regression_loss: 1.3712 - classification_loss 7533/10000 [=====================>........] - ETA: 19:33 - loss: 1.7999 - regression_loss: 1.3712 - classification_loss 7534/10000 [=====================>........] - ETA: 19:33 - loss: 1.7999 - regression_loss: 1.3711 - classification_loss 7535/10000 [=====================>........] - ETA: 19:32 - loss: 1.7998 - regression_loss: 1.3711 - classification_loss 7536/10000 [=====================>........] - ETA: 19:32 - loss: 1.7998 - regression_loss: 1.3710 - classification_loss 7537/10000 [=====================>........] - ETA: 19:31 - loss: 1.7997 - regression_loss: 1.3710 - classification_loss 7538/10000 [=====================>........] - ETA: 19:31 - loss: 1.7996 - regression_loss: 1.3709 - classification_loss 7539/10000 [=====================>........] - ETA: 19:30 - loss: 1.7996 - regression_loss: 1.3709 - classification_loss 7540/10000 [=====================>........] - ETA: 19:30 - loss: 1.7997 - regression_loss: 1.3710 - classification_loss 7541/10000 [=====================>........] - ETA: 19:29 - loss: 1.7997 - regression_loss: 1.3711 - classification_loss 7542/10000 [=====================>........] - ETA: 19:29 - loss: 1.7998 - regression_loss: 1.3712 - classification_loss 7543/10000 [=====================>........] - ETA: 19:28 - loss: 1.7996 - regression_loss: 1.3710 - classification_loss 7544/10000 [=====================>........] - ETA: 19:28 - loss: 1.7996 - regression_loss: 1.3710 - classification_loss 7545/10000 [=====================>........] - ETA: 19:27 - loss: 1.7996 - regression_loss: 1.3710 - classification_loss 7546/10000 [=====================>........] - ETA: 19:27 - loss: 1.7996 - regression_loss: 1.3711 - classification_loss 7547/10000 [=====================>........] - ETA: 19:26 - loss: 1.7995 - regression_loss: 1.3709 - classification_loss 7548/10000 [=====================>........] - ETA: 19:26 - loss: 1.7996 - regression_loss: 1.3710 - classification_loss 7549/10000 [=====================>........] - ETA: 19:25 - loss: 1.7995 - regression_loss: 1.3709 - classification_loss 7550/10000 [=====================>........] - ETA: 19:25 - loss: 1.7995 - regression_loss: 1.3710 - classification_loss 7551/10000 [=====================>........] - ETA: 19:24 - loss: 1.7995 - regression_loss: 1.3710 - classification_loss 7552/10000 [=====================>........] - ETA: 19:24 - loss: 1.7995 - regression_loss: 1.3710 - classification_loss 7553/10000 [=====================>........] - ETA: 19:23 - loss: 1.7996 - regression_loss: 1.3711 - classification_loss 7554/10000 [=====================>........] - ETA: 19:23 - loss: 1.7996 - regression_loss: 1.3711 - classification_loss 7555/10000 [=====================>........] - ETA: 19:22 - loss: 1.7996 - regression_loss: 1.3711 - classification_loss 7556/10000 [=====================>........] - ETA: 19:22 - loss: 1.7995 - regression_loss: 1.3710 - classification_loss 7557/10000 [=====================>........] - ETA: 19:22 - loss: 1.7995 - regression_loss: 1.3710 - classification_loss 7558/10000 [=====================>........] - ETA: 19:21 - loss: 1.7994 - regression_loss: 1.3709 - classification_loss 7559/10000 [=====================>........] - ETA: 19:21 - loss: 1.7995 - regression_loss: 1.3710 - classification_loss 7560/10000 [=====================>........] - ETA: 19:20 - loss: 1.7994 - regression_loss: 1.3710 - classification_loss 7561/10000 [=====================>........] - ETA: 19:20 - loss: 1.7995 - regression_loss: 1.3711 - classification_loss 7562/10000 [=====================>........] - ETA: 19:19 - loss: 1.7995 - regression_loss: 1.3711 - classification_loss 7563/10000 [=====================>........] - ETA: 19:19 - loss: 1.7995 - regression_loss: 1.3711 - classification_loss 7564/10000 [=====================>........] - ETA: 19:18 - loss: 1.7994 - regression_loss: 1.3710 - classification_loss 7565/10000 [=====================>........] - ETA: 19:18 - loss: 1.7992 - regression_loss: 1.3709 - classification_loss 7566/10000 [=====================>........] - ETA: 19:17 - loss: 1.7992 - regression_loss: 1.3708 - classification_loss 7567/10000 [=====================>........] - ETA: 19:17 - loss: 1.7992 - regression_loss: 1.3709 - classification_loss 7568/10000 [=====================>........] - ETA: 19:16 - loss: 1.7993 - regression_loss: 1.3709 - classification_loss 7569/10000 [=====================>........] - ETA: 19:16 - loss: 1.7992 - regression_loss: 1.3709 - classification_loss 7570/10000 [=====================>........] - ETA: 19:15 - loss: 1.7991 - regression_loss: 1.3708 - classification_loss 7571/10000 [=====================>........] - ETA: 19:15 - loss: 1.7992 - regression_loss: 1.3709 - classification_loss 7572/10000 [=====================>........] - ETA: 19:14 - loss: 1.7992 - regression_loss: 1.3709 - classification_loss 7573/10000 [=====================>........] - ETA: 19:14 - loss: 1.7990 - regression_loss: 1.3708 - classification_loss 7574/10000 [=====================>........] - ETA: 19:13 - loss: 1.7991 - regression_loss: 1.3709 - classification_loss 7575/10000 [=====================>........] - ETA: 19:13 - loss: 1.7991 - regression_loss: 1.3709 - classification_loss 7576/10000 [=====================>........] - ETA: 19:12 - loss: 1.7992 - regression_loss: 1.3709 - classification_loss 7577/10000 [=====================>........] - ETA: 19:12 - loss: 1.7992 - regression_loss: 1.3709 - classification_loss 7578/10000 [=====================>........] - ETA: 19:11 - loss: 1.7992 - regression_loss: 1.3710 - classification_loss 7579/10000 [=====================>........] - ETA: 19:11 - loss: 1.7992 - regression_loss: 1.3709 - classification_loss 7580/10000 [=====================>........] - ETA: 19:11 - loss: 1.7993 - regression_loss: 1.3710 - classification_loss 7581/10000 [=====================>........] - ETA: 19:10 - loss: 1.7992 - regression_loss: 1.3709 - classification_loss 7582/10000 [=====================>........] - ETA: 19:10 - loss: 1.7992 - regression_loss: 1.3709 - classification_loss 7583/10000 [=====================>........] - ETA: 19:09 - loss: 1.7991 - regression_loss: 1.3707 - classification_loss 7584/10000 [=====================>........] - ETA: 19:09 - loss: 1.7992 - regression_loss: 1.3708 - classification_loss 7585/10000 [=====================>........] - ETA: 19:08 - loss: 1.7992 - regression_loss: 1.3708 - classification_loss 7586/10000 [=====================>........] - ETA: 19:08 - loss: 1.7992 - regression_loss: 1.3708 - classification_loss 7587/10000 [=====================>........] - ETA: 19:07 - loss: 1.7992 - regression_loss: 1.3709 - classification_loss 7588/10000 [=====================>........] - ETA: 19:07 - loss: 1.7992 - regression_loss: 1.3708 - classification_loss 7589/10000 [=====================>........] - ETA: 19:06 - loss: 1.7992 - regression_loss: 1.3709 - classification_loss 7590/10000 [=====================>........] - ETA: 19:06 - loss: 1.7993 - regression_loss: 1.3709 - classification_loss 7591/10000 [=====================>........] - ETA: 19:05 - loss: 1.7993 - regression_loss: 1.3710 - classification_loss 7592/10000 [=====================>........] - ETA: 19:05 - loss: 1.7994 - regression_loss: 1.3710 - classification_loss 7593/10000 [=====================>........] - ETA: 19:04 - loss: 1.7995 - regression_loss: 1.3711 - classification_loss 7594/10000 [=====================>........] - ETA: 19:04 - loss: 1.7995 - regression_loss: 1.3712 - classification_loss 7595/10000 [=====================>........] - ETA: 19:03 - loss: 1.7994 - regression_loss: 1.3711 - classification_loss 7596/10000 [=====================>........] - ETA: 19:03 - loss: 1.7994 - regression_loss: 1.3711 - classification_loss 7597/10000 [=====================>........] - ETA: 19:02 - loss: 1.7993 - regression_loss: 1.3711 - classification_loss 7598/10000 [=====================>........] - ETA: 19:02 - loss: 1.7993 - regression_loss: 1.3710 - classification_loss 7599/10000 [=====================>........] - ETA: 19:01 - loss: 1.7994 - regression_loss: 1.3711 - classification_loss 7600/10000 [=====================>........] - ETA: 19:01 - loss: 1.7994 - regression_loss: 1.3712 - classification_loss 7601/10000 [=====================>........] - ETA: 19:00 - loss: 1.7994 - regression_loss: 1.3711 - classification_loss 7602/10000 [=====================>........] - ETA: 19:00 - loss: 1.7993 - regression_loss: 1.3710 - classification_loss 7603/10000 [=====================>........] - ETA: 18:59 - loss: 1.7992 - regression_loss: 1.3710 - classification_loss 7604/10000 [=====================>........] - ETA: 18:59 - loss: 1.7991 - regression_loss: 1.3709 - classification_loss 7605/10000 [=====================>........] - ETA: 18:58 - loss: 1.7991 - regression_loss: 1.3708 - classification_loss 7606/10000 [=====================>........] - ETA: 18:58 - loss: 1.7990 - regression_loss: 1.3707 - classification_loss 7607/10000 [=====================>........] - ETA: 18:57 - loss: 1.7989 - regression_loss: 1.3707 - classification_loss 7608/10000 [=====================>........] - ETA: 18:57 - loss: 1.7989 - regression_loss: 1.3707 - classification_loss 7609/10000 [=====================>........] - ETA: 18:56 - loss: 1.7989 - regression_loss: 1.3707 - classification_loss 7610/10000 [=====================>........] - ETA: 18:56 - loss: 1.7990 - regression_loss: 1.3708 - classification_loss 7611/10000 [=====================>........] - ETA: 18:56 - loss: 1.7988 - regression_loss: 1.3706 - classification_loss 7612/10000 [=====================>........] - ETA: 18:55 - loss: 1.7989 - regression_loss: 1.3707 - classification_loss 7613/10000 [=====================>........] - ETA: 18:55 - loss: 1.7989 - regression_loss: 1.3707 - classification_loss 7614/10000 [=====================>........] - ETA: 18:54 - loss: 1.7990 - regression_loss: 1.3708 - classification_loss 7615/10000 [=====================>........] - ETA: 18:54 - loss: 1.7991 - regression_loss: 1.3709 - classification_loss 7616/10000 [=====================>........] - ETA: 18:53 - loss: 1.7992 - regression_loss: 1.3709 - classification_loss 7617/10000 [=====================>........] - ETA: 18:53 - loss: 1.7992 - regression_loss: 1.3709 - classification_loss 7618/10000 [=====================>........] - ETA: 18:52 - loss: 1.7991 - regression_loss: 1.3708 - classification_loss 7619/10000 [=====================>........] - ETA: 18:52 - loss: 1.7990 - regression_loss: 1.3707 - classification_loss 7620/10000 [=====================>........] - ETA: 18:51 - loss: 1.7990 - regression_loss: 1.3708 - classification_loss 7621/10000 [=====================>........] - ETA: 18:51 - loss: 1.7990 - regression_loss: 1.3708 - classification_loss 7622/10000 [=====================>........] - ETA: 18:50 - loss: 1.7990 - regression_loss: 1.3707 - classification_loss 7623/10000 [=====================>........] - ETA: 18:50 - loss: 1.7990 - regression_loss: 1.3708 - classification_loss 7624/10000 [=====================>........] - ETA: 18:49 - loss: 1.7992 - regression_loss: 1.3709 - classification_loss 7625/10000 [=====================>........] - ETA: 18:49 - loss: 1.7991 - regression_loss: 1.3709 - classification_loss 7626/10000 [=====================>........] - ETA: 18:48 - loss: 1.7991 - regression_loss: 1.3708 - classification_loss 7627/10000 [=====================>........] - ETA: 18:48 - loss: 1.7991 - regression_loss: 1.3708 - classification_loss 7628/10000 [=====================>........] - ETA: 18:47 - loss: 1.7991 - regression_loss: 1.3709 - classification_loss 7629/10000 [=====================>........] - ETA: 18:47 - loss: 1.7991 - regression_loss: 1.3709 - classification_loss 7630/10000 [=====================>........] - ETA: 18:46 - loss: 1.7992 - regression_loss: 1.3710 - classification_loss 7631/10000 [=====================>........] - ETA: 18:46 - loss: 1.7991 - regression_loss: 1.3709 - classification_loss 7632/10000 [=====================>........] - ETA: 18:45 - loss: 1.7993 - regression_loss: 1.3710 - classification_loss 7633/10000 [=====================>........] - ETA: 18:45 - loss: 1.7993 - regression_loss: 1.3710 - classification_loss 7634/10000 [=====================>........] - ETA: 18:44 - loss: 1.7992 - regression_loss: 1.3710 - classification_loss 7635/10000 [=====================>........] - ETA: 18:44 - loss: 1.7992 - regression_loss: 1.3710 - classification_loss 7636/10000 [=====================>........] - ETA: 18:44 - loss: 1.7992 - regression_loss: 1.3710 - classification_loss 7637/10000 [=====================>........] - ETA: 18:43 - loss: 1.7993 - regression_loss: 1.3711 - classification_loss 7638/10000 [=====================>........] - ETA: 18:43 - loss: 1.7993 - regression_loss: 1.3711 - classification_loss 7639/10000 [=====================>........] - ETA: 18:42 - loss: 1.7991 - regression_loss: 1.3709 - classification_loss 7640/10000 [=====================>........] - ETA: 18:42 - loss: 1.7991 - regression_loss: 1.3709 - classification_loss 7641/10000 [=====================>........] - ETA: 18:41 - loss: 1.7992 - regression_loss: 1.3710 - classification_loss 7642/10000 [=====================>........] - ETA: 18:41 - loss: 1.7990 - regression_loss: 1.3709 - classification_loss 7643/10000 [=====================>........] - ETA: 18:40 - loss: 1.7991 - regression_loss: 1.3709 - classification_loss 7644/10000 [=====================>........] - ETA: 18:40 - loss: 1.7990 - regression_loss: 1.3709 - classification_loss 7645/10000 [=====================>........] - ETA: 18:39 - loss: 1.7991 - regression_loss: 1.3709 - classification_loss 7646/10000 [=====================>........] - ETA: 18:39 - loss: 1.7992 - regression_loss: 1.3710 - classification_loss 7647/10000 [=====================>........] - ETA: 18:38 - loss: 1.7992 - regression_loss: 1.3710 - classification_loss 7648/10000 [=====================>........] - ETA: 18:38 - loss: 1.7991 - regression_loss: 1.3709 - classification_loss 7649/10000 [=====================>........] - ETA: 18:37 - loss: 1.7991 - regression_loss: 1.3709 - classification_loss 7650/10000 [=====================>........] - ETA: 18:37 - loss: 1.7991 - regression_loss: 1.3710 - classification_loss 7651/10000 [=====================>........] - ETA: 18:36 - loss: 1.7992 - regression_loss: 1.3710 - classification_loss 7652/10000 [=====================>........] - ETA: 18:36 - loss: 1.7993 - regression_loss: 1.3711 - classification_loss 7653/10000 [=====================>........] - ETA: 18:35 - loss: 1.7993 - regression_loss: 1.3712 - classification_loss 7654/10000 [=====================>........] - ETA: 18:35 - loss: 1.7993 - regression_loss: 1.3712 - classification_loss 7655/10000 [=====================>........] - ETA: 18:34 - loss: 1.7994 - regression_loss: 1.3712 - classification_loss 7656/10000 [=====================>........] - ETA: 18:34 - loss: 1.7994 - regression_loss: 1.3713 - classification_loss 7657/10000 [=====================>........] - ETA: 18:33 - loss: 1.7995 - regression_loss: 1.3713 - classification_loss 7658/10000 [=====================>........] - ETA: 18:33 - loss: 1.7994 - regression_loss: 1.3712 - classification_loss 7659/10000 [=====================>........] - ETA: 18:33 - loss: 1.7993 - regression_loss: 1.3711 - classification_loss 7660/10000 [=====================>........] - ETA: 18:32 - loss: 1.7992 - regression_loss: 1.3710 - classification_loss 7661/10000 [=====================>........] - ETA: 18:32 - loss: 1.7993 - regression_loss: 1.3711 - classification_loss 7662/10000 [=====================>........] - ETA: 18:31 - loss: 1.7992 - regression_loss: 1.3710 - classification_loss 7663/10000 [=====================>........] - ETA: 18:31 - loss: 1.7994 - regression_loss: 1.3711 - classification_loss 7664/10000 [=====================>........] - ETA: 18:30 - loss: 1.7993 - regression_loss: 1.3711 - classification_loss 7665/10000 [=====================>........] - ETA: 18:30 - loss: 1.7994 - regression_loss: 1.3711 - classification_loss 7666/10000 [=====================>........] - ETA: 18:29 - loss: 1.7995 - regression_loss: 1.3712 - classification_loss 7667/10000 [======================>.......] - ETA: 18:29 - loss: 1.7995 - regression_loss: 1.3712 - classification_loss 7668/10000 [======================>.......] - ETA: 18:28 - loss: 1.7994 - regression_loss: 1.3711 - classification_loss 7669/10000 [======================>.......] - ETA: 18:28 - loss: 1.7993 - regression_loss: 1.3710 - classification_loss 7670/10000 [======================>.......] - ETA: 18:27 - loss: 1.7992 - regression_loss: 1.3710 - classification_loss 7671/10000 [======================>.......] - ETA: 18:27 - loss: 1.7992 - regression_loss: 1.3709 - classification_loss 7672/10000 [======================>.......] - ETA: 18:26 - loss: 1.7991 - regression_loss: 1.3708 - classification_loss 7673/10000 [======================>.......] - ETA: 18:26 - loss: 1.7991 - regression_loss: 1.3709 - classification_loss 7674/10000 [======================>.......] - ETA: 18:25 - loss: 1.7992 - regression_loss: 1.3709 - classification_loss 7675/10000 [======================>.......] - ETA: 18:25 - loss: 1.7992 - regression_loss: 1.3709 - classification_loss 7676/10000 [======================>.......] - ETA: 18:24 - loss: 1.7991 - regression_loss: 1.3709 - classification_loss 7677/10000 [======================>.......] - ETA: 18:24 - loss: 1.7990 - regression_loss: 1.3708 - classification_loss 7678/10000 [======================>.......] - ETA: 18:23 - loss: 1.7990 - regression_loss: 1.3707 - classification_loss 7679/10000 [======================>.......] - ETA: 18:23 - loss: 1.7989 - regression_loss: 1.3707 - classification_loss 7680/10000 [======================>.......] - ETA: 18:22 - loss: 1.7988 - regression_loss: 1.3706 - classification_loss 7681/10000 [======================>.......] - ETA: 18:22 - loss: 1.7989 - regression_loss: 1.3707 - classification_loss 7682/10000 [======================>.......] - ETA: 18:21 - loss: 1.7989 - regression_loss: 1.3707 - classification_loss 7683/10000 [======================>.......] - ETA: 18:21 - loss: 1.7989 - regression_loss: 1.3707 - classification_loss 7684/10000 [======================>.......] - ETA: 18:21 - loss: 1.7989 - regression_loss: 1.3707 - classification_loss 7685/10000 [======================>.......] - ETA: 18:20 - loss: 1.7989 - regression_loss: 1.3707 - classification_loss 7686/10000 [======================>.......] - ETA: 18:20 - loss: 1.7989 - regression_loss: 1.3707 - classification_loss 7687/10000 [======================>.......] - ETA: 18:19 - loss: 1.7989 - regression_loss: 1.3707 - classification_loss 7688/10000 [======================>.......] - ETA: 18:19 - loss: 1.7988 - regression_loss: 1.3707 - classification_loss 7689/10000 [======================>.......] - ETA: 18:18 - loss: 1.7989 - regression_loss: 1.3707 - classification_loss 7690/10000 [======================>.......] - ETA: 18:18 - loss: 1.7988 - regression_loss: 1.3707 - classification_loss 7691/10000 [======================>.......] - ETA: 18:17 - loss: 1.7987 - regression_loss: 1.3706 - classification_loss 7692/10000 [======================>.......] - ETA: 18:17 - loss: 1.7986 - regression_loss: 1.3705 - classification_loss 7693/10000 [======================>.......] - ETA: 18:16 - loss: 1.7986 - regression_loss: 1.3705 - classification_loss 7694/10000 [======================>.......] - ETA: 18:16 - loss: 1.7985 - regression_loss: 1.3705 - classification_loss 7695/10000 [======================>.......] - ETA: 18:15 - loss: 1.7986 - regression_loss: 1.3705 - classification_loss 7696/10000 [======================>.......] - ETA: 18:15 - loss: 1.7985 - regression_loss: 1.3705 - classification_loss 7697/10000 [======================>.......] - ETA: 18:14 - loss: 1.7985 - regression_loss: 1.3704 - classification_loss 7698/10000 [======================>.......] - ETA: 18:14 - loss: 1.7984 - regression_loss: 1.3704 - classification_loss 7699/10000 [======================>.......] - ETA: 18:13 - loss: 1.7985 - regression_loss: 1.3705 - classification_loss 7700/10000 [======================>.......] - ETA: 18:13 - loss: 1.7986 - regression_loss: 1.3705 - classification_loss 7701/10000 [======================>.......] - ETA: 18:12 - loss: 1.7985 - regression_loss: 1.3705 - classification_loss 7702/10000 [======================>.......] - ETA: 18:12 - loss: 1.7985 - regression_loss: 1.3705 - classification_loss 7703/10000 [======================>.......] - ETA: 18:11 - loss: 1.7986 - regression_loss: 1.3706 - classification_loss 7704/10000 [======================>.......] - ETA: 18:11 - loss: 1.7987 - regression_loss: 1.3707 - classification_loss 7705/10000 [======================>.......] - ETA: 18:10 - loss: 1.7986 - regression_loss: 1.3706 - classification_loss 7706/10000 [======================>.......] - ETA: 18:10 - loss: 1.7987 - regression_loss: 1.3706 - classification_loss 7707/10000 [======================>.......] - ETA: 18:09 - loss: 1.7986 - regression_loss: 1.3706 - classification_loss 7708/10000 [======================>.......] - ETA: 18:09 - loss: 1.7985 - regression_loss: 1.3705 - classification_loss 7709/10000 [======================>.......] - ETA: 18:08 - loss: 1.7984 - regression_loss: 1.3705 - classification_loss 7710/10000 [======================>.......] - ETA: 18:08 - loss: 1.7985 - regression_loss: 1.3705 - classification_loss 7711/10000 [======================>.......] - ETA: 18:08 - loss: 1.7984 - regression_loss: 1.3704 - classification_loss 7712/10000 [======================>.......] - ETA: 18:07 - loss: 1.7983 - regression_loss: 1.3704 - classification_loss 7713/10000 [======================>.......] - ETA: 18:07 - loss: 1.7982 - regression_loss: 1.3703 - classification_loss 7714/10000 [======================>.......] - ETA: 18:06 - loss: 1.7981 - regression_loss: 1.3703 - classification_loss 7715/10000 [======================>.......] - ETA: 18:06 - loss: 1.7981 - regression_loss: 1.3702 - classification_loss 7716/10000 [======================>.......] - ETA: 18:05 - loss: 1.7981 - regression_loss: 1.3702 - classification_loss 7717/10000 [======================>.......] - ETA: 18:05 - loss: 1.7981 - regression_loss: 1.3703 - classification_loss 7718/10000 [======================>.......] - ETA: 18:04 - loss: 1.7980 - regression_loss: 1.3702 - classification_loss 7719/10000 [======================>.......] - ETA: 18:04 - loss: 1.7981 - regression_loss: 1.3703 - classification_loss 7720/10000 [======================>.......] - ETA: 18:03 - loss: 1.7980 - regression_loss: 1.3702 - classification_loss 7721/10000 [======================>.......] - ETA: 18:03 - loss: 1.7980 - regression_loss: 1.3702 - classification_loss 7722/10000 [======================>.......] - ETA: 18:02 - loss: 1.7979 - regression_loss: 1.3702 - classification_loss 7723/10000 [======================>.......] - ETA: 18:02 - loss: 1.7982 - regression_loss: 1.3703 - classification_loss 7724/10000 [======================>.......] - ETA: 18:01 - loss: 1.7981 - regression_loss: 1.3703 - classification_loss 7725/10000 [======================>.......] - ETA: 18:01 - loss: 1.7980 - regression_loss: 1.3703 - classification_loss 7726/10000 [======================>.......] - ETA: 18:00 - loss: 1.7981 - regression_loss: 1.3703 - classification_loss 7727/10000 [======================>.......] - ETA: 18:00 - loss: 1.7982 - regression_loss: 1.3704 - classification_loss 7728/10000 [======================>.......] - ETA: 17:59 - loss: 1.7982 - regression_loss: 1.3704 - classification_loss 7729/10000 [======================>.......] - ETA: 17:59 - loss: 1.7982 - regression_loss: 1.3704 - classification_loss 7730/10000 [======================>.......] - ETA: 17:58 - loss: 1.7982 - regression_loss: 1.3704 - classification_loss 7731/10000 [======================>.......] - ETA: 17:58 - loss: 1.7984 - regression_loss: 1.3706 - classification_loss 7732/10000 [======================>.......] - ETA: 17:58 - loss: 1.7985 - regression_loss: 1.3707 - classification_loss 7733/10000 [======================>.......] - ETA: 17:57 - loss: 1.7987 - regression_loss: 1.3708 - classification_loss 7734/10000 [======================>.......] - ETA: 17:57 - loss: 1.7986 - regression_loss: 1.3707 - classification_loss 7735/10000 [======================>.......] - ETA: 17:56 - loss: 1.7985 - regression_loss: 1.3707 - classification_loss 7736/10000 [======================>.......] - ETA: 17:56 - loss: 1.7986 - regression_loss: 1.3708 - classification_loss 7737/10000 [======================>.......] - ETA: 17:55 - loss: 1.7986 - regression_loss: 1.3708 - classification_loss 7738/10000 [======================>.......] - ETA: 17:55 - loss: 1.7986 - regression_loss: 1.3708 - classification_loss 7739/10000 [======================>.......] - ETA: 17:54 - loss: 1.7984 - regression_loss: 1.3707 - classification_loss 7740/10000 [======================>.......] - ETA: 17:54 - loss: 1.7984 - regression_loss: 1.3706 - classification_loss 7741/10000 [======================>.......] - ETA: 17:53 - loss: 1.7984 - regression_loss: 1.3706 - classification_loss 7742/10000 [======================>.......] - ETA: 17:53 - loss: 1.7984 - regression_loss: 1.3707 - classification_loss 7743/10000 [======================>.......] - ETA: 17:52 - loss: 1.7986 - regression_loss: 1.3708 - classification_loss 7744/10000 [======================>.......] - ETA: 17:52 - loss: 1.7985 - regression_loss: 1.3707 - classification_loss 7745/10000 [======================>.......] - ETA: 17:51 - loss: 1.7985 - regression_loss: 1.3707 - classification_loss 7746/10000 [======================>.......] - ETA: 17:51 - loss: 1.7985 - regression_loss: 1.3707 - classification_loss 7747/10000 [======================>.......] - ETA: 17:50 - loss: 1.7984 - regression_loss: 1.3706 - classification_loss 7748/10000 [======================>.......] - ETA: 17:50 - loss: 1.7984 - regression_loss: 1.3706 - classification_loss 7749/10000 [======================>.......] - ETA: 17:49 - loss: 1.7983 - regression_loss: 1.3705 - classification_loss 7750/10000 [======================>.......] - ETA: 17:49 - loss: 1.7983 - regression_loss: 1.3705 - classification_loss 7751/10000 [======================>.......] - ETA: 17:48 - loss: 1.7983 - regression_loss: 1.3706 - classification_loss 7752/10000 [======================>.......] - ETA: 17:48 - loss: 1.7983 - regression_loss: 1.3706 - classification_loss 7753/10000 [======================>.......] - ETA: 17:47 - loss: 1.7984 - regression_loss: 1.3706 - classification_loss 7754/10000 [======================>.......] - ETA: 17:47 - loss: 1.7984 - regression_loss: 1.3706 - classification_loss 7755/10000 [======================>.......] - ETA: 17:46 - loss: 1.7983 - regression_loss: 1.3705 - classification_loss 7756/10000 [======================>.......] - ETA: 17:46 - loss: 1.7983 - regression_loss: 1.3705 - classification_loss 7757/10000 [======================>.......] - ETA: 17:45 - loss: 1.7983 - regression_loss: 1.3706 - classification_loss 7758/10000 [======================>.......] - ETA: 17:45 - loss: 1.7983 - regression_loss: 1.3705 - classification_loss 7759/10000 [======================>.......] - ETA: 17:44 - loss: 1.7981 - regression_loss: 1.3704 - classification_loss 7760/10000 [======================>.......] - ETA: 17:44 - loss: 1.7980 - regression_loss: 1.3703 - classification_loss 7761/10000 [======================>.......] - ETA: 17:44 - loss: 1.7981 - regression_loss: 1.3704 - classification_loss 7762/10000 [======================>.......] - ETA: 17:43 - loss: 1.7981 - regression_loss: 1.3704 - classification_loss 7763/10000 [======================>.......] - ETA: 17:43 - loss: 1.7980 - regression_loss: 1.3703 - classification_loss 7764/10000 [======================>.......] - ETA: 17:42 - loss: 1.7981 - regression_loss: 1.3704 - classification_loss 7765/10000 [======================>.......] - ETA: 17:42 - loss: 1.7979 - regression_loss: 1.3703 - classification_loss 7766/10000 [======================>.......] - ETA: 17:41 - loss: 1.7979 - regression_loss: 1.3702 - classification_loss 7767/10000 [======================>.......] - ETA: 17:41 - loss: 1.7979 - regression_loss: 1.3702 - classification_loss 7768/10000 [======================>.......] - ETA: 17:40 - loss: 1.7980 - regression_loss: 1.3703 - classification_loss 7769/10000 [======================>.......] - ETA: 17:40 - loss: 1.7980 - regression_loss: 1.3703 - classification_loss 7770/10000 [======================>.......] - ETA: 17:39 - loss: 1.7980 - regression_loss: 1.3704 - classification_loss 7771/10000 [======================>.......] - ETA: 17:39 - loss: 1.7980 - regression_loss: 1.3704 - classification_loss 7772/10000 [======================>.......] - ETA: 17:38 - loss: 1.7981 - regression_loss: 1.3705 - classification_loss 7773/10000 [======================>.......] - ETA: 17:38 - loss: 1.7981 - regression_loss: 1.3704 - classification_loss 7774/10000 [======================>.......] - ETA: 17:37 - loss: 1.7981 - regression_loss: 1.3705 - classification_loss 7775/10000 [======================>.......] - ETA: 17:37 - loss: 1.7981 - regression_loss: 1.3705 - classification_loss 7776/10000 [======================>.......] - ETA: 17:36 - loss: 1.7981 - regression_loss: 1.3705 - classification_loss 7777/10000 [======================>.......] - ETA: 17:36 - loss: 1.7981 - regression_loss: 1.3705 - classification_loss 7778/10000 [======================>.......] - ETA: 17:35 - loss: 1.7981 - regression_loss: 1.3705 - classification_loss 7779/10000 [======================>.......] - ETA: 17:35 - loss: 1.7982 - regression_loss: 1.3706 - classification_loss 7780/10000 [======================>.......] - ETA: 17:34 - loss: 1.7982 - regression_loss: 1.3706 - classification_loss 7781/10000 [======================>.......] - ETA: 17:34 - loss: 1.7981 - regression_loss: 1.3705 - classification_loss 7782/10000 [======================>.......] - ETA: 17:33 - loss: 1.7981 - regression_loss: 1.3705 - classification_loss 7783/10000 [======================>.......] - ETA: 17:33 - loss: 1.7979 - regression_loss: 1.3704 - classification_loss 7784/10000 [======================>.......] - ETA: 17:33 - loss: 1.7980 - regression_loss: 1.3705 - classification_loss 7785/10000 [======================>.......] - ETA: 17:32 - loss: 1.7979 - regression_loss: 1.3704 - classification_loss 7786/10000 [======================>.......] - ETA: 17:32 - loss: 1.7979 - regression_loss: 1.3704 - classification_loss 7787/10000 [======================>.......] - ETA: 17:31 - loss: 1.7980 - regression_loss: 1.3705 - classification_loss 7788/10000 [======================>.......] - ETA: 17:31 - loss: 1.7979 - regression_loss: 1.3704 - classification_loss 7789/10000 [======================>.......] - ETA: 17:30 - loss: 1.7978 - regression_loss: 1.3703 - classification_loss 7790/10000 [======================>.......] - ETA: 17:30 - loss: 1.7978 - regression_loss: 1.3703 - classification_loss 7791/10000 [======================>.......] - ETA: 17:29 - loss: 1.7978 - regression_loss: 1.3703 - classification_loss 7792/10000 [======================>.......] - ETA: 17:29 - loss: 1.7978 - regression_loss: 1.3703 - classification_loss 7793/10000 [======================>.......] - ETA: 17:28 - loss: 1.7977 - regression_loss: 1.3702 - classification_loss 7794/10000 [======================>.......] - ETA: 17:28 - loss: 1.7978 - regression_loss: 1.3702 - classification_loss 7795/10000 [======================>.......] - ETA: 17:27 - loss: 1.7977 - regression_loss: 1.3701 - classification_loss 7796/10000 [======================>.......] - ETA: 17:27 - loss: 1.7975 - regression_loss: 1.3699 - classification_loss 7797/10000 [======================>.......] - ETA: 17:26 - loss: 1.7974 - regression_loss: 1.3698 - classification_loss 7798/10000 [======================>.......] - ETA: 17:26 - loss: 1.7973 - regression_loss: 1.3698 - classification_loss 7799/10000 [======================>.......] - ETA: 17:25 - loss: 1.7973 - regression_loss: 1.3698 - classification_loss 7800/10000 [======================>.......] - ETA: 17:25 - loss: 1.7975 - regression_loss: 1.3699 - classification_loss 7801/10000 [======================>.......] - ETA: 17:24 - loss: 1.7974 - regression_loss: 1.3698 - classification_loss 7802/10000 [======================>.......] - ETA: 17:24 - loss: 1.7974 - regression_loss: 1.3698 - classification_loss 7803/10000 [======================>.......] - ETA: 17:23 - loss: 1.7973 - regression_loss: 1.3697 - classification_loss 7804/10000 [======================>.......] - ETA: 17:23 - loss: 1.7973 - regression_loss: 1.3697 - classification_loss 7805/10000 [======================>.......] - ETA: 17:22 - loss: 1.7973 - regression_loss: 1.3697 - classification_loss 7806/10000 [======================>.......] - ETA: 17:22 - loss: 1.7975 - regression_loss: 1.3698 - classification_loss 7807/10000 [======================>.......] - ETA: 17:21 - loss: 1.7973 - regression_loss: 1.3697 - classification_loss 7808/10000 [======================>.......] - ETA: 17:21 - loss: 1.7972 - regression_loss: 1.3696 - classification_loss 7809/10000 [======================>.......] - ETA: 17:21 - loss: 1.7971 - regression_loss: 1.3696 - classification_loss 7810/10000 [======================>.......] - ETA: 17:20 - loss: 1.7971 - regression_loss: 1.3695 - classification_loss 7811/10000 [======================>.......] - ETA: 17:20 - loss: 1.7971 - regression_loss: 1.3696 - classification_loss 7812/10000 [======================>.......] - ETA: 17:19 - loss: 1.7971 - regression_loss: 1.3696 - classification_loss 7813/10000 [======================>.......] - ETA: 17:19 - loss: 1.7972 - regression_loss: 1.3696 - classification_loss 7814/10000 [======================>.......] - ETA: 17:18 - loss: 1.7972 - regression_loss: 1.3697 - classification_loss 7815/10000 [======================>.......] - ETA: 17:18 - loss: 1.7972 - regression_loss: 1.3697 - classification_loss 7816/10000 [======================>.......] - ETA: 17:17 - loss: 1.7972 - regression_loss: 1.3697 - classification_loss 7817/10000 [======================>.......] - ETA: 17:17 - loss: 1.7972 - regression_loss: 1.3697 - classification_loss 7818/10000 [======================>.......] - ETA: 17:16 - loss: 1.7971 - regression_loss: 1.3696 - classification_loss 7819/10000 [======================>.......] - ETA: 17:16 - loss: 1.7971 - regression_loss: 1.3696 - classification_loss 7820/10000 [======================>.......] - ETA: 17:15 - loss: 1.7970 - regression_loss: 1.3696 - classification_loss 7821/10000 [======================>.......] - ETA: 17:15 - loss: 1.7970 - regression_loss: 1.3696 - classification_loss 7822/10000 [======================>.......] - ETA: 17:14 - loss: 1.7970 - regression_loss: 1.3696 - classification_loss 7823/10000 [======================>.......] - ETA: 17:14 - loss: 1.7969 - regression_loss: 1.3695 - classification_loss 7824/10000 [======================>.......] - ETA: 17:13 - loss: 1.7969 - regression_loss: 1.3695 - classification_loss 7825/10000 [======================>.......] - ETA: 17:13 - loss: 1.7968 - regression_loss: 1.3694 - classification_loss 7826/10000 [======================>.......] - ETA: 17:12 - loss: 1.7967 - regression_loss: 1.3694 - classification_loss 7827/10000 [======================>.......] - ETA: 17:12 - loss: 1.7967 - regression_loss: 1.3694 - classification_loss 7828/10000 [======================>.......] - ETA: 17:11 - loss: 1.7967 - regression_loss: 1.3694 - classification_loss 7829/10000 [======================>.......] - ETA: 17:11 - loss: 1.7966 - regression_loss: 1.3693 - classification_loss 7830/10000 [======================>.......] - ETA: 17:10 - loss: 1.7966 - regression_loss: 1.3693 - classification_loss 7831/10000 [======================>.......] - ETA: 17:10 - loss: 1.7965 - regression_loss: 1.3692 - classification_loss 7832/10000 [======================>.......] - ETA: 17:09 - loss: 1.7965 - regression_loss: 1.3692 - classification_loss 7833/10000 [======================>.......] - ETA: 17:09 - loss: 1.7965 - regression_loss: 1.3692 - classification_loss 7834/10000 [======================>.......] - ETA: 17:09 - loss: 1.7965 - regression_loss: 1.3692 - classification_loss 7835/10000 [======================>.......] - ETA: 17:08 - loss: 1.7966 - regression_loss: 1.3693 - classification_loss 7836/10000 [======================>.......] - ETA: 17:08 - loss: 1.7967 - regression_loss: 1.3694 - classification_loss 7837/10000 [======================>.......] - ETA: 17:07 - loss: 1.7966 - regression_loss: 1.3693 - classification_loss 7838/10000 [======================>.......] - ETA: 17:07 - loss: 1.7966 - regression_loss: 1.3693 - classification_loss 7839/10000 [======================>.......] - ETA: 17:06 - loss: 1.7966 - regression_loss: 1.3693 - classification_loss 7840/10000 [======================>.......] - ETA: 17:06 - loss: 1.7965 - regression_loss: 1.3693 - classification_loss 7841/10000 [======================>.......] - ETA: 17:05 - loss: 1.7966 - regression_loss: 1.3694 - classification_loss 7842/10000 [======================>.......] - ETA: 17:05 - loss: 1.7966 - regression_loss: 1.3694 - classification_loss 7843/10000 [======================>.......] - ETA: 17:04 - loss: 1.7966 - regression_loss: 1.3694 - classification_loss 7844/10000 [======================>.......] - ETA: 17:04 - loss: 1.7966 - regression_loss: 1.3694 - classification_loss 7845/10000 [======================>.......] - ETA: 17:03 - loss: 1.7965 - regression_loss: 1.3693 - classification_loss 7846/10000 [======================>.......] - ETA: 17:03 - loss: 1.7964 - regression_loss: 1.3693 - classification_loss 7847/10000 [======================>.......] - ETA: 17:02 - loss: 1.7965 - regression_loss: 1.3694 - classification_loss 7848/10000 [======================>.......] - ETA: 17:02 - loss: 1.7965 - regression_loss: 1.3693 - classification_loss 7849/10000 [======================>.......] - ETA: 17:01 - loss: 1.7966 - regression_loss: 1.3694 - classification_loss 7850/10000 [======================>.......] - ETA: 17:01 - loss: 1.7966 - regression_loss: 1.3694 - classification_loss 7851/10000 [======================>.......] - ETA: 17:00 - loss: 1.7966 - regression_loss: 1.3694 - classification_loss 7852/10000 [======================>.......] - ETA: 17:00 - loss: 1.7965 - regression_loss: 1.3693 - classification_loss 7853/10000 [======================>.......] - ETA: 16:59 - loss: 1.7964 - regression_loss: 1.3693 - classification_loss 7854/10000 [======================>.......] - ETA: 16:59 - loss: 1.7965 - regression_loss: 1.3694 - classification_loss 7855/10000 [======================>.......] - ETA: 16:58 - loss: 1.7966 - regression_loss: 1.3695 - classification_loss 7856/10000 [======================>.......] - ETA: 16:58 - loss: 1.7966 - regression_loss: 1.3695 - classification_loss 7857/10000 [======================>.......] - ETA: 16:58 - loss: 1.7966 - regression_loss: 1.3695 - classification_loss 7858/10000 [======================>.......] - ETA: 16:57 - loss: 1.7967 - regression_loss: 1.3696 - classification_loss 7859/10000 [======================>.......] - ETA: 16:57 - loss: 1.7967 - regression_loss: 1.3696 - classification_loss 7860/10000 [======================>.......] - ETA: 16:56 - loss: 1.7966 - regression_loss: 1.3695 - classification_loss 7861/10000 [======================>.......] - ETA: 16:56 - loss: 1.7966 - regression_loss: 1.3695 - classification_loss 7862/10000 [======================>.......] - ETA: 16:55 - loss: 1.7966 - regression_loss: 1.3695 - classification_loss 7863/10000 [======================>.......] - ETA: 16:55 - loss: 1.7967 - regression_loss: 1.3696 - classification_loss 7864/10000 [======================>.......] - ETA: 16:54 - loss: 1.7968 - regression_loss: 1.3697 - classification_loss 7865/10000 [======================>.......] - ETA: 16:54 - loss: 1.7969 - regression_loss: 1.3698 - classification_loss 7866/10000 [======================>.......] - ETA: 16:53 - loss: 1.7970 - regression_loss: 1.3699 - classification_loss 7867/10000 [======================>.......] - ETA: 16:53 - loss: 1.7969 - regression_loss: 1.3698 - classification_loss 7868/10000 [======================>.......] - ETA: 16:52 - loss: 1.7968 - regression_loss: 1.3698 - classification_loss 7869/10000 [======================>.......] - ETA: 16:52 - loss: 1.7968 - regression_loss: 1.3697 - classification_loss 7870/10000 [======================>.......] - ETA: 16:51 - loss: 1.7968 - regression_loss: 1.3697 - classification_loss 7871/10000 [======================>.......] - ETA: 16:51 - loss: 1.7968 - regression_loss: 1.3697 - classification_loss 7872/10000 [======================>.......] - ETA: 16:50 - loss: 1.7968 - regression_loss: 1.3697 - classification_loss 7873/10000 [======================>.......] - ETA: 16:50 - loss: 1.7969 - regression_loss: 1.3698 - classification_loss 7874/10000 [======================>.......] - ETA: 16:49 - loss: 1.7968 - regression_loss: 1.3697 - classification_loss 7875/10000 [======================>.......] - ETA: 16:49 - loss: 1.7969 - regression_loss: 1.3697 - classification_loss 7876/10000 [======================>.......] - ETA: 16:48 - loss: 1.7968 - regression_loss: 1.3697 - classification_loss 7877/10000 [======================>.......] - ETA: 16:48 - loss: 1.7968 - regression_loss: 1.3697 - classification_loss 7878/10000 [======================>.......] - ETA: 16:47 - loss: 1.7968 - regression_loss: 1.3697 - classification_loss 7879/10000 [======================>.......] - ETA: 16:47 - loss: 1.7970 - regression_loss: 1.3698 - classification_loss 7880/10000 [======================>.......] - ETA: 16:46 - loss: 1.7970 - regression_loss: 1.3699 - classification_loss 7881/10000 [======================>.......] - ETA: 16:46 - loss: 1.7971 - regression_loss: 1.3699 - classification_loss 7882/10000 [======================>.......] - ETA: 16:46 - loss: 1.7971 - regression_loss: 1.3699 - classification_loss 7883/10000 [======================>.......] - ETA: 16:45 - loss: 1.7971 - regression_loss: 1.3699 - classification_loss 7884/10000 [======================>.......] - ETA: 16:45 - loss: 1.7972 - regression_loss: 1.3700 - classification_loss 7885/10000 [======================>.......] - ETA: 16:44 - loss: 1.7971 - regression_loss: 1.3700 - classification_loss 7886/10000 [======================>.......] - ETA: 16:44 - loss: 1.7971 - regression_loss: 1.3699 - classification_loss 7887/10000 [======================>.......] - ETA: 16:43 - loss: 1.7970 - regression_loss: 1.3698 - classification_loss 7888/10000 [======================>.......] - ETA: 16:43 - loss: 1.7970 - regression_loss: 1.3698 - classification_loss 7889/10000 [======================>.......] - ETA: 16:42 - loss: 1.7970 - regression_loss: 1.3699 - classification_loss 7890/10000 [======================>.......] - ETA: 16:42 - loss: 1.7971 - regression_loss: 1.3700 - classification_loss 7891/10000 [======================>.......] - ETA: 16:41 - loss: 1.7971 - regression_loss: 1.3700 - classification_loss 7892/10000 [======================>.......] - ETA: 16:41 - loss: 1.7971 - regression_loss: 1.3700 - classification_loss 7893/10000 [======================>.......] - ETA: 16:40 - loss: 1.7971 - regression_loss: 1.3700 - classification_loss 7894/10000 [======================>.......] - ETA: 16:40 - loss: 1.7971 - regression_loss: 1.3700 - classification_loss 7895/10000 [======================>.......] - ETA: 16:39 - loss: 1.7971 - regression_loss: 1.3700 - classification_loss 7896/10000 [======================>.......] - ETA: 16:39 - loss: 1.7970 - regression_loss: 1.3700 - classification_loss 7897/10000 [======================>.......] - ETA: 16:38 - loss: 1.7971 - regression_loss: 1.3700 - classification_loss 7898/10000 [======================>.......] - ETA: 16:38 - loss: 1.7970 - regression_loss: 1.3700 - classification_loss 7899/10000 [======================>.......] - ETA: 16:37 - loss: 1.7970 - regression_loss: 1.3700 - classification_loss 7900/10000 [======================>.......] - ETA: 16:37 - loss: 1.7969 - regression_loss: 1.3699 - classification_loss 7901/10000 [======================>.......] - ETA: 16:36 - loss: 1.7969 - regression_loss: 1.3699 - classification_loss 7902/10000 [======================>.......] - ETA: 16:36 - loss: 1.7968 - regression_loss: 1.3699 - classification_loss 7903/10000 [======================>.......] - ETA: 16:35 - loss: 1.7968 - regression_loss: 1.3698 - classification_loss 7904/10000 [======================>.......] - ETA: 16:35 - loss: 1.7968 - regression_loss: 1.3699 - classification_loss 7905/10000 [======================>.......] - ETA: 16:35 - loss: 1.7967 - regression_loss: 1.3698 - classification_loss 7906/10000 [======================>.......] - ETA: 16:34 - loss: 1.7967 - regression_loss: 1.3697 - classification_loss 7907/10000 [======================>.......] - ETA: 16:34 - loss: 1.7966 - regression_loss: 1.3696 - classification_loss 7908/10000 [======================>.......] - ETA: 16:33 - loss: 1.7966 - regression_loss: 1.3696 - classification_loss 7909/10000 [======================>.......] - ETA: 16:33 - loss: 1.7968 - regression_loss: 1.3698 - classification_loss 7910/10000 [======================>.......] - ETA: 16:32 - loss: 1.7968 - regression_loss: 1.3699 - classification_loss 7911/10000 [======================>.......] - ETA: 16:32 - loss: 1.7968 - regression_loss: 1.3698 - classification_loss 7912/10000 [======================>.......] - ETA: 16:31 - loss: 1.7968 - regression_loss: 1.3699 - classification_loss 7913/10000 [======================>.......] - ETA: 16:31 - loss: 1.7968 - regression_loss: 1.3699 - classification_loss 7914/10000 [======================>.......] - ETA: 16:30 - loss: 1.7968 - regression_loss: 1.3699 - classification_loss 7915/10000 [======================>.......] - ETA: 16:30 - loss: 1.7969 - regression_loss: 1.3700 - classification_loss 7916/10000 [======================>.......] - ETA: 16:29 - loss: 1.7968 - regression_loss: 1.3699 - classification_loss 7917/10000 [======================>.......] - ETA: 16:29 - loss: 1.7968 - regression_loss: 1.3699 - classification_loss 7918/10000 [======================>.......] - ETA: 16:28 - loss: 1.7967 - regression_loss: 1.3698 - classification_loss 7919/10000 [======================>.......] - ETA: 16:28 - loss: 1.7969 - regression_loss: 1.3699 - classification_loss 7920/10000 [======================>.......] - ETA: 16:27 - loss: 1.7967 - regression_loss: 1.3698 - classification_loss 7921/10000 [======================>.......] - ETA: 16:27 - loss: 1.7966 - regression_loss: 1.3697 - classification_loss 7922/10000 [======================>.......] - ETA: 16:26 - loss: 1.7967 - regression_loss: 1.3698 - classification_loss 7923/10000 [======================>.......] - ETA: 16:26 - loss: 1.7967 - regression_loss: 1.3698 - classification_loss 7924/10000 [======================>.......] - ETA: 16:25 - loss: 1.7966 - regression_loss: 1.3698 - classification_loss 7925/10000 [======================>.......] - ETA: 16:25 - loss: 1.7966 - regression_loss: 1.3698 - classification_loss 7926/10000 [======================>.......] - ETA: 16:24 - loss: 1.7965 - regression_loss: 1.3698 - classification_loss 7927/10000 [======================>.......] - ETA: 16:24 - loss: 1.7965 - regression_loss: 1.3697 - classification_loss 7928/10000 [======================>.......] - ETA: 16:24 - loss: 1.7964 - regression_loss: 1.3696 - classification_loss 7929/10000 [======================>.......] - ETA: 16:23 - loss: 1.7963 - regression_loss: 1.3696 - classification_loss 7930/10000 [======================>.......] - ETA: 16:23 - loss: 1.7963 - regression_loss: 1.3695 - classification_loss 7931/10000 [======================>.......] - ETA: 16:22 - loss: 1.7964 - regression_loss: 1.3696 - classification_loss 7932/10000 [======================>.......] - ETA: 16:22 - loss: 1.7963 - regression_loss: 1.3695 - classification_loss 7933/10000 [======================>.......] - ETA: 16:21 - loss: 1.7963 - regression_loss: 1.3695 - classification_loss 7934/10000 [======================>.......] - ETA: 16:21 - loss: 1.7963 - regression_loss: 1.3695 - classification_loss 7935/10000 [======================>.......] - ETA: 16:20 - loss: 1.7964 - regression_loss: 1.3695 - classification_loss 7936/10000 [======================>.......] - ETA: 16:20 - loss: 1.7964 - regression_loss: 1.3696 - classification_loss 7937/10000 [======================>.......] - ETA: 16:19 - loss: 1.7964 - regression_loss: 1.3696 - classification_loss 7938/10000 [======================>.......] - ETA: 16:19 - loss: 1.7965 - regression_loss: 1.3696 - classification_loss 7939/10000 [======================>.......] - ETA: 16:18 - loss: 1.7966 - regression_loss: 1.3697 - classification_loss 7940/10000 [======================>.......] - ETA: 16:18 - loss: 1.7965 - regression_loss: 1.3696 - classification_loss 7941/10000 [======================>.......] - ETA: 16:17 - loss: 1.7965 - regression_loss: 1.3696 - classification_loss 7942/10000 [======================>.......] - ETA: 16:17 - loss: 1.7964 - regression_loss: 1.3696 - classification_loss 7943/10000 [======================>.......] - ETA: 16:16 - loss: 1.7965 - regression_loss: 1.3697 - classification_loss 7944/10000 [======================>.......] - ETA: 16:16 - loss: 1.7965 - regression_loss: 1.3696 - classification_loss 7945/10000 [======================>.......] - ETA: 16:15 - loss: 1.7964 - regression_loss: 1.3696 - classification_loss 7946/10000 [======================>.......] - ETA: 16:15 - loss: 1.7963 - regression_loss: 1.3695 - classification_loss 7947/10000 [======================>.......] - ETA: 16:14 - loss: 1.7963 - regression_loss: 1.3695 - classification_loss 7948/10000 [======================>.......] - ETA: 16:14 - loss: 1.7963 - regression_loss: 1.3695 - classification_loss 7949/10000 [======================>.......] - ETA: 16:14 - loss: 1.7963 - regression_loss: 1.3695 - classification_loss 7950/10000 [======================>.......] - ETA: 16:13 - loss: 1.7962 - regression_loss: 1.3694 - classification_loss 7951/10000 [======================>.......] - ETA: 16:13 - loss: 1.7963 - regression_loss: 1.3694 - classification_loss 7952/10000 [======================>.......] - ETA: 16:12 - loss: 1.7965 - regression_loss: 1.3697 - classification_loss 7953/10000 [======================>.......] - ETA: 16:12 - loss: 1.7965 - regression_loss: 1.3697 - classification_loss 7954/10000 [======================>.......] - ETA: 16:11 - loss: 1.7965 - regression_loss: 1.3696 - classification_loss 7955/10000 [======================>.......] - ETA: 16:11 - loss: 1.7964 - regression_loss: 1.3696 - classification_loss 7956/10000 [======================>.......] - ETA: 16:10 - loss: 1.7963 - regression_loss: 1.3695 - classification_loss 7957/10000 [======================>.......] - ETA: 16:10 - loss: 1.7962 - regression_loss: 1.3694 - classification_loss 7958/10000 [======================>.......] - ETA: 16:09 - loss: 1.7961 - regression_loss: 1.3694 - classification_loss 7959/10000 [======================>.......] - ETA: 16:09 - loss: 1.7962 - regression_loss: 1.3694 - classification_loss 7960/10000 [======================>.......] - ETA: 16:08 - loss: 1.7962 - regression_loss: 1.3694 - classification_loss 7961/10000 [======================>.......] - ETA: 16:08 - loss: 1.7962 - regression_loss: 1.3694 - classification_loss 7962/10000 [======================>.......] - ETA: 16:07 - loss: 1.7962 - regression_loss: 1.3694 - classification_loss 7963/10000 [======================>.......] - ETA: 16:07 - loss: 1.7961 - regression_loss: 1.3694 - classification_loss 7964/10000 [======================>.......] - ETA: 16:06 - loss: 1.7961 - regression_loss: 1.3694 - classification_loss 7965/10000 [======================>.......] - ETA: 16:06 - loss: 1.7960 - regression_loss: 1.3693 - classification_loss 7966/10000 [======================>.......] - ETA: 16:05 - loss: 1.7960 - regression_loss: 1.3693 - classification_loss 7967/10000 [======================>.......] - ETA: 16:05 - loss: 1.7959 - regression_loss: 1.3692 - classification_loss 7968/10000 [======================>.......] - ETA: 16:05 - loss: 1.7958 - regression_loss: 1.3691 - classification_loss 7969/10000 [======================>.......] - ETA: 16:04 - loss: 1.7958 - regression_loss: 1.3691 - classification_loss 7970/10000 [======================>.......] - ETA: 16:04 - loss: 1.7958 - regression_loss: 1.3691 - classification_loss 7971/10000 [======================>.......] - ETA: 16:03 - loss: 1.7958 - regression_loss: 1.3692 - classification_loss 7972/10000 [======================>.......] - ETA: 16:03 - loss: 1.7958 - regression_loss: 1.3692 - classification_loss 7973/10000 [======================>.......] - ETA: 16:02 - loss: 1.7958 - regression_loss: 1.3692 - classification_loss 7974/10000 [======================>.......] - ETA: 16:02 - loss: 1.7958 - regression_loss: 1.3691 - classification_loss 7975/10000 [======================>.......] - ETA: 16:01 - loss: 1.7958 - regression_loss: 1.3692 - classification_loss 7976/10000 [======================>.......] - ETA: 16:01 - loss: 1.7957 - regression_loss: 1.3691 - classification_loss 7977/10000 [======================>.......] - ETA: 16:00 - loss: 1.7957 - regression_loss: 1.3691 - classification_loss 7978/10000 [======================>.......] - ETA: 16:00 - loss: 1.7956 - regression_loss: 1.3690 - classification_loss 7979/10000 [======================>.......] - ETA: 15:59 - loss: 1.7956 - regression_loss: 1.3690 - classification_loss 7980/10000 [======================>.......] - ETA: 15:59 - loss: 1.7956 - regression_loss: 1.3691 - classification_loss 7981/10000 [======================>.......] - ETA: 15:58 - loss: 1.7957 - regression_loss: 1.3691 - classification_loss 7982/10000 [======================>.......] - ETA: 15:58 - loss: 1.7956 - regression_loss: 1.3690 - classification_loss 7983/10000 [======================>.......] - ETA: 15:57 - loss: 1.7956 - regression_loss: 1.3690 - classification_loss 7984/10000 [======================>.......] - ETA: 15:57 - loss: 1.7956 - regression_loss: 1.3689 - classification_loss 7985/10000 [======================>.......] - ETA: 15:56 - loss: 1.7956 - regression_loss: 1.3689 - classification_loss 7986/10000 [======================>.......] - ETA: 15:56 - loss: 1.7955 - regression_loss: 1.3689 - classification_loss 7987/10000 [======================>.......] - ETA: 15:55 - loss: 1.7955 - regression_loss: 1.3689 - classification_loss 7988/10000 [======================>.......] - ETA: 15:55 - loss: 1.7955 - regression_loss: 1.3689 - classification_loss 7989/10000 [======================>.......] - ETA: 15:54 - loss: 1.7955 - regression_loss: 1.3689 - classification_loss 7990/10000 [======================>.......] - ETA: 15:54 - loss: 1.7954 - regression_loss: 1.3688 - classification_loss 7991/10000 [======================>.......] - ETA: 15:54 - loss: 1.7954 - regression_loss: 1.3688 - classification_loss 7992/10000 [======================>.......] - ETA: 15:53 - loss: 1.7953 - regression_loss: 1.3687 - classification_loss 7993/10000 [======================>.......] - ETA: 15:53 - loss: 1.7952 - regression_loss: 1.3686 - classification_loss 7994/10000 [======================>.......] - ETA: 15:52 - loss: 1.7952 - regression_loss: 1.3686 - classification_loss 7995/10000 [======================>.......] - ETA: 15:52 - loss: 1.7952 - regression_loss: 1.3687 - classification_loss 7996/10000 [======================>.......] - ETA: 15:51 - loss: 1.7952 - regression_loss: 1.3686 - classification_loss 7997/10000 [======================>.......] - ETA: 15:51 - loss: 1.7950 - regression_loss: 1.3685 - classification_loss 7998/10000 [======================>.......] - ETA: 15:50 - loss: 1.7951 - regression_loss: 1.3685 - classification_loss 7999/10000 [======================>.......] - ETA: 15:50 - loss: 1.7950 - regression_loss: 1.3685 - classification_loss 8000/10000 [=======================>......] - ETA: 15:49 - loss: 1.7951 - regression_loss: 1.3686 - classification_loss 8001/10000 [=======================>......] - ETA: 15:49 - loss: 1.7951 - regression_loss: 1.3686 - classification_loss 8002/10000 [=======================>......] - ETA: 15:48 - loss: 1.7950 - regression_loss: 1.3685 - classification_loss 8003/10000 [=======================>......] - ETA: 15:48 - loss: 1.7950 - regression_loss: 1.3684 - classification_loss 8004/10000 [=======================>......] - ETA: 15:47 - loss: 1.7950 - regression_loss: 1.3685 - classification_loss 8005/10000 [=======================>......] - ETA: 15:47 - loss: 1.7951 - regression_loss: 1.3685 - classification_loss 8006/10000 [=======================>......] - ETA: 15:46 - loss: 1.7951 - regression_loss: 1.3686 - classification_loss 8007/10000 [=======================>......] - ETA: 15:46 - loss: 1.7951 - regression_loss: 1.3685 - classification_loss 8008/10000 [=======================>......] - ETA: 15:45 - loss: 1.7951 - regression_loss: 1.3686 - classification_loss 8009/10000 [=======================>......] - ETA: 15:45 - loss: 1.7950 - regression_loss: 1.3685 - classification_loss 8010/10000 [=======================>......] - ETA: 15:44 - loss: 1.7951 - regression_loss: 1.3685 - classification_loss 8011/10000 [=======================>......] - ETA: 15:44 - loss: 1.7951 - regression_loss: 1.3686 - classification_loss 8012/10000 [=======================>......] - ETA: 15:43 - loss: 1.7951 - regression_loss: 1.3685 - classification_loss 8013/10000 [=======================>......] - ETA: 15:43 - loss: 1.7951 - regression_loss: 1.3686 - classification_loss 8014/10000 [=======================>......] - ETA: 15:43 - loss: 1.7951 - regression_loss: 1.3686 - classification_loss 8015/10000 [=======================>......] - ETA: 15:42 - loss: 1.7951 - regression_loss: 1.3686 - classification_loss 8016/10000 [=======================>......] - ETA: 15:42 - loss: 1.7950 - regression_loss: 1.3685 - classification_loss 8017/10000 [=======================>......] - ETA: 15:41 - loss: 1.7951 - regression_loss: 1.3686 - classification_loss 8018/10000 [=======================>......] - ETA: 15:41 - loss: 1.7951 - regression_loss: 1.3686 - classification_loss 8019/10000 [=======================>......] - ETA: 15:40 - loss: 1.7953 - regression_loss: 1.3687 - classification_loss 8020/10000 [=======================>......] - ETA: 15:40 - loss: 1.7952 - regression_loss: 1.3687 - classification_loss 8021/10000 [=======================>......] - ETA: 15:39 - loss: 1.7953 - regression_loss: 1.3688 - classification_loss 8022/10000 [=======================>......] - ETA: 15:39 - loss: 1.7952 - regression_loss: 1.3687 - classification_loss 8023/10000 [=======================>......] - ETA: 15:38 - loss: 1.7953 - regression_loss: 1.3688 - classification_loss 8024/10000 [=======================>......] - ETA: 15:38 - loss: 1.7955 - regression_loss: 1.3689 - classification_loss 8025/10000 [=======================>......] - ETA: 15:37 - loss: 1.7955 - regression_loss: 1.3689 - classification_loss 8026/10000 [=======================>......] - ETA: 15:37 - loss: 1.7955 - regression_loss: 1.3690 - classification_loss 8027/10000 [=======================>......] - ETA: 15:36 - loss: 1.7955 - regression_loss: 1.3689 - classification_loss 8028/10000 [=======================>......] - ETA: 15:36 - loss: 1.7953 - regression_loss: 1.3688 - classification_loss 8029/10000 [=======================>......] - ETA: 15:35 - loss: 1.7953 - regression_loss: 1.3688 - classification_loss 8030/10000 [=======================>......] - ETA: 15:35 - loss: 1.7952 - regression_loss: 1.3687 - classification_loss 8031/10000 [=======================>......] - ETA: 15:34 - loss: 1.7952 - regression_loss: 1.3688 - classification_loss 8032/10000 [=======================>......] - ETA: 15:34 - loss: 1.7953 - regression_loss: 1.3688 - classification_loss 8033/10000 [=======================>......] - ETA: 15:33 - loss: 1.7952 - regression_loss: 1.3688 - classification_loss 8034/10000 [=======================>......] - ETA: 15:33 - loss: 1.7953 - regression_loss: 1.3688 - classification_loss 8035/10000 [=======================>......] - ETA: 15:32 - loss: 1.7952 - regression_loss: 1.3687 - classification_loss 8036/10000 [=======================>......] - ETA: 15:32 - loss: 1.7952 - regression_loss: 1.3687 - classification_loss 8037/10000 [=======================>......] - ETA: 15:32 - loss: 1.7951 - regression_loss: 1.3686 - classification_loss 8038/10000 [=======================>......] - ETA: 15:31 - loss: 1.7950 - regression_loss: 1.3685 - classification_loss 8039/10000 [=======================>......] - ETA: 15:31 - loss: 1.7950 - regression_loss: 1.3685 - classification_loss 8040/10000 [=======================>......] - ETA: 15:30 - loss: 1.7950 - regression_loss: 1.3685 - classification_loss 8041/10000 [=======================>......] - ETA: 15:30 - loss: 1.7950 - regression_loss: 1.3686 - classification_loss 8042/10000 [=======================>......] - ETA: 15:29 - loss: 1.7949 - regression_loss: 1.3685 - classification_loss 8043/10000 [=======================>......] - ETA: 15:29 - loss: 1.7950 - regression_loss: 1.3686 - classification_loss 8044/10000 [=======================>......] - ETA: 15:28 - loss: 1.7949 - regression_loss: 1.3685 - classification_loss 8045/10000 [=======================>......] - ETA: 15:28 - loss: 1.7950 - regression_loss: 1.3686 - classification_loss 8046/10000 [=======================>......] - ETA: 15:27 - loss: 1.7951 - regression_loss: 1.3686 - classification_loss 8047/10000 [=======================>......] - ETA: 15:27 - loss: 1.7950 - regression_loss: 1.3687 - classification_loss 8048/10000 [=======================>......] - ETA: 15:26 - loss: 1.7952 - regression_loss: 1.3688 - classification_loss 8049/10000 [=======================>......] - ETA: 15:26 - loss: 1.7952 - regression_loss: 1.3688 - classification_loss 8050/10000 [=======================>......] - ETA: 15:25 - loss: 1.7952 - regression_loss: 1.3688 - classification_loss 8051/10000 [=======================>......] - ETA: 15:25 - loss: 1.7951 - regression_loss: 1.3687 - classification_loss 8052/10000 [=======================>......] - ETA: 15:24 - loss: 1.7951 - regression_loss: 1.3687 - classification_loss 8053/10000 [=======================>......] - ETA: 15:24 - loss: 1.7953 - regression_loss: 1.3688 - classification_loss 8054/10000 [=======================>......] - ETA: 15:23 - loss: 1.7954 - regression_loss: 1.3689 - classification_loss 8055/10000 [=======================>......] - ETA: 15:23 - loss: 1.7953 - regression_loss: 1.3689 - classification_loss 8056/10000 [=======================>......] - ETA: 15:22 - loss: 1.7953 - regression_loss: 1.3689 - classification_loss 8057/10000 [=======================>......] - ETA: 15:22 - loss: 1.7952 - regression_loss: 1.3688 - classification_loss 8058/10000 [=======================>......] - ETA: 15:21 - loss: 1.7951 - regression_loss: 1.3687 - classification_loss 8059/10000 [=======================>......] - ETA: 15:21 - loss: 1.7951 - regression_loss: 1.3687 - classification_loss 8060/10000 [=======================>......] - ETA: 15:20 - loss: 1.7952 - regression_loss: 1.3687 - classification_loss 8061/10000 [=======================>......] - ETA: 15:20 - loss: 1.7951 - regression_loss: 1.3687 - classification_loss 8062/10000 [=======================>......] - ETA: 15:20 - loss: 1.7952 - regression_loss: 1.3688 - classification_loss 8063/10000 [=======================>......] - ETA: 15:19 - loss: 1.7953 - regression_loss: 1.3689 - classification_loss 8064/10000 [=======================>......] - ETA: 15:19 - loss: 1.7952 - regression_loss: 1.3688 - classification_loss 8065/10000 [=======================>......] - ETA: 15:18 - loss: 1.7952 - regression_loss: 1.3688 - classification_loss 8066/10000 [=======================>......] - ETA: 15:18 - loss: 1.7951 - regression_loss: 1.3688 - classification_loss 8067/10000 [=======================>......] - ETA: 15:17 - loss: 1.7951 - regression_loss: 1.3687 - classification_loss 8068/10000 [=======================>......] - ETA: 15:17 - loss: 1.7952 - regression_loss: 1.3688 - classification_loss 8069/10000 [=======================>......] - ETA: 15:16 - loss: 1.7951 - regression_loss: 1.3687 - classification_loss 8070/10000 [=======================>......] - ETA: 15:16 - loss: 1.7951 - regression_loss: 1.3688 - classification_loss 8071/10000 [=======================>......] - ETA: 15:15 - loss: 1.7952 - regression_loss: 1.3688 - classification_loss 8072/10000 [=======================>......] - ETA: 15:15 - loss: 1.7951 - regression_loss: 1.3688 - classification_loss 8073/10000 [=======================>......] - ETA: 15:14 - loss: 1.7950 - regression_loss: 1.3687 - classification_loss 8074/10000 [=======================>......] - ETA: 15:14 - loss: 1.7950 - regression_loss: 1.3687 - classification_loss 8075/10000 [=======================>......] - ETA: 15:13 - loss: 1.7950 - regression_loss: 1.3688 - classification_loss 8076/10000 [=======================>......] - ETA: 15:13 - loss: 1.7950 - regression_loss: 1.3687 - classification_loss 8077/10000 [=======================>......] - ETA: 15:12 - loss: 1.7950 - regression_loss: 1.3687 - classification_loss 8078/10000 [=======================>......] - ETA: 15:12 - loss: 1.7949 - regression_loss: 1.3687 - classification_loss 8079/10000 [=======================>......] - ETA: 15:11 - loss: 1.7949 - regression_loss: 1.3687 - classification_loss 8080/10000 [=======================>......] - ETA: 15:11 - loss: 1.7948 - regression_loss: 1.3686 - classification_loss 8081/10000 [=======================>......] - ETA: 15:11 - loss: 1.7947 - regression_loss: 1.3685 - classification_loss 8082/10000 [=======================>......] - ETA: 15:10 - loss: 1.7945 - regression_loss: 1.3684 - classification_loss 8083/10000 [=======================>......] - ETA: 15:10 - loss: 1.7946 - regression_loss: 1.3684 - classification_loss 8084/10000 [=======================>......] - ETA: 15:09 - loss: 1.7947 - regression_loss: 1.3685 - classification_loss 8085/10000 [=======================>......] - ETA: 15:09 - loss: 1.7946 - regression_loss: 1.3685 - classification_loss 8086/10000 [=======================>......] - ETA: 15:08 - loss: 1.7946 - regression_loss: 1.3684 - classification_loss 8087/10000 [=======================>......] - ETA: 15:08 - loss: 1.7946 - regression_loss: 1.3685 - classification_loss 8088/10000 [=======================>......] - ETA: 15:07 - loss: 1.7947 - regression_loss: 1.3685 - classification_loss 8089/10000 [=======================>......] - ETA: 15:07 - loss: 1.7948 - regression_loss: 1.3686 - classification_loss 8090/10000 [=======================>......] - ETA: 15:06 - loss: 1.7948 - regression_loss: 1.3686 - classification_loss 8091/10000 [=======================>......] - ETA: 15:06 - loss: 1.7947 - regression_loss: 1.3686 - classification_loss 8092/10000 [=======================>......] - ETA: 15:05 - loss: 1.7947 - regression_loss: 1.3686 - classification_loss 8093/10000 [=======================>......] - ETA: 15:05 - loss: 1.7948 - regression_loss: 1.3686 - classification_loss 8094/10000 [=======================>......] - ETA: 15:04 - loss: 1.7949 - regression_loss: 1.3687 - classification_loss 8095/10000 [=======================>......] - ETA: 15:04 - loss: 1.7950 - regression_loss: 1.3688 - classification_loss 8096/10000 [=======================>......] - ETA: 15:03 - loss: 1.7949 - regression_loss: 1.3687 - classification_loss 8097/10000 [=======================>......] - ETA: 15:03 - loss: 1.7949 - regression_loss: 1.3688 - classification_loss 8098/10000 [=======================>......] - ETA: 15:02 - loss: 1.7950 - regression_loss: 1.3689 - classification_loss 8099/10000 [=======================>......] - ETA: 15:02 - loss: 1.7951 - regression_loss: 1.3689 - classification_loss 8100/10000 [=======================>......] - ETA: 15:01 - loss: 1.7950 - regression_loss: 1.3688 - classification_loss 8101/10000 [=======================>......] - ETA: 15:01 - loss: 1.7949 - regression_loss: 1.3687 - classification_loss 8102/10000 [=======================>......] - ETA: 15:00 - loss: 1.7949 - regression_loss: 1.3687 - classification_loss 8103/10000 [=======================>......] - ETA: 15:00 - loss: 1.7949 - regression_loss: 1.3687 - classification_loss 8104/10000 [=======================>......] - ETA: 15:00 - loss: 1.7949 - regression_loss: 1.3687 - classification_loss 8105/10000 [=======================>......] - ETA: 14:59 - loss: 1.7949 - regression_loss: 1.3687 - classification_loss 8106/10000 [=======================>......] - ETA: 14:59 - loss: 1.7947 - regression_loss: 1.3686 - classification_loss 8107/10000 [=======================>......] - ETA: 14:58 - loss: 1.7947 - regression_loss: 1.3685 - classification_loss 8108/10000 [=======================>......] - ETA: 14:58 - loss: 1.7947 - regression_loss: 1.3685 - classification_loss 8109/10000 [=======================>......] - ETA: 14:57 - loss: 1.7946 - regression_loss: 1.3685 - classification_loss 8110/10000 [=======================>......] - ETA: 14:57 - loss: 1.7946 - regression_loss: 1.3685 - classification_loss 8111/10000 [=======================>......] - ETA: 14:56 - loss: 1.7946 - regression_loss: 1.3685 - classification_loss 8112/10000 [=======================>......] - ETA: 14:56 - loss: 1.7947 - regression_loss: 1.3686 - classification_loss 8113/10000 [=======================>......] - ETA: 14:55 - loss: 1.7946 - regression_loss: 1.3686 - classification_loss 8114/10000 [=======================>......] - ETA: 14:55 - loss: 1.7947 - regression_loss: 1.3686 - classification_loss 8115/10000 [=======================>......] - ETA: 14:54 - loss: 1.7947 - regression_loss: 1.3687 - classification_loss 8116/10000 [=======================>......] - ETA: 14:54 - loss: 1.7946 - regression_loss: 1.3686 - classification_loss 8117/10000 [=======================>......] - ETA: 14:53 - loss: 1.7947 - regression_loss: 1.3686 - classification_loss 8118/10000 [=======================>......] - ETA: 14:53 - loss: 1.7946 - regression_loss: 1.3686 - classification_loss 8119/10000 [=======================>......] - ETA: 14:52 - loss: 1.7946 - regression_loss: 1.3686 - classification_loss 8120/10000 [=======================>......] - ETA: 14:52 - loss: 1.7946 - regression_loss: 1.3685 - classification_loss 8121/10000 [=======================>......] - ETA: 14:51 - loss: 1.7946 - regression_loss: 1.3686 - classification_loss 8122/10000 [=======================>......] - ETA: 14:51 - loss: 1.7945 - regression_loss: 1.3685 - classification_loss 8123/10000 [=======================>......] - ETA: 14:50 - loss: 1.7945 - regression_loss: 1.3686 - classification_loss 8124/10000 [=======================>......] - ETA: 14:50 - loss: 1.7946 - regression_loss: 1.3687 - classification_loss 8125/10000 [=======================>......] - ETA: 14:50 - loss: 1.7946 - regression_loss: 1.3687 - classification_loss 8126/10000 [=======================>......] - ETA: 14:49 - loss: 1.7947 - regression_loss: 1.3688 - classification_loss 8127/10000 [=======================>......] - ETA: 14:49 - loss: 1.7945 - regression_loss: 1.3686 - classification_loss 8128/10000 [=======================>......] - ETA: 14:48 - loss: 1.7947 - regression_loss: 1.3688 - classification_loss 8129/10000 [=======================>......] - ETA: 14:48 - loss: 1.7947 - regression_loss: 1.3688 - classification_loss 8130/10000 [=======================>......] - ETA: 14:47 - loss: 1.7947 - regression_loss: 1.3688 - classification_loss 8131/10000 [=======================>......] - ETA: 14:47 - loss: 1.7948 - regression_loss: 1.3688 - classification_loss 8132/10000 [=======================>......] - ETA: 14:46 - loss: 1.7948 - regression_loss: 1.3688 - classification_loss 8133/10000 [=======================>......] - ETA: 14:46 - loss: 1.7946 - regression_loss: 1.3687 - classification_loss 8134/10000 [=======================>......] - ETA: 14:45 - loss: 1.7946 - regression_loss: 1.3686 - classification_loss 8135/10000 [=======================>......] - ETA: 14:45 - loss: 1.7946 - regression_loss: 1.3686 - classification_loss 8136/10000 [=======================>......] - ETA: 14:44 - loss: 1.7947 - regression_loss: 1.3687 - classification_loss 8137/10000 [=======================>......] - ETA: 14:44 - loss: 1.7947 - regression_loss: 1.3687 - classification_loss 8138/10000 [=======================>......] - ETA: 14:43 - loss: 1.7946 - regression_loss: 1.3687 - classification_loss 8139/10000 [=======================>......] - ETA: 14:43 - loss: 1.7946 - regression_loss: 1.3687 - classification_loss 8140/10000 [=======================>......] - ETA: 14:42 - loss: 1.7945 - regression_loss: 1.3686 - classification_loss 8141/10000 [=======================>......] - ETA: 14:42 - loss: 1.7946 - regression_loss: 1.3686 - classification_loss 8142/10000 [=======================>......] - ETA: 14:41 - loss: 1.7945 - regression_loss: 1.3686 - classification_loss 8143/10000 [=======================>......] - ETA: 14:41 - loss: 1.7945 - regression_loss: 1.3686 - classification_loss 8144/10000 [=======================>......] - ETA: 14:40 - loss: 1.7946 - regression_loss: 1.3686 - classification_loss 8145/10000 [=======================>......] - ETA: 14:40 - loss: 1.7946 - regression_loss: 1.3687 - classification_loss 8146/10000 [=======================>......] - ETA: 14:40 - loss: 1.7946 - regression_loss: 1.3687 - classification_loss 8147/10000 [=======================>......] - ETA: 14:39 - loss: 1.7945 - regression_loss: 1.3686 - classification_loss 8148/10000 [=======================>......] - ETA: 14:39 - loss: 1.7945 - regression_loss: 1.3687 - classification_loss 8149/10000 [=======================>......] - ETA: 14:38 - loss: 1.7946 - regression_loss: 1.3687 - classification_loss 8150/10000 [=======================>......] - ETA: 14:38 - loss: 1.7946 - regression_loss: 1.3687 - classification_loss 8151/10000 [=======================>......] - ETA: 14:37 - loss: 1.7945 - regression_loss: 1.3687 - classification_loss 8152/10000 [=======================>......] - ETA: 14:37 - loss: 1.7945 - regression_loss: 1.3686 - classification_loss 8153/10000 [=======================>......] - ETA: 14:36 - loss: 1.7945 - regression_loss: 1.3686 - classification_loss 8154/10000 [=======================>......] - ETA: 14:36 - loss: 1.7945 - regression_loss: 1.3686 - classification_loss 8155/10000 [=======================>......] - ETA: 14:35 - loss: 1.7944 - regression_loss: 1.3686 - classification_loss 8156/10000 [=======================>......] - ETA: 14:35 - loss: 1.7944 - regression_loss: 1.3686 - classification_loss 8157/10000 [=======================>......] - ETA: 14:34 - loss: 1.7946 - regression_loss: 1.3687 - classification_loss 8158/10000 [=======================>......] - ETA: 14:34 - loss: 1.7945 - regression_loss: 1.3686 - classification_loss 8159/10000 [=======================>......] - ETA: 14:33 - loss: 1.7943 - regression_loss: 1.3685 - classification_loss 8160/10000 [=======================>......] - ETA: 14:33 - loss: 1.7942 - regression_loss: 1.3684 - classification_loss 8161/10000 [=======================>......] - ETA: 14:32 - loss: 1.7944 - regression_loss: 1.3685 - classification_loss 8162/10000 [=======================>......] - ETA: 14:32 - loss: 1.7944 - regression_loss: 1.3685 - classification_loss 8163/10000 [=======================>......] - ETA: 14:31 - loss: 1.7943 - regression_loss: 1.3684 - classification_loss 8164/10000 [=======================>......] - ETA: 14:31 - loss: 1.7943 - regression_loss: 1.3684 - classification_loss 8165/10000 [=======================>......] - ETA: 14:30 - loss: 1.7943 - regression_loss: 1.3684 - classification_loss 8166/10000 [=======================>......] - ETA: 14:30 - loss: 1.7942 - regression_loss: 1.3683 - classification_loss 8167/10000 [=======================>......] - ETA: 14:30 - loss: 1.7942 - regression_loss: 1.3683 - classification_loss 8168/10000 [=======================>......] - ETA: 14:29 - loss: 1.7943 - regression_loss: 1.3684 - classification_loss 8169/10000 [=======================>......] - ETA: 14:29 - loss: 1.7943 - regression_loss: 1.3684 - classification_loss 8170/10000 [=======================>......] - ETA: 14:28 - loss: 1.7943 - regression_loss: 1.3685 - classification_loss 8171/10000 [=======================>......] - ETA: 14:28 - loss: 1.7943 - regression_loss: 1.3685 - classification_loss 8172/10000 [=======================>......] - ETA: 14:27 - loss: 1.7944 - regression_loss: 1.3686 - classification_loss 8173/10000 [=======================>......] - ETA: 14:27 - loss: 1.7945 - regression_loss: 1.3686 - classification_loss 8174/10000 [=======================>......] - ETA: 14:26 - loss: 1.7946 - regression_loss: 1.3687 - classification_loss 8175/10000 [=======================>......] - ETA: 14:26 - loss: 1.7945 - regression_loss: 1.3687 - classification_loss 8176/10000 [=======================>......] - ETA: 14:25 - loss: 1.7944 - regression_loss: 1.3686 - classification_loss 8177/10000 [=======================>......] - ETA: 14:25 - loss: 1.7944 - regression_loss: 1.3686 - classification_loss 8178/10000 [=======================>......] - ETA: 14:24 - loss: 1.7944 - regression_loss: 1.3685 - classification_loss 8179/10000 [=======================>......] - ETA: 14:24 - loss: 1.7943 - regression_loss: 1.3684 - classification_loss 8180/10000 [=======================>......] - ETA: 14:23 - loss: 1.7942 - regression_loss: 1.3683 - classification_loss 8181/10000 [=======================>......] - ETA: 14:23 - loss: 1.7941 - regression_loss: 1.3683 - classification_loss 8182/10000 [=======================>......] - ETA: 14:22 - loss: 1.7941 - regression_loss: 1.3683 - classification_loss 8183/10000 [=======================>......] - ETA: 14:22 - loss: 1.7941 - regression_loss: 1.3683 - classification_loss 8184/10000 [=======================>......] - ETA: 14:21 - loss: 1.7940 - regression_loss: 1.3682 - classification_loss 8185/10000 [=======================>......] - ETA: 14:21 - loss: 1.7939 - regression_loss: 1.3681 - classification_loss 8186/10000 [=======================>......] - ETA: 14:20 - loss: 1.7938 - regression_loss: 1.3680 - classification_loss 8187/10000 [=======================>......] - ETA: 14:20 - loss: 1.7937 - regression_loss: 1.3680 - classification_loss 8188/10000 [=======================>......] - ETA: 14:20 - loss: 1.7937 - regression_loss: 1.3680 - classification_loss 8189/10000 [=======================>......] - ETA: 14:19 - loss: 1.7939 - regression_loss: 1.3681 - classification_loss 8190/10000 [=======================>......] - ETA: 14:19 - loss: 1.7937 - regression_loss: 1.3680 - classification_loss 8191/10000 [=======================>......] - ETA: 14:18 - loss: 1.7937 - regression_loss: 1.3680 - classification_loss 8192/10000 [=======================>......] - ETA: 14:18 - loss: 1.7937 - regression_loss: 1.3680 - classification_loss 8193/10000 [=======================>......] - ETA: 14:17 - loss: 1.7937 - regression_loss: 1.3680 - classification_loss 8194/10000 [=======================>......] - ETA: 14:17 - loss: 1.7936 - regression_loss: 1.3679 - classification_loss 8195/10000 [=======================>......] - ETA: 14:16 - loss: 1.7936 - regression_loss: 1.3679 - classification_loss 8196/10000 [=======================>......] - ETA: 14:16 - loss: 1.7936 - regression_loss: 1.3679 - classification_loss 8197/10000 [=======================>......] - ETA: 14:15 - loss: 1.7936 - regression_loss: 1.3678 - classification_loss 8198/10000 [=======================>......] - ETA: 14:15 - loss: 1.7936 - regression_loss: 1.3679 - classification_loss 8199/10000 [=======================>......] - ETA: 14:14 - loss: 1.7936 - regression_loss: 1.3678 - classification_loss 8200/10000 [=======================>......] - ETA: 14:14 - loss: 1.7937 - regression_loss: 1.3679 - classification_loss 8201/10000 [=======================>......] - ETA: 14:13 - loss: 1.7937 - regression_loss: 1.3679 - classification_loss 8202/10000 [=======================>......] - ETA: 14:13 - loss: 1.7937 - regression_loss: 1.3679 - classification_loss 8203/10000 [=======================>......] - ETA: 14:12 - loss: 1.7937 - regression_loss: 1.3679 - classification_loss 8204/10000 [=======================>......] - ETA: 14:12 - loss: 1.7937 - regression_loss: 1.3679 - classification_loss 8205/10000 [=======================>......] - ETA: 14:11 - loss: 1.7937 - regression_loss: 1.3679 - classification_loss 8206/10000 [=======================>......] - ETA: 14:11 - loss: 1.7937 - regression_loss: 1.3680 - classification_loss 8207/10000 [=======================>......] - ETA: 14:10 - loss: 1.7937 - regression_loss: 1.3680 - classification_loss 8208/10000 [=======================>......] - ETA: 14:10 - loss: 1.7938 - regression_loss: 1.3679 - classification_loss 8209/10000 [=======================>......] - ETA: 14:09 - loss: 1.7938 - regression_loss: 1.3680 - classification_loss 8210/10000 [=======================>......] - ETA: 14:09 - loss: 1.7937 - regression_loss: 1.3679 - classification_loss 8211/10000 [=======================>......] - ETA: 14:09 - loss: 1.7937 - regression_loss: 1.3679 - classification_loss 8212/10000 [=======================>......] - ETA: 14:08 - loss: 1.7937 - regression_loss: 1.3680 - classification_loss 8213/10000 [=======================>......] - ETA: 14:08 - loss: 1.7936 - regression_loss: 1.3679 - classification_loss 8214/10000 [=======================>......] - ETA: 14:07 - loss: 1.7937 - regression_loss: 1.3679 - classification_loss 8215/10000 [=======================>......] - ETA: 14:07 - loss: 1.7936 - regression_loss: 1.3678 - classification_loss 8216/10000 [=======================>......] - ETA: 14:06 - loss: 1.7936 - regression_loss: 1.3678 - classification_loss 8217/10000 [=======================>......] - ETA: 14:06 - loss: 1.7935 - regression_loss: 1.3678 - classification_loss 8218/10000 [=======================>......] - ETA: 14:05 - loss: 1.7935 - regression_loss: 1.3678 - classification_loss 8219/10000 [=======================>......] - ETA: 14:05 - loss: 1.7936 - regression_loss: 1.3679 - classification_loss 8220/10000 [=======================>......] - ETA: 14:04 - loss: 1.7936 - regression_loss: 1.3679 - classification_loss 8221/10000 [=======================>......] - ETA: 14:04 - loss: 1.7936 - regression_loss: 1.3679 - classification_loss 8222/10000 [=======================>......] - ETA: 14:03 - loss: 1.7935 - regression_loss: 1.3678 - classification_loss 8223/10000 [=======================>......] - ETA: 14:03 - loss: 1.7935 - regression_loss: 1.3678 - classification_loss 8224/10000 [=======================>......] - ETA: 14:02 - loss: 1.7935 - regression_loss: 1.3678 - classification_loss 8225/10000 [=======================>......] - ETA: 14:02 - loss: 1.7934 - regression_loss: 1.3678 - classification_loss 8226/10000 [=======================>......] - ETA: 14:01 - loss: 1.7934 - regression_loss: 1.3677 - classification_loss 8227/10000 [=======================>......] - ETA: 14:01 - loss: 1.7934 - regression_loss: 1.3678 - classification_loss 8228/10000 [=======================>......] - ETA: 14:00 - loss: 1.7934 - regression_loss: 1.3678 - classification_loss 8229/10000 [=======================>......] - ETA: 14:00 - loss: 1.7935 - regression_loss: 1.3679 - classification_loss 8230/10000 [=======================>......] - ETA: 13:59 - loss: 1.7936 - regression_loss: 1.3679 - classification_loss 8231/10000 [=======================>......] - ETA: 13:59 - loss: 1.7935 - regression_loss: 1.3679 - classification_loss 8232/10000 [=======================>......] - ETA: 13:58 - loss: 1.7934 - regression_loss: 1.3678 - classification_loss 8233/10000 [=======================>......] - ETA: 13:58 - loss: 1.7933 - regression_loss: 1.3677 - classification_loss 8234/10000 [=======================>......] - ETA: 13:57 - loss: 1.7934 - regression_loss: 1.3677 - classification_loss 8235/10000 [=======================>......] - ETA: 13:57 - loss: 1.7933 - regression_loss: 1.3677 - classification_loss 8236/10000 [=======================>......] - ETA: 13:57 - loss: 1.7933 - regression_loss: 1.3677 - classification_loss 8237/10000 [=======================>......] - ETA: 13:56 - loss: 1.7933 - regression_loss: 1.3676 - classification_loss 8238/10000 [=======================>......] - ETA: 13:56 - loss: 1.7932 - regression_loss: 1.3676 - classification_loss 8239/10000 [=======================>......] - ETA: 13:55 - loss: 1.7931 - regression_loss: 1.3675 - classification_loss 8240/10000 [=======================>......] - ETA: 13:55 - loss: 1.7930 - regression_loss: 1.3674 - classification_loss 8241/10000 [=======================>......] - ETA: 13:54 - loss: 1.7932 - regression_loss: 1.3676 - classification_loss 8242/10000 [=======================>......] - ETA: 13:54 - loss: 1.7931 - regression_loss: 1.3675 - classification_loss 8243/10000 [=======================>......] - ETA: 13:53 - loss: 1.7931 - regression_loss: 1.3675 - classification_loss 8244/10000 [=======================>......] - ETA: 13:53 - loss: 1.7930 - regression_loss: 1.3675 - classification_loss 8245/10000 [=======================>......] - ETA: 13:52 - loss: 1.7929 - regression_loss: 1.3674 - classification_loss 8246/10000 [=======================>......] - ETA: 13:52 - loss: 1.7929 - regression_loss: 1.3674 - classification_loss 8247/10000 [=======================>......] - ETA: 13:51 - loss: 1.7930 - regression_loss: 1.3674 - classification_loss 8248/10000 [=======================>......] - ETA: 13:51 - loss: 1.7929 - regression_loss: 1.3674 - classification_loss 8249/10000 [=======================>......] - ETA: 13:50 - loss: 1.7928 - regression_loss: 1.3673 - classification_loss 8250/10000 [=======================>......] - ETA: 13:50 - loss: 1.7929 - regression_loss: 1.3673 - classification_loss 8251/10000 [=======================>......] - ETA: 13:49 - loss: 1.7929 - regression_loss: 1.3674 - classification_loss 8252/10000 [=======================>......] - ETA: 13:49 - loss: 1.7929 - regression_loss: 1.3674 - classification_loss 8253/10000 [=======================>......] - ETA: 13:48 - loss: 1.7929 - regression_loss: 1.3673 - classification_loss 8254/10000 [=======================>......] - ETA: 13:48 - loss: 1.7929 - regression_loss: 1.3673 - classification_loss 8255/10000 [=======================>......] - ETA: 13:47 - loss: 1.7929 - regression_loss: 1.3674 - classification_loss 8256/10000 [=======================>......] - ETA: 13:47 - loss: 1.7929 - regression_loss: 1.3673 - classification_loss 8257/10000 [=======================>......] - ETA: 13:47 - loss: 1.7928 - regression_loss: 1.3673 - classification_loss 8258/10000 [=======================>......] - ETA: 13:46 - loss: 1.7929 - regression_loss: 1.3674 - classification_loss 8259/10000 [=======================>......] - ETA: 13:46 - loss: 1.7929 - regression_loss: 1.3674 - classification_loss 8260/10000 [=======================>......] - ETA: 13:45 - loss: 1.7930 - regression_loss: 1.3674 - classification_loss 8261/10000 [=======================>......] - ETA: 13:45 - loss: 1.7929 - regression_loss: 1.3674 - classification_loss 8262/10000 [=======================>......] - ETA: 13:44 - loss: 1.7929 - regression_loss: 1.3674 - classification_loss 8263/10000 [=======================>......] - ETA: 13:44 - loss: 1.7927 - regression_loss: 1.3673 - classification_loss 8264/10000 [=======================>......] - ETA: 13:43 - loss: 1.7927 - regression_loss: 1.3672 - classification_loss 8265/10000 [=======================>......] - ETA: 13:43 - loss: 1.7926 - regression_loss: 1.3672 - classification_loss 8266/10000 [=======================>......] - ETA: 13:42 - loss: 1.7926 - regression_loss: 1.3672 - classification_loss 8267/10000 [=======================>......] - ETA: 13:42 - loss: 1.7925 - regression_loss: 1.3671 - classification_loss 8268/10000 [=======================>......] - ETA: 13:41 - loss: 1.7924 - regression_loss: 1.3670 - classification_loss 8269/10000 [=======================>......] - ETA: 13:41 - loss: 1.7923 - regression_loss: 1.3669 - classification_loss 8270/10000 [=======================>......] - ETA: 13:40 - loss: 1.7923 - regression_loss: 1.3669 - classification_loss 8271/10000 [=======================>......] - ETA: 13:40 - loss: 1.7923 - regression_loss: 1.3669 - classification_loss 8272/10000 [=======================>......] - ETA: 13:39 - loss: 1.7922 - regression_loss: 1.3669 - classification_loss 8273/10000 [=======================>......] - ETA: 13:39 - loss: 1.7923 - regression_loss: 1.3670 - classification_loss 8274/10000 [=======================>......] - ETA: 13:38 - loss: 1.7923 - regression_loss: 1.3669 - classification_loss 8275/10000 [=======================>......] - ETA: 13:38 - loss: 1.7922 - regression_loss: 1.3669 - classification_loss 8276/10000 [=======================>......] - ETA: 13:37 - loss: 1.7922 - regression_loss: 1.3669 - classification_loss 8277/10000 [=======================>......] - ETA: 13:37 - loss: 1.7921 - regression_loss: 1.3668 - classification_loss 8278/10000 [=======================>......] - ETA: 13:36 - loss: 1.7921 - regression_loss: 1.3668 - classification_loss 8279/10000 [=======================>......] - ETA: 13:36 - loss: 1.7921 - regression_loss: 1.3669 - classification_loss 8280/10000 [=======================>......] - ETA: 13:35 - loss: 1.7921 - regression_loss: 1.3669 - classification_loss 8281/10000 [=======================>......] - ETA: 13:35 - loss: 1.7920 - regression_loss: 1.3668 - classification_loss 8282/10000 [=======================>......] - ETA: 13:35 - loss: 1.7920 - regression_loss: 1.3667 - classification_loss 8283/10000 [=======================>......] - ETA: 13:34 - loss: 1.7919 - regression_loss: 1.3667 - classification_loss 8284/10000 [=======================>......] - ETA: 13:34 - loss: 1.7918 - regression_loss: 1.3666 - classification_loss 8285/10000 [=======================>......] - ETA: 13:33 - loss: 1.7917 - regression_loss: 1.3665 - classification_loss 8286/10000 [=======================>......] - ETA: 13:33 - loss: 1.7917 - regression_loss: 1.3665 - classification_loss 8287/10000 [=======================>......] - ETA: 13:32 - loss: 1.7916 - regression_loss: 1.3665 - classification_loss 8288/10000 [=======================>......] - ETA: 13:32 - loss: 1.7915 - regression_loss: 1.3664 - classification_loss 8289/10000 [=======================>......] - ETA: 13:31 - loss: 1.7915 - regression_loss: 1.3664 - classification_loss 8290/10000 [=======================>......] - ETA: 13:31 - loss: 1.7915 - regression_loss: 1.3664 - classification_loss 8291/10000 [=======================>......] - ETA: 13:30 - loss: 1.7915 - regression_loss: 1.3663 - classification_loss 8292/10000 [=======================>......] - ETA: 13:30 - loss: 1.7914 - regression_loss: 1.3663 - classification_loss 8293/10000 [=======================>......] - ETA: 13:29 - loss: 1.7915 - regression_loss: 1.3664 - classification_loss 8294/10000 [=======================>......] - ETA: 13:29 - loss: 1.7915 - regression_loss: 1.3664 - classification_loss 8295/10000 [=======================>......] - ETA: 13:28 - loss: 1.7915 - regression_loss: 1.3664 - classification_loss 8296/10000 [=======================>......] - ETA: 13:28 - loss: 1.7916 - regression_loss: 1.3665 - classification_loss 8297/10000 [=======================>......] - ETA: 13:27 - loss: 1.7915 - regression_loss: 1.3664 - classification_loss 8298/10000 [=======================>......] - ETA: 13:27 - loss: 1.7914 - regression_loss: 1.3663 - classification_loss 8299/10000 [=======================>......] - ETA: 13:26 - loss: 1.7914 - regression_loss: 1.3663 - classification_loss 8300/10000 [=======================>......] - ETA: 13:26 - loss: 1.7915 - regression_loss: 1.3664 - classification_loss 8301/10000 [=======================>......] - ETA: 13:25 - loss: 1.7914 - regression_loss: 1.3663 - classification_loss 8302/10000 [=======================>......] - ETA: 13:25 - loss: 1.7914 - regression_loss: 1.3663 - classification_loss 8303/10000 [=======================>......] - ETA: 13:24 - loss: 1.7914 - regression_loss: 1.3663 - classification_loss 8304/10000 [=======================>......] - ETA: 13:24 - loss: 1.7914 - regression_loss: 1.3663 - classification_loss 8305/10000 [=======================>......] - ETA: 13:24 - loss: 1.7914 - regression_loss: 1.3663 - classification_loss 8306/10000 [=======================>......] - ETA: 13:23 - loss: 1.7914 - regression_loss: 1.3663 - classification_loss 8307/10000 [=======================>......] - ETA: 13:23 - loss: 1.7913 - regression_loss: 1.3663 - classification_loss 8308/10000 [=======================>......] - ETA: 13:22 - loss: 1.7912 - regression_loss: 1.3662 - classification_loss 8309/10000 [=======================>......] - ETA: 13:22 - loss: 1.7913 - regression_loss: 1.3662 - classification_loss 8310/10000 [=======================>......] - ETA: 13:21 - loss: 1.7913 - regression_loss: 1.3662 - classification_loss 8311/10000 [=======================>......] - ETA: 13:21 - loss: 1.7914 - regression_loss: 1.3663 - classification_loss 8312/10000 [=======================>......] - ETA: 13:20 - loss: 1.7914 - regression_loss: 1.3663 - classification_loss 8313/10000 [=======================>......] - ETA: 13:20 - loss: 1.7914 - regression_loss: 1.3663 - classification_loss 8314/10000 [=======================>......] - ETA: 13:19 - loss: 1.7914 - regression_loss: 1.3663 - classification_loss 8315/10000 [=======================>......] - ETA: 13:19 - loss: 1.7914 - regression_loss: 1.3663 - classification_loss 8316/10000 [=======================>......] - ETA: 13:18 - loss: 1.7914 - regression_loss: 1.3663 - classification_loss 8317/10000 [=======================>......] - ETA: 13:18 - loss: 1.7914 - regression_loss: 1.3663 - classification_loss 8318/10000 [=======================>......] - ETA: 13:17 - loss: 1.7913 - regression_loss: 1.3662 - classification_loss 8319/10000 [=======================>......] - ETA: 13:17 - loss: 1.7912 - regression_loss: 1.3662 - classification_loss 8320/10000 [=======================>......] - ETA: 13:16 - loss: 1.7913 - regression_loss: 1.3662 - classification_loss 8321/10000 [=======================>......] - ETA: 13:16 - loss: 1.7913 - regression_loss: 1.3662 - classification_loss 8322/10000 [=======================>......] - ETA: 13:15 - loss: 1.7914 - regression_loss: 1.3663 - classification_loss 8323/10000 [=======================>......] - ETA: 13:15 - loss: 1.7914 - regression_loss: 1.3664 - classification_loss 8324/10000 [=======================>......] - ETA: 13:14 - loss: 1.7914 - regression_loss: 1.3664 - classification_loss 8325/10000 [=======================>......] - ETA: 13:14 - loss: 1.7913 - regression_loss: 1.3663 - classification_loss 8326/10000 [=======================>......] - ETA: 13:14 - loss: 1.7913 - regression_loss: 1.3662 - classification_loss 8327/10000 [=======================>......] - ETA: 13:13 - loss: 1.7912 - regression_loss: 1.3662 - classification_loss 8328/10000 [=======================>......] - ETA: 13:13 - loss: 1.7912 - regression_loss: 1.3662 - classification_loss 8329/10000 [=======================>......] - ETA: 13:12 - loss: 1.7911 - regression_loss: 1.3661 - classification_loss 8330/10000 [=======================>......] - ETA: 13:12 - loss: 1.7910 - regression_loss: 1.3660 - classification_loss 8331/10000 [=======================>......] - ETA: 13:11 - loss: 1.7909 - regression_loss: 1.3659 - classification_loss 8332/10000 [=======================>......] - ETA: 13:11 - loss: 1.7908 - regression_loss: 1.3659 - classification_loss 8333/10000 [=======================>......] - ETA: 13:10 - loss: 1.7908 - regression_loss: 1.3659 - classification_loss 8334/10000 [========================>.....] - ETA: 13:10 - loss: 1.7907 - regression_loss: 1.3658 - classification_loss 8335/10000 [========================>.....] - ETA: 13:09 - loss: 1.7907 - regression_loss: 1.3658 - classification_loss 8336/10000 [========================>.....] - ETA: 13:09 - loss: 1.7906 - regression_loss: 1.3657 - classification_loss 8337/10000 [========================>.....] - ETA: 13:08 - loss: 1.7907 - regression_loss: 1.3658 - classification_loss 8338/10000 [========================>.....] - ETA: 13:08 - loss: 1.7906 - regression_loss: 1.3657 - classification_loss 8339/10000 [========================>.....] - ETA: 13:07 - loss: 1.7906 - regression_loss: 1.3657 - classification_loss 8340/10000 [========================>.....] - ETA: 13:07 - loss: 1.7907 - regression_loss: 1.3658 - classification_loss 8341/10000 [========================>.....] - ETA: 13:06 - loss: 1.7906 - regression_loss: 1.3656 - classification_loss 8342/10000 [========================>.....] - ETA: 13:06 - loss: 1.7906 - regression_loss: 1.3657 - classification_loss 8343/10000 [========================>.....] - ETA: 13:05 - loss: 1.7906 - regression_loss: 1.3657 - classification_loss 8344/10000 [========================>.....] - ETA: 13:05 - loss: 1.7907 - regression_loss: 1.3657 - classification_loss 8345/10000 [========================>.....] - ETA: 13:05 - loss: 1.7905 - regression_loss: 1.3656 - classification_loss 8346/10000 [========================>.....] - ETA: 13:04 - loss: 1.7905 - regression_loss: 1.3656 - classification_loss 8347/10000 [========================>.....] - ETA: 13:04 - loss: 1.7904 - regression_loss: 1.3655 - classification_loss 8348/10000 [========================>.....] - ETA: 13:03 - loss: 1.7904 - regression_loss: 1.3655 - classification_loss 8349/10000 [========================>.....] - ETA: 13:03 - loss: 1.7904 - regression_loss: 1.3655 - classification_loss 8350/10000 [========================>.....] - ETA: 13:02 - loss: 1.7904 - regression_loss: 1.3655 - classification_loss 8351/10000 [========================>.....] - ETA: 13:02 - loss: 1.7904 - regression_loss: 1.3655 - classification_loss 8352/10000 [========================>.....] - ETA: 13:01 - loss: 1.7904 - regression_loss: 1.3655 - classification_loss 8353/10000 [========================>.....] - ETA: 13:01 - loss: 1.7903 - regression_loss: 1.3655 - classification_loss 8354/10000 [========================>.....] - ETA: 13:00 - loss: 1.7903 - regression_loss: 1.3655 - classification_loss 8355/10000 [========================>.....] - ETA: 13:00 - loss: 1.7904 - regression_loss: 1.3656 - classification_loss 8356/10000 [========================>.....] - ETA: 12:59 - loss: 1.7903 - regression_loss: 1.3655 - classification_loss 8357/10000 [========================>.....] - ETA: 12:59 - loss: 1.7903 - regression_loss: 1.3655 - classification_loss 8358/10000 [========================>.....] - ETA: 12:58 - loss: 1.7903 - regression_loss: 1.3655 - classification_loss 8359/10000 [========================>.....] - ETA: 12:58 - loss: 1.7902 - regression_loss: 1.3654 - classification_loss 8360/10000 [========================>.....] - ETA: 12:57 - loss: 1.7902 - regression_loss: 1.3655 - classification_loss 8361/10000 [========================>.....] - ETA: 12:57 - loss: 1.7902 - regression_loss: 1.3655 - classification_loss 8362/10000 [========================>.....] - ETA: 12:56 - loss: 1.7901 - regression_loss: 1.3654 - classification_loss 8363/10000 [========================>.....] - ETA: 12:56 - loss: 1.7902 - regression_loss: 1.3655 - classification_loss 8364/10000 [========================>.....] - ETA: 12:55 - loss: 1.7903 - regression_loss: 1.3655 - classification_loss 8365/10000 [========================>.....] - ETA: 12:55 - loss: 1.7902 - regression_loss: 1.3655 - classification_loss 8366/10000 [========================>.....] - ETA: 12:55 - loss: 1.7904 - regression_loss: 1.3656 - classification_loss 8367/10000 [========================>.....] - ETA: 12:54 - loss: 1.7904 - regression_loss: 1.3656 - classification_loss 8368/10000 [========================>.....] - ETA: 12:54 - loss: 1.7905 - regression_loss: 1.3657 - classification_loss 8369/10000 [========================>.....] - ETA: 12:53 - loss: 1.7904 - regression_loss: 1.3656 - classification_loss 8370/10000 [========================>.....] - ETA: 12:53 - loss: 1.7904 - regression_loss: 1.3657 - classification_loss 8371/10000 [========================>.....] - ETA: 12:52 - loss: 1.7904 - regression_loss: 1.3657 - classification_loss 8372/10000 [========================>.....] - ETA: 12:52 - loss: 1.7903 - regression_loss: 1.3656 - classification_loss 8373/10000 [========================>.....] - ETA: 12:51 - loss: 1.7904 - regression_loss: 1.3657 - classification_loss 8374/10000 [========================>.....] - ETA: 12:51 - loss: 1.7903 - regression_loss: 1.3656 - classification_loss 8375/10000 [========================>.....] - ETA: 12:50 - loss: 1.7902 - regression_loss: 1.3655 - classification_loss 8376/10000 [========================>.....] - ETA: 12:50 - loss: 1.7902 - regression_loss: 1.3655 - classification_loss 8377/10000 [========================>.....] - ETA: 12:49 - loss: 1.7901 - regression_loss: 1.3655 - classification_loss 8378/10000 [========================>.....] - ETA: 12:49 - loss: 1.7901 - regression_loss: 1.3655 - classification_loss 8379/10000 [========================>.....] - ETA: 12:48 - loss: 1.7902 - regression_loss: 1.3655 - classification_loss 8380/10000 [========================>.....] - ETA: 12:48 - loss: 1.7903 - regression_loss: 1.3656 - classification_loss 8381/10000 [========================>.....] - ETA: 12:47 - loss: 1.7903 - regression_loss: 1.3656 - classification_loss 8382/10000 [========================>.....] - ETA: 12:47 - loss: 1.7902 - regression_loss: 1.3656 - classification_loss 8383/10000 [========================>.....] - ETA: 12:46 - loss: 1.7903 - regression_loss: 1.3656 - classification_loss 8384/10000 [========================>.....] - ETA: 12:46 - loss: 1.7902 - regression_loss: 1.3655 - classification_loss 8385/10000 [========================>.....] - ETA: 12:45 - loss: 1.7902 - regression_loss: 1.3655 - classification_loss 8386/10000 [========================>.....] - ETA: 12:45 - loss: 1.7902 - regression_loss: 1.3656 - classification_loss 8387/10000 [========================>.....] - ETA: 12:44 - loss: 1.7902 - regression_loss: 1.3656 - classification_loss 8388/10000 [========================>.....] - ETA: 12:44 - loss: 1.7901 - regression_loss: 1.3656 - classification_loss 8389/10000 [========================>.....] - ETA: 12:44 - loss: 1.7902 - regression_loss: 1.3656 - classification_loss 8390/10000 [========================>.....] - ETA: 12:43 - loss: 1.7901 - regression_loss: 1.3655 - classification_loss 8391/10000 [========================>.....] - ETA: 12:43 - loss: 1.7901 - regression_loss: 1.3655 - classification_loss 8392/10000 [========================>.....] - ETA: 12:42 - loss: 1.7901 - regression_loss: 1.3656 - classification_loss 8393/10000 [========================>.....] - ETA: 12:42 - loss: 1.7901 - regression_loss: 1.3656 - classification_loss 8394/10000 [========================>.....] - ETA: 12:41 - loss: 1.7901 - regression_loss: 1.3656 - classification_loss 8395/10000 [========================>.....] - ETA: 12:41 - loss: 1.7901 - regression_loss: 1.3656 - classification_loss 8396/10000 [========================>.....] - ETA: 12:40 - loss: 1.7901 - regression_loss: 1.3655 - classification_loss 8397/10000 [========================>.....] - ETA: 12:40 - loss: 1.7901 - regression_loss: 1.3656 - classification_loss 8398/10000 [========================>.....] - ETA: 12:39 - loss: 1.7901 - regression_loss: 1.3656 - classification_loss 8399/10000 [========================>.....] - ETA: 12:39 - loss: 1.7901 - regression_loss: 1.3656 - classification_loss 8400/10000 [========================>.....] - ETA: 12:38 - loss: 1.7902 - regression_loss: 1.3657 - classification_loss 8401/10000 [========================>.....] - ETA: 12:38 - loss: 1.7903 - regression_loss: 1.3657 - classification_loss 8402/10000 [========================>.....] - ETA: 12:37 - loss: 1.7901 - regression_loss: 1.3656 - classification_loss 8403/10000 [========================>.....] - ETA: 12:37 - loss: 1.7900 - regression_loss: 1.3656 - classification_loss 8404/10000 [========================>.....] - ETA: 12:36 - loss: 1.7900 - regression_loss: 1.3655 - classification_loss 8405/10000 [========================>.....] - ETA: 12:36 - loss: 1.7900 - regression_loss: 1.3655 - classification_loss 8406/10000 [========================>.....] - ETA: 12:35 - loss: 1.7900 - regression_loss: 1.3656 - classification_loss 8407/10000 [========================>.....] - ETA: 12:35 - loss: 1.7899 - regression_loss: 1.3655 - classification_loss 8408/10000 [========================>.....] - ETA: 12:35 - loss: 1.7899 - regression_loss: 1.3655 - classification_loss 8409/10000 [========================>.....] - ETA: 12:34 - loss: 1.7899 - regression_loss: 1.3655 - classification_loss 8410/10000 [========================>.....] - ETA: 12:34 - loss: 1.7898 - regression_loss: 1.3654 - classification_loss 8411/10000 [========================>.....] - ETA: 12:33 - loss: 1.7898 - regression_loss: 1.3654 - classification_loss 8412/10000 [========================>.....] - ETA: 12:33 - loss: 1.7897 - regression_loss: 1.3653 - classification_loss 8413/10000 [========================>.....] - ETA: 12:32 - loss: 1.7896 - regression_loss: 1.3653 - classification_loss 8414/10000 [========================>.....] - ETA: 12:32 - loss: 1.7895 - regression_loss: 1.3652 - classification_loss 8415/10000 [========================>.....] - ETA: 12:31 - loss: 1.7894 - regression_loss: 1.3651 - classification_loss 8416/10000 [========================>.....] - ETA: 12:31 - loss: 1.7894 - regression_loss: 1.3651 - classification_loss 8417/10000 [========================>.....] - ETA: 12:30 - loss: 1.7893 - regression_loss: 1.3651 - classification_loss 8418/10000 [========================>.....] - ETA: 12:30 - loss: 1.7893 - regression_loss: 1.3650 - classification_loss 8419/10000 [========================>.....] - ETA: 12:29 - loss: 1.7892 - regression_loss: 1.3649 - classification_loss 8420/10000 [========================>.....] - ETA: 12:29 - loss: 1.7891 - regression_loss: 1.3649 - classification_loss 8421/10000 [========================>.....] - ETA: 12:28 - loss: 1.7891 - regression_loss: 1.3648 - classification_loss 8422/10000 [========================>.....] - ETA: 12:28 - loss: 1.7891 - regression_loss: 1.3648 - classification_loss 8423/10000 [========================>.....] - ETA: 12:27 - loss: 1.7891 - regression_loss: 1.3649 - classification_loss 8424/10000 [========================>.....] - ETA: 12:27 - loss: 1.7891 - regression_loss: 1.3649 - classification_loss 8425/10000 [========================>.....] - ETA: 12:26 - loss: 1.7891 - regression_loss: 1.3649 - classification_loss 8426/10000 [========================>.....] - ETA: 12:26 - loss: 1.7891 - regression_loss: 1.3649 - classification_loss 8427/10000 [========================>.....] - ETA: 12:25 - loss: 1.7891 - regression_loss: 1.3649 - classification_loss 8428/10000 [========================>.....] - ETA: 12:25 - loss: 1.7891 - regression_loss: 1.3649 - classification_loss 8429/10000 [========================>.....] - ETA: 12:24 - loss: 1.7890 - regression_loss: 1.3648 - classification_loss 8430/10000 [========================>.....] - ETA: 12:24 - loss: 1.7889 - regression_loss: 1.3648 - classification_loss 8431/10000 [========================>.....] - ETA: 12:24 - loss: 1.7889 - regression_loss: 1.3647 - classification_loss 8432/10000 [========================>.....] - ETA: 12:23 - loss: 1.7888 - regression_loss: 1.3646 - classification_loss 8433/10000 [========================>.....] - ETA: 12:23 - loss: 1.7888 - regression_loss: 1.3646 - classification_loss 8434/10000 [========================>.....] - ETA: 12:22 - loss: 1.7887 - regression_loss: 1.3646 - classification_loss 8435/10000 [========================>.....] - ETA: 12:22 - loss: 1.7886 - regression_loss: 1.3645 - classification_loss 8436/10000 [========================>.....] - ETA: 12:21 - loss: 1.7885 - regression_loss: 1.3644 - classification_loss 8437/10000 [========================>.....] - ETA: 12:21 - loss: 1.7885 - regression_loss: 1.3644 - classification_loss 8438/10000 [========================>.....] - ETA: 12:20 - loss: 1.7885 - regression_loss: 1.3644 - classification_loss 8439/10000 [========================>.....] - ETA: 12:20 - loss: 1.7886 - regression_loss: 1.3645 - classification_loss 8440/10000 [========================>.....] - ETA: 12:19 - loss: 1.7886 - regression_loss: 1.3645 - classification_loss 8441/10000 [========================>.....] - ETA: 12:19 - loss: 1.7886 - regression_loss: 1.3645 - classification_loss 8442/10000 [========================>.....] - ETA: 12:18 - loss: 1.7885 - regression_loss: 1.3644 - classification_loss 8443/10000 [========================>.....] - ETA: 12:18 - loss: 1.7885 - regression_loss: 1.3644 - classification_loss 8444/10000 [========================>.....] - ETA: 12:17 - loss: 1.7886 - regression_loss: 1.3645 - classification_loss 8445/10000 [========================>.....] - ETA: 12:17 - loss: 1.7886 - regression_loss: 1.3645 - classification_loss 8446/10000 [========================>.....] - ETA: 12:16 - loss: 1.7886 - regression_loss: 1.3646 - classification_loss 8447/10000 [========================>.....] - ETA: 12:16 - loss: 1.7885 - regression_loss: 1.3645 - classification_loss 8448/10000 [========================>.....] - ETA: 12:15 - loss: 1.7885 - regression_loss: 1.3645 - classification_loss 8449/10000 [========================>.....] - ETA: 12:15 - loss: 1.7884 - regression_loss: 1.3644 - classification_loss 8450/10000 [========================>.....] - ETA: 12:14 - loss: 1.7884 - regression_loss: 1.3644 - classification_loss 8451/10000 [========================>.....] - ETA: 12:14 - loss: 1.7883 - regression_loss: 1.3643 - classification_loss 8452/10000 [========================>.....] - ETA: 12:14 - loss: 1.7884 - regression_loss: 1.3644 - classification_loss 8453/10000 [========================>.....] - ETA: 12:13 - loss: 1.7885 - regression_loss: 1.3644 - classification_loss 8454/10000 [========================>.....] - ETA: 12:13 - loss: 1.7886 - regression_loss: 1.3645 - classification_loss 8455/10000 [========================>.....] - ETA: 12:12 - loss: 1.7885 - regression_loss: 1.3645 - classification_loss 8456/10000 [========================>.....] - ETA: 12:12 - loss: 1.7886 - regression_loss: 1.3645 - classification_loss 8457/10000 [========================>.....] - ETA: 12:11 - loss: 1.7885 - regression_loss: 1.3644 - classification_loss 8458/10000 [========================>.....] - ETA: 12:11 - loss: 1.7886 - regression_loss: 1.3646 - classification_loss 8459/10000 [========================>.....] - ETA: 12:10 - loss: 1.7887 - regression_loss: 1.3646 - classification_loss 8460/10000 [========================>.....] - ETA: 12:10 - loss: 1.7888 - regression_loss: 1.3647 - classification_loss 8461/10000 [========================>.....] - ETA: 12:09 - loss: 1.7889 - regression_loss: 1.3648 - classification_loss 8462/10000 [========================>.....] - ETA: 12:09 - loss: 1.7889 - regression_loss: 1.3648 - classification_loss 8463/10000 [========================>.....] - ETA: 12:08 - loss: 1.7891 - regression_loss: 1.3649 - classification_loss 8464/10000 [========================>.....] - ETA: 12:08 - loss: 1.7890 - regression_loss: 1.3649 - classification_loss 8465/10000 [========================>.....] - ETA: 12:07 - loss: 1.7890 - regression_loss: 1.3648 - classification_loss 8466/10000 [========================>.....] - ETA: 12:07 - loss: 1.7890 - regression_loss: 1.3648 - classification_loss 8467/10000 [========================>.....] - ETA: 12:06 - loss: 1.7891 - regression_loss: 1.3649 - classification_loss 8468/10000 [========================>.....] - ETA: 12:06 - loss: 1.7890 - regression_loss: 1.3648 - classification_loss 8469/10000 [========================>.....] - ETA: 12:05 - loss: 1.7891 - regression_loss: 1.3649 - classification_loss 8470/10000 [========================>.....] - ETA: 12:05 - loss: 1.7891 - regression_loss: 1.3649 - classification_loss 8471/10000 [========================>.....] - ETA: 12:04 - loss: 1.7891 - regression_loss: 1.3649 - classification_loss 8472/10000 [========================>.....] - ETA: 12:04 - loss: 1.7892 - regression_loss: 1.3649 - classification_loss 8473/10000 [========================>.....] - ETA: 12:04 - loss: 1.7892 - regression_loss: 1.3649 - classification_loss 8474/10000 [========================>.....] - ETA: 12:03 - loss: 1.7891 - regression_loss: 1.3648 - classification_loss 8475/10000 [========================>.....] - ETA: 12:03 - loss: 1.7891 - regression_loss: 1.3648 - classification_loss 8476/10000 [========================>.....] - ETA: 12:02 - loss: 1.7890 - regression_loss: 1.3648 - classification_loss 8477/10000 [========================>.....] - ETA: 12:02 - loss: 1.7891 - regression_loss: 1.3649 - classification_loss 8478/10000 [========================>.....] - ETA: 12:01 - loss: 1.7891 - regression_loss: 1.3649 - classification_loss 8479/10000 [========================>.....] - ETA: 12:01 - loss: 1.7891 - regression_loss: 1.3649 - classification_loss 8480/10000 [========================>.....] - ETA: 12:00 - loss: 1.7891 - regression_loss: 1.3649 - classification_loss 8481/10000 [========================>.....] - ETA: 12:00 - loss: 1.7890 - regression_loss: 1.3648 - classification_loss 8482/10000 [========================>.....] - ETA: 11:59 - loss: 1.7891 - regression_loss: 1.3649 - classification_loss 8483/10000 [========================>.....] - ETA: 11:59 - loss: 1.7892 - regression_loss: 1.3650 - classification_loss 8484/10000 [========================>.....] - ETA: 11:58 - loss: 1.7892 - regression_loss: 1.3650 - classification_loss 8485/10000 [========================>.....] - ETA: 11:58 - loss: 1.7893 - regression_loss: 1.3650 - classification_loss 8486/10000 [========================>.....] - ETA: 11:57 - loss: 1.7893 - regression_loss: 1.3651 - classification_loss 8487/10000 [========================>.....] - ETA: 11:57 - loss: 1.7893 - regression_loss: 1.3651 - classification_loss 8488/10000 [========================>.....] - ETA: 11:56 - loss: 1.7892 - regression_loss: 1.3650 - classification_loss 8489/10000 [========================>.....] - ETA: 11:56 - loss: 1.7892 - regression_loss: 1.3650 - classification_loss 8490/10000 [========================>.....] - ETA: 11:55 - loss: 1.7891 - regression_loss: 1.3650 - classification_loss 8491/10000 [========================>.....] - ETA: 11:55 - loss: 1.7892 - regression_loss: 1.3650 - classification_loss 8492/10000 [========================>.....] - ETA: 11:54 - loss: 1.7891 - regression_loss: 1.3650 - classification_loss 8493/10000 [========================>.....] - ETA: 11:54 - loss: 1.7891 - regression_loss: 1.3650 - classification_loss 8494/10000 [========================>.....] - ETA: 11:54 - loss: 1.7890 - regression_loss: 1.3649 - classification_loss 8495/10000 [========================>.....] - ETA: 11:53 - loss: 1.7891 - regression_loss: 1.3649 - classification_loss 8496/10000 [========================>.....] - ETA: 11:53 - loss: 1.7890 - regression_loss: 1.3648 - classification_loss 8497/10000 [========================>.....] - ETA: 11:52 - loss: 1.7890 - regression_loss: 1.3648 - classification_loss 8498/10000 [========================>.....] - ETA: 11:52 - loss: 1.7890 - regression_loss: 1.3648 - classification_loss 8499/10000 [========================>.....] - ETA: 11:51 - loss: 1.7890 - regression_loss: 1.3648 - classification_loss 8500/10000 [========================>.....] - ETA: 11:51 - loss: 1.7890 - regression_loss: 1.3648 - classification_loss 8501/10000 [========================>.....] - ETA: 11:50 - loss: 1.7889 - regression_loss: 1.3648 - classification_loss 8502/10000 [========================>.....] - ETA: 11:50 - loss: 1.7889 - regression_loss: 1.3647 - classification_loss 8503/10000 [========================>.....] - ETA: 11:49 - loss: 1.7889 - regression_loss: 1.3648 - classification_loss 8504/10000 [========================>.....] - ETA: 11:49 - loss: 1.7890 - regression_loss: 1.3649 - classification_loss 8505/10000 [========================>.....] - ETA: 11:48 - loss: 1.7891 - regression_loss: 1.3649 - classification_loss 8506/10000 [========================>.....] - ETA: 11:48 - loss: 1.7890 - regression_loss: 1.3649 - classification_loss 8507/10000 [========================>.....] - ETA: 11:47 - loss: 1.7891 - regression_loss: 1.3649 - classification_loss 8508/10000 [========================>.....] - ETA: 11:47 - loss: 1.7891 - regression_loss: 1.3649 - classification_loss 8509/10000 [========================>.....] - ETA: 11:46 - loss: 1.7891 - regression_loss: 1.3650 - classification_loss 8510/10000 [========================>.....] - ETA: 11:46 - loss: 1.7891 - regression_loss: 1.3649 - classification_loss 8511/10000 [========================>.....] - ETA: 11:45 - loss: 1.7890 - regression_loss: 1.3649 - classification_loss 8512/10000 [========================>.....] - ETA: 11:45 - loss: 1.7890 - regression_loss: 1.3648 - classification_loss 8513/10000 [========================>.....] - ETA: 11:44 - loss: 1.7890 - regression_loss: 1.3649 - classification_loss 8514/10000 [========================>.....] - ETA: 11:44 - loss: 1.7891 - regression_loss: 1.3650 - classification_loss 8515/10000 [========================>.....] - ETA: 11:44 - loss: 1.7889 - regression_loss: 1.3648 - classification_loss 8516/10000 [========================>.....] - ETA: 11:43 - loss: 1.7888 - regression_loss: 1.3648 - classification_loss 8517/10000 [========================>.....] - ETA: 11:43 - loss: 1.7888 - regression_loss: 1.3647 - classification_loss 8518/10000 [========================>.....] - ETA: 11:42 - loss: 1.7887 - regression_loss: 1.3647 - classification_loss 8519/10000 [========================>.....] - ETA: 11:42 - loss: 1.7888 - regression_loss: 1.3648 - classification_loss 8520/10000 [========================>.....] - ETA: 11:41 - loss: 1.7888 - regression_loss: 1.3648 - classification_loss 8521/10000 [========================>.....] - ETA: 11:41 - loss: 1.7887 - regression_loss: 1.3647 - classification_loss 8522/10000 [========================>.....] - ETA: 11:40 - loss: 1.7886 - regression_loss: 1.3646 - classification_loss 8523/10000 [========================>.....] - ETA: 11:40 - loss: 1.7886 - regression_loss: 1.3647 - classification_loss 8524/10000 [========================>.....] - ETA: 11:39 - loss: 1.7885 - regression_loss: 1.3646 - classification_loss 8525/10000 [========================>.....] - ETA: 11:39 - loss: 1.7886 - regression_loss: 1.3646 - classification_loss 8526/10000 [========================>.....] - ETA: 11:38 - loss: 1.7886 - regression_loss: 1.3646 - classification_loss 8527/10000 [========================>.....] - ETA: 11:38 - loss: 1.7885 - regression_loss: 1.3646 - classification_loss 8528/10000 [========================>.....] - ETA: 11:37 - loss: 1.7885 - regression_loss: 1.3646 - classification_loss 8529/10000 [========================>.....] - ETA: 11:37 - loss: 1.7884 - regression_loss: 1.3645 - classification_loss 8530/10000 [========================>.....] - ETA: 11:36 - loss: 1.7883 - regression_loss: 1.3645 - classification_loss 8531/10000 [========================>.....] - ETA: 11:36 - loss: 1.7883 - regression_loss: 1.3644 - classification_loss 8532/10000 [========================>.....] - ETA: 11:35 - loss: 1.7882 - regression_loss: 1.3643 - classification_loss 8533/10000 [========================>.....] - ETA: 11:35 - loss: 1.7883 - regression_loss: 1.3644 - classification_loss 8534/10000 [========================>.....] - ETA: 11:34 - loss: 1.7882 - regression_loss: 1.3644 - classification_loss 8535/10000 [========================>.....] - ETA: 11:34 - loss: 1.7882 - regression_loss: 1.3644 - classification_loss 8536/10000 [========================>.....] - ETA: 11:34 - loss: 1.7882 - regression_loss: 1.3644 - classification_loss 8537/10000 [========================>.....] - ETA: 11:33 - loss: 1.7882 - regression_loss: 1.3644 - classification_loss 8538/10000 [========================>.....] - ETA: 11:33 - loss: 1.7882 - regression_loss: 1.3644 - classification_loss 8539/10000 [========================>.....] - ETA: 11:32 - loss: 1.7882 - regression_loss: 1.3644 - classification_loss 8540/10000 [========================>.....] - ETA: 11:32 - loss: 1.7882 - regression_loss: 1.3644 - classification_loss 8541/10000 [========================>.....] - ETA: 11:31 - loss: 1.7882 - regression_loss: 1.3644 - classification_loss 8542/10000 [========================>.....] - ETA: 11:31 - loss: 1.7882 - regression_loss: 1.3644 - classification_loss 8543/10000 [========================>.....] - ETA: 11:30 - loss: 1.7881 - regression_loss: 1.3643 - classification_loss 8544/10000 [========================>.....] - ETA: 11:30 - loss: 1.7881 - regression_loss: 1.3642 - classification_loss 8545/10000 [========================>.....] - ETA: 11:29 - loss: 1.7881 - regression_loss: 1.3643 - classification_loss 8546/10000 [========================>.....] - ETA: 11:29 - loss: 1.7880 - regression_loss: 1.3642 - classification_loss 8547/10000 [========================>.....] - ETA: 11:28 - loss: 1.7880 - regression_loss: 1.3642 - classification_loss 8548/10000 [========================>.....] - ETA: 11:28 - loss: 1.7879 - regression_loss: 1.3641 - classification_loss 8549/10000 [========================>.....] - ETA: 11:27 - loss: 1.7878 - regression_loss: 1.3640 - classification_loss 8550/10000 [========================>.....] - ETA: 11:27 - loss: 1.7877 - regression_loss: 1.3640 - classification_loss 8551/10000 [========================>.....] - ETA: 11:26 - loss: 1.7877 - regression_loss: 1.3640 - classification_loss 8552/10000 [========================>.....] - ETA: 11:26 - loss: 1.7877 - regression_loss: 1.3640 - classification_loss 8553/10000 [========================>.....] - ETA: 11:25 - loss: 1.7876 - regression_loss: 1.3639 - classification_loss 8554/10000 [========================>.....] - ETA: 11:25 - loss: 1.7876 - regression_loss: 1.3639 - classification_loss 8555/10000 [========================>.....] - ETA: 11:24 - loss: 1.7876 - regression_loss: 1.3639 - classification_loss 8556/10000 [========================>.....] - ETA: 11:24 - loss: 1.7876 - regression_loss: 1.3639 - classification_loss 8557/10000 [========================>.....] - ETA: 11:24 - loss: 1.7875 - regression_loss: 1.3638 - classification_loss 8558/10000 [========================>.....] - ETA: 11:23 - loss: 1.7875 - regression_loss: 1.3638 - classification_loss 8559/10000 [========================>.....] - ETA: 11:23 - loss: 1.7874 - regression_loss: 1.3638 - classification_loss 8560/10000 [========================>.....] - ETA: 11:22 - loss: 1.7875 - regression_loss: 1.3638 - classification_loss 8561/10000 [========================>.....] - ETA: 11:22 - loss: 1.7874 - regression_loss: 1.3638 - classification_loss 8562/10000 [========================>.....] - ETA: 11:21 - loss: 1.7874 - regression_loss: 1.3637 - classification_loss 8563/10000 [========================>.....] - ETA: 11:21 - loss: 1.7874 - regression_loss: 1.3638 - classification_loss 8564/10000 [========================>.....] - ETA: 11:20 - loss: 1.7873 - regression_loss: 1.3637 - classification_loss 8565/10000 [========================>.....] - ETA: 11:20 - loss: 1.7874 - regression_loss: 1.3637 - classification_loss 8566/10000 [========================>.....] - ETA: 11:19 - loss: 1.7873 - regression_loss: 1.3636 - classification_loss 8567/10000 [========================>.....] - ETA: 11:19 - loss: 1.7873 - regression_loss: 1.3637 - classification_loss 8568/10000 [========================>.....] - ETA: 11:18 - loss: 1.7873 - regression_loss: 1.3636 - classification_loss 8569/10000 [========================>.....] - ETA: 11:18 - loss: 1.7872 - regression_loss: 1.3636 - classification_loss 8570/10000 [========================>.....] - ETA: 11:17 - loss: 1.7873 - regression_loss: 1.3636 - classification_loss 8571/10000 [========================>.....] - ETA: 11:17 - loss: 1.7873 - regression_loss: 1.3637 - classification_loss 8572/10000 [========================>.....] - ETA: 11:16 - loss: 1.7874 - regression_loss: 1.3637 - classification_loss 8573/10000 [========================>.....] - ETA: 11:16 - loss: 1.7873 - regression_loss: 1.3636 - classification_loss 8574/10000 [========================>.....] - ETA: 11:15 - loss: 1.7872 - regression_loss: 1.3636 - classification_loss 8575/10000 [========================>.....] - ETA: 11:15 - loss: 1.7871 - regression_loss: 1.3635 - classification_loss 8576/10000 [========================>.....] - ETA: 11:15 - loss: 1.7870 - regression_loss: 1.3634 - classification_loss 8577/10000 [========================>.....] - ETA: 11:14 - loss: 1.7871 - regression_loss: 1.3635 - classification_loss 8578/10000 [========================>.....] - ETA: 11:14 - loss: 1.7871 - regression_loss: 1.3635 - classification_loss 8579/10000 [========================>.....] - ETA: 11:13 - loss: 1.7869 - regression_loss: 1.3634 - classification_loss 8580/10000 [========================>.....] - ETA: 11:13 - loss: 1.7870 - regression_loss: 1.3634 - classification_loss 8581/10000 [========================>.....] - ETA: 11:12 - loss: 1.7869 - regression_loss: 1.3634 - classification_loss 8582/10000 [========================>.....] - ETA: 11:12 - loss: 1.7869 - regression_loss: 1.3633 - classification_loss 8583/10000 [========================>.....] - ETA: 11:11 - loss: 1.7869 - regression_loss: 1.3634 - classification_loss 8584/10000 [========================>.....] - ETA: 11:11 - loss: 1.7870 - regression_loss: 1.3634 - classification_loss 8585/10000 [========================>.....] - ETA: 11:10 - loss: 1.7870 - regression_loss: 1.3635 - classification_loss 8586/10000 [========================>.....] - ETA: 11:10 - loss: 1.7870 - regression_loss: 1.3634 - classification_loss 8587/10000 [========================>.....] - ETA: 11:09 - loss: 1.7870 - regression_loss: 1.3635 - classification_loss 8588/10000 [========================>.....] - ETA: 11:09 - loss: 1.7869 - regression_loss: 1.3634 - classification_loss 8589/10000 [========================>.....] - ETA: 11:08 - loss: 1.7869 - regression_loss: 1.3634 - classification_loss 8590/10000 [========================>.....] - ETA: 11:08 - loss: 1.7870 - regression_loss: 1.3635 - classification_loss 8591/10000 [========================>.....] - ETA: 11:07 - loss: 1.7870 - regression_loss: 1.3636 - classification_loss 8592/10000 [========================>.....] - ETA: 11:07 - loss: 1.7870 - regression_loss: 1.3635 - classification_loss 8593/10000 [========================>.....] - ETA: 11:06 - loss: 1.7869 - regression_loss: 1.3634 - classification_loss 8594/10000 [========================>.....] - ETA: 11:06 - loss: 1.7869 - regression_loss: 1.3634 - classification_loss 8595/10000 [========================>.....] - ETA: 11:05 - loss: 1.7869 - regression_loss: 1.3634 - classification_loss 8596/10000 [========================>.....] - ETA: 11:05 - loss: 1.7869 - regression_loss: 1.3634 - classification_loss 8597/10000 [========================>.....] - ETA: 11:05 - loss: 1.7868 - regression_loss: 1.3634 - classification_loss 8598/10000 [========================>.....] - ETA: 11:04 - loss: 1.7869 - regression_loss: 1.3634 - classification_loss 8599/10000 [========================>.....] - ETA: 11:04 - loss: 1.7870 - regression_loss: 1.3635 - classification_loss 8600/10000 [========================>.....] - ETA: 11:03 - loss: 1.7870 - regression_loss: 1.3635 - classification_loss 8601/10000 [========================>.....] - ETA: 11:03 - loss: 1.7870 - regression_loss: 1.3635 - classification_loss 8602/10000 [========================>.....] - ETA: 11:02 - loss: 1.7870 - regression_loss: 1.3635 - classification_loss 8603/10000 [========================>.....] - ETA: 11:02 - loss: 1.7869 - regression_loss: 1.3634 - classification_loss 8604/10000 [========================>.....] - ETA: 11:01 - loss: 1.7868 - regression_loss: 1.3634 - classification_loss 8605/10000 [========================>.....] - ETA: 11:01 - loss: 1.7869 - regression_loss: 1.3635 - classification_loss 8606/10000 [========================>.....] - ETA: 11:00 - loss: 1.7868 - regression_loss: 1.3634 - classification_loss 8607/10000 [========================>.....] - ETA: 11:00 - loss: 1.7868 - regression_loss: 1.3634 - classification_loss 8608/10000 [========================>.....] - ETA: 10:59 - loss: 1.7867 - regression_loss: 1.3633 - classification_loss 8609/10000 [========================>.....] - ETA: 10:59 - loss: 1.7866 - regression_loss: 1.3633 - classification_loss 8610/10000 [========================>.....] - ETA: 10:58 - loss: 1.7867 - regression_loss: 1.3634 - classification_loss 8611/10000 [========================>.....] - ETA: 10:58 - loss: 1.7867 - regression_loss: 1.3633 - classification_loss 8612/10000 [========================>.....] - ETA: 10:57 - loss: 1.7867 - regression_loss: 1.3633 - classification_loss 8613/10000 [========================>.....] - ETA: 10:57 - loss: 1.7868 - regression_loss: 1.3634 - classification_loss 8614/10000 [========================>.....] - ETA: 10:56 - loss: 1.7868 - regression_loss: 1.3634 - classification_loss 8615/10000 [========================>.....] - ETA: 10:56 - loss: 1.7868 - regression_loss: 1.3634 - classification_loss 8616/10000 [========================>.....] - ETA: 10:55 - loss: 1.7868 - regression_loss: 1.3634 - classification_loss 8617/10000 [========================>.....] - ETA: 10:55 - loss: 1.7867 - regression_loss: 1.3633 - classification_loss 8618/10000 [========================>.....] - ETA: 10:55 - loss: 1.7867 - regression_loss: 1.3633 - classification_loss 8619/10000 [========================>.....] - ETA: 10:54 - loss: 1.7867 - regression_loss: 1.3633 - classification_loss 8620/10000 [========================>.....] - ETA: 10:54 - loss: 1.7866 - regression_loss: 1.3632 - classification_loss 8621/10000 [========================>.....] - ETA: 10:53 - loss: 1.7866 - regression_loss: 1.3632 - classification_loss 8622/10000 [========================>.....] - ETA: 10:53 - loss: 1.7865 - regression_loss: 1.3632 - classification_loss 8623/10000 [========================>.....] - ETA: 10:52 - loss: 1.7865 - regression_loss: 1.3632 - classification_loss 8624/10000 [========================>.....] - ETA: 10:52 - loss: 1.7865 - regression_loss: 1.3632 - classification_loss 8625/10000 [========================>.....] - ETA: 10:51 - loss: 1.7865 - regression_loss: 1.3632 - classification_loss 8626/10000 [========================>.....] - ETA: 10:51 - loss: 1.7865 - regression_loss: 1.3632 - classification_loss 8627/10000 [========================>.....] - ETA: 10:50 - loss: 1.7865 - regression_loss: 1.3632 - classification_loss 8628/10000 [========================>.....] - ETA: 10:50 - loss: 1.7865 - regression_loss: 1.3633 - classification_loss 8629/10000 [========================>.....] - ETA: 10:49 - loss: 1.7865 - regression_loss: 1.3632 - classification_loss 8630/10000 [========================>.....] - ETA: 10:49 - loss: 1.7865 - regression_loss: 1.3632 - classification_loss 8631/10000 [========================>.....] - ETA: 10:48 - loss: 1.7865 - regression_loss: 1.3632 - classification_loss 8632/10000 [========================>.....] - ETA: 10:48 - loss: 1.7865 - regression_loss: 1.3633 - classification_loss 8633/10000 [========================>.....] - ETA: 10:47 - loss: 1.7866 - regression_loss: 1.3633 - classification_loss 8634/10000 [========================>.....] - ETA: 10:47 - loss: 1.7865 - regression_loss: 1.3633 - classification_loss 8635/10000 [========================>.....] - ETA: 10:46 - loss: 1.7865 - regression_loss: 1.3633 - classification_loss 8636/10000 [========================>.....] - ETA: 10:46 - loss: 1.7865 - regression_loss: 1.3632 - classification_loss 8637/10000 [========================>.....] - ETA: 10:45 - loss: 1.7866 - regression_loss: 1.3633 - classification_loss 8638/10000 [========================>.....] - ETA: 10:45 - loss: 1.7867 - regression_loss: 1.3633 - classification_loss 8639/10000 [========================>.....] - ETA: 10:45 - loss: 1.7866 - regression_loss: 1.3633 - classification_loss 8640/10000 [========================>.....] - ETA: 10:44 - loss: 1.7868 - regression_loss: 1.3634 - classification_loss 8641/10000 [========================>.....] - ETA: 10:44 - loss: 1.7868 - regression_loss: 1.3634 - classification_loss 8642/10000 [========================>.....] - ETA: 10:43 - loss: 1.7868 - regression_loss: 1.3634 - classification_loss 8643/10000 [========================>.....] - ETA: 10:43 - loss: 1.7867 - regression_loss: 1.3633 - classification_loss 8644/10000 [========================>.....] - ETA: 10:42 - loss: 1.7866 - regression_loss: 1.3633 - classification_loss 8645/10000 [========================>.....] - ETA: 10:42 - loss: 1.7868 - regression_loss: 1.3634 - classification_loss 8646/10000 [========================>.....] - ETA: 10:41 - loss: 1.7868 - regression_loss: 1.3634 - classification_loss 8647/10000 [========================>.....] - ETA: 10:41 - loss: 1.7868 - regression_loss: 1.3634 - classification_loss 8648/10000 [========================>.....] - ETA: 10:40 - loss: 1.7867 - regression_loss: 1.3634 - classification_loss 8649/10000 [========================>.....] - ETA: 10:40 - loss: 1.7867 - regression_loss: 1.3634 - classification_loss 8650/10000 [========================>.....] - ETA: 10:39 - loss: 1.7866 - regression_loss: 1.3633 - classification_loss 8651/10000 [========================>.....] - ETA: 10:39 - loss: 1.7866 - regression_loss: 1.3633 - classification_loss 8652/10000 [========================>.....] - ETA: 10:38 - loss: 1.7866 - regression_loss: 1.3633 - classification_loss 8653/10000 [========================>.....] - ETA: 10:38 - loss: 1.7866 - regression_loss: 1.3633 - classification_loss 8654/10000 [========================>.....] - ETA: 10:37 - loss: 1.7866 - regression_loss: 1.3633 - classification_loss 8655/10000 [========================>.....] - ETA: 10:37 - loss: 1.7867 - regression_loss: 1.3634 - classification_loss 8656/10000 [========================>.....] - ETA: 10:36 - loss: 1.7866 - regression_loss: 1.3634 - classification_loss 8657/10000 [========================>.....] - ETA: 10:36 - loss: 1.7865 - regression_loss: 1.3633 - classification_loss 8658/10000 [========================>.....] - ETA: 10:36 - loss: 1.7865 - regression_loss: 1.3633 - classification_loss 8659/10000 [========================>.....] - ETA: 10:35 - loss: 1.7865 - regression_loss: 1.3632 - classification_loss 8660/10000 [========================>.....] - ETA: 10:35 - loss: 1.7864 - regression_loss: 1.3632 - classification_loss 8661/10000 [========================>.....] - ETA: 10:34 - loss: 1.7864 - regression_loss: 1.3632 - classification_loss 8662/10000 [========================>.....] - ETA: 10:34 - loss: 1.7863 - regression_loss: 1.3631 - classification_loss 8663/10000 [========================>.....] - ETA: 10:33 - loss: 1.7862 - regression_loss: 1.3630 - classification_loss 8664/10000 [========================>.....] - ETA: 10:33 - loss: 1.7863 - regression_loss: 1.3631 - classification_loss 8665/10000 [========================>.....] - ETA: 10:32 - loss: 1.7861 - regression_loss: 1.3630 - classification_loss 8666/10000 [========================>.....] - ETA: 10:32 - loss: 1.7862 - regression_loss: 1.3630 - classification_loss 8667/10000 [=========================>....] - ETA: 10:31 - loss: 1.7861 - regression_loss: 1.3629 - classification_loss 8668/10000 [=========================>....] - ETA: 10:31 - loss: 1.7860 - regression_loss: 1.3628 - classification_loss 8669/10000 [=========================>....] - ETA: 10:30 - loss: 1.7859 - regression_loss: 1.3627 - classification_loss 8670/10000 [=========================>....] - ETA: 10:30 - loss: 1.7859 - regression_loss: 1.3627 - classification_loss 8671/10000 [=========================>....] - ETA: 10:29 - loss: 1.7860 - regression_loss: 1.3628 - classification_loss 8672/10000 [=========================>....] - ETA: 10:29 - loss: 1.7858 - regression_loss: 1.3627 - classification_loss 8673/10000 [=========================>....] - ETA: 10:28 - loss: 1.7858 - regression_loss: 1.3626 - classification_loss 8674/10000 [=========================>....] - ETA: 10:28 - loss: 1.7857 - regression_loss: 1.3625 - classification_loss 8675/10000 [=========================>....] - ETA: 10:27 - loss: 1.7856 - regression_loss: 1.3625 - classification_loss 8676/10000 [=========================>....] - ETA: 10:27 - loss: 1.7856 - regression_loss: 1.3625 - classification_loss 8677/10000 [=========================>....] - ETA: 10:26 - loss: 1.7857 - regression_loss: 1.3625 - classification_loss 8678/10000 [=========================>....] - ETA: 10:26 - loss: 1.7856 - regression_loss: 1.3625 - classification_loss 8679/10000 [=========================>....] - ETA: 10:26 - loss: 1.7856 - regression_loss: 1.3625 - classification_loss 8680/10000 [=========================>....] - ETA: 10:25 - loss: 1.7856 - regression_loss: 1.3625 - classification_loss 8681/10000 [=========================>....] - ETA: 10:25 - loss: 1.7856 - regression_loss: 1.3624 - classification_loss 8682/10000 [=========================>....] - ETA: 10:24 - loss: 1.7856 - regression_loss: 1.3625 - classification_loss 8683/10000 [=========================>....] - ETA: 10:24 - loss: 1.7858 - regression_loss: 1.3626 - classification_loss 8684/10000 [=========================>....] - ETA: 10:23 - loss: 1.7858 - regression_loss: 1.3626 - classification_loss 8685/10000 [=========================>....] - ETA: 10:23 - loss: 1.7857 - regression_loss: 1.3625 - classification_loss 8686/10000 [=========================>....] - ETA: 10:22 - loss: 1.7858 - regression_loss: 1.3626 - classification_loss 8687/10000 [=========================>....] - ETA: 10:22 - loss: 1.7858 - regression_loss: 1.3626 - classification_loss 8688/10000 [=========================>....] - ETA: 10:21 - loss: 1.7857 - regression_loss: 1.3625 - classification_loss 8689/10000 [=========================>....] - ETA: 10:21 - loss: 1.7857 - regression_loss: 1.3625 - classification_loss 8690/10000 [=========================>....] - ETA: 10:20 - loss: 1.7856 - regression_loss: 1.3625 - classification_loss 8691/10000 [=========================>....] - ETA: 10:20 - loss: 1.7856 - regression_loss: 1.3625 - classification_loss 8692/10000 [=========================>....] - ETA: 10:19 - loss: 1.7857 - regression_loss: 1.3626 - classification_loss 8693/10000 [=========================>....] - ETA: 10:19 - loss: 1.7857 - regression_loss: 1.3626 - classification_loss 8694/10000 [=========================>....] - ETA: 10:18 - loss: 1.7856 - regression_loss: 1.3625 - classification_loss 8695/10000 [=========================>....] - ETA: 10:18 - loss: 1.7857 - regression_loss: 1.3626 - classification_loss 8696/10000 [=========================>....] - ETA: 10:17 - loss: 1.7856 - regression_loss: 1.3625 - classification_loss 8697/10000 [=========================>....] - ETA: 10:17 - loss: 1.7856 - regression_loss: 1.3625 - classification_loss 8698/10000 [=========================>....] - ETA: 10:16 - loss: 1.7856 - regression_loss: 1.3625 - classification_loss 8699/10000 [=========================>....] - ETA: 10:16 - loss: 1.7856 - regression_loss: 1.3625 - classification_loss 8700/10000 [=========================>....] - ETA: 10:16 - loss: 1.7855 - regression_loss: 1.3624 - classification_loss 8701/10000 [=========================>....] - ETA: 10:15 - loss: 1.7854 - regression_loss: 1.3623 - classification_loss 8702/10000 [=========================>....] - ETA: 10:15 - loss: 1.7854 - regression_loss: 1.3623 - classification_loss 8703/10000 [=========================>....] - ETA: 10:14 - loss: 1.7855 - regression_loss: 1.3625 - classification_loss 8704/10000 [=========================>....] - ETA: 10:14 - loss: 1.7856 - regression_loss: 1.3625 - classification_loss 8705/10000 [=========================>....] - ETA: 10:13 - loss: 1.7855 - regression_loss: 1.3624 - classification_loss 8706/10000 [=========================>....] - ETA: 10:13 - loss: 1.7856 - regression_loss: 1.3625 - classification_loss 8707/10000 [=========================>....] - ETA: 10:12 - loss: 1.7857 - regression_loss: 1.3626 - classification_loss 8708/10000 [=========================>....] - ETA: 10:12 - loss: 1.7857 - regression_loss: 1.3626 - classification_loss 8709/10000 [=========================>....] - ETA: 10:11 - loss: 1.7857 - regression_loss: 1.3626 - classification_loss 8710/10000 [=========================>....] - ETA: 10:11 - loss: 1.7857 - regression_loss: 1.3626 - classification_loss 8711/10000 [=========================>....] - ETA: 10:10 - loss: 1.7857 - regression_loss: 1.3626 - classification_loss 8712/10000 [=========================>....] - ETA: 10:10 - loss: 1.7856 - regression_loss: 1.3625 - classification_loss 8713/10000 [=========================>....] - ETA: 10:09 - loss: 1.7856 - regression_loss: 1.3625 - classification_loss 8714/10000 [=========================>....] - ETA: 10:09 - loss: 1.7855 - regression_loss: 1.3625 - classification_loss 8715/10000 [=========================>....] - ETA: 10:08 - loss: 1.7855 - regression_loss: 1.3625 - classification_loss 8716/10000 [=========================>....] - ETA: 10:08 - loss: 1.7855 - regression_loss: 1.3624 - classification_loss 8717/10000 [=========================>....] - ETA: 10:07 - loss: 1.7854 - regression_loss: 1.3624 - classification_loss 8718/10000 [=========================>....] - ETA: 10:07 - loss: 1.7854 - regression_loss: 1.3624 - classification_loss 8719/10000 [=========================>....] - ETA: 10:06 - loss: 1.7855 - regression_loss: 1.3624 - classification_loss 8720/10000 [=========================>....] - ETA: 10:06 - loss: 1.7854 - regression_loss: 1.3623 - classification_loss 8721/10000 [=========================>....] - ETA: 10:06 - loss: 1.7853 - regression_loss: 1.3622 - classification_loss 8722/10000 [=========================>....] - ETA: 10:05 - loss: 1.7852 - regression_loss: 1.3622 - classification_loss 8723/10000 [=========================>....] - ETA: 10:05 - loss: 1.7852 - regression_loss: 1.3621 - classification_loss 8724/10000 [=========================>....] - ETA: 10:04 - loss: 1.7851 - regression_loss: 1.3620 - classification_loss 8725/10000 [=========================>....] - ETA: 10:04 - loss: 1.7851 - regression_loss: 1.3620 - classification_loss 8726/10000 [=========================>....] - ETA: 10:03 - loss: 1.7851 - regression_loss: 1.3621 - classification_loss 8727/10000 [=========================>....] - ETA: 10:03 - loss: 1.7851 - regression_loss: 1.3621 - classification_loss 8728/10000 [=========================>....] - ETA: 10:02 - loss: 1.7850 - regression_loss: 1.3620 - classification_loss 8729/10000 [=========================>....] - ETA: 10:02 - loss: 1.7850 - regression_loss: 1.3620 - classification_loss 8730/10000 [=========================>....] - ETA: 10:01 - loss: 1.7851 - regression_loss: 1.3620 - classification_loss 8731/10000 [=========================>....] - ETA: 10:01 - loss: 1.7850 - regression_loss: 1.3620 - classification_loss 8732/10000 [=========================>....] - ETA: 10:00 - loss: 1.7851 - regression_loss: 1.3620 - classification_loss 8733/10000 [=========================>....] - ETA: 10:00 - loss: 1.7851 - regression_loss: 1.3620 - classification_loss 8734/10000 [=========================>....] - ETA: 9:59 - loss: 1.7850 - regression_loss: 1.3620 - classification_loss: 8735/10000 [=========================>....] - ETA: 9:59 - loss: 1.7851 - regression_loss: 1.3620 - classification_loss: 8736/10000 [=========================>....] - ETA: 9:58 - loss: 1.7850 - regression_loss: 1.3620 - classification_loss: 8737/10000 [=========================>....] - ETA: 9:58 - loss: 1.7849 - regression_loss: 1.3619 - classification_loss: 8738/10000 [=========================>....] - ETA: 9:57 - loss: 1.7848 - regression_loss: 1.3618 - classification_loss: 8739/10000 [=========================>....] - ETA: 9:57 - loss: 1.7848 - regression_loss: 1.3618 - classification_loss: 8740/10000 [=========================>....] - ETA: 9:57 - loss: 1.7847 - regression_loss: 1.3617 - classification_loss: 8741/10000 [=========================>....] - ETA: 9:56 - loss: 1.7848 - regression_loss: 1.3618 - classification_loss: 8742/10000 [=========================>....] - ETA: 9:56 - loss: 1.7848 - regression_loss: 1.3618 - classification_loss: 8743/10000 [=========================>....] - ETA: 9:55 - loss: 1.7849 - regression_loss: 1.3619 - classification_loss: 8744/10000 [=========================>....] - ETA: 9:55 - loss: 1.7848 - regression_loss: 1.3618 - classification_loss: 8745/10000 [=========================>....] - ETA: 9:54 - loss: 1.7848 - regression_loss: 1.3618 - classification_loss: 8746/10000 [=========================>....] - ETA: 9:54 - loss: 1.7848 - regression_loss: 1.3618 - classification_loss: 8747/10000 [=========================>....] - ETA: 9:53 - loss: 1.7847 - regression_loss: 1.3618 - classification_loss: 8748/10000 [=========================>....] - ETA: 9:53 - loss: 1.7848 - regression_loss: 1.3618 - classification_loss: 8749/10000 [=========================>....] - ETA: 9:52 - loss: 1.7849 - regression_loss: 1.3617 - classification_loss: 8750/10000 [=========================>....] - ETA: 9:52 - loss: 1.7849 - regression_loss: 1.3618 - classification_loss: 8751/10000 [=========================>....] - ETA: 9:51 - loss: 1.7849 - regression_loss: 1.3618 - classification_loss: 8752/10000 [=========================>....] - ETA: 9:51 - loss: 1.7849 - regression_loss: 1.3617 - classification_loss: 8753/10000 [=========================>....] - ETA: 9:50 - loss: 1.7849 - regression_loss: 1.3618 - classification_loss: 8754/10000 [=========================>....] - ETA: 9:50 - loss: 1.7850 - regression_loss: 1.3618 - classification_loss: 8755/10000 [=========================>....] - ETA: 9:49 - loss: 1.7851 - regression_loss: 1.3619 - classification_loss: 8756/10000 [=========================>....] - ETA: 9:49 - loss: 1.7850 - regression_loss: 1.3619 - classification_loss: 8757/10000 [=========================>....] - ETA: 9:48 - loss: 1.7851 - regression_loss: 1.3620 - classification_loss: 8758/10000 [=========================>....] - ETA: 9:48 - loss: 1.7850 - regression_loss: 1.3619 - classification_loss: 8759/10000 [=========================>....] - ETA: 9:47 - loss: 1.7850 - regression_loss: 1.3619 - classification_loss: 8760/10000 [=========================>....] - ETA: 9:47 - loss: 1.7851 - regression_loss: 1.3620 - classification_loss: 8761/10000 [=========================>....] - ETA: 9:46 - loss: 1.7852 - regression_loss: 1.3621 - classification_loss: 8762/10000 [=========================>....] - ETA: 9:46 - loss: 1.7852 - regression_loss: 1.3621 - classification_loss: 8763/10000 [=========================>....] - ETA: 9:46 - loss: 1.7853 - regression_loss: 1.3622 - classification_loss: 8764/10000 [=========================>....] - ETA: 9:45 - loss: 1.7853 - regression_loss: 1.3622 - classification_loss: 8765/10000 [=========================>....] - ETA: 9:45 - loss: 1.7853 - regression_loss: 1.3622 - classification_loss: 8766/10000 [=========================>....] - ETA: 9:44 - loss: 1.7852 - regression_loss: 1.3622 - classification_loss: 8767/10000 [=========================>....] - ETA: 9:44 - loss: 1.7851 - regression_loss: 1.3621 - classification_loss: 8768/10000 [=========================>....] - ETA: 9:43 - loss: 1.7851 - regression_loss: 1.3620 - classification_loss: 8769/10000 [=========================>....] - ETA: 9:43 - loss: 1.7850 - regression_loss: 1.3620 - classification_loss: 8770/10000 [=========================>....] - ETA: 9:42 - loss: 1.7851 - regression_loss: 1.3621 - classification_loss: 8771/10000 [=========================>....] - ETA: 9:42 - loss: 1.7851 - regression_loss: 1.3621 - classification_loss: 8772/10000 [=========================>....] - ETA: 9:41 - loss: 1.7851 - regression_loss: 1.3621 - classification_loss: 8773/10000 [=========================>....] - ETA: 9:41 - loss: 1.7851 - regression_loss: 1.3621 - classification_loss: 8774/10000 [=========================>....] - ETA: 9:40 - loss: 1.7852 - regression_loss: 1.3622 - classification_loss: 8775/10000 [=========================>....] - ETA: 9:40 - loss: 1.7852 - regression_loss: 1.3622 - classification_loss: 8776/10000 [=========================>....] - ETA: 9:39 - loss: 1.7851 - regression_loss: 1.3622 - classification_loss: 8777/10000 [=========================>....] - ETA: 9:39 - loss: 1.7852 - regression_loss: 1.3622 - classification_loss: 8778/10000 [=========================>....] - ETA: 9:38 - loss: 1.7851 - regression_loss: 1.3622 - classification_loss: 8779/10000 [=========================>....] - ETA: 9:38 - loss: 1.7850 - regression_loss: 1.3621 - classification_loss: 8780/10000 [=========================>....] - ETA: 9:37 - loss: 1.7851 - regression_loss: 1.3622 - classification_loss: 8781/10000 [=========================>....] - ETA: 9:37 - loss: 1.7852 - regression_loss: 1.3623 - classification_loss: 8782/10000 [=========================>....] - ETA: 9:36 - loss: 1.7852 - regression_loss: 1.3622 - classification_loss: 8783/10000 [=========================>....] - ETA: 9:36 - loss: 1.7851 - regression_loss: 1.3621 - classification_loss: 8784/10000 [=========================>....] - ETA: 9:36 - loss: 1.7851 - regression_loss: 1.3621 - classification_loss: 8785/10000 [=========================>....] - ETA: 9:35 - loss: 1.7850 - regression_loss: 1.3621 - classification_loss: 8786/10000 [=========================>....] - ETA: 9:35 - loss: 1.7850 - regression_loss: 1.3620 - classification_loss: 8787/10000 [=========================>....] - ETA: 9:34 - loss: 1.7850 - regression_loss: 1.3621 - classification_loss: 8788/10000 [=========================>....] - ETA: 9:34 - loss: 1.7850 - regression_loss: 1.3621 - classification_loss: 8789/10000 [=========================>....] - ETA: 9:33 - loss: 1.7851 - regression_loss: 1.3621 - classification_loss: 8790/10000 [=========================>....] - ETA: 9:33 - loss: 1.7852 - regression_loss: 1.3622 - classification_loss: 8791/10000 [=========================>....] - ETA: 9:32 - loss: 1.7852 - regression_loss: 1.3623 - classification_loss: 8792/10000 [=========================>....] - ETA: 9:32 - loss: 1.7852 - regression_loss: 1.3622 - classification_loss: 8793/10000 [=========================>....] - ETA: 9:31 - loss: 1.7852 - regression_loss: 1.3622 - classification_loss: 8794/10000 [=========================>....] - ETA: 9:31 - loss: 1.7851 - regression_loss: 1.3622 - classification_loss: 8795/10000 [=========================>....] - ETA: 9:30 - loss: 1.7850 - regression_loss: 1.3621 - classification_loss: 8796/10000 [=========================>....] - ETA: 9:30 - loss: 1.7850 - regression_loss: 1.3622 - classification_loss: 8797/10000 [=========================>....] - ETA: 9:29 - loss: 1.7850 - regression_loss: 1.3621 - classification_loss: 8798/10000 [=========================>....] - ETA: 9:29 - loss: 1.7849 - regression_loss: 1.3620 - classification_loss: 8799/10000 [=========================>....] - ETA: 9:28 - loss: 1.7848 - regression_loss: 1.3620 - classification_loss: 8800/10000 [=========================>....] - ETA: 9:28 - loss: 1.7848 - regression_loss: 1.3620 - classification_loss: 8801/10000 [=========================>....] - ETA: 9:27 - loss: 1.7847 - regression_loss: 1.3619 - classification_loss: 8802/10000 [=========================>....] - ETA: 9:27 - loss: 1.7847 - regression_loss: 1.3619 - classification_loss: 8803/10000 [=========================>....] - ETA: 9:26 - loss: 1.7846 - regression_loss: 1.3618 - classification_loss: 8804/10000 [=========================>....] - ETA: 9:26 - loss: 1.7845 - regression_loss: 1.3618 - classification_loss: 8805/10000 [=========================>....] - ETA: 9:26 - loss: 1.7846 - regression_loss: 1.3618 - classification_loss: 8806/10000 [=========================>....] - ETA: 9:25 - loss: 1.7846 - regression_loss: 1.3618 - classification_loss: 8807/10000 [=========================>....] - ETA: 9:25 - loss: 1.7846 - regression_loss: 1.3618 - classification_loss: 8808/10000 [=========================>....] - ETA: 9:24 - loss: 1.7846 - regression_loss: 1.3618 - classification_loss: 8809/10000 [=========================>....] - ETA: 9:24 - loss: 1.7846 - regression_loss: 1.3618 - classification_loss: 8810/10000 [=========================>....] - ETA: 9:23 - loss: 1.7847 - regression_loss: 1.3619 - classification_loss: 8811/10000 [=========================>....] - ETA: 9:23 - loss: 1.7847 - regression_loss: 1.3618 - classification_loss: 8812/10000 [=========================>....] - ETA: 9:22 - loss: 1.7847 - regression_loss: 1.3618 - classification_loss: 8813/10000 [=========================>....] - ETA: 9:22 - loss: 1.7846 - regression_loss: 1.3618 - classification_loss: 8814/10000 [=========================>....] - ETA: 9:21 - loss: 1.7846 - regression_loss: 1.3618 - classification_loss: 8815/10000 [=========================>....] - ETA: 9:21 - loss: 1.7847 - regression_loss: 1.3618 - classification_loss: 8816/10000 [=========================>....] - ETA: 9:20 - loss: 1.7846 - regression_loss: 1.3618 - classification_loss: 8817/10000 [=========================>....] - ETA: 9:20 - loss: 1.7846 - regression_loss: 1.3618 - classification_loss: 8818/10000 [=========================>....] - ETA: 9:19 - loss: 1.7846 - regression_loss: 1.3618 - classification_loss: 8819/10000 [=========================>....] - ETA: 9:19 - loss: 1.7845 - regression_loss: 1.3617 - classification_loss: 8820/10000 [=========================>....] - ETA: 9:18 - loss: 1.7844 - regression_loss: 1.3617 - classification_loss: 8821/10000 [=========================>....] - ETA: 9:18 - loss: 1.7844 - regression_loss: 1.3617 - classification_loss: 8822/10000 [=========================>....] - ETA: 9:17 - loss: 1.7844 - regression_loss: 1.3616 - classification_loss: 8823/10000 [=========================>....] - ETA: 9:17 - loss: 1.7843 - regression_loss: 1.3616 - classification_loss: 8824/10000 [=========================>....] - ETA: 9:16 - loss: 1.7842 - regression_loss: 1.3615 - classification_loss: 8825/10000 [=========================>....] - ETA: 9:16 - loss: 1.7842 - regression_loss: 1.3615 - classification_loss: 8826/10000 [=========================>....] - ETA: 9:16 - loss: 1.7843 - regression_loss: 1.3616 - classification_loss: 8827/10000 [=========================>....] - ETA: 9:15 - loss: 1.7843 - regression_loss: 1.3616 - classification_loss: 8828/10000 [=========================>....] - ETA: 9:15 - loss: 1.7843 - regression_loss: 1.3616 - classification_loss: 8829/10000 [=========================>....] - ETA: 9:14 - loss: 1.7843 - regression_loss: 1.3615 - classification_loss: 8830/10000 [=========================>....] - ETA: 9:14 - loss: 1.7843 - regression_loss: 1.3615 - classification_loss: 8831/10000 [=========================>....] - ETA: 9:13 - loss: 1.7842 - regression_loss: 1.3615 - classification_loss: 8832/10000 [=========================>....] - ETA: 9:13 - loss: 1.7842 - regression_loss: 1.3615 - classification_loss: 8833/10000 [=========================>....] - ETA: 9:12 - loss: 1.7842 - regression_loss: 1.3614 - classification_loss: 8834/10000 [=========================>....] - ETA: 9:12 - loss: 1.7842 - regression_loss: 1.3614 - classification_loss: 8835/10000 [=========================>....] - ETA: 9:11 - loss: 1.7841 - regression_loss: 1.3614 - classification_loss: 8836/10000 [=========================>....] - ETA: 9:11 - loss: 1.7843 - regression_loss: 1.3615 - classification_loss: 8837/10000 [=========================>....] - ETA: 9:10 - loss: 1.7843 - regression_loss: 1.3616 - classification_loss: 8838/10000 [=========================>....] - ETA: 9:10 - loss: 1.7842 - regression_loss: 1.3615 - classification_loss: 8839/10000 [=========================>....] - ETA: 9:09 - loss: 1.7842 - regression_loss: 1.3614 - classification_loss: 8840/10000 [=========================>....] - ETA: 9:09 - loss: 1.7840 - regression_loss: 1.3613 - classification_loss: 8841/10000 [=========================>....] - ETA: 9:08 - loss: 1.7840 - regression_loss: 1.3613 - classification_loss: 8842/10000 [=========================>....] - ETA: 9:08 - loss: 1.7841 - regression_loss: 1.3614 - classification_loss: 8843/10000 [=========================>....] - ETA: 9:07 - loss: 1.7840 - regression_loss: 1.3614 - classification_loss: 8844/10000 [=========================>....] - ETA: 9:07 - loss: 1.7840 - regression_loss: 1.3614 - classification_loss: 8845/10000 [=========================>....] - ETA: 9:06 - loss: 1.7841 - regression_loss: 1.3614 - classification_loss: 8846/10000 [=========================>....] - ETA: 9:06 - loss: 1.7841 - regression_loss: 1.3614 - classification_loss: 8847/10000 [=========================>....] - ETA: 9:06 - loss: 1.7842 - regression_loss: 1.3615 - classification_loss: 8848/10000 [=========================>....] - ETA: 9:05 - loss: 1.7843 - regression_loss: 1.3616 - classification_loss: 8849/10000 [=========================>....] - ETA: 9:05 - loss: 1.7842 - regression_loss: 1.3615 - classification_loss: 8850/10000 [=========================>....] - ETA: 9:04 - loss: 1.7842 - regression_loss: 1.3615 - classification_loss: 8851/10000 [=========================>....] - ETA: 9:04 - loss: 1.7842 - regression_loss: 1.3615 - classification_loss: 8852/10000 [=========================>....] - ETA: 9:03 - loss: 1.7841 - regression_loss: 1.3614 - classification_loss: 8853/10000 [=========================>....] - ETA: 9:03 - loss: 1.7841 - regression_loss: 1.3614 - classification_loss: 8854/10000 [=========================>....] - ETA: 9:02 - loss: 1.7840 - regression_loss: 1.3614 - classification_loss: 8855/10000 [=========================>....] - ETA: 9:02 - loss: 1.7841 - regression_loss: 1.3614 - classification_loss: 8856/10000 [=========================>....] - ETA: 9:01 - loss: 1.7840 - regression_loss: 1.3613 - classification_loss: 8857/10000 [=========================>....] - ETA: 9:01 - loss: 1.7839 - regression_loss: 1.3613 - classification_loss: 8858/10000 [=========================>....] - ETA: 9:00 - loss: 1.7839 - regression_loss: 1.3613 - classification_loss: 8859/10000 [=========================>....] - ETA: 9:00 - loss: 1.7838 - regression_loss: 1.3612 - classification_loss: 8860/10000 [=========================>....] - ETA: 8:59 - loss: 1.7839 - regression_loss: 1.3612 - classification_loss: 8861/10000 [=========================>....] - ETA: 8:59 - loss: 1.7839 - regression_loss: 1.3613 - classification_loss: 8862/10000 [=========================>....] - ETA: 8:58 - loss: 1.7839 - regression_loss: 1.3613 - classification_loss: 8863/10000 [=========================>....] - ETA: 8:58 - loss: 1.7838 - regression_loss: 1.3612 - classification_loss: 8864/10000 [=========================>....] - ETA: 8:57 - loss: 1.7837 - regression_loss: 1.3611 - classification_loss: 8865/10000 [=========================>....] - ETA: 8:57 - loss: 1.7837 - regression_loss: 1.3611 - classification_loss: 8866/10000 [=========================>....] - ETA: 8:57 - loss: 1.7837 - regression_loss: 1.3611 - classification_loss: 8867/10000 [=========================>....] - ETA: 8:56 - loss: 1.7837 - regression_loss: 1.3611 - classification_loss: 8868/10000 [=========================>....] - ETA: 8:56 - loss: 1.7836 - regression_loss: 1.3610 - classification_loss: 8869/10000 [=========================>....] - ETA: 8:55 - loss: 1.7836 - regression_loss: 1.3610 - classification_loss: 8870/10000 [=========================>....] - ETA: 8:55 - loss: 1.7837 - regression_loss: 1.3609 - classification_loss: 8871/10000 [=========================>....] - ETA: 8:54 - loss: 1.7837 - regression_loss: 1.3609 - classification_loss: 8872/10000 [=========================>....] - ETA: 8:54 - loss: 1.7839 - regression_loss: 1.3610 - classification_loss: 8873/10000 [=========================>....] - ETA: 8:53 - loss: 1.7839 - regression_loss: 1.3610 - classification_loss: 8874/10000 [=========================>....] - ETA: 8:53 - loss: 1.7839 - regression_loss: 1.3611 - classification_loss: 8875/10000 [=========================>....] - ETA: 8:52 - loss: 1.7840 - regression_loss: 1.3611 - classification_loss: 8876/10000 [=========================>....] - ETA: 8:52 - loss: 1.7839 - regression_loss: 1.3611 - classification_loss: 8877/10000 [=========================>....] - ETA: 8:51 - loss: 1.7839 - regression_loss: 1.3611 - classification_loss: 8878/10000 [=========================>....] - ETA: 8:51 - loss: 1.7840 - regression_loss: 1.3612 - classification_loss: 8879/10000 [=========================>....] - ETA: 8:50 - loss: 1.7839 - regression_loss: 1.3611 - classification_loss: 8880/10000 [=========================>....] - ETA: 8:50 - loss: 1.7839 - regression_loss: 1.3611 - classification_loss: 8881/10000 [=========================>....] - ETA: 8:49 - loss: 1.7838 - regression_loss: 1.3610 - classification_loss: 8882/10000 [=========================>....] - ETA: 8:49 - loss: 1.7838 - regression_loss: 1.3610 - classification_loss: 8883/10000 [=========================>....] - ETA: 8:48 - loss: 1.7839 - regression_loss: 1.3611 - classification_loss: 8884/10000 [=========================>....] - ETA: 8:48 - loss: 1.7839 - regression_loss: 1.3611 - classification_loss: 8885/10000 [=========================>....] - ETA: 8:47 - loss: 1.7840 - regression_loss: 1.3611 - classification_loss: 8886/10000 [=========================>....] - ETA: 8:47 - loss: 1.7839 - regression_loss: 1.3611 - classification_loss: 8887/10000 [=========================>....] - ETA: 8:47 - loss: 1.7839 - regression_loss: 1.3611 - classification_loss: 8888/10000 [=========================>....] - ETA: 8:46 - loss: 1.7839 - regression_loss: 1.3611 - classification_loss: 8889/10000 [=========================>....] - ETA: 8:46 - loss: 1.7840 - regression_loss: 1.3611 - classification_loss: 8890/10000 [=========================>....] - ETA: 8:45 - loss: 1.7841 - regression_loss: 1.3612 - classification_loss: 8891/10000 [=========================>....] - ETA: 8:45 - loss: 1.7840 - regression_loss: 1.3611 - classification_loss: 8892/10000 [=========================>....] - ETA: 8:44 - loss: 1.7841 - regression_loss: 1.3611 - classification_loss: 8893/10000 [=========================>....] - ETA: 8:44 - loss: 1.7842 - regression_loss: 1.3612 - classification_loss: 8894/10000 [=========================>....] - ETA: 8:43 - loss: 1.7841 - regression_loss: 1.3612 - classification_loss: 8895/10000 [=========================>....] - ETA: 8:43 - loss: 1.7840 - regression_loss: 1.3611 - classification_loss: 8896/10000 [=========================>....] - ETA: 8:42 - loss: 1.7840 - regression_loss: 1.3610 - classification_loss: 8897/10000 [=========================>....] - ETA: 8:42 - loss: 1.7840 - regression_loss: 1.3610 - classification_loss: 8898/10000 [=========================>....] - ETA: 8:41 - loss: 1.7840 - regression_loss: 1.3611 - classification_loss: 8899/10000 [=========================>....] - ETA: 8:41 - loss: 1.7840 - regression_loss: 1.3611 - classification_loss: 8900/10000 [=========================>....] - ETA: 8:40 - loss: 1.7839 - regression_loss: 1.3610 - classification_loss: 8901/10000 [=========================>....] - ETA: 8:40 - loss: 1.7839 - regression_loss: 1.3610 - classification_loss: 8902/10000 [=========================>....] - ETA: 8:39 - loss: 1.7838 - regression_loss: 1.3609 - classification_loss: 8903/10000 [=========================>....] - ETA: 8:39 - loss: 1.7838 - regression_loss: 1.3608 - classification_loss: 8904/10000 [=========================>....] - ETA: 8:38 - loss: 1.7839 - regression_loss: 1.3609 - classification_loss: 8905/10000 [=========================>....] - ETA: 8:38 - loss: 1.7839 - regression_loss: 1.3609 - classification_loss: 8906/10000 [=========================>....] - ETA: 8:37 - loss: 1.7839 - regression_loss: 1.3609 - classification_loss: 8907/10000 [=========================>....] - ETA: 8:37 - loss: 1.7838 - regression_loss: 1.3608 - classification_loss: 8908/10000 [=========================>....] - ETA: 8:37 - loss: 1.7838 - regression_loss: 1.3608 - classification_loss: 8909/10000 [=========================>....] - ETA: 8:36 - loss: 1.7838 - regression_loss: 1.3608 - classification_loss: 8910/10000 [=========================>....] - ETA: 8:36 - loss: 1.7836 - regression_loss: 1.3607 - classification_loss: 8911/10000 [=========================>....] - ETA: 8:35 - loss: 1.7836 - regression_loss: 1.3607 - classification_loss: 8912/10000 [=========================>....] - ETA: 8:35 - loss: 1.7836 - regression_loss: 1.3607 - classification_loss: 8913/10000 [=========================>....] - ETA: 8:34 - loss: 1.7837 - regression_loss: 1.3607 - classification_loss: 8914/10000 [=========================>....] - ETA: 8:34 - loss: 1.7837 - regression_loss: 1.3608 - classification_loss: 8915/10000 [=========================>....] - ETA: 8:33 - loss: 1.7837 - regression_loss: 1.3608 - classification_loss: 8916/10000 [=========================>....] - ETA: 8:33 - loss: 1.7836 - regression_loss: 1.3607 - classification_loss: 8917/10000 [=========================>....] - ETA: 8:32 - loss: 1.7836 - regression_loss: 1.3607 - classification_loss: 8918/10000 [=========================>....] - ETA: 8:32 - loss: 1.7836 - regression_loss: 1.3608 - classification_loss: 8919/10000 [=========================>....] - ETA: 8:31 - loss: 1.7837 - regression_loss: 1.3608 - classification_loss: 8920/10000 [=========================>....] - ETA: 8:31 - loss: 1.7837 - regression_loss: 1.3609 - classification_loss: 8921/10000 [=========================>....] - ETA: 8:30 - loss: 1.7838 - regression_loss: 1.3609 - classification_loss: 8922/10000 [=========================>....] - ETA: 8:30 - loss: 1.7838 - regression_loss: 1.3609 - classification_loss: 8923/10000 [=========================>....] - ETA: 8:29 - loss: 1.7838 - regression_loss: 1.3610 - classification_loss: 8924/10000 [=========================>....] - ETA: 8:29 - loss: 1.7838 - regression_loss: 1.3609 - classification_loss: 8925/10000 [=========================>....] - ETA: 8:28 - loss: 1.7838 - regression_loss: 1.3609 - classification_loss: 8926/10000 [=========================>....] - ETA: 8:28 - loss: 1.7837 - regression_loss: 1.3608 - classification_loss: 8927/10000 [=========================>....] - ETA: 8:28 - loss: 1.7837 - regression_loss: 1.3609 - classification_loss: 8928/10000 [=========================>....] - ETA: 8:27 - loss: 1.7837 - regression_loss: 1.3609 - classification_loss: 8929/10000 [=========================>....] - ETA: 8:27 - loss: 1.7836 - regression_loss: 1.3608 - classification_loss: 8930/10000 [=========================>....] - ETA: 8:26 - loss: 1.7836 - regression_loss: 1.3608 - classification_loss: 8931/10000 [=========================>....] - ETA: 8:26 - loss: 1.7835 - regression_loss: 1.3607 - classification_loss: 8932/10000 [=========================>....] - ETA: 8:25 - loss: 1.7834 - regression_loss: 1.3606 - classification_loss: 8933/10000 [=========================>....] - ETA: 8:25 - loss: 1.7835 - regression_loss: 1.3606 - classification_loss: 8934/10000 [=========================>....] - ETA: 8:24 - loss: 1.7835 - regression_loss: 1.3607 - classification_loss: 8935/10000 [=========================>....] - ETA: 8:24 - loss: 1.7836 - regression_loss: 1.3608 - classification_loss: 8936/10000 [=========================>....] - ETA: 8:23 - loss: 1.7836 - regression_loss: 1.3608 - classification_loss: 8937/10000 [=========================>....] - ETA: 8:23 - loss: 1.7836 - regression_loss: 1.3607 - classification_loss: 8938/10000 [=========================>....] - ETA: 8:22 - loss: 1.7836 - regression_loss: 1.3607 - classification_loss: 8939/10000 [=========================>....] - ETA: 8:22 - loss: 1.7836 - regression_loss: 1.3608 - classification_loss: 8940/10000 [=========================>....] - ETA: 8:21 - loss: 1.7835 - regression_loss: 1.3607 - classification_loss: 8941/10000 [=========================>....] - ETA: 8:21 - loss: 1.7834 - regression_loss: 1.3605 - classification_loss: 8942/10000 [=========================>....] - ETA: 8:20 - loss: 1.7834 - regression_loss: 1.3606 - classification_loss: 8943/10000 [=========================>....] - ETA: 8:20 - loss: 1.7833 - regression_loss: 1.3605 - classification_loss: 8944/10000 [=========================>....] - ETA: 8:19 - loss: 1.7832 - regression_loss: 1.3604 - classification_loss: 8945/10000 [=========================>....] - ETA: 8:19 - loss: 1.7833 - regression_loss: 1.3605 - classification_loss: 8946/10000 [=========================>....] - ETA: 8:19 - loss: 1.7832 - regression_loss: 1.3604 - classification_loss: 8947/10000 [=========================>....] - ETA: 8:18 - loss: 1.7831 - regression_loss: 1.3603 - classification_loss: 8948/10000 [=========================>....] - ETA: 8:18 - loss: 1.7831 - regression_loss: 1.3603 - classification_loss: 8949/10000 [=========================>....] - ETA: 8:17 - loss: 1.7831 - regression_loss: 1.3604 - classification_loss: 8950/10000 [=========================>....] - ETA: 8:17 - loss: 1.7832 - regression_loss: 1.3604 - classification_loss: 8951/10000 [=========================>....] - ETA: 8:16 - loss: 1.7831 - regression_loss: 1.3604 - classification_loss: 8952/10000 [=========================>....] - ETA: 8:16 - loss: 1.7831 - regression_loss: 1.3604 - classification_loss: 8953/10000 [=========================>....] - ETA: 8:15 - loss: 1.7830 - regression_loss: 1.3603 - classification_loss: 8954/10000 [=========================>....] - ETA: 8:15 - loss: 1.7829 - regression_loss: 1.3602 - classification_loss: 8955/10000 [=========================>....] - ETA: 8:14 - loss: 1.7830 - regression_loss: 1.3603 - classification_loss: 8956/10000 [=========================>....] - ETA: 8:14 - loss: 1.7829 - regression_loss: 1.3602 - classification_loss: 8957/10000 [=========================>....] - ETA: 8:13 - loss: 1.7830 - regression_loss: 1.3603 - classification_loss: 8958/10000 [=========================>....] - ETA: 8:13 - loss: 1.7829 - regression_loss: 1.3602 - classification_loss: 8959/10000 [=========================>....] - ETA: 8:12 - loss: 1.7828 - regression_loss: 1.3601 - classification_loss: 8960/10000 [=========================>....] - ETA: 8:12 - loss: 1.7828 - regression_loss: 1.3601 - classification_loss: 8961/10000 [=========================>....] - ETA: 8:11 - loss: 1.7828 - regression_loss: 1.3601 - classification_loss: 8962/10000 [=========================>....] - ETA: 8:11 - loss: 1.7828 - regression_loss: 1.3601 - classification_loss: 8963/10000 [=========================>....] - ETA: 8:10 - loss: 1.7829 - regression_loss: 1.3602 - classification_loss: 8964/10000 [=========================>....] - ETA: 8:10 - loss: 1.7830 - regression_loss: 1.3603 - classification_loss: 8965/10000 [=========================>....] - ETA: 8:10 - loss: 1.7830 - regression_loss: 1.3603 - classification_loss: 8966/10000 [=========================>....] - ETA: 8:09 - loss: 1.7830 - regression_loss: 1.3603 - classification_loss: 8967/10000 [=========================>....] - ETA: 8:09 - loss: 1.7831 - regression_loss: 1.3604 - classification_loss: 8968/10000 [=========================>....] - ETA: 8:08 - loss: 1.7831 - regression_loss: 1.3603 - classification_loss: 8969/10000 [=========================>....] - ETA: 8:08 - loss: 1.7830 - regression_loss: 1.3603 - classification_loss: 8970/10000 [=========================>....] - ETA: 8:07 - loss: 1.7829 - regression_loss: 1.3602 - classification_loss: 8971/10000 [=========================>....] - ETA: 8:07 - loss: 1.7830 - regression_loss: 1.3602 - classification_loss: 8972/10000 [=========================>....] - ETA: 8:06 - loss: 1.7829 - regression_loss: 1.3602 - classification_loss: 8973/10000 [=========================>....] - ETA: 8:06 - loss: 1.7829 - regression_loss: 1.3602 - classification_loss: 8974/10000 [=========================>....] - ETA: 8:05 - loss: 1.7828 - regression_loss: 1.3601 - classification_loss: 8975/10000 [=========================>....] - ETA: 8:05 - loss: 1.7828 - regression_loss: 1.3601 - classification_loss: 8976/10000 [=========================>....] - ETA: 8:04 - loss: 1.7828 - regression_loss: 1.3601 - classification_loss: 8977/10000 [=========================>....] - ETA: 8:04 - loss: 1.7829 - regression_loss: 1.3601 - classification_loss: 8978/10000 [=========================>....] - ETA: 8:03 - loss: 1.7829 - regression_loss: 1.3601 - classification_loss: 8979/10000 [=========================>....] - ETA: 8:03 - loss: 1.7829 - regression_loss: 1.3601 - classification_loss: 8980/10000 [=========================>....] - ETA: 8:02 - loss: 1.7828 - regression_loss: 1.3600 - classification_loss: 8981/10000 [=========================>....] - ETA: 8:02 - loss: 1.7827 - regression_loss: 1.3600 - classification_loss: 8982/10000 [=========================>....] - ETA: 8:01 - loss: 1.7827 - regression_loss: 1.3600 - classification_loss: 8983/10000 [=========================>....] - ETA: 8:01 - loss: 1.7827 - regression_loss: 1.3599 - classification_loss: 8984/10000 [=========================>....] - ETA: 8:00 - loss: 1.7827 - regression_loss: 1.3599 - classification_loss: 8985/10000 [=========================>....] - ETA: 8:00 - loss: 1.7827 - regression_loss: 1.3599 - classification_loss: 8986/10000 [=========================>....] - ETA: 8:00 - loss: 1.7827 - regression_loss: 1.3599 - classification_loss: 8987/10000 [=========================>....] - ETA: 7:59 - loss: 1.7825 - regression_loss: 1.3598 - classification_loss: 8988/10000 [=========================>....] - ETA: 7:59 - loss: 1.7824 - regression_loss: 1.3597 - classification_loss: 8989/10000 [=========================>....] - ETA: 7:58 - loss: 1.7824 - regression_loss: 1.3597 - classification_loss: 8990/10000 [=========================>....] - ETA: 7:58 - loss: 1.7824 - regression_loss: 1.3597 - classification_loss: 8991/10000 [=========================>....] - ETA: 7:57 - loss: 1.7825 - regression_loss: 1.3598 - classification_loss: 8992/10000 [=========================>....] - ETA: 7:57 - loss: 1.7824 - regression_loss: 1.3597 - classification_loss: 8993/10000 [=========================>....] - ETA: 7:56 - loss: 1.7823 - regression_loss: 1.3596 - classification_loss: 8994/10000 [=========================>....] - ETA: 7:56 - loss: 1.7822 - regression_loss: 1.3595 - classification_loss: 8995/10000 [=========================>....] - ETA: 7:55 - loss: 1.7821 - regression_loss: 1.3595 - classification_loss: 8996/10000 [=========================>....] - ETA: 7:55 - loss: 1.7821 - regression_loss: 1.3595 - classification_loss: 8997/10000 [=========================>....] - ETA: 7:54 - loss: 1.7822 - regression_loss: 1.3595 - classification_loss: 8998/10000 [=========================>....] - ETA: 7:54 - loss: 1.7821 - regression_loss: 1.3594 - classification_loss: 8999/10000 [=========================>....] - ETA: 7:53 - loss: 1.7821 - regression_loss: 1.3594 - classification_loss: 9000/10000 [==========================>...] - ETA: 7:53 - loss: 1.7821 - regression_loss: 1.3595 - classification_loss: 9001/10000 [==========================>...] - ETA: 7:52 - loss: 1.7820 - regression_loss: 1.3594 - classification_loss: 9002/10000 [==========================>...] - ETA: 7:52 - loss: 1.7821 - regression_loss: 1.3595 - classification_loss: 9003/10000 [==========================>...] - ETA: 7:51 - loss: 1.7821 - regression_loss: 1.3595 - classification_loss: 9004/10000 [==========================>...] - ETA: 7:51 - loss: 1.7820 - regression_loss: 1.3595 - classification_loss: 9005/10000 [==========================>...] - ETA: 7:51 - loss: 1.7819 - regression_loss: 1.3594 - classification_loss: 9006/10000 [==========================>...] - ETA: 7:50 - loss: 1.7819 - regression_loss: 1.3594 - classification_loss: 9007/10000 [==========================>...] - ETA: 7:50 - loss: 1.7819 - regression_loss: 1.3594 - classification_loss: 9008/10000 [==========================>...] - ETA: 7:49 - loss: 1.7819 - regression_loss: 1.3594 - classification_loss: 9009/10000 [==========================>...] - ETA: 7:49 - loss: 1.7817 - regression_loss: 1.3593 - classification_loss: 9010/10000 [==========================>...] - ETA: 7:48 - loss: 1.7818 - regression_loss: 1.3593 - classification_loss: 9011/10000 [==========================>...] - ETA: 7:48 - loss: 1.7818 - regression_loss: 1.3593 - classification_loss: 9012/10000 [==========================>...] - ETA: 7:47 - loss: 1.7818 - regression_loss: 1.3593 - classification_loss: 9013/10000 [==========================>...] - ETA: 7:47 - loss: 1.7818 - regression_loss: 1.3594 - classification_loss: 9014/10000 [==========================>...] - ETA: 7:46 - loss: 1.7818 - regression_loss: 1.3593 - classification_loss: 9015/10000 [==========================>...] - ETA: 7:46 - loss: 1.7817 - regression_loss: 1.3592 - classification_loss: 9016/10000 [==========================>...] - ETA: 7:45 - loss: 1.7817 - regression_loss: 1.3593 - classification_loss: 9017/10000 [==========================>...] - ETA: 7:45 - loss: 1.7818 - regression_loss: 1.3593 - classification_loss: 9018/10000 [==========================>...] - ETA: 7:44 - loss: 1.7818 - regression_loss: 1.3594 - classification_loss: 9019/10000 [==========================>...] - ETA: 7:44 - loss: 1.7819 - regression_loss: 1.3594 - classification_loss: 9020/10000 [==========================>...] - ETA: 7:43 - loss: 1.7820 - regression_loss: 1.3595 - classification_loss: 9021/10000 [==========================>...] - ETA: 7:43 - loss: 1.7820 - regression_loss: 1.3595 - classification_loss: 9022/10000 [==========================>...] - ETA: 7:42 - loss: 1.7819 - regression_loss: 1.3594 - classification_loss: 9023/10000 [==========================>...] - ETA: 7:42 - loss: 1.7819 - regression_loss: 1.3594 - classification_loss: 9024/10000 [==========================>...] - ETA: 7:42 - loss: 1.7820 - regression_loss: 1.3595 - classification_loss: 9025/10000 [==========================>...] - ETA: 7:41 - loss: 1.7820 - regression_loss: 1.3595 - classification_loss: 9026/10000 [==========================>...] - ETA: 7:41 - loss: 1.7819 - regression_loss: 1.3594 - classification_loss: 9027/10000 [==========================>...] - ETA: 7:40 - loss: 1.7819 - regression_loss: 1.3594 - classification_loss: 9028/10000 [==========================>...] - ETA: 7:40 - loss: 1.7820 - regression_loss: 1.3595 - classification_loss: 9029/10000 [==========================>...] - ETA: 7:39 - loss: 1.7820 - regression_loss: 1.3595 - classification_loss: 9030/10000 [==========================>...] - ETA: 7:39 - loss: 1.7820 - regression_loss: 1.3595 - classification_loss: 9031/10000 [==========================>...] - ETA: 7:38 - loss: 1.7820 - regression_loss: 1.3595 - classification_loss: 9032/10000 [==========================>...] - ETA: 7:38 - loss: 1.7819 - regression_loss: 1.3594 - classification_loss: 9033/10000 [==========================>...] - ETA: 7:37 - loss: 1.7819 - regression_loss: 1.3594 - classification_loss: 9034/10000 [==========================>...] - ETA: 7:37 - loss: 1.7820 - regression_loss: 1.3595 - classification_loss: 9035/10000 [==========================>...] - ETA: 7:36 - loss: 1.7820 - regression_loss: 1.3595 - classification_loss: 9036/10000 [==========================>...] - ETA: 7:36 - loss: 1.7820 - regression_loss: 1.3595 - classification_loss: 9037/10000 [==========================>...] - ETA: 7:35 - loss: 1.7820 - regression_loss: 1.3596 - classification_loss: 9038/10000 [==========================>...] - ETA: 7:35 - loss: 1.7820 - regression_loss: 1.3596 - classification_loss: 9039/10000 [==========================>...] - ETA: 7:34 - loss: 1.7820 - regression_loss: 1.3595 - classification_loss: 9040/10000 [==========================>...] - ETA: 7:34 - loss: 1.7820 - regression_loss: 1.3596 - classification_loss: 9041/10000 [==========================>...] - ETA: 7:33 - loss: 1.7820 - regression_loss: 1.3596 - classification_loss: 9042/10000 [==========================>...] - ETA: 7:33 - loss: 1.7820 - regression_loss: 1.3596 - classification_loss: 9043/10000 [==========================>...] - ETA: 7:33 - loss: 1.7820 - regression_loss: 1.3595 - classification_loss: 9044/10000 [==========================>...] - ETA: 7:32 - loss: 1.7819 - regression_loss: 1.3595 - classification_loss: 9045/10000 [==========================>...] - ETA: 7:32 - loss: 1.7818 - regression_loss: 1.3594 - classification_loss: 9046/10000 [==========================>...] - ETA: 7:31 - loss: 1.7819 - regression_loss: 1.3595 - classification_loss: 9047/10000 [==========================>...] - ETA: 7:31 - loss: 1.7819 - regression_loss: 1.3595 - classification_loss: 9048/10000 [==========================>...] - ETA: 7:30 - loss: 1.7818 - regression_loss: 1.3594 - classification_loss: 9049/10000 [==========================>...] - ETA: 7:30 - loss: 1.7817 - regression_loss: 1.3594 - classification_loss: 9050/10000 [==========================>...] - ETA: 7:29 - loss: 1.7816 - regression_loss: 1.3593 - classification_loss: 9051/10000 [==========================>...] - ETA: 7:29 - loss: 1.7816 - regression_loss: 1.3593 - classification_loss: 9052/10000 [==========================>...] - ETA: 7:28 - loss: 1.7815 - regression_loss: 1.3592 - classification_loss: 9053/10000 [==========================>...] - ETA: 7:28 - loss: 1.7814 - regression_loss: 1.3591 - classification_loss: 9054/10000 [==========================>...] - ETA: 7:27 - loss: 1.7814 - regression_loss: 1.3591 - classification_loss: 9055/10000 [==========================>...] - ETA: 7:27 - loss: 1.7815 - regression_loss: 1.3592 - classification_loss: 9056/10000 [==========================>...] - ETA: 7:26 - loss: 1.7814 - regression_loss: 1.3591 - classification_loss: 9057/10000 [==========================>...] - ETA: 7:26 - loss: 1.7815 - regression_loss: 1.3591 - classification_loss: 9058/10000 [==========================>...] - ETA: 7:25 - loss: 1.7813 - regression_loss: 1.3590 - classification_loss: 9059/10000 [==========================>...] - ETA: 7:25 - loss: 1.7813 - regression_loss: 1.3590 - classification_loss: 9060/10000 [==========================>...] - ETA: 7:24 - loss: 1.7813 - regression_loss: 1.3590 - classification_loss: 9061/10000 [==========================>...] - ETA: 7:24 - loss: 1.7813 - regression_loss: 1.3591 - classification_loss: 9062/10000 [==========================>...] - ETA: 7:24 - loss: 1.7812 - regression_loss: 1.3590 - classification_loss: 9063/10000 [==========================>...] - ETA: 7:23 - loss: 1.7812 - regression_loss: 1.3589 - classification_loss: 9064/10000 [==========================>...] - ETA: 7:23 - loss: 1.7811 - regression_loss: 1.3589 - classification_loss: 9065/10000 [==========================>...] - ETA: 7:22 - loss: 1.7811 - regression_loss: 1.3589 - classification_loss: 9066/10000 [==========================>...] - ETA: 7:22 - loss: 1.7811 - regression_loss: 1.3589 - classification_loss: 9067/10000 [==========================>...] - ETA: 7:21 - loss: 1.7811 - regression_loss: 1.3589 - classification_loss: 9068/10000 [==========================>...] - ETA: 7:21 - loss: 1.7811 - regression_loss: 1.3589 - classification_loss: 9069/10000 [==========================>...] - ETA: 7:20 - loss: 1.7810 - regression_loss: 1.3588 - classification_loss: 9070/10000 [==========================>...] - ETA: 7:20 - loss: 1.7810 - regression_loss: 1.3589 - classification_loss: 9071/10000 [==========================>...] - ETA: 7:19 - loss: 1.7810 - regression_loss: 1.3589 - classification_loss: 9072/10000 [==========================>...] - ETA: 7:19 - loss: 1.7809 - regression_loss: 1.3588 - classification_loss: 9073/10000 [==========================>...] - ETA: 7:18 - loss: 1.7809 - regression_loss: 1.3588 - classification_loss: 9074/10000 [==========================>...] - ETA: 7:18 - loss: 1.7809 - regression_loss: 1.3589 - classification_loss: 9075/10000 [==========================>...] - ETA: 7:17 - loss: 1.7809 - regression_loss: 1.3589 - classification_loss: 9076/10000 [==========================>...] - ETA: 7:17 - loss: 1.7810 - regression_loss: 1.3589 - classification_loss: 9077/10000 [==========================>...] - ETA: 7:16 - loss: 1.7810 - regression_loss: 1.3590 - classification_loss: 9078/10000 [==========================>...] - ETA: 7:16 - loss: 1.7810 - regression_loss: 1.3590 - classification_loss: 9079/10000 [==========================>...] - ETA: 7:15 - loss: 1.7811 - regression_loss: 1.3590 - classification_loss: 9080/10000 [==========================>...] - ETA: 7:15 - loss: 1.7810 - regression_loss: 1.3590 - classification_loss: 9081/10000 [==========================>...] - ETA: 7:14 - loss: 1.7810 - regression_loss: 1.3590 - classification_loss: 9082/10000 [==========================>...] - ETA: 7:14 - loss: 1.7811 - regression_loss: 1.3590 - classification_loss: 9083/10000 [==========================>...] - ETA: 7:14 - loss: 1.7810 - regression_loss: 1.3590 - classification_loss: 9084/10000 [==========================>...] - ETA: 7:13 - loss: 1.7810 - regression_loss: 1.3589 - classification_loss: 9085/10000 [==========================>...] - ETA: 7:13 - loss: 1.7809 - regression_loss: 1.3589 - classification_loss: 9086/10000 [==========================>...] - ETA: 7:12 - loss: 1.7809 - regression_loss: 1.3589 - classification_loss: 9087/10000 [==========================>...] - ETA: 7:12 - loss: 1.7808 - regression_loss: 1.3588 - classification_loss: 9088/10000 [==========================>...] - ETA: 7:11 - loss: 1.7809 - regression_loss: 1.3589 - classification_loss: 9089/10000 [==========================>...] - ETA: 7:11 - loss: 1.7809 - regression_loss: 1.3589 - classification_loss: 9090/10000 [==========================>...] - ETA: 7:10 - loss: 1.7809 - regression_loss: 1.3589 - classification_loss: 9091/10000 [==========================>...] - ETA: 7:10 - loss: 1.7809 - regression_loss: 1.3589 - classification_loss: 9092/10000 [==========================>...] - ETA: 7:09 - loss: 1.7808 - regression_loss: 1.3588 - classification_loss: 9093/10000 [==========================>...] - ETA: 7:09 - loss: 1.7808 - regression_loss: 1.3588 - classification_loss: 9094/10000 [==========================>...] - ETA: 7:08 - loss: 1.7808 - regression_loss: 1.3588 - classification_loss: 9095/10000 [==========================>...] - ETA: 7:08 - loss: 1.7809 - regression_loss: 1.3589 - classification_loss: 9096/10000 [==========================>...] - ETA: 7:07 - loss: 1.7809 - regression_loss: 1.3589 - classification_loss: 9097/10000 [==========================>...] - ETA: 7:07 - loss: 1.7809 - regression_loss: 1.3590 - classification_loss: 9098/10000 [==========================>...] - ETA: 7:06 - loss: 1.7809 - regression_loss: 1.3589 - classification_loss: 9099/10000 [==========================>...] - ETA: 7:06 - loss: 1.7810 - regression_loss: 1.3590 - classification_loss: 9100/10000 [==========================>...] - ETA: 7:05 - loss: 1.7809 - regression_loss: 1.3589 - classification_loss: 9101/10000 [==========================>...] - ETA: 7:05 - loss: 1.7808 - regression_loss: 1.3588 - classification_loss: 9102/10000 [==========================>...] - ETA: 7:05 - loss: 1.7807 - regression_loss: 1.3588 - classification_loss: 9103/10000 [==========================>...] - ETA: 7:04 - loss: 1.7807 - regression_loss: 1.3587 - classification_loss: 9104/10000 [==========================>...] - ETA: 7:04 - loss: 1.7807 - regression_loss: 1.3588 - classification_loss: 9105/10000 [==========================>...] - ETA: 7:03 - loss: 1.7806 - regression_loss: 1.3587 - classification_loss: 9106/10000 [==========================>...] - ETA: 7:03 - loss: 1.7807 - regression_loss: 1.3587 - classification_loss: 9107/10000 [==========================>...] - ETA: 7:02 - loss: 1.7806 - regression_loss: 1.3587 - classification_loss: 9108/10000 [==========================>...] - ETA: 7:02 - loss: 1.7807 - regression_loss: 1.3588 - classification_loss: 9109/10000 [==========================>...] - ETA: 7:01 - loss: 1.7806 - regression_loss: 1.3587 - classification_loss: 9110/10000 [==========================>...] - ETA: 7:01 - loss: 1.7806 - regression_loss: 1.3587 - classification_loss: 9111/10000 [==========================>...] - ETA: 7:00 - loss: 1.7805 - regression_loss: 1.3586 - classification_loss: 9112/10000 [==========================>...] - ETA: 7:00 - loss: 1.7805 - regression_loss: 1.3586 - classification_loss: 9113/10000 [==========================>...] - ETA: 6:59 - loss: 1.7806 - regression_loss: 1.3587 - classification_loss: 9114/10000 [==========================>...] - ETA: 6:59 - loss: 1.7806 - regression_loss: 1.3587 - classification_loss: 9115/10000 [==========================>...] - ETA: 6:58 - loss: 1.7807 - regression_loss: 1.3588 - classification_loss: 9116/10000 [==========================>...] - ETA: 6:58 - loss: 1.7806 - regression_loss: 1.3587 - classification_loss: 9117/10000 [==========================>...] - ETA: 6:57 - loss: 1.7808 - regression_loss: 1.3588 - classification_loss: 9118/10000 [==========================>...] - ETA: 6:57 - loss: 1.7807 - regression_loss: 1.3588 - classification_loss: 9119/10000 [==========================>...] - ETA: 6:56 - loss: 1.7807 - regression_loss: 1.3587 - classification_loss: 9120/10000 [==========================>...] - ETA: 6:56 - loss: 1.7806 - regression_loss: 1.3587 - classification_loss: 9121/10000 [==========================>...] - ETA: 6:56 - loss: 1.7805 - regression_loss: 1.3586 - classification_loss: 9122/10000 [==========================>...] - ETA: 6:55 - loss: 1.7804 - regression_loss: 1.3585 - classification_loss: 9123/10000 [==========================>...] - ETA: 6:55 - loss: 1.7804 - regression_loss: 1.3585 - classification_loss: 9124/10000 [==========================>...] - ETA: 6:54 - loss: 1.7804 - regression_loss: 1.3585 - classification_loss: 9125/10000 [==========================>...] - ETA: 6:54 - loss: 1.7804 - regression_loss: 1.3585 - classification_loss: 9126/10000 [==========================>...] - ETA: 6:53 - loss: 1.7803 - regression_loss: 1.3584 - classification_loss: 9127/10000 [==========================>...] - ETA: 6:53 - loss: 1.7803 - regression_loss: 1.3584 - classification_loss: 9128/10000 [==========================>...] - ETA: 6:52 - loss: 1.7804 - regression_loss: 1.3585 - classification_loss: 9129/10000 [==========================>...] - ETA: 6:52 - loss: 1.7803 - regression_loss: 1.3584 - classification_loss: 9130/10000 [==========================>...] - ETA: 6:51 - loss: 1.7803 - regression_loss: 1.3584 - classification_loss: 9131/10000 [==========================>...] - ETA: 6:51 - loss: 1.7803 - regression_loss: 1.3584 - classification_loss: 9132/10000 [==========================>...] - ETA: 6:50 - loss: 1.7802 - regression_loss: 1.3583 - classification_loss: 9133/10000 [==========================>...] - ETA: 6:50 - loss: 1.7802 - regression_loss: 1.3583 - classification_loss: 9134/10000 [==========================>...] - ETA: 6:49 - loss: 1.7801 - regression_loss: 1.3583 - classification_loss: 9135/10000 [==========================>...] - ETA: 6:49 - loss: 1.7801 - regression_loss: 1.3583 - classification_loss: 9136/10000 [==========================>...] - ETA: 6:48 - loss: 1.7803 - regression_loss: 1.3584 - classification_loss: 9137/10000 [==========================>...] - ETA: 6:48 - loss: 1.7803 - regression_loss: 1.3585 - classification_loss: 9138/10000 [==========================>...] - ETA: 6:47 - loss: 1.7804 - regression_loss: 1.3585 - classification_loss: 9139/10000 [==========================>...] - ETA: 6:47 - loss: 1.7805 - regression_loss: 1.3586 - classification_loss: 9140/10000 [==========================>...] - ETA: 6:46 - loss: 1.7803 - regression_loss: 1.3585 - classification_loss: 9141/10000 [==========================>...] - ETA: 6:46 - loss: 1.7804 - regression_loss: 1.3585 - classification_loss: 9142/10000 [==========================>...] - ETA: 6:46 - loss: 1.7805 - regression_loss: 1.3585 - classification_loss: 9143/10000 [==========================>...] - ETA: 6:45 - loss: 1.7804 - regression_loss: 1.3585 - classification_loss: 9144/10000 [==========================>...] - ETA: 6:45 - loss: 1.7804 - regression_loss: 1.3585 - classification_loss: 9145/10000 [==========================>...] - ETA: 6:44 - loss: 1.7803 - regression_loss: 1.3584 - classification_loss: 9146/10000 [==========================>...] - ETA: 6:44 - loss: 1.7803 - regression_loss: 1.3585 - classification_loss: 9147/10000 [==========================>...] - ETA: 6:43 - loss: 1.7803 - regression_loss: 1.3584 - classification_loss: 9148/10000 [==========================>...] - ETA: 6:43 - loss: 1.7802 - regression_loss: 1.3583 - classification_loss: 9149/10000 [==========================>...] - ETA: 6:42 - loss: 1.7802 - regression_loss: 1.3584 - classification_loss: 9150/10000 [==========================>...] - ETA: 6:42 - loss: 1.7802 - regression_loss: 1.3584 - classification_loss: 9151/10000 [==========================>...] - ETA: 6:41 - loss: 1.7802 - regression_loss: 1.3583 - classification_loss: 9152/10000 [==========================>...] - ETA: 6:41 - loss: 1.7802 - regression_loss: 1.3583 - classification_loss: 9153/10000 [==========================>...] - ETA: 6:40 - loss: 1.7801 - regression_loss: 1.3583 - classification_loss: 9154/10000 [==========================>...] - ETA: 6:40 - loss: 1.7801 - regression_loss: 1.3583 - classification_loss: 9155/10000 [==========================>...] - ETA: 6:39 - loss: 1.7800 - regression_loss: 1.3582 - classification_loss: 9156/10000 [==========================>...] - ETA: 6:39 - loss: 1.7799 - regression_loss: 1.3582 - classification_loss: 9157/10000 [==========================>...] - ETA: 6:38 - loss: 1.7799 - regression_loss: 1.3581 - classification_loss: 9158/10000 [==========================>...] - ETA: 6:38 - loss: 1.7798 - regression_loss: 1.3581 - classification_loss: 9159/10000 [==========================>...] - ETA: 6:37 - loss: 1.7798 - regression_loss: 1.3581 - classification_loss: 9160/10000 [==========================>...] - ETA: 6:37 - loss: 1.7798 - regression_loss: 1.3581 - classification_loss: 9161/10000 [==========================>...] - ETA: 6:37 - loss: 1.7797 - regression_loss: 1.3580 - classification_loss: 9162/10000 [==========================>...] - ETA: 6:36 - loss: 1.7797 - regression_loss: 1.3580 - classification_loss: 9163/10000 [==========================>...] - ETA: 6:36 - loss: 1.7796 - regression_loss: 1.3580 - classification_loss: 9164/10000 [==========================>...] - ETA: 6:35 - loss: 1.7796 - regression_loss: 1.3580 - classification_loss: 9165/10000 [==========================>...] - ETA: 6:35 - loss: 1.7795 - regression_loss: 1.3579 - classification_loss: 9166/10000 [==========================>...] - ETA: 6:34 - loss: 1.7796 - regression_loss: 1.3580 - classification_loss: 9167/10000 [==========================>...] - ETA: 6:34 - loss: 1.7796 - regression_loss: 1.3580 - classification_loss: 9168/10000 [==========================>...] - ETA: 6:33 - loss: 1.7795 - regression_loss: 1.3579 - classification_loss: 9169/10000 [==========================>...] - ETA: 6:33 - loss: 1.7796 - regression_loss: 1.3579 - classification_loss: 9170/10000 [==========================>...] - ETA: 6:32 - loss: 1.7796 - regression_loss: 1.3579 - classification_loss: 9171/10000 [==========================>...] - ETA: 6:32 - loss: 1.7796 - regression_loss: 1.3580 - classification_loss: 9172/10000 [==========================>...] - ETA: 6:31 - loss: 1.7795 - regression_loss: 1.3579 - classification_loss: 9173/10000 [==========================>...] - ETA: 6:31 - loss: 1.7796 - regression_loss: 1.3580 - classification_loss: 9174/10000 [==========================>...] - ETA: 6:30 - loss: 1.7796 - regression_loss: 1.3580 - classification_loss: 9175/10000 [==========================>...] - ETA: 6:30 - loss: 1.7797 - regression_loss: 1.3581 - classification_loss: 9176/10000 [==========================>...] - ETA: 6:29 - loss: 1.7797 - regression_loss: 1.3581 - classification_loss: 9177/10000 [==========================>...] - ETA: 6:29 - loss: 1.7798 - regression_loss: 1.3582 - classification_loss: 9178/10000 [==========================>...] - ETA: 6:28 - loss: 1.7799 - regression_loss: 1.3582 - classification_loss: 9179/10000 [==========================>...] - ETA: 6:28 - loss: 1.7798 - regression_loss: 1.3582 - classification_loss: 9180/10000 [==========================>...] - ETA: 6:28 - loss: 1.7798 - regression_loss: 1.3581 - classification_loss: 9181/10000 [==========================>...] - ETA: 6:27 - loss: 1.7796 - regression_loss: 1.3581 - classification_loss: 9182/10000 [==========================>...] - ETA: 6:27 - loss: 1.7796 - regression_loss: 1.3580 - classification_loss: 9183/10000 [==========================>...] - ETA: 6:26 - loss: 1.7795 - regression_loss: 1.3580 - classification_loss: 9184/10000 [==========================>...] - ETA: 6:26 - loss: 1.7795 - regression_loss: 1.3579 - classification_loss: 9185/10000 [==========================>...] - ETA: 6:25 - loss: 1.7794 - regression_loss: 1.3579 - classification_loss: 9186/10000 [==========================>...] - ETA: 6:25 - loss: 1.7795 - regression_loss: 1.3579 - classification_loss: 9187/10000 [==========================>...] - ETA: 6:24 - loss: 1.7795 - regression_loss: 1.3579 - classification_loss: 9188/10000 [==========================>...] - ETA: 6:24 - loss: 1.7794 - regression_loss: 1.3579 - classification_loss: 9189/10000 [==========================>...] - ETA: 6:23 - loss: 1.7795 - regression_loss: 1.3579 - classification_loss: 9190/10000 [==========================>...] - ETA: 6:23 - loss: 1.7794 - regression_loss: 1.3579 - classification_loss: 9191/10000 [==========================>...] - ETA: 6:22 - loss: 1.7795 - regression_loss: 1.3580 - classification_loss: 9192/10000 [==========================>...] - ETA: 6:22 - loss: 1.7794 - regression_loss: 1.3579 - classification_loss: 9193/10000 [==========================>...] - ETA: 6:21 - loss: 1.7793 - regression_loss: 1.3578 - classification_loss: 9194/10000 [==========================>...] - ETA: 6:21 - loss: 1.7792 - regression_loss: 1.3577 - classification_loss: 9195/10000 [==========================>...] - ETA: 6:20 - loss: 1.7791 - regression_loss: 1.3576 - classification_loss: 9196/10000 [==========================>...] - ETA: 6:20 - loss: 1.7790 - regression_loss: 1.3576 - classification_loss: 9197/10000 [==========================>...] - ETA: 6:19 - loss: 1.7792 - regression_loss: 1.3577 - classification_loss: 9198/10000 [==========================>...] - ETA: 6:19 - loss: 1.7792 - regression_loss: 1.3577 - classification_loss: 9199/10000 [==========================>...] - ETA: 6:19 - loss: 1.7792 - regression_loss: 1.3577 - classification_loss: 9200/10000 [==========================>...] - ETA: 6:18 - loss: 1.7792 - regression_loss: 1.3576 - classification_loss: 9201/10000 [==========================>...] - ETA: 6:18 - loss: 1.7791 - regression_loss: 1.3575 - classification_loss: 9202/10000 [==========================>...] - ETA: 6:17 - loss: 1.7791 - regression_loss: 1.3575 - classification_loss: 9203/10000 [==========================>...] - ETA: 6:17 - loss: 1.7791 - regression_loss: 1.3576 - classification_loss: 9204/10000 [==========================>...] - ETA: 6:16 - loss: 1.7790 - regression_loss: 1.3575 - classification_loss: 9205/10000 [==========================>...] - ETA: 6:16 - loss: 1.7790 - regression_loss: 1.3574 - classification_loss: 9206/10000 [==========================>...] - ETA: 6:15 - loss: 1.7788 - regression_loss: 1.3573 - classification_loss: 9207/10000 [==========================>...] - ETA: 6:15 - loss: 1.7789 - regression_loss: 1.3574 - classification_loss: 9208/10000 [==========================>...] - ETA: 6:14 - loss: 1.7789 - regression_loss: 1.3574 - classification_loss: 9209/10000 [==========================>...] - ETA: 6:14 - loss: 1.7789 - regression_loss: 1.3574 - classification_loss: 9210/10000 [==========================>...] - ETA: 6:13 - loss: 1.7789 - regression_loss: 1.3574 - classification_loss: 9211/10000 [==========================>...] - ETA: 6:13 - loss: 1.7788 - regression_loss: 1.3573 - classification_loss: 9212/10000 [==========================>...] - ETA: 6:12 - loss: 1.7789 - regression_loss: 1.3574 - classification_loss: 9213/10000 [==========================>...] - ETA: 6:12 - loss: 1.7788 - regression_loss: 1.3573 - classification_loss: 9214/10000 [==========================>...] - ETA: 6:11 - loss: 1.7787 - regression_loss: 1.3572 - classification_loss: 9215/10000 [==========================>...] - ETA: 6:11 - loss: 1.7786 - regression_loss: 1.3572 - classification_loss: 9216/10000 [==========================>...] - ETA: 6:10 - loss: 1.7786 - regression_loss: 1.3572 - classification_loss: 9217/10000 [==========================>...] - ETA: 6:10 - loss: 1.7787 - regression_loss: 1.3572 - classification_loss: 9218/10000 [==========================>...] - ETA: 6:10 - loss: 1.7787 - regression_loss: 1.3572 - classification_loss: 9219/10000 [==========================>...] - ETA: 6:09 - loss: 1.7787 - regression_loss: 1.3573 - classification_loss: 9220/10000 [==========================>...] - ETA: 6:09 - loss: 1.7787 - regression_loss: 1.3572 - classification_loss: 9221/10000 [==========================>...] - ETA: 6:08 - loss: 1.7787 - regression_loss: 1.3572 - classification_loss: 9222/10000 [==========================>...] - ETA: 6:08 - loss: 1.7787 - regression_loss: 1.3572 - classification_loss: 9223/10000 [==========================>...] - ETA: 6:07 - loss: 1.7787 - regression_loss: 1.3572 - classification_loss: 9224/10000 [==========================>...] - ETA: 6:07 - loss: 1.7785 - regression_loss: 1.3571 - classification_loss: 9225/10000 [==========================>...] - ETA: 6:06 - loss: 1.7786 - regression_loss: 1.3572 - classification_loss: 9226/10000 [==========================>...] - ETA: 6:06 - loss: 1.7786 - regression_loss: 1.3571 - classification_loss: 9227/10000 [==========================>...] - ETA: 6:05 - loss: 1.7787 - regression_loss: 1.3572 - classification_loss: 9228/10000 [==========================>...] - ETA: 6:05 - loss: 1.7787 - regression_loss: 1.3572 - classification_loss: 9229/10000 [==========================>...] - ETA: 6:04 - loss: 1.7787 - regression_loss: 1.3572 - classification_loss: 9230/10000 [==========================>...] - ETA: 6:04 - loss: 1.7786 - regression_loss: 1.3571 - classification_loss: 9231/10000 [==========================>...] - ETA: 6:03 - loss: 1.7786 - regression_loss: 1.3570 - classification_loss: 9232/10000 [==========================>...] - ETA: 6:03 - loss: 1.7786 - regression_loss: 1.3571 - classification_loss: 9233/10000 [==========================>...] - ETA: 6:02 - loss: 1.7786 - regression_loss: 1.3570 - classification_loss: 9234/10000 [==========================>...] - ETA: 6:02 - loss: 1.7786 - regression_loss: 1.3571 - classification_loss: 9235/10000 [==========================>...] - ETA: 6:01 - loss: 1.7785 - regression_loss: 1.3570 - classification_loss: 9236/10000 [==========================>...] - ETA: 6:01 - loss: 1.7785 - regression_loss: 1.3570 - classification_loss: 9237/10000 [==========================>...] - ETA: 6:01 - loss: 1.7785 - regression_loss: 1.3570 - classification_loss: 9238/10000 [==========================>...] - ETA: 6:00 - loss: 1.7786 - regression_loss: 1.3570 - classification_loss: 9239/10000 [==========================>...] - ETA: 6:00 - loss: 1.7786 - regression_loss: 1.3571 - classification_loss: 9240/10000 [==========================>...] - ETA: 5:59 - loss: 1.7786 - regression_loss: 1.3570 - classification_loss: 9241/10000 [==========================>...] - ETA: 5:59 - loss: 1.7786 - regression_loss: 1.3571 - classification_loss: 9242/10000 [==========================>...] - ETA: 5:58 - loss: 1.7786 - regression_loss: 1.3570 - classification_loss: 9243/10000 [==========================>...] - ETA: 5:58 - loss: 1.7785 - regression_loss: 1.3571 - classification_loss: 9244/10000 [==========================>...] - ETA: 5:57 - loss: 1.7785 - regression_loss: 1.3571 - classification_loss: 9245/10000 [==========================>...] - ETA: 5:57 - loss: 1.7785 - regression_loss: 1.3570 - classification_loss: 9246/10000 [==========================>...] - ETA: 5:56 - loss: 1.7785 - regression_loss: 1.3570 - classification_loss: 9247/10000 [==========================>...] - ETA: 5:56 - loss: 1.7784 - regression_loss: 1.3569 - classification_loss: 9248/10000 [==========================>...] - ETA: 5:55 - loss: 1.7784 - regression_loss: 1.3569 - classification_loss: 9249/10000 [==========================>...] - ETA: 5:55 - loss: 1.7783 - regression_loss: 1.3569 - classification_loss: 9250/10000 [==========================>...] - ETA: 5:54 - loss: 1.7783 - regression_loss: 1.3569 - classification_loss: 9251/10000 [==========================>...] - ETA: 5:54 - loss: 1.7782 - regression_loss: 1.3568 - classification_loss: 9252/10000 [==========================>...] - ETA: 5:53 - loss: 1.7782 - regression_loss: 1.3568 - classification_loss: 9253/10000 [==========================>...] - ETA: 5:53 - loss: 1.7782 - regression_loss: 1.3568 - classification_loss: 9254/10000 [==========================>...] - ETA: 5:52 - loss: 1.7781 - regression_loss: 1.3567 - classification_loss: 9255/10000 [==========================>...] - ETA: 5:52 - loss: 1.7780 - regression_loss: 1.3567 - classification_loss: 9256/10000 [==========================>...] - ETA: 5:52 - loss: 1.7780 - regression_loss: 1.3567 - classification_loss: 9257/10000 [==========================>...] - ETA: 5:51 - loss: 1.7779 - regression_loss: 1.3566 - classification_loss: 9258/10000 [==========================>...] - ETA: 5:51 - loss: 1.7779 - regression_loss: 1.3567 - classification_loss: 9259/10000 [==========================>...] - ETA: 5:50 - loss: 1.7779 - regression_loss: 1.3567 - classification_loss: 9260/10000 [==========================>...] - ETA: 5:50 - loss: 1.7779 - regression_loss: 1.3567 - classification_loss: 9261/10000 [==========================>...] - ETA: 5:49 - loss: 1.7779 - regression_loss: 1.3566 - classification_loss: 9262/10000 [==========================>...] - ETA: 5:49 - loss: 1.7779 - regression_loss: 1.3567 - classification_loss: 9263/10000 [==========================>...] - ETA: 5:48 - loss: 1.7779 - regression_loss: 1.3567 - classification_loss: 9264/10000 [==========================>...] - ETA: 5:48 - loss: 1.7778 - regression_loss: 1.3566 - classification_loss: 9265/10000 [==========================>...] - ETA: 5:47 - loss: 1.7779 - regression_loss: 1.3567 - classification_loss: 9266/10000 [==========================>...] - ETA: 5:47 - loss: 1.7780 - regression_loss: 1.3568 - classification_loss: 9267/10000 [==========================>...] - ETA: 5:46 - loss: 1.7779 - regression_loss: 1.3567 - classification_loss: 9268/10000 [==========================>...] - ETA: 5:46 - loss: 1.7780 - regression_loss: 1.3568 - classification_loss: 9269/10000 [==========================>...] - ETA: 5:45 - loss: 1.7780 - regression_loss: 1.3568 - classification_loss: 9270/10000 [==========================>...] - ETA: 5:45 - loss: 1.7779 - regression_loss: 1.3567 - classification_loss: 9271/10000 [==========================>...] - ETA: 5:44 - loss: 1.7779 - regression_loss: 1.3567 - classification_loss: 9272/10000 [==========================>...] - ETA: 5:44 - loss: 1.7779 - regression_loss: 1.3567 - classification_loss: 9273/10000 [==========================>...] - ETA: 5:43 - loss: 1.7779 - regression_loss: 1.3567 - classification_loss: 9274/10000 [==========================>...] - ETA: 5:43 - loss: 1.7779 - regression_loss: 1.3567 - classification_loss: 9275/10000 [==========================>...] - ETA: 5:42 - loss: 1.7780 - regression_loss: 1.3567 - classification_loss: 9276/10000 [==========================>...] - ETA: 5:42 - loss: 1.7779 - regression_loss: 1.3567 - classification_loss: 9277/10000 [==========================>...] - ETA: 5:42 - loss: 1.7779 - regression_loss: 1.3567 - classification_loss: 9278/10000 [==========================>...] - ETA: 5:41 - loss: 1.7779 - regression_loss: 1.3567 - classification_loss: 9279/10000 [==========================>...] - ETA: 5:41 - loss: 1.7779 - regression_loss: 1.3568 - classification_loss: 9280/10000 [==========================>...] - ETA: 5:40 - loss: 1.7780 - regression_loss: 1.3568 - classification_loss: 9281/10000 [==========================>...] - ETA: 5:40 - loss: 1.7780 - regression_loss: 1.3568 - classification_loss: 9282/10000 [==========================>...] - ETA: 5:39 - loss: 1.7780 - regression_loss: 1.3568 - classification_loss: 9283/10000 [==========================>...] - ETA: 5:39 - loss: 1.7780 - regression_loss: 1.3569 - classification_loss: 9284/10000 [==========================>...] - ETA: 5:38 - loss: 1.7780 - regression_loss: 1.3568 - classification_loss: 9285/10000 [==========================>...] - ETA: 5:38 - loss: 1.7780 - regression_loss: 1.3569 - classification_loss: 9286/10000 [==========================>...] - ETA: 5:37 - loss: 1.7779 - regression_loss: 1.3568 - classification_loss: 9287/10000 [==========================>...] - ETA: 5:37 - loss: 1.7779 - regression_loss: 1.3567 - classification_loss: 9288/10000 [==========================>...] - ETA: 5:36 - loss: 1.7778 - regression_loss: 1.3567 - classification_loss: 9289/10000 [==========================>...] - ETA: 5:36 - loss: 1.7779 - regression_loss: 1.3568 - classification_loss: 9290/10000 [==========================>...] - ETA: 5:35 - loss: 1.7779 - regression_loss: 1.3568 - classification_loss: 9291/10000 [==========================>...] - ETA: 5:35 - loss: 1.7779 - regression_loss: 1.3568 - classification_loss: 9292/10000 [==========================>...] - ETA: 5:34 - loss: 1.7778 - regression_loss: 1.3567 - classification_loss: 9293/10000 [==========================>...] - ETA: 5:34 - loss: 1.7777 - regression_loss: 1.3566 - classification_loss: 9294/10000 [==========================>...] - ETA: 5:33 - loss: 1.7777 - regression_loss: 1.3566 - classification_loss: 9295/10000 [==========================>...] - ETA: 5:33 - loss: 1.7777 - regression_loss: 1.3567 - classification_loss: 9296/10000 [==========================>...] - ETA: 5:33 - loss: 1.7777 - regression_loss: 1.3567 - classification_loss: 9297/10000 [==========================>...] - ETA: 5:32 - loss: 1.7777 - regression_loss: 1.3567 - classification_loss: 9298/10000 [==========================>...] - ETA: 5:32 - loss: 1.7776 - regression_loss: 1.3566 - classification_loss: 9299/10000 [==========================>...] - ETA: 5:31 - loss: 1.7776 - regression_loss: 1.3566 - classification_loss: 9300/10000 [==========================>...] - ETA: 5:31 - loss: 1.7776 - regression_loss: 1.3566 - classification_loss: 9301/10000 [==========================>...] - ETA: 5:30 - loss: 1.7777 - regression_loss: 1.3567 - classification_loss: 9302/10000 [==========================>...] - ETA: 5:30 - loss: 1.7777 - regression_loss: 1.3567 - classification_loss: 9303/10000 [==========================>...] - ETA: 5:29 - loss: 1.7777 - regression_loss: 1.3567 - classification_loss: 9304/10000 [==========================>...] - ETA: 5:29 - loss: 1.7778 - regression_loss: 1.3568 - classification_loss: 9305/10000 [==========================>...] - ETA: 5:28 - loss: 1.7780 - regression_loss: 1.3569 - classification_loss: 9306/10000 [==========================>...] - ETA: 5:28 - loss: 1.7780 - regression_loss: 1.3569 - classification_loss: 9307/10000 [==========================>...] - ETA: 5:27 - loss: 1.7779 - regression_loss: 1.3568 - classification_loss: 9308/10000 [==========================>...] - ETA: 5:27 - loss: 1.7778 - regression_loss: 1.3568 - classification_loss: 9309/10000 [==========================>...] - ETA: 5:26 - loss: 1.7778 - regression_loss: 1.3567 - classification_loss: 9310/10000 [==========================>...] - ETA: 5:26 - loss: 1.7779 - regression_loss: 1.3568 - classification_loss: 9311/10000 [==========================>...] - ETA: 5:25 - loss: 1.7780 - regression_loss: 1.3569 - classification_loss: 9312/10000 [==========================>...] - ETA: 5:25 - loss: 1.7780 - regression_loss: 1.3569 - classification_loss: 9313/10000 [==========================>...] - ETA: 5:24 - loss: 1.7780 - regression_loss: 1.3569 - classification_loss: 9314/10000 [==========================>...] - ETA: 5:24 - loss: 1.7779 - regression_loss: 1.3568 - classification_loss: 9315/10000 [==========================>...] - ETA: 5:24 - loss: 1.7779 - regression_loss: 1.3568 - classification_loss: 9316/10000 [==========================>...] - ETA: 5:23 - loss: 1.7779 - regression_loss: 1.3568 - classification_loss: 9317/10000 [==========================>...] - ETA: 5:23 - loss: 1.7778 - regression_loss: 1.3567 - classification_loss: 9318/10000 [==========================>...] - ETA: 5:22 - loss: 1.7779 - regression_loss: 1.3568 - classification_loss: 9319/10000 [==========================>...] - ETA: 5:22 - loss: 1.7778 - regression_loss: 1.3568 - classification_loss: 9320/10000 [==========================>...] - ETA: 5:21 - loss: 1.7779 - regression_loss: 1.3568 - classification_loss: 9321/10000 [==========================>...] - ETA: 5:21 - loss: 1.7778 - regression_loss: 1.3568 - classification_loss: 9322/10000 [==========================>...] - ETA: 5:20 - loss: 1.7778 - regression_loss: 1.3567 - classification_loss: 9323/10000 [==========================>...] - ETA: 5:20 - loss: 1.7777 - regression_loss: 1.3567 - classification_loss: 9324/10000 [==========================>...] - ETA: 5:19 - loss: 1.7778 - regression_loss: 1.3567 - classification_loss: 9325/10000 [==========================>...] - ETA: 5:19 - loss: 1.7777 - regression_loss: 1.3567 - classification_loss: 9326/10000 [==========================>...] - ETA: 5:18 - loss: 1.7777 - regression_loss: 1.3567 - classification_loss: 9327/10000 [==========================>...] - ETA: 5:18 - loss: 1.7778 - regression_loss: 1.3568 - classification_loss: 9328/10000 [==========================>...] - ETA: 5:17 - loss: 1.7781 - regression_loss: 1.3566 - classification_loss: 9329/10000 [==========================>...] - ETA: 5:17 - loss: 1.7780 - regression_loss: 1.3566 - classification_loss: 9330/10000 [==========================>...] - ETA: 5:16 - loss: 1.7780 - regression_loss: 1.3565 - classification_loss: 9331/10000 [==========================>...] - ETA: 5:16 - loss: 1.7779 - regression_loss: 1.3565 - classification_loss: 9332/10000 [==========================>...] - ETA: 5:15 - loss: 1.7779 - regression_loss: 1.3565 - classification_loss: 9333/10000 [==========================>...] - ETA: 5:15 - loss: 1.7780 - regression_loss: 1.3565 - classification_loss: 9334/10000 [===========================>..] - ETA: 5:15 - loss: 1.7781 - regression_loss: 1.3566 - classification_loss: 9335/10000 [===========================>..] - ETA: 5:14 - loss: 1.7781 - regression_loss: 1.3566 - classification_loss: 9336/10000 [===========================>..] - ETA: 5:14 - loss: 1.7780 - regression_loss: 1.3566 - classification_loss: 9337/10000 [===========================>..] - ETA: 5:13 - loss: 1.7780 - regression_loss: 1.3566 - classification_loss: 9338/10000 [===========================>..] - ETA: 5:13 - loss: 1.7779 - regression_loss: 1.3565 - classification_loss: 9339/10000 [===========================>..] - ETA: 5:12 - loss: 1.7779 - regression_loss: 1.3565 - classification_loss: 9340/10000 [===========================>..] - ETA: 5:12 - loss: 1.7779 - regression_loss: 1.3565 - classification_loss: 9341/10000 [===========================>..] - ETA: 5:11 - loss: 1.7778 - regression_loss: 1.3564 - classification_loss: 9342/10000 [===========================>..] - ETA: 5:11 - loss: 1.7778 - regression_loss: 1.3565 - classification_loss: 9343/10000 [===========================>..] - ETA: 5:10 - loss: 1.7779 - regression_loss: 1.3565 - classification_loss: 9344/10000 [===========================>..] - ETA: 5:10 - loss: 1.7780 - regression_loss: 1.3566 - classification_loss: 9345/10000 [===========================>..] - ETA: 5:09 - loss: 1.7780 - regression_loss: 1.3566 - classification_loss: 9346/10000 [===========================>..] - ETA: 5:09 - loss: 1.7780 - regression_loss: 1.3566 - classification_loss: 9347/10000 [===========================>..] - ETA: 5:08 - loss: 1.7779 - regression_loss: 1.3566 - classification_loss: 9348/10000 [===========================>..] - ETA: 5:08 - loss: 1.7779 - regression_loss: 1.3565 - classification_loss: 9349/10000 [===========================>..] - ETA: 5:07 - loss: 1.7778 - regression_loss: 1.3565 - classification_loss: 9350/10000 [===========================>..] - ETA: 5:07 - loss: 1.7777 - regression_loss: 1.3563 - classification_loss: 9351/10000 [===========================>..] - ETA: 5:06 - loss: 1.7776 - regression_loss: 1.3563 - classification_loss: 9352/10000 [===========================>..] - ETA: 5:06 - loss: 1.7776 - regression_loss: 1.3562 - classification_loss: 9353/10000 [===========================>..] - ETA: 5:06 - loss: 1.7776 - regression_loss: 1.3563 - classification_loss: 9354/10000 [===========================>..] - ETA: 5:05 - loss: 1.7776 - regression_loss: 1.3562 - classification_loss: 9355/10000 [===========================>..] - ETA: 5:05 - loss: 1.7775 - regression_loss: 1.3562 - classification_loss: 9356/10000 [===========================>..] - ETA: 5:04 - loss: 1.7774 - regression_loss: 1.3561 - classification_loss: 9357/10000 [===========================>..] - ETA: 5:04 - loss: 1.7773 - regression_loss: 1.3561 - classification_loss: 9358/10000 [===========================>..] - ETA: 5:03 - loss: 1.7773 - regression_loss: 1.3560 - classification_loss: 9359/10000 [===========================>..] - ETA: 5:03 - loss: 1.7772 - regression_loss: 1.3559 - classification_loss: 9360/10000 [===========================>..] - ETA: 5:02 - loss: 1.7772 - regression_loss: 1.3559 - classification_loss: 9361/10000 [===========================>..] - ETA: 5:02 - loss: 1.7772 - regression_loss: 1.3560 - classification_loss: 9362/10000 [===========================>..] - ETA: 5:01 - loss: 1.7774 - regression_loss: 1.3561 - classification_loss: 9363/10000 [===========================>..] - ETA: 5:01 - loss: 1.7774 - regression_loss: 1.3561 - classification_loss: 9364/10000 [===========================>..] - ETA: 5:00 - loss: 1.7774 - regression_loss: 1.3561 - classification_loss: 9365/10000 [===========================>..] - ETA: 5:00 - loss: 1.7773 - regression_loss: 1.3561 - classification_loss: 9366/10000 [===========================>..] - ETA: 4:59 - loss: 1.7773 - regression_loss: 1.3560 - classification_loss: 9367/10000 [===========================>..] - ETA: 4:59 - loss: 1.7773 - regression_loss: 1.3561 - classification_loss: 9368/10000 [===========================>..] - ETA: 4:58 - loss: 1.7772 - regression_loss: 1.3560 - classification_loss: 9369/10000 [===========================>..] - ETA: 4:58 - loss: 1.7772 - regression_loss: 1.3559 - classification_loss: 9370/10000 [===========================>..] - ETA: 4:57 - loss: 1.7773 - regression_loss: 1.3560 - classification_loss: 9371/10000 [===========================>..] - ETA: 4:57 - loss: 1.7772 - regression_loss: 1.3560 - classification_loss: 9372/10000 [===========================>..] - ETA: 4:57 - loss: 1.7772 - regression_loss: 1.3559 - classification_loss: 9373/10000 [===========================>..] - ETA: 4:56 - loss: 1.7773 - regression_loss: 1.3560 - classification_loss: 9374/10000 [===========================>..] - ETA: 4:56 - loss: 1.7773 - regression_loss: 1.3560 - classification_loss: 9375/10000 [===========================>..] - ETA: 4:55 - loss: 1.7773 - regression_loss: 1.3560 - classification_loss: 9376/10000 [===========================>..] - ETA: 4:55 - loss: 1.7773 - regression_loss: 1.3560 - classification_loss: 9377/10000 [===========================>..] - ETA: 4:54 - loss: 1.7773 - regression_loss: 1.3560 - classification_loss: 9378/10000 [===========================>..] - ETA: 4:54 - loss: 1.7773 - regression_loss: 1.3560 - classification_loss: 9379/10000 [===========================>..] - ETA: 4:53 - loss: 1.7772 - regression_loss: 1.3559 - classification_loss: 9380/10000 [===========================>..] - ETA: 4:53 - loss: 1.7772 - regression_loss: 1.3560 - classification_loss: 9381/10000 [===========================>..] - ETA: 4:52 - loss: 1.7773 - regression_loss: 1.3560 - classification_loss: 9382/10000 [===========================>..] - ETA: 4:52 - loss: 1.7774 - regression_loss: 1.3561 - classification_loss: 9383/10000 [===========================>..] - ETA: 4:51 - loss: 1.7773 - regression_loss: 1.3560 - classification_loss: 9384/10000 [===========================>..] - ETA: 4:51 - loss: 1.7773 - regression_loss: 1.3560 - classification_loss: 9385/10000 [===========================>..] - ETA: 4:50 - loss: 1.7773 - regression_loss: 1.3561 - classification_loss: 9386/10000 [===========================>..] - ETA: 4:50 - loss: 1.7773 - regression_loss: 1.3560 - classification_loss: 9387/10000 [===========================>..] - ETA: 4:49 - loss: 1.7772 - regression_loss: 1.3560 - classification_loss: 9388/10000 [===========================>..] - ETA: 4:49 - loss: 1.7773 - regression_loss: 1.3560 - classification_loss: 9389/10000 [===========================>..] - ETA: 4:48 - loss: 1.7772 - regression_loss: 1.3560 - classification_loss: 9390/10000 [===========================>..] - ETA: 4:48 - loss: 1.7773 - regression_loss: 1.3560 - classification_loss: 9391/10000 [===========================>..] - ETA: 4:48 - loss: 1.7773 - regression_loss: 1.3561 - classification_loss: 9392/10000 [===========================>..] - ETA: 4:47 - loss: 1.7774 - regression_loss: 1.3561 - classification_loss: 9393/10000 [===========================>..] - ETA: 4:47 - loss: 1.7774 - regression_loss: 1.3562 - classification_loss: 9394/10000 [===========================>..] - ETA: 4:46 - loss: 1.7773 - regression_loss: 1.3562 - classification_loss: 9395/10000 [===========================>..] - ETA: 4:46 - loss: 1.7773 - regression_loss: 1.3562 - classification_loss: 9396/10000 [===========================>..] - ETA: 4:45 - loss: 1.7774 - regression_loss: 1.3562 - classification_loss: 9397/10000 [===========================>..] - ETA: 4:45 - loss: 1.7775 - regression_loss: 1.3563 - classification_loss: 9398/10000 [===========================>..] - ETA: 4:44 - loss: 1.7774 - regression_loss: 1.3562 - classification_loss: 9399/10000 [===========================>..] - ETA: 4:44 - loss: 1.7775 - regression_loss: 1.3563 - classification_loss: 9400/10000 [===========================>..] - ETA: 4:43 - loss: 1.7775 - regression_loss: 1.3563 - classification_loss: 9401/10000 [===========================>..] - ETA: 4:43 - loss: 1.7775 - regression_loss: 1.3563 - classification_loss: 9402/10000 [===========================>..] - ETA: 4:42 - loss: 1.7775 - regression_loss: 1.3563 - classification_loss: 9403/10000 [===========================>..] - ETA: 4:42 - loss: 1.7775 - regression_loss: 1.3562 - classification_loss: 9404/10000 [===========================>..] - ETA: 4:41 - loss: 1.7774 - regression_loss: 1.3562 - classification_loss: 9405/10000 [===========================>..] - ETA: 4:41 - loss: 1.7774 - regression_loss: 1.3563 - classification_loss: 9406/10000 [===========================>..] - ETA: 4:40 - loss: 1.7774 - regression_loss: 1.3562 - classification_loss: 9407/10000 [===========================>..] - ETA: 4:40 - loss: 1.7774 - regression_loss: 1.3562 - classification_loss: 9408/10000 [===========================>..] - ETA: 4:39 - loss: 1.7773 - regression_loss: 1.3561 - classification_loss: 9409/10000 [===========================>..] - ETA: 4:39 - loss: 1.7774 - regression_loss: 1.3562 - classification_loss: 9410/10000 [===========================>..] - ETA: 4:39 - loss: 1.7774 - regression_loss: 1.3562 - classification_loss: 9411/10000 [===========================>..] - ETA: 4:38 - loss: 1.7775 - regression_loss: 1.3563 - classification_loss: 9412/10000 [===========================>..] - ETA: 4:38 - loss: 1.7773 - regression_loss: 1.3562 - classification_loss: 9413/10000 [===========================>..] - ETA: 4:37 - loss: 1.7773 - regression_loss: 1.3562 - classification_loss: 9414/10000 [===========================>..] - ETA: 4:37 - loss: 1.7774 - regression_loss: 1.3562 - classification_loss: 9415/10000 [===========================>..] - ETA: 4:36 - loss: 1.7774 - regression_loss: 1.3562 - classification_loss: 9416/10000 [===========================>..] - ETA: 4:36 - loss: 1.7774 - regression_loss: 1.3563 - classification_loss: 9417/10000 [===========================>..] - ETA: 4:35 - loss: 1.7773 - regression_loss: 1.3562 - classification_loss: 9418/10000 [===========================>..] - ETA: 4:35 - loss: 1.7773 - regression_loss: 1.3562 - classification_loss: 9419/10000 [===========================>..] - ETA: 4:34 - loss: 1.7773 - regression_loss: 1.3563 - classification_loss: 9420/10000 [===========================>..] - ETA: 4:34 - loss: 1.7773 - regression_loss: 1.3563 - classification_loss: 9421/10000 [===========================>..] - ETA: 4:33 - loss: 1.7772 - regression_loss: 1.3562 - classification_loss: 9422/10000 [===========================>..] - ETA: 4:33 - loss: 1.7772 - regression_loss: 1.3562 - classification_loss: 9423/10000 [===========================>..] - ETA: 4:32 - loss: 1.7773 - regression_loss: 1.3563 - classification_loss: 9424/10000 [===========================>..] - ETA: 4:32 - loss: 1.7773 - regression_loss: 1.3562 - classification_loss: 9425/10000 [===========================>..] - ETA: 4:31 - loss: 1.7773 - regression_loss: 1.3563 - classification_loss: 9426/10000 [===========================>..] - ETA: 4:31 - loss: 1.7772 - regression_loss: 1.3562 - classification_loss: 9427/10000 [===========================>..] - ETA: 4:30 - loss: 1.7773 - regression_loss: 1.3563 - classification_loss: 9428/10000 [===========================>..] - ETA: 4:30 - loss: 1.7772 - regression_loss: 1.3562 - classification_loss: 9429/10000 [===========================>..] - ETA: 4:30 - loss: 1.7772 - regression_loss: 1.3562 - classification_loss: 9430/10000 [===========================>..] - ETA: 4:29 - loss: 1.7772 - regression_loss: 1.3562 - classification_loss: 9431/10000 [===========================>..] - ETA: 4:29 - loss: 1.7773 - regression_loss: 1.3563 - classification_loss: 9432/10000 [===========================>..] - ETA: 4:28 - loss: 1.7771 - regression_loss: 1.3562 - classification_loss: 9433/10000 [===========================>..] - ETA: 4:28 - loss: 1.7770 - regression_loss: 1.3561 - classification_loss: 9434/10000 [===========================>..] - ETA: 4:27 - loss: 1.7771 - regression_loss: 1.3562 - classification_loss: 9435/10000 [===========================>..] - ETA: 4:27 - loss: 1.7771 - regression_loss: 1.3562 - classification_loss: 9436/10000 [===========================>..] - ETA: 4:26 - loss: 1.7771 - regression_loss: 1.3561 - classification_loss: 9437/10000 [===========================>..] - ETA: 4:26 - loss: 1.7770 - regression_loss: 1.3561 - classification_loss: 9438/10000 [===========================>..] - ETA: 4:25 - loss: 1.7770 - regression_loss: 1.3561 - classification_loss: 9439/10000 [===========================>..] - ETA: 4:25 - loss: 1.7770 - regression_loss: 1.3561 - classification_loss: 9440/10000 [===========================>..] - ETA: 4:24 - loss: 1.7771 - regression_loss: 1.3562 - classification_loss: 9441/10000 [===========================>..] - ETA: 4:24 - loss: 1.7771 - regression_loss: 1.3562 - classification_loss: 9442/10000 [===========================>..] - ETA: 4:23 - loss: 1.7770 - regression_loss: 1.3561 - classification_loss: 9443/10000 [===========================>..] - ETA: 4:23 - loss: 1.7770 - regression_loss: 1.3561 - classification_loss: 9444/10000 [===========================>..] - ETA: 4:22 - loss: 1.7769 - regression_loss: 1.3560 - classification_loss: 9445/10000 [===========================>..] - ETA: 4:22 - loss: 1.7769 - regression_loss: 1.3560 - classification_loss: 9446/10000 [===========================>..] - ETA: 4:21 - loss: 1.7768 - regression_loss: 1.3559 - classification_loss: 9447/10000 [===========================>..] - ETA: 4:21 - loss: 1.7769 - regression_loss: 1.3560 - classification_loss: 9448/10000 [===========================>..] - ETA: 4:21 - loss: 1.7768 - regression_loss: 1.3559 - classification_loss: 9449/10000 [===========================>..] - ETA: 4:20 - loss: 1.7767 - regression_loss: 1.3558 - classification_loss: 9450/10000 [===========================>..] - ETA: 4:20 - loss: 1.7767 - regression_loss: 1.3558 - classification_loss: 9451/10000 [===========================>..] - ETA: 4:19 - loss: 1.7767 - regression_loss: 1.3558 - classification_loss: 9452/10000 [===========================>..] - ETA: 4:19 - loss: 1.7766 - regression_loss: 1.3557 - classification_loss: 9453/10000 [===========================>..] - ETA: 4:18 - loss: 1.7764 - regression_loss: 1.3556 - classification_loss: 9454/10000 [===========================>..] - ETA: 4:18 - loss: 1.7764 - regression_loss: 1.3556 - classification_loss: 9455/10000 [===========================>..] - ETA: 4:17 - loss: 1.7764 - regression_loss: 1.3556 - classification_loss: 9456/10000 [===========================>..] - ETA: 4:17 - loss: 1.7764 - regression_loss: 1.3556 - classification_loss: 9457/10000 [===========================>..] - ETA: 4:16 - loss: 1.7764 - regression_loss: 1.3556 - classification_loss: 9458/10000 [===========================>..] - ETA: 4:16 - loss: 1.7764 - regression_loss: 1.3556 - classification_loss: 9459/10000 [===========================>..] - ETA: 4:15 - loss: 1.7762 - regression_loss: 1.3555 - classification_loss: 9460/10000 [===========================>..] - ETA: 4:15 - loss: 1.7762 - regression_loss: 1.3555 - classification_loss: 9461/10000 [===========================>..] - ETA: 4:14 - loss: 1.7761 - regression_loss: 1.3554 - classification_loss: 9462/10000 [===========================>..] - ETA: 4:14 - loss: 1.7760 - regression_loss: 1.3553 - classification_loss: 9463/10000 [===========================>..] - ETA: 4:13 - loss: 1.7762 - regression_loss: 1.3555 - classification_loss: 9464/10000 [===========================>..] - ETA: 4:13 - loss: 1.7760 - regression_loss: 1.3553 - classification_loss: 9465/10000 [===========================>..] - ETA: 4:12 - loss: 1.7760 - regression_loss: 1.3553 - classification_loss: 9466/10000 [===========================>..] - ETA: 4:12 - loss: 1.7759 - regression_loss: 1.3552 - classification_loss: 9467/10000 [===========================>..] - ETA: 4:12 - loss: 1.7759 - regression_loss: 1.3552 - classification_loss: 9468/10000 [===========================>..] - ETA: 4:11 - loss: 1.7760 - regression_loss: 1.3553 - classification_loss: 9469/10000 [===========================>..] - ETA: 4:11 - loss: 1.7760 - regression_loss: 1.3553 - classification_loss: 9470/10000 [===========================>..] - ETA: 4:10 - loss: 1.7759 - regression_loss: 1.3552 - classification_loss: 9471/10000 [===========================>..] - ETA: 4:10 - loss: 1.7759 - regression_loss: 1.3552 - classification_loss: 9472/10000 [===========================>..] - ETA: 4:09 - loss: 1.7759 - regression_loss: 1.3552 - classification_loss: 9473/10000 [===========================>..] - ETA: 4:09 - loss: 1.7759 - regression_loss: 1.3552 - classification_loss: 9474/10000 [===========================>..] - ETA: 4:08 - loss: 1.7758 - regression_loss: 1.3551 - classification_loss: 9475/10000 [===========================>..] - ETA: 4:08 - loss: 1.7758 - regression_loss: 1.3550 - classification_loss: 9476/10000 [===========================>..] - ETA: 4:07 - loss: 1.7758 - regression_loss: 1.3551 - classification_loss: 9477/10000 [===========================>..] - ETA: 4:07 - loss: 1.7758 - regression_loss: 1.3551 - classification_loss: 9478/10000 [===========================>..] - ETA: 4:06 - loss: 1.7758 - regression_loss: 1.3551 - classification_loss: 9479/10000 [===========================>..] - ETA: 4:06 - loss: 1.7757 - regression_loss: 1.3550 - classification_loss: 9480/10000 [===========================>..] - ETA: 4:05 - loss: 1.7758 - regression_loss: 1.3551 - classification_loss: 9481/10000 [===========================>..] - ETA: 4:05 - loss: 1.7757 - regression_loss: 1.3550 - classification_loss: 9482/10000 [===========================>..] - ETA: 4:04 - loss: 1.7756 - regression_loss: 1.3550 - classification_loss: 9483/10000 [===========================>..] - ETA: 4:04 - loss: 1.7757 - regression_loss: 1.3550 - classification_loss: 9484/10000 [===========================>..] - ETA: 4:03 - loss: 1.7756 - regression_loss: 1.3549 - classification_loss: 9485/10000 [===========================>..] - ETA: 4:03 - loss: 1.7756 - regression_loss: 1.3549 - classification_loss: 9486/10000 [===========================>..] - ETA: 4:03 - loss: 1.7756 - regression_loss: 1.3549 - classification_loss: 9487/10000 [===========================>..] - ETA: 4:02 - loss: 1.7756 - regression_loss: 1.3550 - classification_loss: 9488/10000 [===========================>..] - ETA: 4:02 - loss: 1.7757 - regression_loss: 1.3550 - classification_loss: 9489/10000 [===========================>..] - ETA: 4:01 - loss: 1.7756 - regression_loss: 1.3549 - classification_loss: 9490/10000 [===========================>..] - ETA: 4:01 - loss: 1.7756 - regression_loss: 1.3549 - classification_loss: 9491/10000 [===========================>..] - ETA: 4:00 - loss: 1.7755 - regression_loss: 1.3548 - classification_loss: 9492/10000 [===========================>..] - ETA: 4:00 - loss: 1.7755 - regression_loss: 1.3549 - classification_loss: 9493/10000 [===========================>..] - ETA: 3:59 - loss: 1.7754 - regression_loss: 1.3548 - classification_loss: 9494/10000 [===========================>..] - ETA: 3:59 - loss: 1.7754 - regression_loss: 1.3548 - classification_loss: 9495/10000 [===========================>..] - ETA: 3:58 - loss: 1.7754 - regression_loss: 1.3548 - classification_loss: 9496/10000 [===========================>..] - ETA: 3:58 - loss: 1.7753 - regression_loss: 1.3547 - classification_loss: 9497/10000 [===========================>..] - ETA: 3:57 - loss: 1.7754 - regression_loss: 1.3548 - classification_loss: 9498/10000 [===========================>..] - ETA: 3:57 - loss: 1.7753 - regression_loss: 1.3548 - classification_loss: 9499/10000 [===========================>..] - ETA: 3:56 - loss: 1.7754 - regression_loss: 1.3548 - classification_loss: 9500/10000 [===========================>..] - ETA: 3:56 - loss: 1.7754 - regression_loss: 1.3548 - classification_loss: 9501/10000 [===========================>..] - ETA: 3:55 - loss: 1.7754 - regression_loss: 1.3548 - classification_loss: 9502/10000 [===========================>..] - ETA: 3:55 - loss: 1.7755 - regression_loss: 1.3549 - classification_loss: 9503/10000 [===========================>..] - ETA: 3:54 - loss: 1.7755 - regression_loss: 1.3549 - classification_loss: 9504/10000 [===========================>..] - ETA: 3:54 - loss: 1.7754 - regression_loss: 1.3548 - classification_loss: 9505/10000 [===========================>..] - ETA: 3:54 - loss: 1.7754 - regression_loss: 1.3548 - classification_loss: 9506/10000 [===========================>..] - ETA: 3:53 - loss: 1.7754 - regression_loss: 1.3548 - classification_loss: 9507/10000 [===========================>..] - ETA: 3:53 - loss: 1.7754 - regression_loss: 1.3548 - classification_loss: 9508/10000 [===========================>..] - ETA: 3:52 - loss: 1.7753 - regression_loss: 1.3548 - classification_loss: 9509/10000 [===========================>..] - ETA: 3:52 - loss: 1.7754 - regression_loss: 1.3549 - classification_loss: 9510/10000 [===========================>..] - ETA: 3:51 - loss: 1.7754 - regression_loss: 1.3548 - classification_loss: 9511/10000 [===========================>..] - ETA: 3:51 - loss: 1.7753 - regression_loss: 1.3548 - classification_loss: 9512/10000 [===========================>..] - ETA: 3:50 - loss: 1.7754 - regression_loss: 1.3548 - classification_loss: 9513/10000 [===========================>..] - ETA: 3:50 - loss: 1.7754 - regression_loss: 1.3548 - classification_loss: 9514/10000 [===========================>..] - ETA: 3:49 - loss: 1.7754 - regression_loss: 1.3548 - classification_loss: 9515/10000 [===========================>..] - ETA: 3:49 - loss: 1.7754 - regression_loss: 1.3548 - classification_loss: 9516/10000 [===========================>..] - ETA: 3:48 - loss: 1.7753 - regression_loss: 1.3547 - classification_loss: 9517/10000 [===========================>..] - ETA: 3:48 - loss: 1.7752 - regression_loss: 1.3547 - classification_loss: 9518/10000 [===========================>..] - ETA: 3:47 - loss: 1.7752 - regression_loss: 1.3547 - classification_loss: 9519/10000 [===========================>..] - ETA: 3:47 - loss: 1.7752 - regression_loss: 1.3546 - classification_loss: 9520/10000 [===========================>..] - ETA: 3:46 - loss: 1.7751 - regression_loss: 1.3546 - classification_loss: 9521/10000 [===========================>..] - ETA: 3:46 - loss: 1.7751 - regression_loss: 1.3546 - classification_loss: 9522/10000 [===========================>..] - ETA: 3:45 - loss: 1.7751 - regression_loss: 1.3546 - classification_loss: 9523/10000 [===========================>..] - ETA: 3:45 - loss: 1.7750 - regression_loss: 1.3545 - classification_loss: 9524/10000 [===========================>..] - ETA: 3:45 - loss: 1.7750 - regression_loss: 1.3545 - classification_loss: 9525/10000 [===========================>..] - ETA: 3:44 - loss: 1.7750 - regression_loss: 1.3545 - classification_loss: 9526/10000 [===========================>..] - ETA: 3:44 - loss: 1.7751 - regression_loss: 1.3546 - classification_loss: 9527/10000 [===========================>..] - ETA: 3:43 - loss: 1.7750 - regression_loss: 1.3545 - classification_loss: 9528/10000 [===========================>..] - ETA: 3:43 - loss: 1.7749 - regression_loss: 1.3545 - classification_loss: 9529/10000 [===========================>..] - ETA: 3:42 - loss: 1.7749 - regression_loss: 1.3544 - classification_loss: 9530/10000 [===========================>..] - ETA: 3:42 - loss: 1.7748 - regression_loss: 1.3544 - classification_loss: 9531/10000 [===========================>..] - ETA: 3:41 - loss: 1.7747 - regression_loss: 1.3543 - classification_loss: 9532/10000 [===========================>..] - ETA: 3:41 - loss: 1.7747 - regression_loss: 1.3543 - classification_loss: 9533/10000 [===========================>..] - ETA: 3:40 - loss: 1.7747 - regression_loss: 1.3543 - classification_loss: 9534/10000 [===========================>..] - ETA: 3:40 - loss: 1.7747 - regression_loss: 1.3543 - classification_loss: 9535/10000 [===========================>..] - ETA: 3:39 - loss: 1.7747 - regression_loss: 1.3543 - classification_loss: 9536/10000 [===========================>..] - ETA: 3:39 - loss: 1.7746 - regression_loss: 1.3542 - classification_loss: 9537/10000 [===========================>..] - ETA: 3:38 - loss: 1.7746 - regression_loss: 1.3542 - classification_loss: 9538/10000 [===========================>..] - ETA: 3:38 - loss: 1.7747 - regression_loss: 1.3543 - classification_loss: 9539/10000 [===========================>..] - ETA: 3:37 - loss: 1.7747 - regression_loss: 1.3543 - classification_loss: 9540/10000 [===========================>..] - ETA: 3:37 - loss: 1.7747 - regression_loss: 1.3543 - classification_loss: 9541/10000 [===========================>..] - ETA: 3:37 - loss: 1.7747 - regression_loss: 1.3542 - classification_loss: 9542/10000 [===========================>..] - ETA: 3:36 - loss: 1.7747 - regression_loss: 1.3543 - classification_loss: 9543/10000 [===========================>..] - ETA: 3:36 - loss: 1.7747 - regression_loss: 1.3542 - classification_loss: 9544/10000 [===========================>..] - ETA: 3:35 - loss: 1.7746 - regression_loss: 1.3541 - classification_loss: 9545/10000 [===========================>..] - ETA: 3:35 - loss: 1.7746 - regression_loss: 1.3542 - classification_loss: 9546/10000 [===========================>..] - ETA: 3:34 - loss: 1.7746 - regression_loss: 1.3542 - classification_loss: 9547/10000 [===========================>..] - ETA: 3:34 - loss: 1.7746 - regression_loss: 1.3542 - classification_loss: 9548/10000 [===========================>..] - ETA: 3:33 - loss: 1.7746 - regression_loss: 1.3542 - classification_loss: 9549/10000 [===========================>..] - ETA: 3:33 - loss: 1.7746 - regression_loss: 1.3542 - classification_loss: 9550/10000 [===========================>..] - ETA: 3:32 - loss: 1.7746 - regression_loss: 1.3542 - classification_loss: 9551/10000 [===========================>..] - ETA: 3:32 - loss: 1.7746 - regression_loss: 1.3541 - classification_loss: 9552/10000 [===========================>..] - ETA: 3:31 - loss: 1.7746 - regression_loss: 1.3541 - classification_loss: 9553/10000 [===========================>..] - ETA: 3:31 - loss: 1.7745 - regression_loss: 1.3540 - classification_loss: 9554/10000 [===========================>..] - ETA: 3:30 - loss: 1.7746 - regression_loss: 1.3539 - classification_loss: 9555/10000 [===========================>..] - ETA: 3:30 - loss: 1.7746 - regression_loss: 1.3539 - classification_loss: 9556/10000 [===========================>..] - ETA: 3:29 - loss: 1.7745 - regression_loss: 1.3538 - classification_loss: 9557/10000 [===========================>..] - ETA: 3:29 - loss: 1.7745 - regression_loss: 1.3538 - classification_loss: 9558/10000 [===========================>..] - ETA: 3:28 - loss: 1.7744 - regression_loss: 1.3538 - classification_loss: 9559/10000 [===========================>..] - ETA: 3:28 - loss: 1.7743 - regression_loss: 1.3537 - classification_loss: 9560/10000 [===========================>..] - ETA: 3:28 - loss: 1.7744 - regression_loss: 1.3538 - classification_loss: 9561/10000 [===========================>..] - ETA: 3:27 - loss: 1.7744 - regression_loss: 1.3538 - classification_loss: 9562/10000 [===========================>..] - ETA: 3:27 - loss: 1.7744 - regression_loss: 1.3538 - classification_loss: 9563/10000 [===========================>..] - ETA: 3:26 - loss: 1.7743 - regression_loss: 1.3537 - classification_loss: 9564/10000 [===========================>..] - ETA: 3:26 - loss: 1.7743 - regression_loss: 1.3537 - classification_loss: 9565/10000 [===========================>..] - ETA: 3:25 - loss: 1.7743 - regression_loss: 1.3537 - classification_loss: 9566/10000 [===========================>..] - ETA: 3:25 - loss: 1.7746 - regression_loss: 1.3539 - classification_loss: 9567/10000 [===========================>..] - ETA: 3:24 - loss: 1.7746 - regression_loss: 1.3539 - classification_loss: 9568/10000 [===========================>..] - ETA: 3:24 - loss: 1.7746 - regression_loss: 1.3539 - classification_loss: 9569/10000 [===========================>..] - ETA: 3:23 - loss: 1.7746 - regression_loss: 1.3539 - classification_loss: 9570/10000 [===========================>..] - ETA: 3:23 - loss: 1.7746 - regression_loss: 1.3539 - classification_loss: 9571/10000 [===========================>..] - ETA: 3:22 - loss: 1.7746 - regression_loss: 1.3539 - classification_loss: 9572/10000 [===========================>..] - ETA: 3:22 - loss: 1.7746 - regression_loss: 1.3539 - classification_loss: 9573/10000 [===========================>..] - ETA: 3:21 - loss: 1.7746 - regression_loss: 1.3539 - classification_loss: 9574/10000 [===========================>..] - ETA: 3:21 - loss: 1.7746 - regression_loss: 1.3538 - classification_loss: 9575/10000 [===========================>..] - ETA: 3:20 - loss: 1.7745 - regression_loss: 1.3538 - classification_loss: 9576/10000 [===========================>..] - ETA: 3:20 - loss: 1.7744 - regression_loss: 1.3537 - classification_loss: 9577/10000 [===========================>..] - ETA: 3:19 - loss: 1.7745 - regression_loss: 1.3538 - classification_loss: 9578/10000 [===========================>..] - ETA: 3:19 - loss: 1.7744 - regression_loss: 1.3537 - classification_loss: 9579/10000 [===========================>..] - ETA: 3:19 - loss: 1.7745 - regression_loss: 1.3538 - classification_loss: 9580/10000 [===========================>..] - ETA: 3:18 - loss: 1.7745 - regression_loss: 1.3537 - classification_loss: 9581/10000 [===========================>..] - ETA: 3:18 - loss: 1.7744 - regression_loss: 1.3537 - classification_loss: 9582/10000 [===========================>..] - ETA: 3:17 - loss: 1.7745 - regression_loss: 1.3538 - classification_loss: 9583/10000 [===========================>..] - ETA: 3:17 - loss: 1.7744 - regression_loss: 1.3537 - classification_loss: 9584/10000 [===========================>..] - ETA: 3:16 - loss: 1.7744 - regression_loss: 1.3537 - classification_loss: 9585/10000 [===========================>..] - ETA: 3:16 - loss: 1.7744 - regression_loss: 1.3537 - classification_loss: 9586/10000 [===========================>..] - ETA: 3:15 - loss: 1.7744 - regression_loss: 1.3537 - classification_loss: 9587/10000 [===========================>..] - ETA: 3:15 - loss: 1.7744 - regression_loss: 1.3537 - classification_loss: 9588/10000 [===========================>..] - ETA: 3:14 - loss: 1.7743 - regression_loss: 1.3536 - classification_loss: 9589/10000 [===========================>..] - ETA: 3:14 - loss: 1.7743 - regression_loss: 1.3536 - classification_loss: 9590/10000 [===========================>..] - ETA: 3:13 - loss: 1.7743 - regression_loss: 1.3536 - classification_loss: 9591/10000 [===========================>..] - ETA: 3:13 - loss: 1.7744 - regression_loss: 1.3536 - classification_loss: 9592/10000 [===========================>..] - ETA: 3:12 - loss: 1.7744 - regression_loss: 1.3537 - classification_loss: 9593/10000 [===========================>..] - ETA: 3:12 - loss: 1.7743 - regression_loss: 1.3536 - classification_loss: 9594/10000 [===========================>..] - ETA: 3:11 - loss: 1.7743 - regression_loss: 1.3536 - classification_loss: 9595/10000 [===========================>..] - ETA: 3:11 - loss: 1.7743 - regression_loss: 1.3536 - classification_loss: 9596/10000 [===========================>..] - ETA: 3:10 - loss: 1.7743 - regression_loss: 1.3536 - classification_loss: 9597/10000 [===========================>..] - ETA: 3:10 - loss: 1.7743 - regression_loss: 1.3536 - classification_loss: 9598/10000 [===========================>..] - ETA: 3:10 - loss: 1.7743 - regression_loss: 1.3536 - classification_loss: 9599/10000 [===========================>..] - ETA: 3:09 - loss: 1.7742 - regression_loss: 1.3535 - classification_loss: 9600/10000 [===========================>..] - ETA: 3:09 - loss: 1.7742 - regression_loss: 1.3535 - classification_loss: 9601/10000 [===========================>..] - ETA: 3:08 - loss: 1.7741 - regression_loss: 1.3534 - classification_loss: 9602/10000 [===========================>..] - ETA: 3:08 - loss: 1.7741 - regression_loss: 1.3534 - classification_loss: 9603/10000 [===========================>..] - ETA: 3:07 - loss: 1.7742 - regression_loss: 1.3535 - classification_loss: 9604/10000 [===========================>..] - ETA: 3:07 - loss: 1.7742 - regression_loss: 1.3535 - classification_loss: 9605/10000 [===========================>..] - ETA: 3:06 - loss: 1.7741 - regression_loss: 1.3535 - classification_loss: 9606/10000 [===========================>..] - ETA: 3:06 - loss: 1.7742 - regression_loss: 1.3535 - classification_loss: 9607/10000 [===========================>..] - ETA: 3:05 - loss: 1.7742 - regression_loss: 1.3535 - classification_loss: 9608/10000 [===========================>..] - ETA: 3:05 - loss: 1.7743 - regression_loss: 1.3536 - classification_loss: 9609/10000 [===========================>..] - ETA: 3:04 - loss: 1.7743 - regression_loss: 1.3536 - classification_loss: 9610/10000 [===========================>..] - ETA: 3:04 - loss: 1.7743 - regression_loss: 1.3536 - classification_loss: 9611/10000 [===========================>..] - ETA: 3:03 - loss: 1.7743 - regression_loss: 1.3536 - classification_loss: 9612/10000 [===========================>..] - ETA: 3:03 - loss: 1.7743 - regression_loss: 1.3536 - classification_loss: 9613/10000 [===========================>..] - ETA: 3:02 - loss: 1.7742 - regression_loss: 1.3536 - classification_loss: 9614/10000 [===========================>..] - ETA: 3:02 - loss: 1.7742 - regression_loss: 1.3536 - classification_loss: 9615/10000 [===========================>..] - ETA: 3:02 - loss: 1.7743 - regression_loss: 1.3536 - classification_loss: 9616/10000 [===========================>..] - ETA: 3:01 - loss: 1.7743 - regression_loss: 1.3536 - classification_loss: 9617/10000 [===========================>..] - ETA: 3:01 - loss: 1.7744 - regression_loss: 1.3537 - classification_loss: 9618/10000 [===========================>..] - ETA: 3:00 - loss: 1.7744 - regression_loss: 1.3537 - classification_loss: 9619/10000 [===========================>..] - ETA: 3:00 - loss: 1.7744 - regression_loss: 1.3537 - classification_loss: 9620/10000 [===========================>..] - ETA: 2:59 - loss: 1.7743 - regression_loss: 1.3536 - classification_loss: 9621/10000 [===========================>..] - ETA: 2:59 - loss: 1.7743 - regression_loss: 1.3536 - classification_loss: 9622/10000 [===========================>..] - ETA: 2:58 - loss: 1.7743 - regression_loss: 1.3536 - classification_loss: 9623/10000 [===========================>..] - ETA: 2:58 - loss: 1.7743 - regression_loss: 1.3536 - classification_loss: 9624/10000 [===========================>..] - ETA: 2:57 - loss: 1.7742 - regression_loss: 1.3535 - classification_loss: 9625/10000 [===========================>..] - ETA: 2:57 - loss: 1.7741 - regression_loss: 1.3535 - classification_loss: 9626/10000 [===========================>..] - ETA: 2:56 - loss: 1.7742 - regression_loss: 1.3535 - classification_loss: 9627/10000 [===========================>..] - ETA: 2:56 - loss: 1.7742 - regression_loss: 1.3536 - classification_loss: 9628/10000 [===========================>..] - ETA: 2:55 - loss: 1.7741 - regression_loss: 1.3535 - classification_loss: 9629/10000 [===========================>..] - ETA: 2:55 - loss: 1.7741 - regression_loss: 1.3535 - classification_loss: 9630/10000 [===========================>..] - ETA: 2:54 - loss: 1.7742 - regression_loss: 1.3535 - classification_loss: 9631/10000 [===========================>..] - ETA: 2:54 - loss: 1.7740 - regression_loss: 1.3534 - classification_loss: 9632/10000 [===========================>..] - ETA: 2:53 - loss: 1.7740 - regression_loss: 1.3533 - classification_loss: 9633/10000 [===========================>..] - ETA: 2:53 - loss: 1.7740 - regression_loss: 1.3534 - classification_loss: 9634/10000 [===========================>..] - ETA: 2:53 - loss: 1.7740 - regression_loss: 1.3534 - classification_loss: 9635/10000 [===========================>..] - ETA: 2:52 - loss: 1.7740 - regression_loss: 1.3534 - classification_loss: 9636/10000 [===========================>..] - ETA: 2:52 - loss: 1.7739 - regression_loss: 1.3533 - classification_loss: 9637/10000 [===========================>..] - ETA: 2:51 - loss: 1.7739 - regression_loss: 1.3533 - classification_loss: 9638/10000 [===========================>..] - ETA: 2:51 - loss: 1.7739 - regression_loss: 1.3533 - classification_loss: 9639/10000 [===========================>..] - ETA: 2:50 - loss: 1.7740 - regression_loss: 1.3534 - classification_loss: 9640/10000 [===========================>..] - ETA: 2:50 - loss: 1.7739 - regression_loss: 1.3533 - classification_loss: 9641/10000 [===========================>..] - ETA: 2:49 - loss: 1.7739 - regression_loss: 1.3534 - classification_loss: 9642/10000 [===========================>..] - ETA: 2:49 - loss: 1.7739 - regression_loss: 1.3534 - classification_loss: 9643/10000 [===========================>..] - ETA: 2:48 - loss: 1.7738 - regression_loss: 1.3533 - classification_loss: 9644/10000 [===========================>..] - ETA: 2:48 - loss: 1.7739 - regression_loss: 1.3533 - classification_loss: 9645/10000 [===========================>..] - ETA: 2:47 - loss: 1.7739 - regression_loss: 1.3533 - classification_loss: 9646/10000 [===========================>..] - ETA: 2:47 - loss: 1.7738 - regression_loss: 1.3533 - classification_loss: 9647/10000 [===========================>..] - ETA: 2:46 - loss: 1.7737 - regression_loss: 1.3532 - classification_loss: 9648/10000 [===========================>..] - ETA: 2:46 - loss: 1.7737 - regression_loss: 1.3532 - classification_loss: 9649/10000 [===========================>..] - ETA: 2:45 - loss: 1.7737 - regression_loss: 1.3532 - classification_loss: 9650/10000 [===========================>..] - ETA: 2:45 - loss: 1.7737 - regression_loss: 1.3532 - classification_loss: 9651/10000 [===========================>..] - ETA: 2:44 - loss: 1.7736 - regression_loss: 1.3532 - classification_loss: 9652/10000 [===========================>..] - ETA: 2:44 - loss: 1.7737 - regression_loss: 1.3532 - classification_loss: 9653/10000 [===========================>..] - ETA: 2:44 - loss: 1.7736 - regression_loss: 1.3531 - classification_loss: 9654/10000 [===========================>..] - ETA: 2:43 - loss: 1.7737 - regression_loss: 1.3532 - classification_loss: 9655/10000 [===========================>..] - ETA: 2:43 - loss: 1.7737 - regression_loss: 1.3532 - classification_loss: 9656/10000 [===========================>..] - ETA: 2:42 - loss: 1.7736 - regression_loss: 1.3531 - classification_loss: 9657/10000 [===========================>..] - ETA: 2:42 - loss: 1.7736 - regression_loss: 1.3531 - classification_loss: 9658/10000 [===========================>..] - ETA: 2:41 - loss: 1.7736 - regression_loss: 1.3531 - classification_loss: 9659/10000 [===========================>..] - ETA: 2:41 - loss: 1.7737 - regression_loss: 1.3532 - classification_loss: 9660/10000 [===========================>..] - ETA: 2:40 - loss: 1.7736 - regression_loss: 1.3532 - classification_loss: 9661/10000 [===========================>..] - ETA: 2:40 - loss: 1.7736 - regression_loss: 1.3531 - classification_loss: 9662/10000 [===========================>..] - ETA: 2:39 - loss: 1.7735 - regression_loss: 1.3531 - classification_loss: 9663/10000 [===========================>..] - ETA: 2:39 - loss: 1.7734 - regression_loss: 1.3530 - classification_loss: 9664/10000 [===========================>..] - ETA: 2:38 - loss: 1.7734 - regression_loss: 1.3530 - classification_loss: 9665/10000 [===========================>..] - ETA: 2:38 - loss: 1.7734 - regression_loss: 1.3530 - classification_loss: 9666/10000 [===========================>..] - ETA: 2:37 - loss: 1.7733 - regression_loss: 1.3529 - classification_loss: 9667/10000 [============================>.] - ETA: 2:37 - loss: 1.7733 - regression_loss: 1.3529 - classification_loss: 9668/10000 [============================>.] - ETA: 2:36 - loss: 1.7733 - regression_loss: 1.3529 - classification_loss: 9669/10000 [============================>.] - ETA: 2:36 - loss: 1.7733 - regression_loss: 1.3529 - classification_loss: 9670/10000 [============================>.] - ETA: 2:35 - loss: 1.7733 - regression_loss: 1.3529 - classification_loss: 9671/10000 [============================>.] - ETA: 2:35 - loss: 1.7733 - regression_loss: 1.3529 - classification_loss: 9672/10000 [============================>.] - ETA: 2:35 - loss: 1.7733 - regression_loss: 1.3529 - classification_loss: 9673/10000 [============================>.] - ETA: 2:34 - loss: 1.7732 - regression_loss: 1.3528 - classification_loss: 9674/10000 [============================>.] - ETA: 2:34 - loss: 1.7731 - regression_loss: 1.3527 - classification_loss: 9675/10000 [============================>.] - ETA: 2:33 - loss: 1.7730 - regression_loss: 1.3527 - classification_loss: 9676/10000 [============================>.] - ETA: 2:33 - loss: 1.7730 - regression_loss: 1.3527 - classification_loss: 9677/10000 [============================>.] - ETA: 2:32 - loss: 1.7731 - regression_loss: 1.3528 - classification_loss: 9678/10000 [============================>.] - ETA: 2:32 - loss: 1.7732 - regression_loss: 1.3528 - classification_loss: 9679/10000 [============================>.] - ETA: 2:31 - loss: 1.7732 - regression_loss: 1.3528 - classification_loss: 9680/10000 [============================>.] - ETA: 2:31 - loss: 1.7731 - regression_loss: 1.3528 - classification_loss: 9681/10000 [============================>.] - ETA: 2:30 - loss: 1.7731 - regression_loss: 1.3528 - classification_loss: 9682/10000 [============================>.] - ETA: 2:30 - loss: 1.7731 - regression_loss: 1.3528 - classification_loss: 9683/10000 [============================>.] - ETA: 2:29 - loss: 1.7732 - regression_loss: 1.3529 - classification_loss: 9684/10000 [============================>.] - ETA: 2:29 - loss: 1.7733 - regression_loss: 1.3529 - classification_loss: 9685/10000 [============================>.] - ETA: 2:28 - loss: 1.7733 - regression_loss: 1.3530 - classification_loss: 9686/10000 [============================>.] - ETA: 2:28 - loss: 1.7734 - regression_loss: 1.3530 - classification_loss: 9687/10000 [============================>.] - ETA: 2:27 - loss: 1.7734 - regression_loss: 1.3530 - classification_loss: 9688/10000 [============================>.] - ETA: 2:27 - loss: 1.7733 - regression_loss: 1.3530 - classification_loss: 9689/10000 [============================>.] - ETA: 2:26 - loss: 1.7734 - regression_loss: 1.3530 - classification_loss: 9690/10000 [============================>.] - ETA: 2:26 - loss: 1.7734 - regression_loss: 1.3530 - classification_loss: 9691/10000 [============================>.] - ETA: 2:26 - loss: 1.7733 - regression_loss: 1.3529 - classification_loss: 9692/10000 [============================>.] - ETA: 2:25 - loss: 1.7733 - regression_loss: 1.3529 - classification_loss: 9693/10000 [============================>.] - ETA: 2:25 - loss: 1.7733 - regression_loss: 1.3530 - classification_loss: 9694/10000 [============================>.] - ETA: 2:24 - loss: 1.7733 - regression_loss: 1.3530 - classification_loss: 9695/10000 [============================>.] - ETA: 2:24 - loss: 1.7733 - regression_loss: 1.3529 - classification_loss: 9696/10000 [============================>.] - ETA: 2:23 - loss: 1.7732 - regression_loss: 1.3529 - classification_loss: 9697/10000 [============================>.] - ETA: 2:23 - loss: 1.7731 - regression_loss: 1.3528 - classification_loss: 9698/10000 [============================>.] - ETA: 2:22 - loss: 1.7731 - regression_loss: 1.3528 - classification_loss: 9699/10000 [============================>.] - ETA: 2:22 - loss: 1.7730 - regression_loss: 1.3528 - classification_loss: 9700/10000 [============================>.] - ETA: 2:21 - loss: 1.7730 - regression_loss: 1.3527 - classification_loss: 9701/10000 [============================>.] - ETA: 2:21 - loss: 1.7729 - regression_loss: 1.3527 - classification_loss: 9702/10000 [============================>.] - ETA: 2:20 - loss: 1.7729 - regression_loss: 1.3527 - classification_loss: 9703/10000 [============================>.] - ETA: 2:20 - loss: 1.7728 - regression_loss: 1.3526 - classification_loss: 9704/10000 [============================>.] - ETA: 2:19 - loss: 1.7728 - regression_loss: 1.3526 - classification_loss: 9705/10000 [============================>.] - ETA: 2:19 - loss: 1.7728 - regression_loss: 1.3526 - classification_loss: 9706/10000 [============================>.] - ETA: 2:18 - loss: 1.7729 - regression_loss: 1.3527 - classification_loss: 9707/10000 [============================>.] - ETA: 2:18 - loss: 1.7730 - regression_loss: 1.3527 - classification_loss: 9708/10000 [============================>.] - ETA: 2:17 - loss: 1.7729 - regression_loss: 1.3526 - classification_loss: 9709/10000 [============================>.] - ETA: 2:17 - loss: 1.7727 - regression_loss: 1.3525 - classification_loss: 9710/10000 [============================>.] - ETA: 2:17 - loss: 1.7727 - regression_loss: 1.3525 - classification_loss: 9711/10000 [============================>.] - ETA: 2:16 - loss: 1.7726 - regression_loss: 1.3524 - classification_loss: 9712/10000 [============================>.] - ETA: 2:16 - loss: 1.7726 - regression_loss: 1.3524 - classification_loss: 9713/10000 [============================>.] - ETA: 2:15 - loss: 1.7726 - regression_loss: 1.3524 - classification_loss: 9714/10000 [============================>.] - ETA: 2:15 - loss: 1.7726 - regression_loss: 1.3523 - classification_loss: 9715/10000 [============================>.] - ETA: 2:14 - loss: 1.7726 - regression_loss: 1.3523 - classification_loss: 9716/10000 [============================>.] - ETA: 2:14 - loss: 1.7725 - regression_loss: 1.3522 - classification_loss: 9717/10000 [============================>.] - ETA: 2:13 - loss: 1.7724 - regression_loss: 1.3521 - classification_loss: 9718/10000 [============================>.] - ETA: 2:13 - loss: 1.7725 - regression_loss: 1.3522 - classification_loss: 9719/10000 [============================>.] - ETA: 2:12 - loss: 1.7724 - regression_loss: 1.3521 - classification_loss: 9720/10000 [============================>.] - ETA: 2:12 - loss: 1.7724 - regression_loss: 1.3522 - classification_loss: 9721/10000 [============================>.] - ETA: 2:11 - loss: 1.7723 - regression_loss: 1.3521 - classification_loss: 9722/10000 [============================>.] - ETA: 2:11 - loss: 1.7722 - regression_loss: 1.3520 - classification_loss: 9723/10000 [============================>.] - ETA: 2:10 - loss: 1.7722 - regression_loss: 1.3519 - classification_loss: 9724/10000 [============================>.] - ETA: 2:10 - loss: 1.7722 - regression_loss: 1.3520 - classification_loss: 9725/10000 [============================>.] - ETA: 2:09 - loss: 1.7722 - regression_loss: 1.3519 - classification_loss: 9726/10000 [============================>.] - ETA: 2:09 - loss: 1.7722 - regression_loss: 1.3520 - classification_loss: 9727/10000 [============================>.] - ETA: 2:09 - loss: 1.7723 - regression_loss: 1.3520 - classification_loss: 9728/10000 [============================>.] - ETA: 2:08 - loss: 1.7722 - regression_loss: 1.3520 - classification_loss: 9729/10000 [============================>.] - ETA: 2:08 - loss: 1.7721 - regression_loss: 1.3519 - classification_loss: 9730/10000 [============================>.] - ETA: 2:07 - loss: 1.7720 - regression_loss: 1.3519 - classification_loss: 9731/10000 [============================>.] - ETA: 2:07 - loss: 1.7721 - regression_loss: 1.3519 - classification_loss: 9732/10000 [============================>.] - ETA: 2:06 - loss: 1.7721 - regression_loss: 1.3519 - classification_loss: 9733/10000 [============================>.] - ETA: 2:06 - loss: 1.7720 - regression_loss: 1.3519 - classification_loss: 9734/10000 [============================>.] - ETA: 2:05 - loss: 1.7721 - regression_loss: 1.3519 - classification_loss: 9735/10000 [============================>.] - ETA: 2:05 - loss: 1.7721 - regression_loss: 1.3520 - classification_loss: 9736/10000 [============================>.] - ETA: 2:04 - loss: 1.7720 - regression_loss: 1.3519 - classification_loss: 9737/10000 [============================>.] - ETA: 2:04 - loss: 1.7720 - regression_loss: 1.3519 - classification_loss: 9738/10000 [============================>.] - ETA: 2:03 - loss: 1.7721 - regression_loss: 1.3520 - classification_loss: 9739/10000 [============================>.] - ETA: 2:03 - loss: 1.7720 - regression_loss: 1.3520 - classification_loss: 9740/10000 [============================>.] - ETA: 2:02 - loss: 1.7720 - regression_loss: 1.3520 - classification_loss: 9741/10000 [============================>.] - ETA: 2:02 - loss: 1.7721 - regression_loss: 1.3521 - classification_loss: 9742/10000 [============================>.] - ETA: 2:01 - loss: 1.7720 - regression_loss: 1.3520 - classification_loss: 9743/10000 [============================>.] - ETA: 2:01 - loss: 1.7719 - regression_loss: 1.3519 - classification_loss: 9744/10000 [============================>.] - ETA: 2:00 - loss: 1.7719 - regression_loss: 1.3519 - classification_loss: 9745/10000 [============================>.] - ETA: 2:00 - loss: 1.7719 - regression_loss: 1.3518 - classification_loss: 9746/10000 [============================>.] - ETA: 2:00 - loss: 1.7719 - regression_loss: 1.3518 - classification_loss: 9747/10000 [============================>.] - ETA: 1:59 - loss: 1.7719 - regression_loss: 1.3519 - classification_loss: 9748/10000 [============================>.] - ETA: 1:59 - loss: 1.7720 - regression_loss: 1.3520 - classification_loss: 9749/10000 [============================>.] - ETA: 1:58 - loss: 1.7720 - regression_loss: 1.3520 - classification_loss: 9750/10000 [============================>.] - ETA: 1:58 - loss: 1.7720 - regression_loss: 1.3520 - classification_loss: 9751/10000 [============================>.] - ETA: 1:57 - loss: 1.7720 - regression_loss: 1.3520 - classification_loss: 9752/10000 [============================>.] - ETA: 1:57 - loss: 1.7720 - regression_loss: 1.3519 - classification_loss: 9753/10000 [============================>.] - ETA: 1:56 - loss: 1.7719 - regression_loss: 1.3518 - classification_loss: 9754/10000 [============================>.] - ETA: 1:56 - loss: 1.7719 - regression_loss: 1.3519 - classification_loss: 9755/10000 [============================>.] - ETA: 1:55 - loss: 1.7719 - regression_loss: 1.3519 - classification_loss: 9756/10000 [============================>.] - ETA: 1:55 - loss: 1.7720 - regression_loss: 1.3519 - classification_loss: 9757/10000 [============================>.] - ETA: 1:54 - loss: 1.7719 - regression_loss: 1.3519 - classification_loss: 9758/10000 [============================>.] - ETA: 1:54 - loss: 1.7719 - regression_loss: 1.3519 - classification_loss: 9759/10000 [============================>.] - ETA: 1:53 - loss: 1.7718 - regression_loss: 1.3518 - classification_loss: 9760/10000 [============================>.] - ETA: 1:53 - loss: 1.7718 - regression_loss: 1.3518 - classification_loss: 9761/10000 [============================>.] - ETA: 1:52 - loss: 1.7718 - regression_loss: 1.3518 - classification_loss: 9762/10000 [============================>.] - ETA: 1:52 - loss: 1.7718 - regression_loss: 1.3518 - classification_loss: 9763/10000 [============================>.] - ETA: 1:51 - loss: 1.7718 - regression_loss: 1.3518 - classification_loss: 9764/10000 [============================>.] - ETA: 1:51 - loss: 1.7717 - regression_loss: 1.3517 - classification_loss: 9765/10000 [============================>.] - ETA: 1:51 - loss: 1.7718 - regression_loss: 1.3518 - classification_loss: 9766/10000 [============================>.] - ETA: 1:50 - loss: 1.7718 - regression_loss: 1.3518 - classification_loss: 9767/10000 [============================>.] - ETA: 1:50 - loss: 1.7718 - regression_loss: 1.3518 - classification_loss: 9768/10000 [============================>.] - ETA: 1:49 - loss: 1.7718 - regression_loss: 1.3518 - classification_loss: 9769/10000 [============================>.] - ETA: 1:49 - loss: 1.7718 - regression_loss: 1.3518 - classification_loss: 9770/10000 [============================>.] - ETA: 1:48 - loss: 1.7719 - regression_loss: 1.3519 - classification_loss: 9771/10000 [============================>.] - ETA: 1:48 - loss: 1.7718 - regression_loss: 1.3518 - classification_loss: 9772/10000 [============================>.] - ETA: 1:47 - loss: 1.7719 - regression_loss: 1.3519 - classification_loss: 9773/10000 [============================>.] - ETA: 1:47 - loss: 1.7720 - regression_loss: 1.3520 - classification_loss: 9774/10000 [============================>.] - ETA: 1:46 - loss: 1.7720 - regression_loss: 1.3520 - classification_loss: 9775/10000 [============================>.] - ETA: 1:46 - loss: 1.7721 - regression_loss: 1.3521 - classification_loss: 9776/10000 [============================>.] - ETA: 1:45 - loss: 1.7721 - regression_loss: 1.3521 - classification_loss: 9777/10000 [============================>.] - ETA: 1:45 - loss: 1.7721 - regression_loss: 1.3521 - classification_loss: 9778/10000 [============================>.] - ETA: 1:44 - loss: 1.7721 - regression_loss: 1.3521 - classification_loss: 9779/10000 [============================>.] - ETA: 1:44 - loss: 1.7721 - regression_loss: 1.3521 - classification_loss: 9780/10000 [============================>.] - ETA: 1:43 - loss: 1.7720 - regression_loss: 1.3520 - classification_loss: 9781/10000 [============================>.] - ETA: 1:43 - loss: 1.7720 - regression_loss: 1.3520 - classification_loss: 9782/10000 [============================>.] - ETA: 1:42 - loss: 1.7720 - regression_loss: 1.3520 - classification_loss: 9783/10000 [============================>.] - ETA: 1:42 - loss: 1.7719 - regression_loss: 1.3519 - classification_loss: 9784/10000 [============================>.] - ETA: 1:42 - loss: 1.7719 - regression_loss: 1.3519 - classification_loss: 9785/10000 [============================>.] - ETA: 1:41 - loss: 1.7720 - regression_loss: 1.3520 - classification_loss: 9786/10000 [============================>.] - ETA: 1:41 - loss: 1.7719 - regression_loss: 1.3520 - classification_loss: 9787/10000 [============================>.] - ETA: 1:40 - loss: 1.7720 - regression_loss: 1.3520 - classification_loss: 9788/10000 [============================>.] - ETA: 1:40 - loss: 1.7719 - regression_loss: 1.3520 - classification_loss: 9789/10000 [============================>.] - ETA: 1:39 - loss: 1.7719 - regression_loss: 1.3520 - classification_loss: 9790/10000 [============================>.] - ETA: 1:39 - loss: 1.7719 - regression_loss: 1.3520 - classification_loss: 9791/10000 [============================>.] - ETA: 1:38 - loss: 1.7719 - regression_loss: 1.3520 - classification_loss: 9792/10000 [============================>.] - ETA: 1:38 - loss: 1.7719 - regression_loss: 1.3520 - classification_loss: 9793/10000 [============================>.] - ETA: 1:37 - loss: 1.7719 - regression_loss: 1.3520 - classification_loss: 9794/10000 [============================>.] - ETA: 1:37 - loss: 1.7717 - regression_loss: 1.3518 - classification_loss: 9795/10000 [============================>.] - ETA: 1:36 - loss: 1.7717 - regression_loss: 1.3518 - classification_loss: 9796/10000 [============================>.] - ETA: 1:36 - loss: 1.7717 - regression_loss: 1.3518 - classification_loss: 9797/10000 [============================>.] - ETA: 1:35 - loss: 1.7716 - regression_loss: 1.3517 - classification_loss: 9798/10000 [============================>.] - ETA: 1:35 - loss: 1.7717 - regression_loss: 1.3518 - classification_loss: 9799/10000 [============================>.] - ETA: 1:34 - loss: 1.7716 - regression_loss: 1.3517 - classification_loss: 9800/10000 [============================>.] - ETA: 1:34 - loss: 1.7716 - regression_loss: 1.3517 - classification_loss: 9801/10000 [============================>.] - ETA: 1:34 - loss: 1.7715 - regression_loss: 1.3517 - classification_loss: 9802/10000 [============================>.] - ETA: 1:33 - loss: 1.7715 - regression_loss: 1.3516 - classification_loss: 9803/10000 [============================>.] - ETA: 1:33 - loss: 1.7714 - regression_loss: 1.3516 - classification_loss: 9804/10000 [============================>.] - ETA: 1:32 - loss: 1.7713 - regression_loss: 1.3515 - classification_loss: 9805/10000 [============================>.] - ETA: 1:32 - loss: 1.7713 - regression_loss: 1.3515 - classification_loss: 9806/10000 [============================>.] - ETA: 1:31 - loss: 1.7714 - regression_loss: 1.3515 - classification_loss: 9807/10000 [============================>.] - ETA: 1:31 - loss: 1.7713 - regression_loss: 1.3515 - classification_loss: 9808/10000 [============================>.] - ETA: 1:30 - loss: 1.7713 - regression_loss: 1.3515 - classification_loss: 9809/10000 [============================>.] - ETA: 1:30 - loss: 1.7712 - regression_loss: 1.3514 - classification_loss: 9810/10000 [============================>.] - ETA: 1:29 - loss: 1.7712 - regression_loss: 1.3514 - classification_loss: 9811/10000 [============================>.] - ETA: 1:29 - loss: 1.7712 - regression_loss: 1.3514 - classification_loss: 9812/10000 [============================>.] - ETA: 1:28 - loss: 1.7712 - regression_loss: 1.3514 - classification_loss: 9813/10000 [============================>.] - ETA: 1:28 - loss: 1.7712 - regression_loss: 1.3513 - classification_loss: 9814/10000 [============================>.] - ETA: 1:27 - loss: 1.7711 - regression_loss: 1.3513 - classification_loss: 9815/10000 [============================>.] - ETA: 1:27 - loss: 1.7710 - regression_loss: 1.3512 - classification_loss: 9816/10000 [============================>.] - ETA: 1:26 - loss: 1.7709 - regression_loss: 1.3511 - classification_loss: 9817/10000 [============================>.] - ETA: 1:26 - loss: 1.7709 - regression_loss: 1.3511 - classification_loss: 9818/10000 [============================>.] - ETA: 1:25 - loss: 1.7710 - regression_loss: 1.3512 - classification_loss: 9819/10000 [============================>.] - ETA: 1:25 - loss: 1.7709 - regression_loss: 1.3512 - classification_loss: 9820/10000 [============================>.] - ETA: 1:25 - loss: 1.7709 - regression_loss: 1.3511 - classification_loss: 9821/10000 [============================>.] - ETA: 1:24 - loss: 1.7709 - regression_loss: 1.3511 - classification_loss: 9822/10000 [============================>.] - ETA: 1:24 - loss: 1.7710 - regression_loss: 1.3512 - classification_loss: 9823/10000 [============================>.] - ETA: 1:23 - loss: 1.7709 - regression_loss: 1.3512 - classification_loss: 9824/10000 [============================>.] - ETA: 1:23 - loss: 1.7709 - regression_loss: 1.3511 - classification_loss: 9825/10000 [============================>.] - ETA: 1:22 - loss: 1.7709 - regression_loss: 1.3511 - classification_loss: 9826/10000 [============================>.] - ETA: 1:22 - loss: 1.7709 - regression_loss: 1.3511 - classification_loss: 9827/10000 [============================>.] - ETA: 1:21 - loss: 1.7708 - regression_loss: 1.3511 - classification_loss: 9828/10000 [============================>.] - ETA: 1:21 - loss: 1.7709 - regression_loss: 1.3511 - classification_loss: 9829/10000 [============================>.] - ETA: 1:20 - loss: 1.7709 - regression_loss: 1.3511 - classification_loss: 9830/10000 [============================>.] - ETA: 1:20 - loss: 1.7709 - regression_loss: 1.3511 - classification_loss: 9831/10000 [============================>.] - ETA: 1:19 - loss: 1.7708 - regression_loss: 1.3511 - classification_loss: 9832/10000 [============================>.] - ETA: 1:19 - loss: 1.7708 - regression_loss: 1.3511 - classification_loss: 9833/10000 [============================>.] - ETA: 1:18 - loss: 1.7709 - regression_loss: 1.3511 - classification_loss: 9834/10000 [============================>.] - ETA: 1:18 - loss: 1.7708 - regression_loss: 1.3510 - classification_loss: 9835/10000 [============================>.] - ETA: 1:17 - loss: 1.7708 - regression_loss: 1.3511 - classification_loss: 9836/10000 [============================>.] - ETA: 1:17 - loss: 1.7708 - regression_loss: 1.3511 - classification_loss: 9837/10000 [============================>.] - ETA: 1:16 - loss: 1.7708 - regression_loss: 1.3511 - classification_loss: 9838/10000 [============================>.] - ETA: 1:16 - loss: 1.7708 - regression_loss: 1.3511 - classification_loss: 9839/10000 [============================>.] - ETA: 1:16 - loss: 1.7708 - regression_loss: 1.3511 - classification_loss: 9840/10000 [============================>.] - ETA: 1:15 - loss: 1.7707 - regression_loss: 1.3510 - classification_loss: 9841/10000 [============================>.] - ETA: 1:15 - loss: 1.7707 - regression_loss: 1.3510 - classification_loss: 9842/10000 [============================>.] - ETA: 1:14 - loss: 1.7707 - regression_loss: 1.3510 - classification_loss: 9843/10000 [============================>.] - ETA: 1:14 - loss: 1.7707 - regression_loss: 1.3510 - classification_loss: 9844/10000 [============================>.] - ETA: 1:13 - loss: 1.7707 - regression_loss: 1.3510 - classification_loss: 9845/10000 [============================>.] - ETA: 1:13 - loss: 1.7707 - regression_loss: 1.3510 - classification_loss: 9846/10000 [============================>.] - ETA: 1:12 - loss: 1.7707 - regression_loss: 1.3510 - classification_loss: 9847/10000 [============================>.] - ETA: 1:12 - loss: 1.7708 - regression_loss: 1.3510 - classification_loss: 9848/10000 [============================>.] - ETA: 1:11 - loss: 1.7708 - regression_loss: 1.3511 - classification_loss: 9849/10000 [============================>.] - ETA: 1:11 - loss: 1.7708 - regression_loss: 1.3511 - classification_loss: 9850/10000 [============================>.] - ETA: 1:10 - loss: 1.7708 - regression_loss: 1.3511 - classification_loss: 9851/10000 [============================>.] - ETA: 1:10 - loss: 1.7708 - regression_loss: 1.3512 - classification_loss: 9852/10000 [============================>.] - ETA: 1:09 - loss: 1.7708 - regression_loss: 1.3511 - classification_loss: 9853/10000 [============================>.] - ETA: 1:09 - loss: 1.7708 - regression_loss: 1.3511 - classification_loss: 9854/10000 [============================>.] - ETA: 1:08 - loss: 1.7708 - regression_loss: 1.3512 - classification_loss: 9855/10000 [============================>.] - ETA: 1:08 - loss: 1.7707 - regression_loss: 1.3511 - classification_loss: 9856/10000 [============================>.] - ETA: 1:08 - loss: 1.7707 - regression_loss: 1.3510 - classification_loss: 9857/10000 [============================>.] - ETA: 1:07 - loss: 1.7707 - regression_loss: 1.3511 - classification_loss: 9858/10000 [============================>.] - ETA: 1:07 - loss: 1.7706 - regression_loss: 1.3510 - classification_loss: 9859/10000 [============================>.] - ETA: 1:06 - loss: 1.7706 - regression_loss: 1.3510 - classification_loss: 9860/10000 [============================>.] - ETA: 1:06 - loss: 1.7706 - regression_loss: 1.3510 - classification_loss: 9861/10000 [============================>.] - ETA: 1:05 - loss: 1.7706 - regression_loss: 1.3510 - classification_loss: 9862/10000 [============================>.] - ETA: 1:05 - loss: 1.7706 - regression_loss: 1.3510 - classification_loss: 9863/10000 [============================>.] - ETA: 1:04 - loss: 1.7705 - regression_loss: 1.3509 - classification_loss: 9864/10000 [============================>.] - ETA: 1:04 - loss: 1.7706 - regression_loss: 1.3510 - classification_loss: 9865/10000 [============================>.] - ETA: 1:03 - loss: 1.7706 - regression_loss: 1.3510 - classification_loss: 9866/10000 [============================>.] - ETA: 1:03 - loss: 1.7706 - regression_loss: 1.3510 - classification_loss: 9867/10000 [============================>.] - ETA: 1:02 - loss: 1.7706 - regression_loss: 1.3510 - classification_loss: 9868/10000 [============================>.] - ETA: 1:02 - loss: 1.7706 - regression_loss: 1.3510 - classification_loss: 9869/10000 [============================>.] - ETA: 1:01 - loss: 1.7706 - regression_loss: 1.3510 - classification_loss: 9870/10000 [============================>.] - ETA: 1:01 - loss: 1.7705 - regression_loss: 1.3510 - classification_loss: 9871/10000 [============================>.] - ETA: 1:00 - loss: 1.7705 - regression_loss: 1.3510 - classification_loss: 9872/10000 [============================>.] - ETA: 1:00 - loss: 1.7706 - regression_loss: 1.3510 - classification_loss: 9873/10000 [============================>.] - ETA: 59s - loss: 1.7705 - regression_loss: 1.3510 - classification_loss:  9874/10000 [============================>.] - ETA: 59s - loss: 1.7705 - regression_loss: 1.3510 - classification_loss:  9875/10000 [============================>.] - ETA: 59s - loss: 1.7706 - regression_loss: 1.3510 - classification_loss:  9876/10000 [============================>.] - ETA: 58s - loss: 1.7706 - regression_loss: 1.3510 - classification_loss:  9877/10000 [============================>.] - ETA: 58s - loss: 1.7706 - regression_loss: 1.3511 - classification_loss:  9878/10000 [============================>.] - ETA: 57s - loss: 1.7705 - regression_loss: 1.3510 - classification_loss:  9879/10000 [============================>.] - ETA: 57s - loss: 1.7705 - regression_loss: 1.3510 - classification_loss:  9880/10000 [============================>.] - ETA: 56s - loss: 1.7705 - regression_loss: 1.3510 - classification_loss:  9881/10000 [============================>.] - ETA: 56s - loss: 1.7704 - regression_loss: 1.3509 - classification_loss:  9882/10000 [============================>.] - ETA: 55s - loss: 1.7704 - regression_loss: 1.3509 - classification_loss:  9883/10000 [============================>.] - ETA: 55s - loss: 1.7704 - regression_loss: 1.3509 - classification_loss:  9884/10000 [============================>.] - ETA: 54s - loss: 1.7703 - regression_loss: 1.3508 - classification_loss:  9885/10000 [============================>.] - ETA: 54s - loss: 1.7704 - regression_loss: 1.3508 - classification_loss:  9886/10000 [============================>.] - ETA: 53s - loss: 1.7703 - regression_loss: 1.3507 - classification_loss:  9887/10000 [============================>.] - ETA: 53s - loss: 1.7702 - regression_loss: 1.3506 - classification_loss:  9888/10000 [============================>.] - ETA: 52s - loss: 1.7701 - regression_loss: 1.3506 - classification_loss:  9889/10000 [============================>.] - ETA: 52s - loss: 1.7701 - regression_loss: 1.3506 - classification_loss:  9890/10000 [============================>.] - ETA: 51s - loss: 1.7701 - regression_loss: 1.3506 - classification_loss:  9891/10000 [============================>.] - ETA: 51s - loss: 1.7700 - regression_loss: 1.3505 - classification_loss:  9892/10000 [============================>.] - ETA: 51s - loss: 1.7701 - regression_loss: 1.3506 - classification_loss:  9893/10000 [============================>.] - ETA: 50s - loss: 1.7701 - regression_loss: 1.3507 - classification_loss:  9894/10000 [============================>.] - ETA: 50s - loss: 1.7700 - regression_loss: 1.3506 - classification_loss:  9895/10000 [============================>.] - ETA: 49s - loss: 1.7700 - regression_loss: 1.3506 - classification_loss:  9896/10000 [============================>.] - ETA: 49s - loss: 1.7700 - regression_loss: 1.3506 - classification_loss:  9897/10000 [============================>.] - ETA: 48s - loss: 1.7699 - regression_loss: 1.3505 - classification_loss:  9898/10000 [============================>.] - ETA: 48s - loss: 1.7700 - regression_loss: 1.3506 - classification_loss:  9899/10000 [============================>.] - ETA: 47s - loss: 1.7700 - regression_loss: 1.3506 - classification_loss:  9900/10000 [============================>.] - ETA: 47s - loss: 1.7699 - regression_loss: 1.3506 - classification_loss:  9901/10000 [============================>.] - ETA: 46s - loss: 1.7699 - regression_loss: 1.3505 - classification_loss:  9902/10000 [============================>.] - ETA: 46s - loss: 1.7698 - regression_loss: 1.3505 - classification_loss:  9903/10000 [============================>.] - ETA: 45s - loss: 1.7699 - regression_loss: 1.3505 - classification_loss:  9904/10000 [============================>.] - ETA: 45s - loss: 1.7699 - regression_loss: 1.3506 - classification_loss:  9905/10000 [============================>.] - ETA: 44s - loss: 1.7698 - regression_loss: 1.3505 - classification_loss:  9906/10000 [============================>.] - ETA: 44s - loss: 1.7699 - regression_loss: 1.3506 - classification_loss:  9907/10000 [============================>.] - ETA: 43s - loss: 1.7699 - regression_loss: 1.3506 - classification_loss:  9908/10000 [============================>.] - ETA: 43s - loss: 1.7699 - regression_loss: 1.3505 - classification_loss:  9909/10000 [============================>.] - ETA: 42s - loss: 1.7698 - regression_loss: 1.3505 - classification_loss:  9910/10000 [============================>.] - ETA: 42s - loss: 1.7698 - regression_loss: 1.3505 - classification_loss:  9911/10000 [============================>.] - ETA: 42s - loss: 1.7697 - regression_loss: 1.3504 - classification_loss:  9912/10000 [============================>.] - ETA: 41s - loss: 1.7697 - regression_loss: 1.3504 - classification_loss:  9913/10000 [============================>.] - ETA: 41s - loss: 1.7697 - regression_loss: 1.3504 - classification_loss:  9914/10000 [============================>.] - ETA: 40s - loss: 1.7696 - regression_loss: 1.3503 - classification_loss:  9915/10000 [============================>.] - ETA: 40s - loss: 1.7696 - regression_loss: 1.3503 - classification_loss:  9916/10000 [============================>.] - ETA: 39s - loss: 1.7695 - regression_loss: 1.3503 - classification_loss:  9917/10000 [============================>.] - ETA: 39s - loss: 1.7695 - regression_loss: 1.3503 - classification_loss:  9918/10000 [============================>.] - ETA: 38s - loss: 1.7695 - regression_loss: 1.3502 - classification_loss:  9919/10000 [============================>.] - ETA: 38s - loss: 1.7695 - regression_loss: 1.3502 - classification_loss:  9920/10000 [============================>.] - ETA: 37s - loss: 1.7695 - regression_loss: 1.3502 - classification_loss:  9921/10000 [============================>.] - ETA: 37s - loss: 1.7695 - regression_loss: 1.3502 - classification_loss:  9922/10000 [============================>.] - ETA: 36s - loss: 1.7694 - regression_loss: 1.3502 - classification_loss:  9923/10000 [============================>.] - ETA: 36s - loss: 1.7694 - regression_loss: 1.3502 - classification_loss:  9924/10000 [============================>.] - ETA: 35s - loss: 1.7694 - regression_loss: 1.3502 - classification_loss:  9925/10000 [============================>.] - ETA: 35s - loss: 1.7693 - regression_loss: 1.3501 - classification_loss:  9926/10000 [============================>.] - ETA: 34s - loss: 1.7693 - regression_loss: 1.3501 - classification_loss:  9927/10000 [============================>.] - ETA: 34s - loss: 1.7694 - regression_loss: 1.3502 - classification_loss:  9928/10000 [============================>.] - ETA: 33s - loss: 1.7694 - regression_loss: 1.3502 - classification_loss:  9929/10000 [============================>.] - ETA: 33s - loss: 1.7694 - regression_loss: 1.3502 - classification_loss:  9930/10000 [============================>.] - ETA: 33s - loss: 1.7693 - regression_loss: 1.3501 - classification_loss:  9931/10000 [============================>.] - ETA: 32s - loss: 1.7694 - regression_loss: 1.3502 - classification_loss:  9932/10000 [============================>.] - ETA: 32s - loss: 1.7695 - regression_loss: 1.3502 - classification_loss:  9933/10000 [============================>.] - ETA: 31s - loss: 1.7694 - regression_loss: 1.3502 - classification_loss:  9934/10000 [============================>.] - ETA: 31s - loss: 1.7693 - regression_loss: 1.3501 - classification_loss:  9935/10000 [============================>.] - ETA: 30s - loss: 1.7693 - regression_loss: 1.3501 - classification_loss:  9936/10000 [============================>.] - ETA: 30s - loss: 1.7693 - regression_loss: 1.3501 - classification_loss:  9937/10000 [============================>.] - ETA: 29s - loss: 1.7693 - regression_loss: 1.3501 - classification_loss:  9938/10000 [============================>.] - ETA: 29s - loss: 1.7692 - regression_loss: 1.3500 - classification_loss:  9939/10000 [============================>.] - ETA: 28s - loss: 1.7691 - regression_loss: 1.3499 - classification_loss:  9940/10000 [============================>.] - ETA: 28s - loss: 1.7690 - regression_loss: 1.3498 - classification_loss:  9941/10000 [============================>.] - ETA: 27s - loss: 1.7690 - regression_loss: 1.3498 - classification_loss:  9942/10000 [============================>.] - ETA: 27s - loss: 1.7690 - regression_loss: 1.3498 - classification_loss:  9943/10000 [============================>.] - ETA: 26s - loss: 1.7689 - regression_loss: 1.3497 - classification_loss:  9944/10000 [============================>.] - ETA: 26s - loss: 1.7689 - regression_loss: 1.3497 - classification_loss:  9945/10000 [============================>.] - ETA: 25s - loss: 1.7689 - regression_loss: 1.3497 - classification_loss:  9946/10000 [============================>.] - ETA: 25s - loss: 1.7689 - regression_loss: 1.3497 - classification_loss:  9947/10000 [============================>.] - ETA: 25s - loss: 1.7688 - regression_loss: 1.3496 - classification_loss:  9948/10000 [============================>.] - ETA: 24s - loss: 1.7687 - regression_loss: 1.3495 - classification_loss:  9949/10000 [============================>.] - ETA: 24s - loss: 1.7686 - regression_loss: 1.3494 - classification_loss:  9950/10000 [============================>.] - ETA: 23s - loss: 1.7687 - regression_loss: 1.3495 - classification_loss:  9951/10000 [============================>.] - ETA: 23s - loss: 1.7686 - regression_loss: 1.3495 - classification_loss:  9952/10000 [============================>.] - ETA: 22s - loss: 1.7687 - regression_loss: 1.3495 - classification_loss:  9953/10000 [============================>.] - ETA: 22s - loss: 1.7686 - regression_loss: 1.3494 - classification_loss:  9954/10000 [============================>.] - ETA: 21s - loss: 1.7686 - regression_loss: 1.3494 - classification_loss:  9955/10000 [============================>.] - ETA: 21s - loss: 1.7686 - regression_loss: 1.3495 - classification_loss:  9956/10000 [============================>.] - ETA: 20s - loss: 1.7687 - regression_loss: 1.3496 - classification_loss:  9957/10000 [============================>.] - ETA: 20s - loss: 1.7686 - regression_loss: 1.3495 - classification_loss:  9958/10000 [============================>.] - ETA: 19s - loss: 1.7685 - regression_loss: 1.3494 - classification_loss:  9959/10000 [============================>.] - ETA: 19s - loss: 1.7685 - regression_loss: 1.3495 - classification_loss:  9960/10000 [============================>.] - ETA: 18s - loss: 1.7685 - regression_loss: 1.3495 - classification_loss:  9961/10000 [============================>.] - ETA: 18s - loss: 1.7685 - regression_loss: 1.3495 - classification_loss:  9962/10000 [============================>.] - ETA: 17s - loss: 1.7685 - regression_loss: 1.3495 - classification_loss:  9963/10000 [============================>.] - ETA: 17s - loss: 1.7684 - regression_loss: 1.3494 - classification_loss:  9964/10000 [============================>.] - ETA: 16s - loss: 1.7684 - regression_loss: 1.3494 - classification_loss:  9965/10000 [============================>.] - ETA: 16s - loss: 1.7684 - regression_loss: 1.3494 - classification_loss:  9966/10000 [============================>.] - ETA: 16s - loss: 1.7684 - regression_loss: 1.3494 - classification_loss:  9967/10000 [============================>.] - ETA: 15s - loss: 1.7685 - regression_loss: 1.3494 - classification_loss:  9968/10000 [============================>.] - ETA: 15s - loss: 1.7686 - regression_loss: 1.3495 - classification_loss:  9969/10000 [============================>.] - ETA: 14s - loss: 1.7686 - regression_loss: 1.3495 - classification_loss:  9970/10000 [============================>.] - ETA: 14s - loss: 1.7687 - regression_loss: 1.3496 - classification_loss:  9971/10000 [============================>.] - ETA: 13s - loss: 1.7689 - regression_loss: 1.3498 - classification_loss:  9972/10000 [============================>.] - ETA: 13s - loss: 1.7689 - regression_loss: 1.3498 - classification_loss:  9973/10000 [============================>.] - ETA: 12s - loss: 1.7690 - regression_loss: 1.3498 - classification_loss:  9974/10000 [============================>.] - ETA: 12s - loss: 1.7690 - regression_loss: 1.3499 - classification_loss:  9975/10000 [============================>.] - ETA: 11s - loss: 1.7691 - regression_loss: 1.3500 - classification_loss:  9976/10000 [============================>.] - ETA: 11s - loss: 1.7691 - regression_loss: 1.3500 - classification_loss:  9977/10000 [============================>.] - ETA: 10s - loss: 1.7692 - regression_loss: 1.3500 - classification_loss:  9978/10000 [============================>.] - ETA: 10s - loss: 1.7692 - regression_loss: 1.3501 - classification_loss:  9979/10000 [============================>.] - ETA: 9s - loss: 1.7692 - regression_loss: 1.3500 - classification_loss: 0 9980/10000 [============================>.] - ETA: 9s - loss: 1.7692 - regression_loss: 1.3500 - classification_loss: 0 9981/10000 [============================>.] - ETA: 8s - loss: 1.7692 - regression_loss: 1.3500 - classification_loss: 0 9982/10000 [============================>.] - ETA: 8s - loss: 1.7693 - regression_loss: 1.3501 - classification_loss: 0 9983/10000 [============================>.] - ETA: 8s - loss: 1.7693 - regression_loss: 1.3501 - classification_loss: 0 9984/10000 [============================>.] - ETA: 7s - loss: 1.7693 - regression_loss: 1.3501 - classification_loss: 0 9985/10000 [============================>.] - ETA: 7s - loss: 1.7693 - regression_loss: 1.3501 - classification_loss: 0 9986/10000 [============================>.] - ETA: 6s - loss: 1.7693 - regression_loss: 1.3501 - classification_loss: 0 9987/10000 [============================>.] - ETA: 6s - loss: 1.7693 - regression_loss: 1.3501 - classification_loss: 0 9988/10000 [============================>.] - ETA: 5s - loss: 1.7693 - regression_loss: 1.3501 - classification_loss: 0 9989/10000 [============================>.] - ETA: 5s - loss: 1.7692 - regression_loss: 1.3501 - classification_loss: 0 9990/10000 [============================>.] - ETA: 4s - loss: 1.7693 - regression_loss: 1.3501 - classification_loss: 0 9991/10000 [============================>.] - ETA: 4s - loss: 1.7693 - regression_loss: 1.3501 - classification_loss: 0 9992/10000 [============================>.] - ETA: 3s - loss: 1.7692 - regression_loss: 1.3500 - classification_loss: 0 9993/10000 [============================>.] - ETA: 3s - loss: 1.7692 - regression_loss: 1.3500 - classification_loss: 0 9994/10000 [============================>.] - ETA: 2s - loss: 1.7691 - regression_loss: 1.3499 - classification_loss: 0 9995/10000 [============================>.] - ETA: 2s - loss: 1.7690 - regression_loss: 1.3499 - classification_loss: 0 9996/10000 [============================>.] - ETA: 1s - loss: 1.7690 - regression_loss: 1.3499 - classification_loss: 0 9997/10000 [============================>.] - ETA: 1s - loss: 1.7690 - regression_loss: 1.3499 - classification_loss: 0 9998/10000 [============================>.] - ETA: 0s - loss: 1.7689 - regression_loss: 1.3498 - classification_loss: 0 9999/10000 [============================>.] - ETA: 0s - loss: 1.7688 - regression_loss: 1.3498 - classification_loss: 010000/10000 [==============================] - 4721s 472ms/step - loss: 1.7688 - regression_loss: 1.3498 - classification_loss: 0.4190
aeroplane 0.3709
bicycle 0.2726
bird 0.1327
boat 0.1124
bottle 0.3001
bus 0.2488
car 0.6085
cat 0.4171
chair 0.2551
cow 0.1793
diningtable 0.2366
dog 0.2228
horse 0.2326
motorbike 0.3593
person 0.6711
pottedplant 0.1171
sheep 0.1936
sofa 0.3583
train 0.2925
tvmonitor 0.3732
mAP: 0.2977

Epoch 00001: saving model to ./snapshots\resnet50_pascal_01.h5










 8322/10000 [=======================>......] - ETA: 13:13 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8323/10000 [=======================>......] - ETA: 13:13 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8324/10000 [=======================>......] - ETA: 13:12 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8325/10000 [=======================>......] - ETA: 13:12 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8326/10000 [=======================>......] - ETA: 13:12 - loss: 1.4950 - regression_loss: 1.1542 - classification_loss 8327/10000 [=======================>......] - ETA: 13:11 - loss: 1.4951 - regression_loss: 1.1543 - classification_loss 8328/10000 [=======================>......] - ETA: 13:11 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8329/10000 [=======================>......] - ETA: 13:10 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8330/10000 [=======================>......] - ETA: 13:10 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8331/10000 [=======================>......] - ETA: 13:09 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8332/10000 [=======================>......] - ETA: 13:09 - loss: 1.4949 - regression_loss: 1.1542 - classification_loss 8333/10000 [=======================>......] - ETA: 13:08 - loss: 1.4949 - regression_loss: 1.1543 - classification_loss 8334/10000 [========================>.....] - ETA: 13:08 - loss: 1.4949 - regression_loss: 1.1542 - classification_loss 8335/10000 [========================>.....] - ETA: 13:07 - loss: 1.4949 - regression_loss: 1.1542 - classification_loss 8336/10000 [========================>.....] - ETA: 13:07 - loss: 1.4948 - regression_loss: 1.1542 - classification_loss 8337/10000 [========================>.....] - ETA: 13:06 - loss: 1.4949 - regression_loss: 1.1543 - classification_loss 8338/10000 [========================>.....] - ETA: 13:06 - loss: 1.4948 - regression_loss: 1.1542 - classification_loss 8339/10000 [========================>.....] - ETA: 13:05 - loss: 1.4949 - regression_loss: 1.1543 - classification_loss 8340/10000 [========================>.....] - ETA: 13:05 - loss: 1.4949 - regression_loss: 1.1543 - classification_loss 8341/10000 [========================>.....] - ETA: 13:04 - loss: 1.4951 - regression_loss: 1.1544 - classification_loss 8342/10000 [========================>.....] - ETA: 13:04 - loss: 1.4951 - regression_loss: 1.1544 - classification_loss 8343/10000 [========================>.....] - ETA: 13:03 - loss: 1.4952 - regression_loss: 1.1545 - classification_loss 8344/10000 [========================>.....] - ETA: 13:03 - loss: 1.4952 - regression_loss: 1.1545 - classification_loss 8345/10000 [========================>.....] - ETA: 13:02 - loss: 1.4952 - regression_loss: 1.1545 - classification_loss 8346/10000 [========================>.....] - ETA: 13:02 - loss: 1.4951 - regression_loss: 1.1544 - classification_loss 8347/10000 [========================>.....] - ETA: 13:01 - loss: 1.4952 - regression_loss: 1.1544 - classification_loss 8348/10000 [========================>.....] - ETA: 13:01 - loss: 1.4952 - regression_loss: 1.1545 - classification_loss 8349/10000 [========================>.....] - ETA: 13:01 - loss: 1.4953 - regression_loss: 1.1545 - classification_loss 8350/10000 [========================>.....] - ETA: 13:00 - loss: 1.4954 - regression_loss: 1.1546 - classification_loss 8351/10000 [========================>.....] - ETA: 13:00 - loss: 1.4954 - regression_loss: 1.1546 - classification_loss 8352/10000 [========================>.....] - ETA: 12:59 - loss: 1.4955 - regression_loss: 1.1547 - classification_loss 8353/10000 [========================>.....] - ETA: 12:59 - loss: 1.4955 - regression_loss: 1.1547 - classification_loss 8354/10000 [========================>.....] - ETA: 12:58 - loss: 1.4955 - regression_loss: 1.1547 - classification_loss 8355/10000 [========================>.....] - ETA: 12:58 - loss: 1.4954 - regression_loss: 1.1547 - classification_loss 8356/10000 [========================>.....] - ETA: 12:57 - loss: 1.4953 - regression_loss: 1.1546 - classification_loss 8357/10000 [========================>.....] - ETA: 12:57 - loss: 1.4954 - regression_loss: 1.1547 - classification_loss 8358/10000 [========================>.....] - ETA: 12:56 - loss: 1.4954 - regression_loss: 1.1547 - classification_loss 8359/10000 [========================>.....] - ETA: 12:56 - loss: 1.4954 - regression_loss: 1.1547 - classification_loss 8360/10000 [========================>.....] - ETA: 12:55 - loss: 1.4953 - regression_loss: 1.1546 - classification_loss 8361/10000 [========================>.....] - ETA: 12:55 - loss: 1.4952 - regression_loss: 1.1545 - classification_loss 8362/10000 [========================>.....] - ETA: 12:54 - loss: 1.4952 - regression_loss: 1.1545 - classification_loss 8363/10000 [========================>.....] - ETA: 12:54 - loss: 1.4952 - regression_loss: 1.1545 - classification_loss 8364/10000 [========================>.....] - ETA: 12:53 - loss: 1.4953 - regression_loss: 1.1545 - classification_loss 8365/10000 [========================>.....] - ETA: 12:53 - loss: 1.4954 - regression_loss: 1.1546 - classification_loss 8366/10000 [========================>.....] - ETA: 12:52 - loss: 1.4954 - regression_loss: 1.1546 - classification_loss 8367/10000 [========================>.....] - ETA: 12:52 - loss: 1.4953 - regression_loss: 1.1546 - classification_loss 8368/10000 [========================>.....] - ETA: 12:51 - loss: 1.4952 - regression_loss: 1.1545 - classification_loss 8369/10000 [========================>.....] - ETA: 12:51 - loss: 1.4952 - regression_loss: 1.1545 - classification_loss 8370/10000 [========================>.....] - ETA: 12:51 - loss: 1.4952 - regression_loss: 1.1545 - classification_loss 8371/10000 [========================>.....] - ETA: 12:50 - loss: 1.4952 - regression_loss: 1.1545 - classification_loss 8372/10000 [========================>.....] - ETA: 12:50 - loss: 1.4952 - regression_loss: 1.1546 - classification_loss 8373/10000 [========================>.....] - ETA: 12:49 - loss: 1.4953 - regression_loss: 1.1546 - classification_loss 8374/10000 [========================>.....] - ETA: 12:49 - loss: 1.4952 - regression_loss: 1.1545 - classification_loss 8375/10000 [========================>.....] - ETA: 12:48 - loss: 1.4952 - regression_loss: 1.1545 - classification_loss 8376/10000 [========================>.....] - ETA: 12:48 - loss: 1.4952 - regression_loss: 1.1545 - classification_loss 8377/10000 [========================>.....] - ETA: 12:47 - loss: 1.4952 - regression_loss: 1.1545 - classification_loss 8378/10000 [========================>.....] - ETA: 12:47 - loss: 1.4952 - regression_loss: 1.1545 - classification_loss 8379/10000 [========================>.....] - ETA: 12:46 - loss: 1.4951 - regression_loss: 1.1544 - classification_loss 8380/10000 [========================>.....] - ETA: 12:46 - loss: 1.4950 - regression_loss: 1.1544 - classification_loss 8381/10000 [========================>.....] - ETA: 12:45 - loss: 1.4950 - regression_loss: 1.1544 - classification_loss 8382/10000 [========================>.....] - ETA: 12:45 - loss: 1.4950 - regression_loss: 1.1544 - classification_loss 8383/10000 [========================>.....] - ETA: 12:44 - loss: 1.4950 - regression_loss: 1.1544 - classification_loss 8384/10000 [========================>.....] - ETA: 12:44 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8385/10000 [========================>.....] - ETA: 12:43 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8386/10000 [========================>.....] - ETA: 12:43 - loss: 1.4951 - regression_loss: 1.1544 - classification_loss 8387/10000 [========================>.....] - ETA: 12:42 - loss: 1.4950 - regression_loss: 1.1544 - classification_loss 8388/10000 [========================>.....] - ETA: 12:42 - loss: 1.4950 - regression_loss: 1.1544 - classification_loss 8389/10000 [========================>.....] - ETA: 12:42 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8390/10000 [========================>.....] - ETA: 12:41 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8391/10000 [========================>.....] - ETA: 12:41 - loss: 1.4949 - regression_loss: 1.1543 - classification_loss 8392/10000 [========================>.....] - ETA: 12:40 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8393/10000 [========================>.....] - ETA: 12:40 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8394/10000 [========================>.....] - ETA: 12:39 - loss: 1.4950 - regression_loss: 1.1544 - classification_loss 8395/10000 [========================>.....] - ETA: 12:39 - loss: 1.4949 - regression_loss: 1.1543 - classification_loss 8396/10000 [========================>.....] - ETA: 12:38 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8397/10000 [========================>.....] - ETA: 12:38 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8398/10000 [========================>.....] - ETA: 12:37 - loss: 1.4949 - regression_loss: 1.1543 - classification_loss 8399/10000 [========================>.....] - ETA: 12:37 - loss: 1.4949 - regression_loss: 1.1542 - classification_loss 8400/10000 [========================>.....] - ETA: 12:36 - loss: 1.4949 - regression_loss: 1.1542 - classification_loss 8401/10000 [========================>.....] - ETA: 12:36 - loss: 1.4949 - regression_loss: 1.1543 - classification_loss 8402/10000 [========================>.....] - ETA: 12:35 - loss: 1.4949 - regression_loss: 1.1543 - classification_loss 8403/10000 [========================>.....] - ETA: 12:35 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8404/10000 [========================>.....] - ETA: 12:34 - loss: 1.4949 - regression_loss: 1.1543 - classification_loss 8405/10000 [========================>.....] - ETA: 12:34 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8406/10000 [========================>.....] - ETA: 12:33 - loss: 1.4950 - regression_loss: 1.1544 - classification_loss 8407/10000 [========================>.....] - ETA: 12:33 - loss: 1.4951 - regression_loss: 1.1544 - classification_loss 8408/10000 [========================>.....] - ETA: 12:32 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8409/10000 [========================>.....] - ETA: 12:32 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8410/10000 [========================>.....] - ETA: 12:32 - loss: 1.4951 - regression_loss: 1.1544 - classification_loss 8411/10000 [========================>.....] - ETA: 12:31 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8412/10000 [========================>.....] - ETA: 12:31 - loss: 1.4949 - regression_loss: 1.1542 - classification_loss 8413/10000 [========================>.....] - ETA: 12:30 - loss: 1.4949 - regression_loss: 1.1543 - classification_loss 8414/10000 [========================>.....] - ETA: 12:30 - loss: 1.4949 - regression_loss: 1.1543 - classification_loss 8415/10000 [========================>.....] - ETA: 12:29 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8416/10000 [========================>.....] - ETA: 12:29 - loss: 1.4949 - regression_loss: 1.1543 - classification_loss 8417/10000 [========================>.....] - ETA: 12:28 - loss: 1.4950 - regression_loss: 1.1544 - classification_loss 8418/10000 [========================>.....] - ETA: 12:28 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8419/10000 [========================>.....] - ETA: 12:27 - loss: 1.4951 - regression_loss: 1.1545 - classification_loss 8420/10000 [========================>.....] - ETA: 12:27 - loss: 1.4951 - regression_loss: 1.1545 - classification_loss 8421/10000 [========================>.....] - ETA: 12:26 - loss: 1.4950 - regression_loss: 1.1544 - classification_loss 8422/10000 [========================>.....] - ETA: 12:26 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8423/10000 [========================>.....] - ETA: 12:25 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8424/10000 [========================>.....] - ETA: 12:25 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8425/10000 [========================>.....] - ETA: 12:24 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8426/10000 [========================>.....] - ETA: 12:24 - loss: 1.4951 - regression_loss: 1.1544 - classification_loss 8427/10000 [========================>.....] - ETA: 12:23 - loss: 1.4951 - regression_loss: 1.1544 - classification_loss 8428/10000 [========================>.....] - ETA: 12:23 - loss: 1.4949 - regression_loss: 1.1543 - classification_loss 8429/10000 [========================>.....] - ETA: 12:23 - loss: 1.4949 - regression_loss: 1.1542 - classification_loss 8430/10000 [========================>.....] - ETA: 12:22 - loss: 1.4949 - regression_loss: 1.1542 - classification_loss 8431/10000 [========================>.....] - ETA: 12:22 - loss: 1.4949 - regression_loss: 1.1543 - classification_loss 8432/10000 [========================>.....] - ETA: 12:21 - loss: 1.4949 - regression_loss: 1.1543 - classification_loss 8433/10000 [========================>.....] - ETA: 12:21 - loss: 1.4949 - regression_loss: 1.1543 - classification_loss 8434/10000 [========================>.....] - ETA: 12:20 - loss: 1.4949 - regression_loss: 1.1543 - classification_loss 8435/10000 [========================>.....] - ETA: 12:20 - loss: 1.4948 - regression_loss: 1.1542 - classification_loss 8436/10000 [========================>.....] - ETA: 12:19 - loss: 1.4948 - regression_loss: 1.1542 - classification_loss 8437/10000 [========================>.....] - ETA: 12:19 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8438/10000 [========================>.....] - ETA: 12:18 - loss: 1.4951 - regression_loss: 1.1544 - classification_loss 8439/10000 [========================>.....] - ETA: 12:18 - loss: 1.4952 - regression_loss: 1.1545 - classification_loss 8440/10000 [========================>.....] - ETA: 12:17 - loss: 1.4952 - regression_loss: 1.1545 - classification_loss 8441/10000 [========================>.....] - ETA: 12:17 - loss: 1.4953 - regression_loss: 1.1546 - classification_loss 8442/10000 [========================>.....] - ETA: 12:16 - loss: 1.4952 - regression_loss: 1.1545 - classification_loss 8443/10000 [========================>.....] - ETA: 12:16 - loss: 1.4952 - regression_loss: 1.1545 - classification_loss 8444/10000 [========================>.....] - ETA: 12:15 - loss: 1.4952 - regression_loss: 1.1545 - classification_loss 8445/10000 [========================>.....] - ETA: 12:15 - loss: 1.4952 - regression_loss: 1.1545 - classification_loss 8446/10000 [========================>.....] - ETA: 12:14 - loss: 1.4950 - regression_loss: 1.1544 - classification_loss 8447/10000 [========================>.....] - ETA: 12:14 - loss: 1.4950 - regression_loss: 1.1544 - classification_loss 8448/10000 [========================>.....] - ETA: 12:13 - loss: 1.4950 - regression_loss: 1.1544 - classification_loss 8449/10000 [========================>.....] - ETA: 12:13 - loss: 1.4949 - regression_loss: 1.1543 - classification_loss 8450/10000 [========================>.....] - ETA: 12:13 - loss: 1.4950 - regression_loss: 1.1544 - classification_loss 8451/10000 [========================>.....] - ETA: 12:12 - loss: 1.4950 - regression_loss: 1.1544 - classification_loss 8452/10000 [========================>.....] - ETA: 12:12 - loss: 1.4950 - regression_loss: 1.1545 - classification_loss 8453/10000 [========================>.....] - ETA: 12:11 - loss: 1.4950 - regression_loss: 1.1545 - classification_loss 8454/10000 [========================>.....] - ETA: 12:11 - loss: 1.4951 - regression_loss: 1.1545 - classification_loss 8455/10000 [========================>.....] - ETA: 12:10 - loss: 1.4951 - regression_loss: 1.1545 - classification_loss 8456/10000 [========================>.....] - ETA: 12:10 - loss: 1.4951 - regression_loss: 1.1546 - classification_loss 8457/10000 [========================>.....] - ETA: 12:09 - loss: 1.4951 - regression_loss: 1.1546 - classification_loss 8458/10000 [========================>.....] - ETA: 12:09 - loss: 1.4951 - regression_loss: 1.1546 - classification_loss 8459/10000 [========================>.....] - ETA: 12:08 - loss: 1.4950 - regression_loss: 1.1545 - classification_loss 8460/10000 [========================>.....] - ETA: 12:08 - loss: 1.4949 - regression_loss: 1.1544 - classification_loss 8461/10000 [========================>.....] - ETA: 12:07 - loss: 1.4948 - regression_loss: 1.1544 - classification_loss 8462/10000 [========================>.....] - ETA: 12:07 - loss: 1.4948 - regression_loss: 1.1543 - classification_loss 8463/10000 [========================>.....] - ETA: 12:06 - loss: 1.4948 - regression_loss: 1.1543 - classification_loss 8464/10000 [========================>.....] - ETA: 12:06 - loss: 1.4948 - regression_loss: 1.1543 - classification_loss 8465/10000 [========================>.....] - ETA: 12:05 - loss: 1.4948 - regression_loss: 1.1544 - classification_loss 8466/10000 [========================>.....] - ETA: 12:05 - loss: 1.4947 - regression_loss: 1.1543 - classification_loss 8467/10000 [========================>.....] - ETA: 12:04 - loss: 1.4947 - regression_loss: 1.1542 - classification_loss 8468/10000 [========================>.....] - ETA: 12:04 - loss: 1.4947 - regression_loss: 1.1542 - classification_loss 8469/10000 [========================>.....] - ETA: 12:04 - loss: 1.4946 - regression_loss: 1.1542 - classification_loss 8470/10000 [========================>.....] - ETA: 12:03 - loss: 1.4946 - regression_loss: 1.1542 - classification_loss 8471/10000 [========================>.....] - ETA: 12:03 - loss: 1.4945 - regression_loss: 1.1541 - classification_loss 8472/10000 [========================>.....] - ETA: 12:02 - loss: 1.4946 - regression_loss: 1.1542 - classification_loss 8473/10000 [========================>.....] - ETA: 12:02 - loss: 1.4947 - regression_loss: 1.1543 - classification_loss 8474/10000 [========================>.....] - ETA: 12:01 - loss: 1.4947 - regression_loss: 1.1543 - classification_loss 8475/10000 [========================>.....] - ETA: 12:01 - loss: 1.4947 - regression_loss: 1.1543 - classification_loss 8476/10000 [========================>.....] - ETA: 12:00 - loss: 1.4947 - regression_loss: 1.1543 - classification_loss 8477/10000 [========================>.....] - ETA: 12:00 - loss: 1.4947 - regression_loss: 1.1543 - classification_loss 8478/10000 [========================>.....] - ETA: 11:59 - loss: 1.4947 - regression_loss: 1.1543 - classification_loss 8479/10000 [========================>.....] - ETA: 11:59 - loss: 1.4946 - regression_loss: 1.1542 - classification_loss 8480/10000 [========================>.....] - ETA: 11:58 - loss: 1.4946 - regression_loss: 1.1542 - classification_loss 8481/10000 [========================>.....] - ETA: 11:58 - loss: 1.4948 - regression_loss: 1.1543 - classification_loss 8482/10000 [========================>.....] - ETA: 11:57 - loss: 1.4947 - regression_loss: 1.1542 - classification_loss 8483/10000 [========================>.....] - ETA: 11:57 - loss: 1.4947 - regression_loss: 1.1542 - classification_loss 8484/10000 [========================>.....] - ETA: 11:56 - loss: 1.4947 - regression_loss: 1.1542 - classification_loss 8485/10000 [========================>.....] - ETA: 11:56 - loss: 1.4947 - regression_loss: 1.1543 - classification_loss 8486/10000 [========================>.....] - ETA: 11:55 - loss: 1.4947 - regression_loss: 1.1543 - classification_loss 8487/10000 [========================>.....] - ETA: 11:55 - loss: 1.4946 - regression_loss: 1.1542 - classification_loss 8488/10000 [========================>.....] - ETA: 11:55 - loss: 1.4946 - regression_loss: 1.1542 - classification_loss 8489/10000 [========================>.....] - ETA: 11:54 - loss: 1.4945 - regression_loss: 1.1541 - classification_loss 8490/10000 [========================>.....] - ETA: 11:54 - loss: 1.4945 - regression_loss: 1.1541 - classification_loss 8491/10000 [========================>.....] - ETA: 11:53 - loss: 1.4945 - regression_loss: 1.1541 - classification_loss 8492/10000 [========================>.....] - ETA: 11:53 - loss: 1.4944 - regression_loss: 1.1541 - classification_loss 8493/10000 [========================>.....] - ETA: 11:52 - loss: 1.4944 - regression_loss: 1.1540 - classification_loss 8494/10000 [========================>.....] - ETA: 11:52 - loss: 1.4944 - regression_loss: 1.1541 - classification_loss 8495/10000 [========================>.....] - ETA: 11:51 - loss: 1.4945 - regression_loss: 1.1541 - classification_loss 8496/10000 [========================>.....] - ETA: 11:51 - loss: 1.4945 - regression_loss: 1.1541 - classification_loss 8497/10000 [========================>.....] - ETA: 11:50 - loss: 1.4945 - regression_loss: 1.1541 - classification_loss 8498/10000 [========================>.....] - ETA: 11:50 - loss: 1.4944 - regression_loss: 1.1541 - classification_loss 8499/10000 [========================>.....] - ETA: 11:49 - loss: 1.4946 - regression_loss: 1.1542 - classification_loss 8500/10000 [========================>.....] - ETA: 11:49 - loss: 1.4944 - regression_loss: 1.1541 - classification_loss 8501/10000 [========================>.....] - ETA: 11:48 - loss: 1.4946 - regression_loss: 1.1543 - classification_loss 8502/10000 [========================>.....] - ETA: 11:48 - loss: 1.4945 - regression_loss: 1.1542 - classification_loss 8503/10000 [========================>.....] - ETA: 11:47 - loss: 1.4946 - regression_loss: 1.1543 - classification_loss 8504/10000 [========================>.....] - ETA: 11:47 - loss: 1.4946 - regression_loss: 1.1543 - classification_loss 8505/10000 [========================>.....] - ETA: 11:46 - loss: 1.4946 - regression_loss: 1.1543 - classification_loss 8506/10000 [========================>.....] - ETA: 11:46 - loss: 1.4947 - regression_loss: 1.1544 - classification_loss 8507/10000 [========================>.....] - ETA: 11:45 - loss: 1.4947 - regression_loss: 1.1544 - classification_loss 8508/10000 [========================>.....] - ETA: 11:45 - loss: 1.4948 - regression_loss: 1.1545 - classification_loss 8509/10000 [========================>.....] - ETA: 11:44 - loss: 1.4948 - regression_loss: 1.1545 - classification_loss 8510/10000 [========================>.....] - ETA: 11:44 - loss: 1.4948 - regression_loss: 1.1545 - classification_loss 8511/10000 [========================>.....] - ETA: 11:44 - loss: 1.4948 - regression_loss: 1.1545 - classification_loss 8512/10000 [========================>.....] - ETA: 11:43 - loss: 1.4948 - regression_loss: 1.1544 - classification_loss 8513/10000 [========================>.....] - ETA: 11:43 - loss: 1.4948 - regression_loss: 1.1544 - classification_loss 8514/10000 [========================>.....] - ETA: 11:42 - loss: 1.4947 - regression_loss: 1.1544 - classification_loss 8515/10000 [========================>.....] - ETA: 11:42 - loss: 1.4948 - regression_loss: 1.1544 - classification_loss 8516/10000 [========================>.....] - ETA: 11:41 - loss: 1.4948 - regression_loss: 1.1544 - classification_loss 8517/10000 [========================>.....] - ETA: 11:41 - loss: 1.4948 - regression_loss: 1.1544 - classification_loss 8518/10000 [========================>.....] - ETA: 11:40 - loss: 1.4948 - regression_loss: 1.1544 - classification_loss 8519/10000 [========================>.....] - ETA: 11:40 - loss: 1.4948 - regression_loss: 1.1545 - classification_loss 8520/10000 [========================>.....] - ETA: 11:39 - loss: 1.4948 - regression_loss: 1.1545 - classification_loss 8521/10000 [========================>.....] - ETA: 11:39 - loss: 1.4948 - regression_loss: 1.1544 - classification_loss 8522/10000 [========================>.....] - ETA: 11:38 - loss: 1.4947 - regression_loss: 1.1544 - classification_loss 8523/10000 [========================>.....] - ETA: 11:38 - loss: 1.4947 - regression_loss: 1.1543 - classification_loss 8524/10000 [========================>.....] - ETA: 11:37 - loss: 1.4946 - regression_loss: 1.1543 - classification_loss 8525/10000 [========================>.....] - ETA: 11:37 - loss: 1.4946 - regression_loss: 1.1543 - classification_loss 8526/10000 [========================>.....] - ETA: 11:36 - loss: 1.4946 - regression_loss: 1.1543 - classification_loss 8527/10000 [========================>.....] - ETA: 11:36 - loss: 1.4947 - regression_loss: 1.1543 - classification_loss 8528/10000 [========================>.....] - ETA: 11:35 - loss: 1.4947 - regression_loss: 1.1544 - classification_loss 8529/10000 [========================>.....] - ETA: 11:35 - loss: 1.4947 - regression_loss: 1.1544 - classification_loss 8530/10000 [========================>.....] - ETA: 11:34 - loss: 1.4946 - regression_loss: 1.1543 - classification_loss 8531/10000 [========================>.....] - ETA: 11:34 - loss: 1.4946 - regression_loss: 1.1543 - classification_loss 8532/10000 [========================>.....] - ETA: 11:34 - loss: 1.4946 - regression_loss: 1.1543 - classification_loss 8533/10000 [========================>.....] - ETA: 11:33 - loss: 1.4946 - regression_loss: 1.1543 - classification_loss 8534/10000 [========================>.....] - ETA: 11:33 - loss: 1.4946 - regression_loss: 1.1543 - classification_loss 8535/10000 [========================>.....] - ETA: 11:32 - loss: 1.4945 - regression_loss: 1.1542 - classification_loss 8536/10000 [========================>.....] - ETA: 11:32 - loss: 1.4946 - regression_loss: 1.1543 - classification_loss 8537/10000 [========================>.....] - ETA: 11:31 - loss: 1.4946 - regression_loss: 1.1543 - classification_loss 8538/10000 [========================>.....] - ETA: 11:31 - loss: 1.4945 - regression_loss: 1.1543 - classification_loss 8539/10000 [========================>.....] - ETA: 11:30 - loss: 1.4944 - regression_loss: 1.1542 - classification_loss 8540/10000 [========================>.....] - ETA: 11:30 - loss: 1.4944 - regression_loss: 1.1542 - classification_loss 8541/10000 [========================>.....] - ETA: 11:29 - loss: 1.4944 - regression_loss: 1.1541 - classification_loss 8542/10000 [========================>.....] - ETA: 11:29 - loss: 1.4944 - regression_loss: 1.1541 - classification_loss 8543/10000 [========================>.....] - ETA: 11:28 - loss: 1.4945 - regression_loss: 1.1542 - classification_loss 8544/10000 [========================>.....] - ETA: 11:28 - loss: 1.4945 - regression_loss: 1.1542 - classification_loss 8545/10000 [========================>.....] - ETA: 11:27 - loss: 1.4945 - regression_loss: 1.1542 - classification_loss 8546/10000 [========================>.....] - ETA: 11:27 - loss: 1.4947 - regression_loss: 1.1543 - classification_loss 8547/10000 [========================>.....] - ETA: 11:26 - loss: 1.4947 - regression_loss: 1.1544 - classification_loss 8548/10000 [========================>.....] - ETA: 11:26 - loss: 1.4948 - regression_loss: 1.1544 - classification_loss 8549/10000 [========================>.....] - ETA: 11:25 - loss: 1.4948 - regression_loss: 1.1545 - classification_loss 8550/10000 [========================>.....] - ETA: 11:25 - loss: 1.4949 - regression_loss: 1.1545 - classification_loss 8551/10000 [========================>.....] - ETA: 11:25 - loss: 1.4948 - regression_loss: 1.1545 - classification_loss 8552/10000 [========================>.....] - ETA: 11:24 - loss: 1.4947 - regression_loss: 1.1544 - classification_loss 8553/10000 [========================>.....] - ETA: 11:24 - loss: 1.4947 - regression_loss: 1.1544 - classification_loss 8554/10000 [========================>.....] - ETA: 11:23 - loss: 1.4946 - regression_loss: 1.1543 - classification_loss 8555/10000 [========================>.....] - ETA: 11:23 - loss: 1.4946 - regression_loss: 1.1543 - classification_loss 8556/10000 [========================>.....] - ETA: 11:22 - loss: 1.4949 - regression_loss: 1.1545 - classification_loss 8557/10000 [========================>.....] - ETA: 11:22 - loss: 1.4949 - regression_loss: 1.1546 - classification_loss 8558/10000 [========================>.....] - ETA: 11:21 - loss: 1.4949 - regression_loss: 1.1545 - classification_loss 8559/10000 [========================>.....] - ETA: 11:21 - loss: 1.4949 - regression_loss: 1.1545 - classification_loss 8560/10000 [========================>.....] - ETA: 11:20 - loss: 1.4949 - regression_loss: 1.1545 - classification_loss 8561/10000 [========================>.....] - ETA: 11:20 - loss: 1.4948 - regression_loss: 1.1544 - classification_loss 8562/10000 [========================>.....] - ETA: 11:19 - loss: 1.4949 - regression_loss: 1.1545 - classification_loss 8563/10000 [========================>.....] - ETA: 11:19 - loss: 1.4948 - regression_loss: 1.1544 - classification_loss 8564/10000 [========================>.....] - ETA: 11:18 - loss: 1.4948 - regression_loss: 1.1544 - classification_loss 8565/10000 [========================>.....] - ETA: 11:18 - loss: 1.4947 - regression_loss: 1.1544 - classification_loss 8566/10000 [========================>.....] - ETA: 11:17 - loss: 1.4948 - regression_loss: 1.1544 - classification_loss 8567/10000 [========================>.....] - ETA: 11:17 - loss: 1.4947 - regression_loss: 1.1543 - classification_loss 8568/10000 [========================>.....] - ETA: 11:16 - loss: 1.4947 - regression_loss: 1.1544 - classification_loss 8569/10000 [========================>.....] - ETA: 11:16 - loss: 1.4946 - regression_loss: 1.1543 - classification_loss 8570/10000 [========================>.....] - ETA: 11:15 - loss: 1.4946 - regression_loss: 1.1543 - classification_loss 8571/10000 [========================>.....] - ETA: 11:15 - loss: 1.4947 - regression_loss: 1.1544 - classification_loss 8572/10000 [========================>.....] - ETA: 11:15 - loss: 1.4948 - regression_loss: 1.1545 - classification_loss 8573/10000 [========================>.....] - ETA: 11:14 - loss: 1.4949 - regression_loss: 1.1545 - classification_loss 8574/10000 [========================>.....] - ETA: 11:14 - loss: 1.4949 - regression_loss: 1.1545 - classification_loss 8575/10000 [========================>.....] - ETA: 11:13 - loss: 1.4950 - regression_loss: 1.1546 - classification_loss 8576/10000 [========================>.....] - ETA: 11:13 - loss: 1.4950 - regression_loss: 1.1547 - classification_loss 8577/10000 [========================>.....] - ETA: 11:12 - loss: 1.4949 - regression_loss: 1.1545 - classification_loss 8578/10000 [========================>.....] - ETA: 11:12 - loss: 1.4948 - regression_loss: 1.1544 - classification_loss 8579/10000 [========================>.....] - ETA: 11:11 - loss: 1.4947 - regression_loss: 1.1543 - classification_loss 8580/10000 [========================>.....] - ETA: 11:11 - loss: 1.4947 - regression_loss: 1.1544 - classification_loss 8581/10000 [========================>.....] - ETA: 11:10 - loss: 1.4948 - regression_loss: 1.1544 - classification_loss 8582/10000 [========================>.....] - ETA: 11:10 - loss: 1.4947 - regression_loss: 1.1544 - classification_loss 8583/10000 [========================>.....] - ETA: 11:09 - loss: 1.4946 - regression_loss: 1.1543 - classification_loss 8584/10000 [========================>.....] - ETA: 11:09 - loss: 1.4946 - regression_loss: 1.1542 - classification_loss 8585/10000 [========================>.....] - ETA: 11:08 - loss: 1.4947 - regression_loss: 1.1543 - classification_loss 8586/10000 [========================>.....] - ETA: 11:08 - loss: 1.4948 - regression_loss: 1.1544 - classification_loss 8587/10000 [========================>.....] - ETA: 11:07 - loss: 1.4948 - regression_loss: 1.1544 - classification_loss 8588/10000 [========================>.....] - ETA: 11:07 - loss: 1.4948 - regression_loss: 1.1544 - classification_loss 8589/10000 [========================>.....] - ETA: 11:07 - loss: 1.4948 - regression_loss: 1.1544 - classification_loss 8590/10000 [========================>.....] - ETA: 11:06 - loss: 1.4949 - regression_loss: 1.1545 - classification_loss 8591/10000 [========================>.....] - ETA: 11:06 - loss: 1.4949 - regression_loss: 1.1545 - classification_loss 8592/10000 [========================>.....] - ETA: 11:05 - loss: 1.4950 - regression_loss: 1.1546 - classification_loss 8593/10000 [========================>.....] - ETA: 11:05 - loss: 1.4950 - regression_loss: 1.1546 - classification_loss 8594/10000 [========================>.....] - ETA: 11:04 - loss: 1.4950 - regression_loss: 1.1546 - classification_loss 8595/10000 [========================>.....] - ETA: 11:04 - loss: 1.4949 - regression_loss: 1.1545 - classification_loss 8596/10000 [========================>.....] - ETA: 11:03 - loss: 1.4950 - regression_loss: 1.1546 - classification_loss 8597/10000 [========================>.....] - ETA: 11:03 - loss: 1.4950 - regression_loss: 1.1546 - classification_loss 8598/10000 [========================>.....] - ETA: 11:02 - loss: 1.4951 - regression_loss: 1.1546 - classification_loss 8599/10000 [========================>.....] - ETA: 11:02 - loss: 1.4950 - regression_loss: 1.1545 - classification_loss 8600/10000 [========================>.....] - ETA: 11:01 - loss: 1.4950 - regression_loss: 1.1546 - classification_loss 8601/10000 [========================>.....] - ETA: 11:01 - loss: 1.4951 - regression_loss: 1.1546 - classification_loss 8602/10000 [========================>.....] - ETA: 11:00 - loss: 1.4950 - regression_loss: 1.1545 - classification_loss 8603/10000 [========================>.....] - ETA: 11:00 - loss: 1.4953 - regression_loss: 1.1546 - classification_loss 8604/10000 [========================>.....] - ETA: 10:59 - loss: 1.4953 - regression_loss: 1.1546 - classification_loss 8605/10000 [========================>.....] - ETA: 10:59 - loss: 1.4953 - regression_loss: 1.1546 - classification_loss 8606/10000 [========================>.....] - ETA: 10:58 - loss: 1.4954 - regression_loss: 1.1547 - classification_loss 8607/10000 [========================>.....] - ETA: 10:58 - loss: 1.4954 - regression_loss: 1.1548 - classification_loss 8608/10000 [========================>.....] - ETA: 10:57 - loss: 1.4954 - regression_loss: 1.1547 - classification_loss 8609/10000 [========================>.....] - ETA: 10:57 - loss: 1.4954 - regression_loss: 1.1547 - classification_loss 8610/10000 [========================>.....] - ETA: 10:57 - loss: 1.4953 - regression_loss: 1.1547 - classification_loss 8611/10000 [========================>.....] - ETA: 10:56 - loss: 1.4955 - regression_loss: 1.1547 - classification_loss 8612/10000 [========================>.....] - ETA: 10:56 - loss: 1.4955 - regression_loss: 1.1548 - classification_loss 8613/10000 [========================>.....] - ETA: 10:55 - loss: 1.4956 - regression_loss: 1.1548 - classification_loss 8614/10000 [========================>.....] - ETA: 10:55 - loss: 1.4956 - regression_loss: 1.1548 - classification_loss 8615/10000 [========================>.....] - ETA: 10:54 - loss: 1.4956 - regression_loss: 1.1548 - classification_loss 8616/10000 [========================>.....] - ETA: 10:54 - loss: 1.4955 - regression_loss: 1.1548 - classification_loss 8617/10000 [========================>.....] - ETA: 10:53 - loss: 1.4955 - regression_loss: 1.1547 - classification_loss 8618/10000 [========================>.....] - ETA: 10:53 - loss: 1.4955 - regression_loss: 1.1548 - classification_loss 8619/10000 [========================>.....] - ETA: 10:52 - loss: 1.4955 - regression_loss: 1.1548 - classification_loss 8620/10000 [========================>.....] - ETA: 10:52 - loss: 1.4956 - regression_loss: 1.1548 - classification_loss 8621/10000 [========================>.....] - ETA: 10:51 - loss: 1.4956 - regression_loss: 1.1548 - classification_loss 8622/10000 [========================>.....] - ETA: 10:51 - loss: 1.4955 - regression_loss: 1.1547 - classification_loss 8623/10000 [========================>.....] - ETA: 10:50 - loss: 1.4954 - regression_loss: 1.1546 - classification_loss 8624/10000 [========================>.....] - ETA: 10:50 - loss: 1.4954 - regression_loss: 1.1547 - classification_loss 8625/10000 [========================>.....] - ETA: 10:49 - loss: 1.4954 - regression_loss: 1.1547 - classification_loss 8626/10000 [========================>.....] - ETA: 10:49 - loss: 1.4953 - regression_loss: 1.1546 - classification_loss 8627/10000 [========================>.....] - ETA: 10:48 - loss: 1.4953 - regression_loss: 1.1546 - classification_loss 8628/10000 [========================>.....] - ETA: 10:48 - loss: 1.4952 - regression_loss: 1.1545 - classification_loss 8629/10000 [========================>.....] - ETA: 10:48 - loss: 1.4951 - regression_loss: 1.1544 - classification_loss 8630/10000 [========================>.....] - ETA: 10:47 - loss: 1.4951 - regression_loss: 1.1544 - classification_loss 8631/10000 [========================>.....] - ETA: 10:47 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8632/10000 [========================>.....] - ETA: 10:46 - loss: 1.4949 - regression_loss: 1.1542 - classification_loss 8633/10000 [========================>.....] - ETA: 10:46 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8634/10000 [========================>.....] - ETA: 10:45 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8635/10000 [========================>.....] - ETA: 10:45 - loss: 1.4949 - regression_loss: 1.1543 - classification_loss 8636/10000 [========================>.....] - ETA: 10:44 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8637/10000 [========================>.....] - ETA: 10:44 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8638/10000 [========================>.....] - ETA: 10:43 - loss: 1.4949 - regression_loss: 1.1542 - classification_loss 8639/10000 [========================>.....] - ETA: 10:43 - loss: 1.4949 - regression_loss: 1.1543 - classification_loss 8640/10000 [========================>.....] - ETA: 10:42 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8641/10000 [========================>.....] - ETA: 10:42 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8642/10000 [========================>.....] - ETA: 10:41 - loss: 1.4950 - regression_loss: 1.1544 - classification_loss 8643/10000 [========================>.....] - ETA: 10:41 - loss: 1.4951 - regression_loss: 1.1544 - classification_loss 8644/10000 [========================>.....] - ETA: 10:40 - loss: 1.4951 - regression_loss: 1.1544 - classification_loss 8645/10000 [========================>.....] - ETA: 10:40 - loss: 1.4950 - regression_loss: 1.1544 - classification_loss 8646/10000 [========================>.....] - ETA: 10:39 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8647/10000 [========================>.....] - ETA: 10:39 - loss: 1.4948 - regression_loss: 1.1542 - classification_loss 8648/10000 [========================>.....] - ETA: 10:38 - loss: 1.4947 - regression_loss: 1.1542 - classification_loss 8649/10000 [========================>.....] - ETA: 10:38 - loss: 1.4948 - regression_loss: 1.1542 - classification_loss 8650/10000 [========================>.....] - ETA: 10:38 - loss: 1.4948 - regression_loss: 1.1542 - classification_loss 8651/10000 [========================>.....] - ETA: 10:37 - loss: 1.4950 - regression_loss: 1.1544 - classification_loss 8652/10000 [========================>.....] - ETA: 10:37 - loss: 1.4951 - regression_loss: 1.1544 - classification_loss 8653/10000 [========================>.....] - ETA: 10:36 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8654/10000 [========================>.....] - ETA: 10:36 - loss: 1.4949 - regression_loss: 1.1542 - classification_loss 8655/10000 [========================>.....] - ETA: 10:35 - loss: 1.4949 - regression_loss: 1.1542 - classification_loss 8656/10000 [========================>.....] - ETA: 10:35 - loss: 1.4950 - regression_loss: 1.1543 - classification_loss 8657/10000 [========================>.....] - ETA: 10:34 - loss: 1.4949 - regression_loss: 1.1542 - classification_loss 8658/10000 [========================>.....] - ETA: 10:34 - loss: 1.4948 - regression_loss: 1.1541 - classification_loss 8659/10000 [========================>.....] - ETA: 10:33 - loss: 1.4947 - regression_loss: 1.1541 - classification_loss 8660/10000 [========================>.....] - ETA: 10:33 - loss: 1.4947 - regression_loss: 1.1541 - classification_loss 8661/10000 [========================>.....] - ETA: 10:32 - loss: 1.4946 - regression_loss: 1.1540 - classification_loss 8662/10000 [========================>.....] - ETA: 10:32 - loss: 1.4945 - regression_loss: 1.1539 - classification_loss 8663/10000 [========================>.....] - ETA: 10:31 - loss: 1.4945 - regression_loss: 1.1539 - classification_loss 8664/10000 [========================>.....] - ETA: 10:31 - loss: 1.4944 - regression_loss: 1.1539 - classification_loss 8665/10000 [========================>.....] - ETA: 10:30 - loss: 1.4944 - regression_loss: 1.1539 - classification_loss 8666/10000 [========================>.....] - ETA: 10:30 - loss: 1.4944 - regression_loss: 1.1539 - classification_loss 8667/10000 [=========================>....] - ETA: 10:29 - loss: 1.4945 - regression_loss: 1.1539 - classification_loss 8668/10000 [=========================>....] - ETA: 10:29 - loss: 1.4945 - regression_loss: 1.1540 - classification_loss 8669/10000 [=========================>....] - ETA: 10:28 - loss: 1.4945 - regression_loss: 1.1540 - classification_loss 8670/10000 [=========================>....] - ETA: 10:28 - loss: 1.4943 - regression_loss: 1.1539 - classification_loss 8671/10000 [=========================>....] - ETA: 10:28 - loss: 1.4944 - regression_loss: 1.1539 - classification_loss 8672/10000 [=========================>....] - ETA: 10:27 - loss: 1.4944 - regression_loss: 1.1539 - classification_loss 8673/10000 [=========================>....] - ETA: 10:27 - loss: 1.4945 - regression_loss: 1.1539 - classification_loss 8674/10000 [=========================>....] - ETA: 10:26 - loss: 1.4944 - regression_loss: 1.1539 - classification_loss 8675/10000 [=========================>....] - ETA: 10:26 - loss: 1.4943 - regression_loss: 1.1538 - classification_loss 8676/10000 [=========================>....] - ETA: 10:25 - loss: 1.4942 - regression_loss: 1.1538 - classification_loss 8677/10000 [=========================>....] - ETA: 10:25 - loss: 1.4943 - regression_loss: 1.1539 - classification_loss 8678/10000 [=========================>....] - ETA: 10:24 - loss: 1.4943 - regression_loss: 1.1538 - classification_loss 8679/10000 [=========================>....] - ETA: 10:24 - loss: 1.4943 - regression_loss: 1.1539 - classification_loss 8680/10000 [=========================>....] - ETA: 10:23 - loss: 1.4943 - regression_loss: 1.1539 - classification_loss 8681/10000 [=========================>....] - ETA: 10:23 - loss: 1.4943 - regression_loss: 1.1539 - classification_loss 8682/10000 [=========================>....] - ETA: 10:22 - loss: 1.4943 - regression_loss: 1.1539 - classification_loss 8683/10000 [=========================>....] - ETA: 10:22 - loss: 1.4944 - regression_loss: 1.1539 - classification_loss 8684/10000 [=========================>....] - ETA: 10:21 - loss: 1.4944 - regression_loss: 1.1539 - classification_loss 8685/10000 [=========================>....] - ETA: 10:21 - loss: 1.4944 - regression_loss: 1.1539 - classification_loss 8686/10000 [=========================>....] - ETA: 10:20 - loss: 1.4944 - regression_loss: 1.1539 - classification_loss 8687/10000 [=========================>....] - ETA: 10:20 - loss: 1.4944 - regression_loss: 1.1539 - classification_loss 8688/10000 [=========================>....] - ETA: 10:19 - loss: 1.4943 - regression_loss: 1.1539 - classification_loss 8689/10000 [=========================>....] - ETA: 10:19 - loss: 1.4944 - regression_loss: 1.1539 - classification_loss 8690/10000 [=========================>....] - ETA: 10:18 - loss: 1.4944 - regression_loss: 1.1539 - classification_loss 8691/10000 [=========================>....] - ETA: 10:18 - loss: 1.4944 - regression_loss: 1.1539 - classification_loss 8692/10000 [=========================>....] - ETA: 10:18 - loss: 1.4943 - regression_loss: 1.1539 - classification_loss 8693/10000 [=========================>....] - ETA: 10:17 - loss: 1.4942 - regression_loss: 1.1538 - classification_loss 8694/10000 [=========================>....] - ETA: 10:17 - loss: 1.4944 - regression_loss: 1.1540 - classification_loss 8695/10000 [=========================>....] - ETA: 10:16 - loss: 1.4943 - regression_loss: 1.1539 - classification_loss 8696/10000 [=========================>....] - ETA: 10:16 - loss: 1.4942 - regression_loss: 1.1539 - classification_loss 8697/10000 [=========================>....] - ETA: 10:15 - loss: 1.4942 - regression_loss: 1.1539 - classification_loss 8698/10000 [=========================>....] - ETA: 10:15 - loss: 1.4942 - regression_loss: 1.1539 - classification_loss 8699/10000 [=========================>....] - ETA: 10:14 - loss: 1.4943 - regression_loss: 1.1539 - classification_loss 8700/10000 [=========================>....] - ETA: 10:14 - loss: 1.4943 - regression_loss: 1.1540 - classification_loss 8701/10000 [=========================>....] - ETA: 10:13 - loss: 1.4942 - regression_loss: 1.1539 - classification_loss 8702/10000 [=========================>....] - ETA: 10:13 - loss: 1.4943 - regression_loss: 1.1539 - classification_loss 8703/10000 [=========================>....] - ETA: 10:12 - loss: 1.4943 - regression_loss: 1.1540 - classification_loss 8704/10000 [=========================>....] - ETA: 10:12 - loss: 1.4944 - regression_loss: 1.1541 - classification_loss 8705/10000 [=========================>....] - ETA: 10:11 - loss: 1.4944 - regression_loss: 1.1541 - classification_loss 8706/10000 [=========================>....] - ETA: 10:11 - loss: 1.4945 - regression_loss: 1.1542 - classification_loss 8707/10000 [=========================>....] - ETA: 10:10 - loss: 1.4945 - regression_loss: 1.1542 - classification_loss 8708/10000 [=========================>....] - ETA: 10:10 - loss: 1.4945 - regression_loss: 1.1542 - classification_loss 8709/10000 [=========================>....] - ETA: 10:09 - loss: 1.4944 - regression_loss: 1.1541 - classification_loss 8710/10000 [=========================>....] - ETA: 10:09 - loss: 1.4944 - regression_loss: 1.1542 - classification_loss 8711/10000 [=========================>....] - ETA: 10:08 - loss: 1.4943 - regression_loss: 1.1541 - classification_loss 8712/10000 [=========================>....] - ETA: 10:08 - loss: 1.4944 - regression_loss: 1.1542 - classification_loss 8713/10000 [=========================>....] - ETA: 10:08 - loss: 1.4944 - regression_loss: 1.1541 - classification_loss 8714/10000 [=========================>....] - ETA: 10:07 - loss: 1.4943 - regression_loss: 1.1541 - classification_loss 8715/10000 [=========================>....] - ETA: 10:07 - loss: 1.4944 - regression_loss: 1.1541 - classification_loss 8716/10000 [=========================>....] - ETA: 10:06 - loss: 1.4944 - regression_loss: 1.1542 - classification_loss 8717/10000 [=========================>....] - ETA: 10:06 - loss: 1.4945 - regression_loss: 1.1542 - classification_loss 8718/10000 [=========================>....] - ETA: 10:05 - loss: 1.4944 - regression_loss: 1.1542 - classification_loss 8719/10000 [=========================>....] - ETA: 10:05 - loss: 1.4943 - regression_loss: 1.1541 - classification_loss 8720/10000 [=========================>....] - ETA: 10:04 - loss: 1.4944 - regression_loss: 1.1542 - classification_loss 8721/10000 [=========================>....] - ETA: 10:04 - loss: 1.4943 - regression_loss: 1.1541 - classification_loss 8722/10000 [=========================>....] - ETA: 10:03 - loss: 1.4942 - regression_loss: 1.1540 - classification_loss 8723/10000 [=========================>....] - ETA: 10:03 - loss: 1.4941 - regression_loss: 1.1540 - classification_loss 8724/10000 [=========================>....] - ETA: 10:02 - loss: 1.4942 - regression_loss: 1.1540 - classification_loss 8725/10000 [=========================>....] - ETA: 10:02 - loss: 1.4941 - regression_loss: 1.1540 - classification_loss 8726/10000 [=========================>....] - ETA: 10:01 - loss: 1.4941 - regression_loss: 1.1540 - classification_loss 8727/10000 [=========================>....] - ETA: 10:01 - loss: 1.4941 - regression_loss: 1.1539 - classification_loss 8728/10000 [=========================>....] - ETA: 10:00 - loss: 1.4940 - regression_loss: 1.1539 - classification_loss 8729/10000 [=========================>....] - ETA: 10:00 - loss: 1.4940 - regression_loss: 1.1539 - classification_loss 8730/10000 [=========================>....] - ETA: 9:59 - loss: 1.4940 - regression_loss: 1.1538 - classification_loss: 8731/10000 [=========================>....] - ETA: 9:59 - loss: 1.4941 - regression_loss: 1.1539 - classification_loss: 8732/10000 [=========================>....] - ETA: 9:59 - loss: 1.4941 - regression_loss: 1.1539 - classification_loss: 8733/10000 [=========================>....] - ETA: 9:58 - loss: 1.4941 - regression_loss: 1.1539 - classification_loss: 8734/10000 [=========================>....] - ETA: 9:58 - loss: 1.4941 - regression_loss: 1.1539 - classification_loss: 8735/10000 [=========================>....] - ETA: 9:57 - loss: 1.4941 - regression_loss: 1.1539 - classification_loss: 8736/10000 [=========================>....] - ETA: 9:57 - loss: 1.4941 - regression_loss: 1.1539 - classification_loss: 8737/10000 [=========================>....] - ETA: 9:56 - loss: 1.4940 - regression_loss: 1.1538 - classification_loss: 8738/10000 [=========================>....] - ETA: 9:56 - loss: 1.4939 - regression_loss: 1.1538 - classification_loss: 8739/10000 [=========================>....] - ETA: 9:55 - loss: 1.4940 - regression_loss: 1.1538 - classification_loss: 8740/10000 [=========================>....] - ETA: 9:55 - loss: 1.4939 - regression_loss: 1.1537 - classification_loss: 8741/10000 [=========================>....] - ETA: 9:54 - loss: 1.4939 - regression_loss: 1.1538 - classification_loss: 8742/10000 [=========================>....] - ETA: 9:54 - loss: 1.4939 - regression_loss: 1.1538 - classification_loss: 8743/10000 [=========================>....] - ETA: 9:53 - loss: 1.4938 - regression_loss: 1.1537 - classification_loss: 8744/10000 [=========================>....] - ETA: 9:53 - loss: 1.4939 - regression_loss: 1.1537 - classification_loss: 8745/10000 [=========================>....] - ETA: 9:52 - loss: 1.4939 - regression_loss: 1.1538 - classification_loss: 8746/10000 [=========================>....] - ETA: 9:52 - loss: 1.4940 - regression_loss: 1.1538 - classification_loss: 8747/10000 [=========================>....] - ETA: 9:51 - loss: 1.4941 - regression_loss: 1.1538 - classification_loss: 8748/10000 [=========================>....] - ETA: 9:51 - loss: 1.4941 - regression_loss: 1.1539 - classification_loss: 8749/10000 [=========================>....] - ETA: 9:50 - loss: 1.4940 - regression_loss: 1.1538 - classification_loss: 8750/10000 [=========================>....] - ETA: 9:50 - loss: 1.4941 - regression_loss: 1.1539 - classification_loss: 8751/10000 [=========================>....] - ETA: 9:50 - loss: 1.4942 - regression_loss: 1.1540 - classification_loss: 8752/10000 [=========================>....] - ETA: 9:49 - loss: 1.4942 - regression_loss: 1.1540 - classification_loss: 8753/10000 [=========================>....] - ETA: 9:49 - loss: 1.4942 - regression_loss: 1.1539 - classification_loss: 8754/10000 [=========================>....] - ETA: 9:48 - loss: 1.4941 - regression_loss: 1.1539 - classification_loss: 8755/10000 [=========================>....] - ETA: 9:48 - loss: 1.4942 - regression_loss: 1.1540 - classification_loss: 8756/10000 [=========================>....] - ETA: 9:47 - loss: 1.4942 - regression_loss: 1.1540 - classification_loss: 8757/10000 [=========================>....] - ETA: 9:47 - loss: 1.4942 - regression_loss: 1.1539 - classification_loss: 8758/10000 [=========================>....] - ETA: 9:46 - loss: 1.4942 - regression_loss: 1.1539 - classification_loss: 8759/10000 [=========================>....] - ETA: 9:46 - loss: 1.4942 - regression_loss: 1.1539 - classification_loss: 8760/10000 [=========================>....] - ETA: 9:45 - loss: 1.4941 - regression_loss: 1.1538 - classification_loss: 8761/10000 [=========================>....] - ETA: 9:45 - loss: 1.4940 - regression_loss: 1.1538 - classification_loss: 8762/10000 [=========================>....] - ETA: 9:44 - loss: 1.4941 - regression_loss: 1.1538 - classification_loss: 8763/10000 [=========================>....] - ETA: 9:44 - loss: 1.4941 - regression_loss: 1.1539 - classification_loss: 8764/10000 [=========================>....] - ETA: 9:43 - loss: 1.4940 - regression_loss: 1.1538 - classification_loss: 8765/10000 [=========================>....] - ETA: 9:43 - loss: 1.4941 - regression_loss: 1.1539 - classification_loss: 8766/10000 [=========================>....] - ETA: 9:42 - loss: 1.4941 - regression_loss: 1.1538 - classification_loss: 8767/10000 [=========================>....] - ETA: 9:42 - loss: 1.4941 - regression_loss: 1.1538 - classification_loss: 8768/10000 [=========================>....] - ETA: 9:41 - loss: 1.4941 - regression_loss: 1.1538 - classification_loss: 8769/10000 [=========================>....] - ETA: 9:41 - loss: 1.4941 - regression_loss: 1.1538 - classification_loss: 8770/10000 [=========================>....] - ETA: 9:41 - loss: 1.4941 - regression_loss: 1.1538 - classification_loss: 8771/10000 [=========================>....] - ETA: 9:40 - loss: 1.4940 - regression_loss: 1.1537 - classification_loss: 8772/10000 [=========================>....] - ETA: 9:40 - loss: 1.4940 - regression_loss: 1.1537 - classification_loss: 8773/10000 [=========================>....] - ETA: 9:39 - loss: 1.4940 - regression_loss: 1.1537 - classification_loss: 8774/10000 [=========================>....] - ETA: 9:39 - loss: 1.4940 - regression_loss: 1.1538 - classification_loss: 8775/10000 [=========================>....] - ETA: 9:38 - loss: 1.4939 - regression_loss: 1.1537 - classification_loss: 8776/10000 [=========================>....] - ETA: 9:38 - loss: 1.4940 - regression_loss: 1.1537 - classification_loss: 8777/10000 [=========================>....] - ETA: 9:37 - loss: 1.4939 - regression_loss: 1.1537 - classification_loss: 8778/10000 [=========================>....] - ETA: 9:37 - loss: 1.4939 - regression_loss: 1.1537 - classification_loss: 8779/10000 [=========================>....] - ETA: 9:36 - loss: 1.4940 - regression_loss: 1.1538 - classification_loss: 8780/10000 [=========================>....] - ETA: 9:36 - loss: 1.4940 - regression_loss: 1.1538 - classification_loss: 8781/10000 [=========================>....] - ETA: 9:35 - loss: 1.4940 - regression_loss: 1.1538 - classification_loss: 8782/10000 [=========================>....] - ETA: 9:35 - loss: 1.4940 - regression_loss: 1.1538 - classification_loss: 8783/10000 [=========================>....] - ETA: 9:34 - loss: 1.4940 - regression_loss: 1.1538 - classification_loss: 8784/10000 [=========================>....] - ETA: 9:34 - loss: 1.4941 - regression_loss: 1.1539 - classification_loss: 8785/10000 [=========================>....] - ETA: 9:33 - loss: 1.4940 - regression_loss: 1.1538 - classification_loss: 8786/10000 [=========================>....] - ETA: 9:33 - loss: 1.4940 - regression_loss: 1.1538 - classification_loss: 8787/10000 [=========================>....] - ETA: 9:32 - loss: 1.4939 - regression_loss: 1.1537 - classification_loss: 8788/10000 [=========================>....] - ETA: 9:32 - loss: 1.4939 - regression_loss: 1.1537 - classification_loss: 8789/10000 [=========================>....] - ETA: 9:31 - loss: 1.4939 - regression_loss: 1.1537 - classification_loss: 8790/10000 [=========================>....] - ETA: 9:31 - loss: 1.4939 - regression_loss: 1.1537 - classification_loss: 8791/10000 [=========================>....] - ETA: 9:31 - loss: 1.4939 - regression_loss: 1.1538 - classification_loss: 8792/10000 [=========================>....] - ETA: 9:30 - loss: 1.4940 - regression_loss: 1.1538 - classification_loss: 8793/10000 [=========================>....] - ETA: 9:30 - loss: 1.4939 - regression_loss: 1.1537 - classification_loss: 8794/10000 [=========================>....] - ETA: 9:29 - loss: 1.4939 - regression_loss: 1.1538 - classification_loss: 8795/10000 [=========================>....] - ETA: 9:29 - loss: 1.4939 - regression_loss: 1.1537 - classification_loss: 8796/10000 [=========================>....] - ETA: 9:28 - loss: 1.4940 - regression_loss: 1.1538 - classification_loss: 8797/10000 [=========================>....] - ETA: 9:28 - loss: 1.4940 - regression_loss: 1.1538 - classification_loss: 8798/10000 [=========================>....] - ETA: 9:27 - loss: 1.4939 - regression_loss: 1.1537 - classification_loss: 8799/10000 [=========================>....] - ETA: 9:27 - loss: 1.4938 - regression_loss: 1.1536 - classification_loss: 8800/10000 [=========================>....] - ETA: 9:26 - loss: 1.4937 - regression_loss: 1.1536 - classification_loss: 8801/10000 [=========================>....] - ETA: 9:26 - loss: 1.4937 - regression_loss: 1.1535 - classification_loss: 8802/10000 [=========================>....] - ETA: 9:25 - loss: 1.4937 - regression_loss: 1.1536 - classification_loss: 8803/10000 [=========================>....] - ETA: 9:25 - loss: 1.4937 - regression_loss: 1.1536 - classification_loss: 8804/10000 [=========================>....] - ETA: 9:24 - loss: 1.4938 - regression_loss: 1.1536 - classification_loss: 8805/10000 [=========================>....] - ETA: 9:24 - loss: 1.4937 - regression_loss: 1.1536 - classification_loss: 8806/10000 [=========================>....] - ETA: 9:23 - loss: 1.4937 - regression_loss: 1.1536 - classification_loss: 8807/10000 [=========================>....] - ETA: 9:23 - loss: 1.4937 - regression_loss: 1.1535 - classification_loss: 8808/10000 [=========================>....] - ETA: 9:22 - loss: 1.4936 - regression_loss: 1.1535 - classification_loss: 8809/10000 [=========================>....] - ETA: 9:22 - loss: 1.4937 - regression_loss: 1.1535 - classification_loss: 8810/10000 [=========================>....] - ETA: 9:22 - loss: 1.4936 - regression_loss: 1.1535 - classification_loss: 8811/10000 [=========================>....] - ETA: 9:21 - loss: 1.4936 - regression_loss: 1.1535 - classification_loss: 8812/10000 [=========================>....] - ETA: 9:21 - loss: 1.4937 - regression_loss: 1.1536 - classification_loss: 8813/10000 [=========================>....] - ETA: 9:20 - loss: 1.4936 - regression_loss: 1.1535 - classification_loss: 8814/10000 [=========================>....] - ETA: 9:20 - loss: 1.4936 - regression_loss: 1.1535 - classification_loss: 8815/10000 [=========================>....] - ETA: 9:19 - loss: 1.4937 - regression_loss: 1.1536 - classification_loss: 8816/10000 [=========================>....] - ETA: 9:19 - loss: 1.4937 - regression_loss: 1.1536 - classification_loss: 8817/10000 [=========================>....] - ETA: 9:18 - loss: 1.4938 - regression_loss: 1.1536 - classification_loss: 8818/10000 [=========================>....] - ETA: 9:18 - loss: 1.4937 - regression_loss: 1.1536 - classification_loss: 8819/10000 [=========================>....] - ETA: 9:17 - loss: 1.4936 - regression_loss: 1.1535 - classification_loss: 8820/10000 [=========================>....] - ETA: 9:17 - loss: 1.4937 - regression_loss: 1.1535 - classification_loss: 8821/10000 [=========================>....] - ETA: 9:16 - loss: 1.4937 - regression_loss: 1.1536 - classification_loss: 8822/10000 [=========================>....] - ETA: 9:16 - loss: 1.4937 - regression_loss: 1.1536 - classification_loss: 8823/10000 [=========================>....] - ETA: 9:15 - loss: 1.4937 - regression_loss: 1.1536 - classification_loss: 8824/10000 [=========================>....] - ETA: 9:15 - loss: 1.4937 - regression_loss: 1.1537 - classification_loss: 8825/10000 [=========================>....] - ETA: 9:14 - loss: 1.4937 - regression_loss: 1.1537 - classification_loss: 8826/10000 [=========================>....] - ETA: 9:14 - loss: 1.4936 - regression_loss: 1.1536 - classification_loss: 8827/10000 [=========================>....] - ETA: 9:13 - loss: 1.4935 - regression_loss: 1.1535 - classification_loss: 8828/10000 [=========================>....] - ETA: 9:13 - loss: 1.4936 - regression_loss: 1.1536 - classification_loss: 8829/10000 [=========================>....] - ETA: 9:13 - loss: 1.4936 - regression_loss: 1.1536 - classification_loss: 8830/10000 [=========================>....] - ETA: 9:12 - loss: 1.4936 - regression_loss: 1.1536 - classification_loss: 8831/10000 [=========================>....] - ETA: 9:12 - loss: 1.4936 - regression_loss: 1.1536 - classification_loss: 8832/10000 [=========================>....] - ETA: 9:11 - loss: 1.4937 - regression_loss: 1.1537 - classification_loss: 8833/10000 [=========================>....] - ETA: 9:11 - loss: 1.4937 - regression_loss: 1.1537 - classification_loss: 8834/10000 [=========================>....] - ETA: 9:10 - loss: 1.4936 - regression_loss: 1.1537 - classification_loss: 8835/10000 [=========================>....] - ETA: 9:10 - loss: 1.4937 - regression_loss: 1.1538 - classification_loss: 8836/10000 [=========================>....] - ETA: 9:09 - loss: 1.4938 - regression_loss: 1.1538 - classification_loss: 8837/10000 [=========================>....] - ETA: 9:09 - loss: 1.4939 - regression_loss: 1.1539 - classification_loss: 8838/10000 [=========================>....] - ETA: 9:08 - loss: 1.4940 - regression_loss: 1.1540 - classification_loss: 8839/10000 [=========================>....] - ETA: 9:08 - loss: 1.4941 - regression_loss: 1.1541 - classification_loss: 8840/10000 [=========================>....] - ETA: 9:07 - loss: 1.4941 - regression_loss: 1.1540 - classification_loss: 8841/10000 [=========================>....] - ETA: 9:07 - loss: 1.4940 - regression_loss: 1.1540 - classification_loss: 8842/10000 [=========================>....] - ETA: 9:06 - loss: 1.4941 - regression_loss: 1.1540 - classification_loss: 8843/10000 [=========================>....] - ETA: 9:06 - loss: 1.4941 - regression_loss: 1.1540 - classification_loss: 8844/10000 [=========================>....] - ETA: 9:05 - loss: 1.4941 - regression_loss: 1.1540 - classification_loss: 8845/10000 [=========================>....] - ETA: 9:05 - loss: 1.4942 - regression_loss: 1.1541 - classification_loss: 8846/10000 [=========================>....] - ETA: 9:04 - loss: 1.4941 - regression_loss: 1.1541 - classification_loss: 8847/10000 [=========================>....] - ETA: 9:04 - loss: 1.4941 - regression_loss: 1.1540 - classification_loss: 8848/10000 [=========================>....] - ETA: 9:04 - loss: 1.4940 - regression_loss: 1.1539 - classification_loss: 8849/10000 [=========================>....] - ETA: 9:03 - loss: 1.4941 - regression_loss: 1.1540 - classification_loss: 8850/10000 [=========================>....] - ETA: 9:03 - loss: 1.4940 - regression_loss: 1.1539 - classification_loss: 8851/10000 [=========================>....] - ETA: 9:02 - loss: 1.4939 - regression_loss: 1.1539 - classification_loss: 8852/10000 [=========================>....] - ETA: 9:02 - loss: 1.4939 - regression_loss: 1.1539 - classification_loss: 8853/10000 [=========================>....] - ETA: 9:01 - loss: 1.4940 - regression_loss: 1.1539 - classification_loss: 8854/10000 [=========================>....] - ETA: 9:01 - loss: 1.4940 - regression_loss: 1.1540 - classification_loss: 8855/10000 [=========================>....] - ETA: 9:00 - loss: 1.4941 - regression_loss: 1.1541 - classification_loss: 8856/10000 [=========================>....] - ETA: 9:00 - loss: 1.4941 - regression_loss: 1.1541 - classification_loss: 8857/10000 [=========================>....] - ETA: 8:59 - loss: 1.4941 - regression_loss: 1.1541 - classification_loss: 8858/10000 [=========================>....] - ETA: 8:59 - loss: 1.4941 - regression_loss: 1.1541 - classification_loss: 8859/10000 [=========================>....] - ETA: 8:58 - loss: 1.4941 - regression_loss: 1.1541 - classification_loss: 8860/10000 [=========================>....] - ETA: 8:58 - loss: 1.4942 - regression_loss: 1.1542 - classification_loss: 8861/10000 [=========================>....] - ETA: 8:57 - loss: 1.4942 - regression_loss: 1.1542 - classification_loss: 8862/10000 [=========================>....] - ETA: 8:57 - loss: 1.4943 - regression_loss: 1.1543 - classification_loss: 8863/10000 [=========================>....] - ETA: 8:56 - loss: 1.4945 - regression_loss: 1.1544 - classification_loss: 8864/10000 [=========================>....] - ETA: 8:56 - loss: 1.4945 - regression_loss: 1.1545 - classification_loss: 8865/10000 [=========================>....] - ETA: 8:55 - loss: 1.4946 - regression_loss: 1.1545 - classification_loss: 8866/10000 [=========================>....] - ETA: 8:55 - loss: 1.4947 - regression_loss: 1.1547 - classification_loss: 8867/10000 [=========================>....] - ETA: 8:55 - loss: 1.4947 - regression_loss: 1.1547 - classification_loss: 8868/10000 [=========================>....] - ETA: 8:54 - loss: 1.4948 - regression_loss: 1.1547 - classification_loss: 8869/10000 [=========================>....] - ETA: 8:54 - loss: 1.4948 - regression_loss: 1.1547 - classification_loss: 8870/10000 [=========================>....] - ETA: 8:53 - loss: 1.4947 - regression_loss: 1.1547 - classification_loss: 8871/10000 [=========================>....] - ETA: 8:53 - loss: 1.4946 - regression_loss: 1.1546 - classification_loss: 8872/10000 [=========================>....] - ETA: 8:52 - loss: 1.4946 - regression_loss: 1.1545 - classification_loss: 8873/10000 [=========================>....] - ETA: 8:52 - loss: 1.4945 - regression_loss: 1.1545 - classification_loss: 8874/10000 [=========================>....] - ETA: 8:51 - loss: 1.4946 - regression_loss: 1.1545 - classification_loss: 8875/10000 [=========================>....] - ETA: 8:51 - loss: 1.4946 - regression_loss: 1.1545 - classification_loss: 8876/10000 [=========================>....] - ETA: 8:50 - loss: 1.4947 - regression_loss: 1.1546 - classification_loss: 8877/10000 [=========================>....] - ETA: 8:50 - loss: 1.4947 - regression_loss: 1.1546 - classification_loss: 8878/10000 [=========================>....] - ETA: 8:49 - loss: 1.4946 - regression_loss: 1.1545 - classification_loss: 8879/10000 [=========================>....] - ETA: 8:49 - loss: 1.4946 - regression_loss: 1.1545 - classification_loss: 8880/10000 [=========================>....] - ETA: 8:48 - loss: 1.4946 - regression_loss: 1.1545 - classification_loss: 8881/10000 [=========================>....] - ETA: 8:48 - loss: 1.4945 - regression_loss: 1.1545 - classification_loss: 8882/10000 [=========================>....] - ETA: 8:47 - loss: 1.4945 - regression_loss: 1.1544 - classification_loss: 8883/10000 [=========================>....] - ETA: 8:47 - loss: 1.4944 - regression_loss: 1.1544 - classification_loss: 8884/10000 [=========================>....] - ETA: 8:46 - loss: 1.4945 - regression_loss: 1.1544 - classification_loss: 8885/10000 [=========================>....] - ETA: 8:46 - loss: 1.4945 - regression_loss: 1.1545 - classification_loss: 8886/10000 [=========================>....] - ETA: 8:46 - loss: 1.4945 - regression_loss: 1.1544 - classification_loss: 8887/10000 [=========================>....] - ETA: 8:45 - loss: 1.4944 - regression_loss: 1.1544 - classification_loss: 8888/10000 [=========================>....] - ETA: 8:45 - loss: 1.4944 - regression_loss: 1.1544 - classification_loss: 8889/10000 [=========================>....] - ETA: 8:44 - loss: 1.4944 - regression_loss: 1.1544 - classification_loss: 8890/10000 [=========================>....] - ETA: 8:44 - loss: 1.4944 - regression_loss: 1.1543 - classification_loss: 8891/10000 [=========================>....] - ETA: 8:43 - loss: 1.4944 - regression_loss: 1.1543 - classification_loss: 8892/10000 [=========================>....] - ETA: 8:43 - loss: 1.4945 - regression_loss: 1.1544 - classification_loss: 8893/10000 [=========================>....] - ETA: 8:42 - loss: 1.4944 - regression_loss: 1.1544 - classification_loss: 8894/10000 [=========================>....] - ETA: 8:42 - loss: 1.4944 - regression_loss: 1.1543 - classification_loss: 8895/10000 [=========================>....] - ETA: 8:41 - loss: 1.4944 - regression_loss: 1.1544 - classification_loss: 8896/10000 [=========================>....] - ETA: 8:41 - loss: 1.4945 - regression_loss: 1.1544 - classification_loss: 8897/10000 [=========================>....] - ETA: 8:40 - loss: 1.4944 - regression_loss: 1.1544 - classification_loss: 8898/10000 [=========================>....] - ETA: 8:40 - loss: 1.4944 - regression_loss: 1.1544 - classification_loss: 8899/10000 [=========================>....] - ETA: 8:39 - loss: 1.4943 - regression_loss: 1.1543 - classification_loss: 8900/10000 [=========================>....] - ETA: 8:39 - loss: 1.4943 - regression_loss: 1.1542 - classification_loss: 8901/10000 [=========================>....] - ETA: 8:38 - loss: 1.4942 - regression_loss: 1.1542 - classification_loss: 8902/10000 [=========================>....] - ETA: 8:38 - loss: 1.4943 - regression_loss: 1.1543 - classification_loss: 8903/10000 [=========================>....] - ETA: 8:37 - loss: 1.4944 - regression_loss: 1.1543 - classification_loss: 8904/10000 [=========================>....] - ETA: 8:37 - loss: 1.4942 - regression_loss: 1.1542 - classification_loss: 8905/10000 [=========================>....] - ETA: 8:37 - loss: 1.4943 - regression_loss: 1.1543 - classification_loss: 8906/10000 [=========================>....] - ETA: 8:36 - loss: 1.4942 - regression_loss: 1.1542 - classification_loss: 8907/10000 [=========================>....] - ETA: 8:36 - loss: 1.4943 - regression_loss: 1.1543 - classification_loss: 8908/10000 [=========================>....] - ETA: 8:35 - loss: 1.4942 - regression_loss: 1.1542 - classification_loss: 8909/10000 [=========================>....] - ETA: 8:35 - loss: 1.4942 - regression_loss: 1.1542 - classification_loss: 8910/10000 [=========================>....] - ETA: 8:34 - loss: 1.4943 - regression_loss: 1.1543 - classification_loss: 8911/10000 [=========================>....] - ETA: 8:34 - loss: 1.4942 - regression_loss: 1.1542 - classification_loss: 8912/10000 [=========================>....] - ETA: 8:33 - loss: 1.4942 - regression_loss: 1.1542 - classification_loss: 8913/10000 [=========================>....] - ETA: 8:33 - loss: 1.4942 - regression_loss: 1.1542 - classification_loss: 8914/10000 [=========================>....] - ETA: 8:32 - loss: 1.4943 - regression_loss: 1.1543 - classification_loss: 8915/10000 [=========================>....] - ETA: 8:32 - loss: 1.4943 - regression_loss: 1.1543 - classification_loss: 8916/10000 [=========================>....] - ETA: 8:31 - loss: 1.4943 - regression_loss: 1.1542 - classification_loss: 8917/10000 [=========================>....] - ETA: 8:31 - loss: 1.4944 - regression_loss: 1.1543 - classification_loss: 8918/10000 [=========================>....] - ETA: 8:30 - loss: 1.4944 - regression_loss: 1.1543 - classification_loss: 8919/10000 [=========================>....] - ETA: 8:30 - loss: 1.4944 - regression_loss: 1.1543 - classification_loss: 8920/10000 [=========================>....] - ETA: 8:29 - loss: 1.4943 - regression_loss: 1.1542 - classification_loss: 8921/10000 [=========================>....] - ETA: 8:29 - loss: 1.4943 - regression_loss: 1.1542 - classification_loss: 8922/10000 [=========================>....] - ETA: 8:28 - loss: 1.4943 - regression_loss: 1.1542 - classification_loss: 8923/10000 [=========================>....] - ETA: 8:28 - loss: 1.4942 - regression_loss: 1.1542 - classification_loss: 8924/10000 [=========================>....] - ETA: 8:28 - loss: 1.4943 - regression_loss: 1.1542 - classification_loss: 8925/10000 [=========================>....] - ETA: 8:27 - loss: 1.4943 - regression_loss: 1.1542 - classification_loss: 8926/10000 [=========================>....] - ETA: 8:27 - loss: 1.4942 - regression_loss: 1.1542 - classification_loss: 8927/10000 [=========================>....] - ETA: 8:26 - loss: 1.4942 - regression_loss: 1.1542 - classification_loss: 8928/10000 [=========================>....] - ETA: 8:26 - loss: 1.4943 - regression_loss: 1.1543 - classification_loss: 8929/10000 [=========================>....] - ETA: 8:25 - loss: 1.4943 - regression_loss: 1.1543 - classification_loss: 8930/10000 [=========================>....] - ETA: 8:25 - loss: 1.4943 - regression_loss: 1.1543 - classification_loss: 8931/10000 [=========================>....] - ETA: 8:24 - loss: 1.4942 - regression_loss: 1.1542 - classification_loss: 8932/10000 [=========================>....] - ETA: 8:24 - loss: 1.4942 - regression_loss: 1.1541 - classification_loss: 8933/10000 [=========================>....] - ETA: 8:23 - loss: 1.4941 - regression_loss: 1.1541 - classification_loss: 8934/10000 [=========================>....] - ETA: 8:23 - loss: 1.4940 - regression_loss: 1.1541 - classification_loss: 8935/10000 [=========================>....] - ETA: 8:22 - loss: 1.4941 - regression_loss: 1.1541 - classification_loss: 8936/10000 [=========================>....] - ETA: 8:22 - loss: 1.4939 - regression_loss: 1.1540 - classification_loss: 8937/10000 [=========================>....] - ETA: 8:21 - loss: 1.4939 - regression_loss: 1.1540 - classification_loss: 8938/10000 [=========================>....] - ETA: 8:21 - loss: 1.4939 - regression_loss: 1.1539 - classification_loss: 8939/10000 [=========================>....] - ETA: 8:20 - loss: 1.4938 - regression_loss: 1.1538 - classification_loss: 8940/10000 [=========================>....] - ETA: 8:20 - loss: 1.4938 - regression_loss: 1.1539 - classification_loss: 8941/10000 [=========================>....] - ETA: 8:19 - loss: 1.4938 - regression_loss: 1.1539 - classification_loss: 8942/10000 [=========================>....] - ETA: 8:19 - loss: 1.4938 - regression_loss: 1.1539 - classification_loss: 8943/10000 [=========================>....] - ETA: 8:19 - loss: 1.4937 - regression_loss: 1.1538 - classification_loss: 8944/10000 [=========================>....] - ETA: 8:18 - loss: 1.4938 - regression_loss: 1.1538 - classification_loss: 8945/10000 [=========================>....] - ETA: 8:18 - loss: 1.4937 - regression_loss: 1.1538 - classification_loss: 8946/10000 [=========================>....] - ETA: 8:17 - loss: 1.4936 - regression_loss: 1.1537 - classification_loss: 8947/10000 [=========================>....] - ETA: 8:17 - loss: 1.4936 - regression_loss: 1.1537 - classification_loss: 8948/10000 [=========================>....] - ETA: 8:16 - loss: 1.4937 - regression_loss: 1.1538 - classification_loss: 8949/10000 [=========================>....] - ETA: 8:16 - loss: 1.4936 - regression_loss: 1.1537 - classification_loss: 8950/10000 [=========================>....] - ETA: 8:15 - loss: 1.4936 - regression_loss: 1.1537 - classification_loss: 8951/10000 [=========================>....] - ETA: 8:15 - loss: 1.4936 - regression_loss: 1.1537 - classification_loss: 8952/10000 [=========================>....] - ETA: 8:14 - loss: 1.4936 - regression_loss: 1.1537 - classification_loss: 8953/10000 [=========================>....] - ETA: 8:14 - loss: 1.4936 - regression_loss: 1.1537 - classification_loss: 8954/10000 [=========================>....] - ETA: 8:13 - loss: 1.4935 - regression_loss: 1.1536 - classification_loss: 8955/10000 [=========================>....] - ETA: 8:13 - loss: 1.4936 - regression_loss: 1.1537 - classification_loss: 8956/10000 [=========================>....] - ETA: 8:12 - loss: 1.4936 - regression_loss: 1.1537 - classification_loss: 8957/10000 [=========================>....] - ETA: 8:12 - loss: 1.4938 - regression_loss: 1.1536 - classification_loss: 8958/10000 [=========================>....] - ETA: 8:11 - loss: 1.4937 - regression_loss: 1.1535 - classification_loss: 8959/10000 [=========================>....] - ETA: 8:11 - loss: 1.4937 - regression_loss: 1.1536 - classification_loss: 8960/10000 [=========================>....] - ETA: 8:10 - loss: 1.4936 - regression_loss: 1.1535 - classification_loss: 8961/10000 [=========================>....] - ETA: 8:10 - loss: 1.4935 - regression_loss: 1.1534 - classification_loss: 8962/10000 [=========================>....] - ETA: 8:10 - loss: 1.4935 - regression_loss: 1.1534 - classification_loss: 8963/10000 [=========================>....] - ETA: 8:09 - loss: 1.4936 - regression_loss: 1.1535 - classification_loss: 8964/10000 [=========================>....] - ETA: 8:09 - loss: 1.4936 - regression_loss: 1.1535 - classification_loss: 8965/10000 [=========================>....] - ETA: 8:08 - loss: 1.4937 - regression_loss: 1.1536 - classification_loss: 8966/10000 [=========================>....] - ETA: 8:08 - loss: 1.4938 - regression_loss: 1.1537 - classification_loss: 8967/10000 [=========================>....] - ETA: 8:07 - loss: 1.4937 - regression_loss: 1.1536 - classification_loss: 8968/10000 [=========================>....] - ETA: 8:07 - loss: 1.4938 - regression_loss: 1.1537 - classification_loss: 8969/10000 [=========================>....] - ETA: 8:06 - loss: 1.4939 - regression_loss: 1.1538 - classification_loss: 8970/10000 [=========================>....] - ETA: 8:06 - loss: 1.4939 - regression_loss: 1.1538 - classification_loss: 8971/10000 [=========================>....] - ETA: 8:05 - loss: 1.4939 - regression_loss: 1.1538 - classification_loss: 8972/10000 [=========================>....] - ETA: 8:05 - loss: 1.4938 - regression_loss: 1.1537 - classification_loss: 8973/10000 [=========================>....] - ETA: 8:04 - loss: 1.4938 - regression_loss: 1.1538 - classification_loss: 8974/10000 [=========================>....] - ETA: 8:04 - loss: 1.4938 - regression_loss: 1.1537 - classification_loss: 8975/10000 [=========================>....] - ETA: 8:03 - loss: 1.4938 - regression_loss: 1.1537 - classification_loss: 8976/10000 [=========================>....] - ETA: 8:03 - loss: 1.4939 - regression_loss: 1.1538 - classification_loss: 8977/10000 [=========================>....] - ETA: 8:02 - loss: 1.4939 - regression_loss: 1.1538 - classification_loss: 8978/10000 [=========================>....] - ETA: 8:02 - loss: 1.4940 - regression_loss: 1.1538 - classification_loss: 8979/10000 [=========================>....] - ETA: 8:01 - loss: 1.4940 - regression_loss: 1.1539 - classification_loss: 8980/10000 [=========================>....] - ETA: 8:01 - loss: 1.4940 - regression_loss: 1.1539 - classification_loss: 8981/10000 [=========================>....] - ETA: 8:01 - loss: 1.4939 - regression_loss: 1.1539 - classification_loss: 8982/10000 [=========================>....] - ETA: 8:00 - loss: 1.4939 - regression_loss: 1.1539 - classification_loss: 8983/10000 [=========================>....] - ETA: 8:00 - loss: 1.4940 - regression_loss: 1.1539 - classification_loss: 8984/10000 [=========================>....] - ETA: 7:59 - loss: 1.4940 - regression_loss: 1.1539 - classification_loss: 8985/10000 [=========================>....] - ETA: 7:59 - loss: 1.4939 - regression_loss: 1.1539 - classification_loss: 8986/10000 [=========================>....] - ETA: 7:58 - loss: 1.4940 - regression_loss: 1.1539 - classification_loss: 8987/10000 [=========================>....] - ETA: 7:58 - loss: 1.4940 - regression_loss: 1.1539 - classification_loss: 8988/10000 [=========================>....] - ETA: 7:57 - loss: 1.4939 - regression_loss: 1.1538 - classification_loss: 8989/10000 [=========================>....] - ETA: 7:57 - loss: 1.4938 - regression_loss: 1.1538 - classification_loss: 8990/10000 [=========================>....] - ETA: 7:56 - loss: 1.4938 - regression_loss: 1.1538 - classification_loss: 8991/10000 [=========================>....] - ETA: 7:56 - loss: 1.4937 - regression_loss: 1.1537 - classification_loss: 8992/10000 [=========================>....] - ETA: 7:55 - loss: 1.4936 - regression_loss: 1.1537 - classification_loss: 8993/10000 [=========================>....] - ETA: 7:55 - loss: 1.4937 - regression_loss: 1.1537 - classification_loss: 8994/10000 [=========================>....] - ETA: 7:54 - loss: 1.4937 - regression_loss: 1.1538 - classification_loss: 8995/10000 [=========================>....] - ETA: 7:54 - loss: 1.4938 - regression_loss: 1.1538 - classification_loss: 8996/10000 [=========================>....] - ETA: 7:53 - loss: 1.4938 - regression_loss: 1.1539 - classification_loss: 8997/10000 [=========================>....] - ETA: 7:53 - loss: 1.4937 - regression_loss: 1.1538 - classification_loss: 8998/10000 [=========================>....] - ETA: 7:52 - loss: 1.4939 - regression_loss: 1.1539 - classification_loss: 8999/10000 [=========================>....] - ETA: 7:52 - loss: 1.4939 - regression_loss: 1.1539 - classification_loss: 9000/10000 [==========================>...] - ETA: 7:52 - loss: 1.4939 - regression_loss: 1.1540 - classification_loss: 9001/10000 [==========================>...] - ETA: 7:51 - loss: 1.4938 - regression_loss: 1.1539 - classification_loss: 9002/10000 [==========================>...] - ETA: 7:51 - loss: 1.4937 - regression_loss: 1.1538 - classification_loss: 9003/10000 [==========================>...] - ETA: 7:50 - loss: 1.4937 - regression_loss: 1.1538 - classification_loss: 9004/10000 [==========================>...] - ETA: 7:50 - loss: 1.4936 - regression_loss: 1.1537 - classification_loss: 9005/10000 [==========================>...] - ETA: 7:49 - loss: 1.4936 - regression_loss: 1.1537 - classification_loss: 9006/10000 [==========================>...] - ETA: 7:49 - loss: 1.4937 - regression_loss: 1.1538 - classification_loss: 9007/10000 [==========================>...] - ETA: 7:48 - loss: 1.4937 - regression_loss: 1.1538 - classification_loss: 9008/10000 [==========================>...] - ETA: 7:48 - loss: 1.4937 - regression_loss: 1.1538 - classification_loss: 9009/10000 [==========================>...] - ETA: 7:47 - loss: 1.4937 - regression_loss: 1.1538 - classification_loss: 9010/10000 [==========================>...] - ETA: 7:47 - loss: 1.4938 - regression_loss: 1.1538 - classification_loss: 9011/10000 [==========================>...] - ETA: 7:46 - loss: 1.4938 - regression_loss: 1.1539 - classification_loss: 9012/10000 [==========================>...] - ETA: 7:46 - loss: 1.4939 - regression_loss: 1.1539 - classification_loss: 9013/10000 [==========================>...] - ETA: 7:45 - loss: 1.4938 - regression_loss: 1.1539 - classification_loss: 9014/10000 [==========================>...] - ETA: 7:45 - loss: 1.4938 - regression_loss: 1.1538 - classification_loss: 9015/10000 [==========================>...] - ETA: 7:44 - loss: 1.4939 - regression_loss: 1.1539 - classification_loss: 9016/10000 [==========================>...] - ETA: 7:44 - loss: 1.4938 - regression_loss: 1.1539 - classification_loss: 9017/10000 [==========================>...] - ETA: 7:43 - loss: 1.4938 - regression_loss: 1.1538 - classification_loss: 9018/10000 [==========================>...] - ETA: 7:43 - loss: 1.4939 - regression_loss: 1.1539 - classification_loss: 9019/10000 [==========================>...] - ETA: 7:43 - loss: 1.4940 - regression_loss: 1.1540 - classification_loss: 9020/10000 [==========================>...] - ETA: 7:42 - loss: 1.4940 - regression_loss: 1.1540 - classification_loss: 9021/10000 [==========================>...] - ETA: 7:42 - loss: 1.4939 - regression_loss: 1.1540 - classification_loss: 9022/10000 [==========================>...] - ETA: 7:41 - loss: 1.4939 - regression_loss: 1.1540 - classification_loss: 9023/10000 [==========================>...] - ETA: 7:41 - loss: 1.4939 - regression_loss: 1.1539 - classification_loss: 9024/10000 [==========================>...] - ETA: 7:40 - loss: 1.4939 - regression_loss: 1.1539 - classification_loss: 9025/10000 [==========================>...] - ETA: 7:40 - loss: 1.4940 - regression_loss: 1.1540 - classification_loss: 9026/10000 [==========================>...] - ETA: 7:39 - loss: 1.4939 - regression_loss: 1.1540 - classification_loss: 9027/10000 [==========================>...] - ETA: 7:39 - loss: 1.4940 - regression_loss: 1.1540 - classification_loss: 9028/10000 [==========================>...] - ETA: 7:38 - loss: 1.4940 - regression_loss: 1.1540 - classification_loss: 9029/10000 [==========================>...] - ETA: 7:38 - loss: 1.4939 - regression_loss: 1.1540 - classification_loss: 9030/10000 [==========================>...] - ETA: 7:37 - loss: 1.4939 - regression_loss: 1.1539 - classification_loss: 9031/10000 [==========================>...] - ETA: 7:37 - loss: 1.4938 - regression_loss: 1.1539 - classification_loss: 9032/10000 [==========================>...] - ETA: 7:36 - loss: 1.4938 - regression_loss: 1.1538 - classification_loss: 9033/10000 [==========================>...] - ETA: 7:36 - loss: 1.4937 - regression_loss: 1.1538 - classification_loss: 9034/10000 [==========================>...] - ETA: 7:35 - loss: 1.4937 - regression_loss: 1.1538 - classification_loss: 9035/10000 [==========================>...] - ETA: 7:35 - loss: 1.4937 - regression_loss: 1.1538 - classification_loss: 9036/10000 [==========================>...] - ETA: 7:35 - loss: 1.4936 - regression_loss: 1.1537 - classification_loss: 9037/10000 [==========================>...] - ETA: 7:34 - loss: 1.4935 - regression_loss: 1.1536 - classification_loss: 9038/10000 [==========================>...] - ETA: 7:34 - loss: 1.4936 - regression_loss: 1.1537 - classification_loss: 9039/10000 [==========================>...] - ETA: 7:33 - loss: 1.4937 - regression_loss: 1.1538 - classification_loss: 9040/10000 [==========================>...] - ETA: 7:33 - loss: 1.4936 - regression_loss: 1.1537 - classification_loss: 9041/10000 [==========================>...] - ETA: 7:32 - loss: 1.4935 - regression_loss: 1.1537 - classification_loss: 9042/10000 [==========================>...] - ETA: 7:32 - loss: 1.4934 - regression_loss: 1.1536 - classification_loss: 9043/10000 [==========================>...] - ETA: 7:31 - loss: 1.4935 - regression_loss: 1.1536 - classification_loss: 9044/10000 [==========================>...] - ETA: 7:31 - loss: 1.4936 - regression_loss: 1.1537 - classification_loss: 9045/10000 [==========================>...] - ETA: 7:30 - loss: 1.4935 - regression_loss: 1.1537 - classification_loss: 9046/10000 [==========================>...] - ETA: 7:30 - loss: 1.4934 - regression_loss: 1.1536 - classification_loss: 9047/10000 [==========================>...] - ETA: 7:29 - loss: 1.4934 - regression_loss: 1.1536 - classification_loss: 9048/10000 [==========================>...] - ETA: 7:29 - loss: 1.4934 - regression_loss: 1.1535 - classification_loss: 9049/10000 [==========================>...] - ETA: 7:28 - loss: 1.4933 - regression_loss: 1.1535 - classification_loss: 9050/10000 [==========================>...] - ETA: 7:28 - loss: 1.4933 - regression_loss: 1.1535 - classification_loss: 9051/10000 [==========================>...] - ETA: 7:27 - loss: 1.4935 - regression_loss: 1.1536 - classification_loss: 9052/10000 [==========================>...] - ETA: 7:27 - loss: 1.4934 - regression_loss: 1.1535 - classification_loss: 9053/10000 [==========================>...] - ETA: 7:26 - loss: 1.4934 - regression_loss: 1.1536 - classification_loss: 9054/10000 [==========================>...] - ETA: 7:26 - loss: 1.4934 - regression_loss: 1.1535 - classification_loss: 9055/10000 [==========================>...] - ETA: 7:26 - loss: 1.4934 - regression_loss: 1.1536 - classification_loss: 9056/10000 [==========================>...] - ETA: 7:25 - loss: 1.4935 - regression_loss: 1.1536 - classification_loss: 9057/10000 [==========================>...] - ETA: 7:25 - loss: 1.4934 - regression_loss: 1.1535 - classification_loss: 9058/10000 [==========================>...] - ETA: 7:24 - loss: 1.4934 - regression_loss: 1.1535 - classification_loss: 9059/10000 [==========================>...] - ETA: 7:24 - loss: 1.4934 - regression_loss: 1.1535 - classification_loss: 9060/10000 [==========================>...] - ETA: 7:23 - loss: 1.4934 - regression_loss: 1.1535 - classification_loss: 9061/10000 [==========================>...] - ETA: 7:23 - loss: 1.4934 - regression_loss: 1.1535 - classification_loss: 9062/10000 [==========================>...] - ETA: 7:22 - loss: 1.4933 - regression_loss: 1.1535 - classification_loss: 9063/10000 [==========================>...] - ETA: 7:22 - loss: 1.4934 - regression_loss: 1.1535 - classification_loss: 9064/10000 [==========================>...] - ETA: 7:21 - loss: 1.4935 - regression_loss: 1.1536 - classification_loss: 9065/10000 [==========================>...] - ETA: 7:21 - loss: 1.4936 - regression_loss: 1.1536 - classification_loss: 9066/10000 [==========================>...] - ETA: 7:20 - loss: 1.4936 - regression_loss: 1.1537 - classification_loss: 9067/10000 [==========================>...] - ETA: 7:20 - loss: 1.4935 - regression_loss: 1.1536 - classification_loss: 9068/10000 [==========================>...] - ETA: 7:19 - loss: 1.4936 - regression_loss: 1.1536 - classification_loss: 9069/10000 [==========================>...] - ETA: 7:19 - loss: 1.4936 - regression_loss: 1.1537 - classification_loss: 9070/10000 [==========================>...] - ETA: 7:18 - loss: 1.4936 - regression_loss: 1.1537 - classification_loss: 9071/10000 [==========================>...] - ETA: 7:18 - loss: 1.4937 - regression_loss: 1.1537 - classification_loss: 9072/10000 [==========================>...] - ETA: 7:17 - loss: 1.4937 - regression_loss: 1.1538 - classification_loss: 9073/10000 [==========================>...] - ETA: 7:17 - loss: 1.4937 - regression_loss: 1.1537 - classification_loss: 9074/10000 [==========================>...] - ETA: 7:17 - loss: 1.4937 - regression_loss: 1.1538 - classification_loss: 9075/10000 [==========================>...] - ETA: 7:16 - loss: 1.4937 - regression_loss: 1.1537 - classification_loss: 9076/10000 [==========================>...] - ETA: 7:16 - loss: 1.4936 - regression_loss: 1.1537 - classification_loss: 9077/10000 [==========================>...] - ETA: 7:15 - loss: 1.4935 - regression_loss: 1.1536 - classification_loss: 9078/10000 [==========================>...] - ETA: 7:15 - loss: 1.4935 - regression_loss: 1.1535 - classification_loss: 9079/10000 [==========================>...] - ETA: 7:14 - loss: 1.4935 - regression_loss: 1.1536 - classification_loss: 9080/10000 [==========================>...] - ETA: 7:14 - loss: 1.4934 - regression_loss: 1.1535 - classification_loss: 9081/10000 [==========================>...] - ETA: 7:13 - loss: 1.4934 - regression_loss: 1.1535 - classification_loss: 9082/10000 [==========================>...] - ETA: 7:13 - loss: 1.4934 - regression_loss: 1.1536 - classification_loss: 9083/10000 [==========================>...] - ETA: 7:12 - loss: 1.4934 - regression_loss: 1.1536 - classification_loss: 9084/10000 [==========================>...] - ETA: 7:12 - loss: 1.4934 - regression_loss: 1.1535 - classification_loss: 9085/10000 [==========================>...] - ETA: 7:11 - loss: 1.4933 - regression_loss: 1.1534 - classification_loss: 9086/10000 [==========================>...] - ETA: 7:11 - loss: 1.4932 - regression_loss: 1.1534 - classification_loss: 9087/10000 [==========================>...] - ETA: 7:10 - loss: 1.4934 - regression_loss: 1.1535 - classification_loss: 9088/10000 [==========================>...] - ETA: 7:10 - loss: 1.4933 - regression_loss: 1.1534 - classification_loss: 9089/10000 [==========================>...] - ETA: 7:09 - loss: 1.4932 - regression_loss: 1.1534 - classification_loss: 9090/10000 [==========================>...] - ETA: 7:09 - loss: 1.4932 - regression_loss: 1.1534 - classification_loss: 9091/10000 [==========================>...] - ETA: 7:08 - loss: 1.4932 - regression_loss: 1.1534 - classification_loss: 9092/10000 [==========================>...] - ETA: 7:08 - loss: 1.4933 - regression_loss: 1.1534 - classification_loss: 9093/10000 [==========================>...] - ETA: 7:08 - loss: 1.4934 - regression_loss: 1.1535 - classification_loss: 9094/10000 [==========================>...] - ETA: 7:07 - loss: 1.4934 - regression_loss: 1.1535 - classification_loss: 9095/10000 [==========================>...] - ETA: 7:07 - loss: 1.4935 - regression_loss: 1.1535 - classification_loss: 9096/10000 [==========================>...] - ETA: 7:06 - loss: 1.4934 - regression_loss: 1.1535 - classification_loss: 9097/10000 [==========================>...] - ETA: 7:06 - loss: 1.4934 - regression_loss: 1.1535 - classification_loss: 9098/10000 [==========================>...] - ETA: 7:05 - loss: 1.4934 - regression_loss: 1.1535 - classification_loss: 9099/10000 [==========================>...] - ETA: 7:05 - loss: 1.4934 - regression_loss: 1.1535 - classification_loss: 9100/10000 [==========================>...] - ETA: 7:04 - loss: 1.4934 - regression_loss: 1.1535 - classification_loss: 9101/10000 [==========================>...] - ETA: 7:04 - loss: 1.4934 - regression_loss: 1.1535 - classification_loss: 9102/10000 [==========================>...] - ETA: 7:03 - loss: 1.4933 - regression_loss: 1.1535 - classification_loss: 9103/10000 [==========================>...] - ETA: 7:03 - loss: 1.4933 - regression_loss: 1.1535 - classification_loss: 9104/10000 [==========================>...] - ETA: 7:02 - loss: 1.4934 - regression_loss: 1.1535 - classification_loss: 9105/10000 [==========================>...] - ETA: 7:02 - loss: 1.4935 - regression_loss: 1.1536 - classification_loss: 9106/10000 [==========================>...] - ETA: 7:01 - loss: 1.4934 - regression_loss: 1.1536 - classification_loss: 9107/10000 [==========================>...] - ETA: 7:01 - loss: 1.4933 - regression_loss: 1.1535 - classification_loss: 9108/10000 [==========================>...] - ETA: 7:00 - loss: 1.4934 - regression_loss: 1.1535 - classification_loss: 9109/10000 [==========================>...] - ETA: 7:00 - loss: 1.4934 - regression_loss: 1.1535 - classification_loss: 9110/10000 [==========================>...] - ETA: 7:00 - loss: 1.4933 - regression_loss: 1.1535 - classification_loss: 9111/10000 [==========================>...] - ETA: 6:59 - loss: 1.4933 - regression_loss: 1.1534 - classification_loss: 9112/10000 [==========================>...] - ETA: 6:59 - loss: 1.4933 - regression_loss: 1.1534 - classification_loss: 9113/10000 [==========================>...] - ETA: 6:58 - loss: 1.4932 - regression_loss: 1.1534 - classification_loss: 9114/10000 [==========================>...] - ETA: 6:58 - loss: 1.4933 - regression_loss: 1.1534 - classification_loss: 9115/10000 [==========================>...] - ETA: 6:57 - loss: 1.4932 - regression_loss: 1.1534 - classification_loss: 9116/10000 [==========================>...] - ETA: 6:57 - loss: 1.4931 - regression_loss: 1.1533 - classification_loss: 9117/10000 [==========================>...] - ETA: 6:56 - loss: 1.4931 - regression_loss: 1.1533 - classification_loss: 9118/10000 [==========================>...] - ETA: 6:56 - loss: 1.4931 - regression_loss: 1.1532 - classification_loss: 9119/10000 [==========================>...] - ETA: 6:55 - loss: 1.4930 - regression_loss: 1.1532 - classification_loss: 9120/10000 [==========================>...] - ETA: 6:55 - loss: 1.4930 - regression_loss: 1.1531 - classification_loss: 9121/10000 [==========================>...] - ETA: 6:54 - loss: 1.4929 - regression_loss: 1.1531 - classification_loss: 9122/10000 [==========================>...] - ETA: 6:54 - loss: 1.4929 - regression_loss: 1.1531 - classification_loss: 9123/10000 [==========================>...] - ETA: 6:53 - loss: 1.4929 - regression_loss: 1.1531 - classification_loss: 9124/10000 [==========================>...] - ETA: 6:53 - loss: 1.4930 - regression_loss: 1.1532 - classification_loss: 9125/10000 [==========================>...] - ETA: 6:52 - loss: 1.4930 - regression_loss: 1.1532 - classification_loss: 9126/10000 [==========================>...] - ETA: 6:52 - loss: 1.4931 - regression_loss: 1.1532 - classification_loss: 9127/10000 [==========================>...] - ETA: 6:51 - loss: 1.4932 - regression_loss: 1.1533 - classification_loss: 9128/10000 [==========================>...] - ETA: 6:51 - loss: 1.4932 - regression_loss: 1.1533 - classification_loss: 9129/10000 [==========================>...] - ETA: 6:51 - loss: 1.4932 - regression_loss: 1.1533 - classification_loss: 9130/10000 [==========================>...] - ETA: 6:50 - loss: 1.4931 - regression_loss: 1.1533 - classification_loss: 9131/10000 [==========================>...] - ETA: 6:50 - loss: 1.4930 - regression_loss: 1.1532 - classification_loss: 9132/10000 [==========================>...] - ETA: 6:49 - loss: 1.4930 - regression_loss: 1.1531 - classification_loss: 9133/10000 [==========================>...] - ETA: 6:49 - loss: 1.4931 - regression_loss: 1.1532 - classification_loss: 9134/10000 [==========================>...] - ETA: 6:48 - loss: 1.4931 - regression_loss: 1.1532 - classification_loss: 9135/10000 [==========================>...] - ETA: 6:48 - loss: 1.4931 - regression_loss: 1.1532 - classification_loss: 9136/10000 [==========================>...] - ETA: 6:47 - loss: 1.4931 - regression_loss: 1.1532 - classification_loss: 9137/10000 [==========================>...] - ETA: 6:47 - loss: 1.4931 - regression_loss: 1.1533 - classification_loss: 9138/10000 [==========================>...] - ETA: 6:46 - loss: 1.4931 - regression_loss: 1.1533 - classification_loss: 9139/10000 [==========================>...] - ETA: 6:46 - loss: 1.4931 - regression_loss: 1.1533 - classification_loss: 9140/10000 [==========================>...] - ETA: 6:45 - loss: 1.4931 - regression_loss: 1.1533 - classification_loss: 9141/10000 [==========================>...] - ETA: 6:45 - loss: 1.4932 - regression_loss: 1.1534 - classification_loss: 9142/10000 [==========================>...] - ETA: 6:44 - loss: 1.4932 - regression_loss: 1.1534 - classification_loss: 9143/10000 [==========================>...] - ETA: 6:44 - loss: 1.4932 - regression_loss: 1.1534 - classification_loss: 9144/10000 [==========================>...] - ETA: 6:43 - loss: 1.4931 - regression_loss: 1.1533 - classification_loss: 9145/10000 [==========================>...] - ETA: 6:43 - loss: 1.4930 - regression_loss: 1.1533 - classification_loss: 9146/10000 [==========================>...] - ETA: 6:42 - loss: 1.4931 - regression_loss: 1.1534 - classification_loss: 9147/10000 [==========================>...] - ETA: 6:42 - loss: 1.4930 - regression_loss: 1.1533 - classification_loss: 9148/10000 [==========================>...] - ETA: 6:42 - loss: 1.4931 - regression_loss: 1.1533 - classification_loss: 9149/10000 [==========================>...] - ETA: 6:41 - loss: 1.4931 - regression_loss: 1.1533 - classification_loss: 9150/10000 [==========================>...] - ETA: 6:41 - loss: 1.4931 - regression_loss: 1.1533 - classification_loss: 9151/10000 [==========================>...] - ETA: 6:40 - loss: 1.4932 - regression_loss: 1.1534 - classification_loss: 9152/10000 [==========================>...] - ETA: 6:40 - loss: 1.4931 - regression_loss: 1.1534 - classification_loss: 9153/10000 [==========================>...] - ETA: 6:39 - loss: 1.4931 - regression_loss: 1.1534 - classification_loss: 9154/10000 [==========================>...] - ETA: 6:39 - loss: 1.4932 - regression_loss: 1.1534 - classification_loss: 9155/10000 [==========================>...] - ETA: 6:38 - loss: 1.4932 - regression_loss: 1.1535 - classification_loss: 9156/10000 [==========================>...] - ETA: 6:38 - loss: 1.4932 - regression_loss: 1.1534 - classification_loss: 9157/10000 [==========================>...] - ETA: 6:37 - loss: 1.4932 - regression_loss: 1.1534 - classification_loss: 9158/10000 [==========================>...] - ETA: 6:37 - loss: 1.4931 - regression_loss: 1.1534 - classification_loss: 9159/10000 [==========================>...] - ETA: 6:36 - loss: 1.4931 - regression_loss: 1.1533 - classification_loss: 9160/10000 [==========================>...] - ETA: 6:36 - loss: 1.4930 - regression_loss: 1.1532 - classification_loss: 9161/10000 [==========================>...] - ETA: 6:35 - loss: 1.4929 - regression_loss: 1.1532 - classification_loss: 9162/10000 [==========================>...] - ETA: 6:35 - loss: 1.4929 - regression_loss: 1.1531 - classification_loss: 9163/10000 [==========================>...] - ETA: 6:34 - loss: 1.4929 - regression_loss: 1.1532 - classification_loss: 9164/10000 [==========================>...] - ETA: 6:34 - loss: 1.4929 - regression_loss: 1.1532 - classification_loss: 9165/10000 [==========================>...] - ETA: 6:33 - loss: 1.4929 - regression_loss: 1.1532 - classification_loss: 9166/10000 [==========================>...] - ETA: 6:33 - loss: 1.4929 - regression_loss: 1.1532 - classification_loss: 9167/10000 [==========================>...] - ETA: 6:33 - loss: 1.4929 - regression_loss: 1.1532 - classification_loss: 9168/10000 [==========================>...] - ETA: 6:32 - loss: 1.4929 - regression_loss: 1.1532 - classification_loss: 9169/10000 [==========================>...] - ETA: 6:32 - loss: 1.4929 - regression_loss: 1.1532 - classification_loss: 9170/10000 [==========================>...] - ETA: 6:31 - loss: 1.4929 - regression_loss: 1.1532 - classification_loss: 9171/10000 [==========================>...] - ETA: 6:31 - loss: 1.4928 - regression_loss: 1.1531 - classification_loss: 9172/10000 [==========================>...] - ETA: 6:30 - loss: 1.4928 - regression_loss: 1.1531 - classification_loss: 9173/10000 [==========================>...] - ETA: 6:30 - loss: 1.4927 - regression_loss: 1.1531 - classification_loss: 9174/10000 [==========================>...] - ETA: 6:29 - loss: 1.4927 - regression_loss: 1.1531 - classification_loss: 9175/10000 [==========================>...] - ETA: 6:29 - loss: 1.4927 - regression_loss: 1.1531 - classification_loss: 9176/10000 [==========================>...] - ETA: 6:28 - loss: 1.4928 - regression_loss: 1.1532 - classification_loss: 9177/10000 [==========================>...] - ETA: 6:28 - loss: 1.4928 - regression_loss: 1.1532 - classification_loss: 9178/10000 [==========================>...] - ETA: 6:27 - loss: 1.4928 - regression_loss: 1.1532 - classification_loss: 9179/10000 [==========================>...] - ETA: 6:27 - loss: 1.4928 - regression_loss: 1.1531 - classification_loss: 9180/10000 [==========================>...] - ETA: 6:26 - loss: 1.4927 - regression_loss: 1.1530 - classification_loss: 9181/10000 [==========================>...] - ETA: 6:26 - loss: 1.4927 - regression_loss: 1.1530 - classification_loss: 9182/10000 [==========================>...] - ETA: 6:25 - loss: 1.4928 - regression_loss: 1.1531 - classification_loss: 9183/10000 [==========================>...] - ETA: 6:25 - loss: 1.4927 - regression_loss: 1.1530 - classification_loss: 9184/10000 [==========================>...] - ETA: 6:24 - loss: 1.4928 - regression_loss: 1.1531 - classification_loss: 9185/10000 [==========================>...] - ETA: 6:24 - loss: 1.4927 - regression_loss: 1.1531 - classification_loss: 9186/10000 [==========================>...] - ETA: 6:24 - loss: 1.4927 - regression_loss: 1.1530 - classification_loss: 9187/10000 [==========================>...] - ETA: 6:23 - loss: 1.4927 - regression_loss: 1.1530 - classification_loss: 9188/10000 [==========================>...] - ETA: 6:23 - loss: 1.4926 - regression_loss: 1.1530 - classification_loss: 9189/10000 [==========================>...] - ETA: 6:22 - loss: 1.4926 - regression_loss: 1.1530 - classification_loss: 9190/10000 [==========================>...] - ETA: 6:22 - loss: 1.4927 - regression_loss: 1.1531 - classification_loss: 9191/10000 [==========================>...] - ETA: 6:21 - loss: 1.4927 - regression_loss: 1.1530 - classification_loss: 9192/10000 [==========================>...] - ETA: 6:21 - loss: 1.4926 - regression_loss: 1.1529 - classification_loss: 9193/10000 [==========================>...] - ETA: 6:20 - loss: 1.4927 - regression_loss: 1.1530 - classification_loss: 9194/10000 [==========================>...] - ETA: 6:20 - loss: 1.4927 - regression_loss: 1.1530 - classification_loss: 9195/10000 [==========================>...] - ETA: 6:19 - loss: 1.4926 - regression_loss: 1.1529 - classification_loss: 9196/10000 [==========================>...] - ETA: 6:19 - loss: 1.4925 - regression_loss: 1.1529 - classification_loss: 9197/10000 [==========================>...] - ETA: 6:18 - loss: 1.4925 - regression_loss: 1.1529 - classification_loss: 9198/10000 [==========================>...] - ETA: 6:18 - loss: 1.4926 - regression_loss: 1.1529 - classification_loss: 9199/10000 [==========================>...] - ETA: 6:17 - loss: 1.4925 - regression_loss: 1.1529 - classification_loss: 9200/10000 [==========================>...] - ETA: 6:17 - loss: 1.4925 - regression_loss: 1.1529 - classification_loss: 9201/10000 [==========================>...] - ETA: 6:16 - loss: 1.4926 - regression_loss: 1.1530 - classification_loss: 9202/10000 [==========================>...] - ETA: 6:16 - loss: 1.4925 - regression_loss: 1.1529 - classification_loss: 9203/10000 [==========================>...] - ETA: 6:16 - loss: 1.4925 - regression_loss: 1.1529 - classification_loss: 9204/10000 [==========================>...] - ETA: 6:15 - loss: 1.4924 - regression_loss: 1.1528 - classification_loss: 9205/10000 [==========================>...] - ETA: 6:15 - loss: 1.4925 - regression_loss: 1.1529 - classification_loss: 9206/10000 [==========================>...] - ETA: 6:14 - loss: 1.4925 - regression_loss: 1.1529 - classification_loss: 9207/10000 [==========================>...] - ETA: 6:14 - loss: 1.4924 - regression_loss: 1.1529 - classification_loss: 9208/10000 [==========================>...] - ETA: 6:13 - loss: 1.4924 - regression_loss: 1.1528 - classification_loss: 9209/10000 [==========================>...] - ETA: 6:13 - loss: 1.4923 - regression_loss: 1.1527 - classification_loss: 9210/10000 [==========================>...] - ETA: 6:12 - loss: 1.4922 - regression_loss: 1.1527 - classification_loss: 9211/10000 [==========================>...] - ETA: 6:12 - loss: 1.4922 - regression_loss: 1.1527 - classification_loss: 9212/10000 [==========================>...] - ETA: 6:11 - loss: 1.4921 - regression_loss: 1.1526 - classification_loss: 9213/10000 [==========================>...] - ETA: 6:11 - loss: 1.4921 - regression_loss: 1.1526 - classification_loss: 9214/10000 [==========================>...] - ETA: 6:10 - loss: 1.4920 - regression_loss: 1.1525 - classification_loss: 9215/10000 [==========================>...] - ETA: 6:10 - loss: 1.4920 - regression_loss: 1.1525 - classification_loss: 9216/10000 [==========================>...] - ETA: 6:09 - loss: 1.4921 - regression_loss: 1.1526 - classification_loss: 9217/10000 [==========================>...] - ETA: 6:09 - loss: 1.4921 - regression_loss: 1.1526 - classification_loss: 9218/10000 [==========================>...] - ETA: 6:08 - loss: 1.4921 - regression_loss: 1.1526 - classification_loss: 9219/10000 [==========================>...] - ETA: 6:08 - loss: 1.4921 - regression_loss: 1.1526 - classification_loss: 9220/10000 [==========================>...] - ETA: 6:07 - loss: 1.4920 - regression_loss: 1.1525 - classification_loss: 9221/10000 [==========================>...] - ETA: 6:07 - loss: 1.4920 - regression_loss: 1.1524 - classification_loss: 9222/10000 [==========================>...] - ETA: 6:07 - loss: 1.4920 - regression_loss: 1.1525 - classification_loss: 9223/10000 [==========================>...] - ETA: 6:06 - loss: 1.4921 - regression_loss: 1.1525 - classification_loss: 9224/10000 [==========================>...] - ETA: 6:06 - loss: 1.4920 - regression_loss: 1.1525 - classification_loss: 9225/10000 [==========================>...] - ETA: 6:05 - loss: 1.4919 - regression_loss: 1.1524 - classification_loss: 9226/10000 [==========================>...] - ETA: 6:05 - loss: 1.4918 - regression_loss: 1.1523 - classification_loss: 9227/10000 [==========================>...] - ETA: 6:04 - loss: 1.4918 - regression_loss: 1.1523 - classification_loss: 9228/10000 [==========================>...] - ETA: 6:04 - loss: 1.4919 - regression_loss: 1.1524 - classification_loss: 9229/10000 [==========================>...] - ETA: 6:03 - loss: 1.4918 - regression_loss: 1.1524 - classification_loss: 9230/10000 [==========================>...] - ETA: 6:03 - loss: 1.4919 - regression_loss: 1.1524 - classification_loss: 9231/10000 [==========================>...] - ETA: 6:02 - loss: 1.4919 - regression_loss: 1.1524 - classification_loss: 9232/10000 [==========================>...] - ETA: 6:02 - loss: 1.4920 - regression_loss: 1.1525 - classification_loss: 9233/10000 [==========================>...] - ETA: 6:01 - loss: 1.4920 - regression_loss: 1.1526 - classification_loss: 9234/10000 [==========================>...] - ETA: 6:01 - loss: 1.4920 - regression_loss: 1.1525 - classification_loss: 9235/10000 [==========================>...] - ETA: 6:00 - loss: 1.4920 - regression_loss: 1.1525 - classification_loss: 9236/10000 [==========================>...] - ETA: 6:00 - loss: 1.4920 - regression_loss: 1.1525 - classification_loss: 9237/10000 [==========================>...] - ETA: 5:59 - loss: 1.4920 - regression_loss: 1.1526 - classification_loss: 9238/10000 [==========================>...] - ETA: 5:59 - loss: 1.4920 - regression_loss: 1.1526 - classification_loss: 9239/10000 [==========================>...] - ETA: 5:58 - loss: 1.4920 - regression_loss: 1.1525 - classification_loss: 9240/10000 [==========================>...] - ETA: 5:58 - loss: 1.4919 - regression_loss: 1.1525 - classification_loss: 9241/10000 [==========================>...] - ETA: 5:58 - loss: 1.4918 - regression_loss: 1.1524 - classification_loss: 9242/10000 [==========================>...] - ETA: 5:57 - loss: 1.4917 - regression_loss: 1.1523 - classification_loss: 9243/10000 [==========================>...] - ETA: 5:57 - loss: 1.4918 - regression_loss: 1.1523 - classification_loss: 9244/10000 [==========================>...] - ETA: 5:56 - loss: 1.4918 - regression_loss: 1.1523 - classification_loss: 9245/10000 [==========================>...] - ETA: 5:56 - loss: 1.4917 - regression_loss: 1.1523 - classification_loss: 9246/10000 [==========================>...] - ETA: 5:55 - loss: 1.4918 - regression_loss: 1.1524 - classification_loss: 9247/10000 [==========================>...] - ETA: 5:55 - loss: 1.4918 - regression_loss: 1.1524 - classification_loss: 9248/10000 [==========================>...] - ETA: 5:54 - loss: 1.4917 - regression_loss: 1.1524 - classification_loss: 9249/10000 [==========================>...] - ETA: 5:54 - loss: 1.4917 - regression_loss: 1.1523 - classification_loss: 9250/10000 [==========================>...] - ETA: 5:53 - loss: 1.4917 - regression_loss: 1.1523 - classification_loss: 9251/10000 [==========================>...] - ETA: 5:53 - loss: 1.4917 - regression_loss: 1.1523 - classification_loss: 9252/10000 [==========================>...] - ETA: 5:52 - loss: 1.4917 - regression_loss: 1.1523 - classification_loss: 9253/10000 [==========================>...] - ETA: 5:52 - loss: 1.4917 - regression_loss: 1.1523 - classification_loss: 9254/10000 [==========================>...] - ETA: 5:51 - loss: 1.4918 - regression_loss: 1.1523 - classification_loss: 9255/10000 [==========================>...] - ETA: 5:51 - loss: 1.4919 - regression_loss: 1.1524 - classification_loss: 9256/10000 [==========================>...] - ETA: 5:50 - loss: 1.4918 - regression_loss: 1.1524 - classification_loss: 9257/10000 [==========================>...] - ETA: 5:50 - loss: 1.4917 - regression_loss: 1.1523 - classification_loss: 9258/10000 [==========================>...] - ETA: 5:49 - loss: 1.4917 - regression_loss: 1.1523 - classification_loss: 9259/10000 [==========================>...] - ETA: 5:49 - loss: 1.4917 - regression_loss: 1.1523 - classification_loss: 9260/10000 [==========================>...] - ETA: 5:49 - loss: 1.4917 - regression_loss: 1.1523 - classification_loss: 9261/10000 [==========================>...] - ETA: 5:48 - loss: 1.4917 - regression_loss: 1.1522 - classification_loss: 9262/10000 [==========================>...] - ETA: 5:48 - loss: 1.4917 - regression_loss: 1.1523 - classification_loss: 9263/10000 [==========================>...] - ETA: 5:47 - loss: 1.4917 - regression_loss: 1.1522 - classification_loss: 9264/10000 [==========================>...] - ETA: 5:47 - loss: 1.4916 - regression_loss: 1.1522 - classification_loss: 9265/10000 [==========================>...] - ETA: 5:46 - loss: 1.4916 - regression_loss: 1.1522 - classification_loss: 9266/10000 [==========================>...] - ETA: 5:46 - loss: 1.4916 - regression_loss: 1.1522 - classification_loss: 9267/10000 [==========================>...] - ETA: 5:45 - loss: 1.4915 - regression_loss: 1.1521 - classification_loss: 9268/10000 [==========================>...] - ETA: 5:45 - loss: 1.4914 - regression_loss: 1.1521 - classification_loss: 9269/10000 [==========================>...] - ETA: 5:44 - loss: 1.4914 - regression_loss: 1.1521 - classification_loss: 9270/10000 [==========================>...] - ETA: 5:44 - loss: 1.4915 - regression_loss: 1.1521 - classification_loss: 9271/10000 [==========================>...] - ETA: 5:43 - loss: 1.4914 - regression_loss: 1.1521 - classification_loss: 9272/10000 [==========================>...] - ETA: 5:43 - loss: 1.4916 - regression_loss: 1.1522 - classification_loss: 9273/10000 [==========================>...] - ETA: 5:42 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9274/10000 [==========================>...] - ETA: 5:42 - loss: 1.4916 - regression_loss: 1.1522 - classification_loss: 9275/10000 [==========================>...] - ETA: 5:41 - loss: 1.4916 - regression_loss: 1.1523 - classification_loss: 9276/10000 [==========================>...] - ETA: 5:41 - loss: 1.4916 - regression_loss: 1.1522 - classification_loss: 9277/10000 [==========================>...] - ETA: 5:41 - loss: 1.4916 - regression_loss: 1.1522 - classification_loss: 9278/10000 [==========================>...] - ETA: 5:40 - loss: 1.4915 - regression_loss: 1.1521 - classification_loss: 9279/10000 [==========================>...] - ETA: 5:40 - loss: 1.4916 - regression_loss: 1.1522 - classification_loss: 9280/10000 [==========================>...] - ETA: 5:39 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9281/10000 [==========================>...] - ETA: 5:39 - loss: 1.4914 - regression_loss: 1.1521 - classification_loss: 9282/10000 [==========================>...] - ETA: 5:38 - loss: 1.4914 - regression_loss: 1.1521 - classification_loss: 9283/10000 [==========================>...] - ETA: 5:38 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9284/10000 [==========================>...] - ETA: 5:37 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9285/10000 [==========================>...] - ETA: 5:37 - loss: 1.4914 - regression_loss: 1.1521 - classification_loss: 9286/10000 [==========================>...] - ETA: 5:36 - loss: 1.4914 - regression_loss: 1.1521 - classification_loss: 9287/10000 [==========================>...] - ETA: 5:36 - loss: 1.4913 - regression_loss: 1.1520 - classification_loss: 9288/10000 [==========================>...] - ETA: 5:35 - loss: 1.4915 - regression_loss: 1.1521 - classification_loss: 9289/10000 [==========================>...] - ETA: 5:35 - loss: 1.4913 - regression_loss: 1.1521 - classification_loss: 9290/10000 [==========================>...] - ETA: 5:34 - loss: 1.4913 - regression_loss: 1.1520 - classification_loss: 9291/10000 [==========================>...] - ETA: 5:34 - loss: 1.4912 - regression_loss: 1.1520 - classification_loss: 9292/10000 [==========================>...] - ETA: 5:33 - loss: 1.4912 - regression_loss: 1.1520 - classification_loss: 9293/10000 [==========================>...] - ETA: 5:33 - loss: 1.4913 - regression_loss: 1.1520 - classification_loss: 9294/10000 [==========================>...] - ETA: 5:32 - loss: 1.4914 - regression_loss: 1.1521 - classification_loss: 9295/10000 [==========================>...] - ETA: 5:32 - loss: 1.4914 - regression_loss: 1.1521 - classification_loss: 9296/10000 [==========================>...] - ETA: 5:32 - loss: 1.4914 - regression_loss: 1.1521 - classification_loss: 9297/10000 [==========================>...] - ETA: 5:31 - loss: 1.4916 - regression_loss: 1.1522 - classification_loss: 9298/10000 [==========================>...] - ETA: 5:31 - loss: 1.4916 - regression_loss: 1.1522 - classification_loss: 9299/10000 [==========================>...] - ETA: 5:30 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9300/10000 [==========================>...] - ETA: 5:30 - loss: 1.4914 - regression_loss: 1.1521 - classification_loss: 9301/10000 [==========================>...] - ETA: 5:29 - loss: 1.4915 - regression_loss: 1.1521 - classification_loss: 9302/10000 [==========================>...] - ETA: 5:29 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9303/10000 [==========================>...] - ETA: 5:28 - loss: 1.4914 - regression_loss: 1.1521 - classification_loss: 9304/10000 [==========================>...] - ETA: 5:28 - loss: 1.4914 - regression_loss: 1.1521 - classification_loss: 9305/10000 [==========================>...] - ETA: 5:27 - loss: 1.4915 - regression_loss: 1.1521 - classification_loss: 9306/10000 [==========================>...] - ETA: 5:27 - loss: 1.4916 - regression_loss: 1.1522 - classification_loss: 9307/10000 [==========================>...] - ETA: 5:26 - loss: 1.4915 - regression_loss: 1.1521 - classification_loss: 9308/10000 [==========================>...] - ETA: 5:26 - loss: 1.4914 - regression_loss: 1.1521 - classification_loss: 9309/10000 [==========================>...] - ETA: 5:25 - loss: 1.4915 - regression_loss: 1.1521 - classification_loss: 9310/10000 [==========================>...] - ETA: 5:25 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9311/10000 [==========================>...] - ETA: 5:24 - loss: 1.4915 - regression_loss: 1.1521 - classification_loss: 9312/10000 [==========================>...] - ETA: 5:24 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9313/10000 [==========================>...] - ETA: 5:24 - loss: 1.4916 - regression_loss: 1.1522 - classification_loss: 9314/10000 [==========================>...] - ETA: 5:23 - loss: 1.4916 - regression_loss: 1.1522 - classification_loss: 9315/10000 [==========================>...] - ETA: 5:23 - loss: 1.4917 - regression_loss: 1.1523 - classification_loss: 9316/10000 [==========================>...] - ETA: 5:22 - loss: 1.4917 - regression_loss: 1.1523 - classification_loss: 9317/10000 [==========================>...] - ETA: 5:22 - loss: 1.4917 - regression_loss: 1.1523 - classification_loss: 9318/10000 [==========================>...] - ETA: 5:21 - loss: 1.4917 - regression_loss: 1.1524 - classification_loss: 9319/10000 [==========================>...] - ETA: 5:21 - loss: 1.4917 - regression_loss: 1.1523 - classification_loss: 9320/10000 [==========================>...] - ETA: 5:20 - loss: 1.4916 - regression_loss: 1.1523 - classification_loss: 9321/10000 [==========================>...] - ETA: 5:20 - loss: 1.4916 - regression_loss: 1.1523 - classification_loss: 9322/10000 [==========================>...] - ETA: 5:19 - loss: 1.4915 - regression_loss: 1.1523 - classification_loss: 9323/10000 [==========================>...] - ETA: 5:19 - loss: 1.4917 - regression_loss: 1.1524 - classification_loss: 9324/10000 [==========================>...] - ETA: 5:18 - loss: 1.4917 - regression_loss: 1.1524 - classification_loss: 9325/10000 [==========================>...] - ETA: 5:18 - loss: 1.4916 - regression_loss: 1.1523 - classification_loss: 9326/10000 [==========================>...] - ETA: 5:17 - loss: 1.4917 - regression_loss: 1.1524 - classification_loss: 9327/10000 [==========================>...] - ETA: 5:17 - loss: 1.4917 - regression_loss: 1.1524 - classification_loss: 9328/10000 [==========================>...] - ETA: 5:16 - loss: 1.4916 - regression_loss: 1.1523 - classification_loss: 9329/10000 [==========================>...] - ETA: 5:16 - loss: 1.4916 - regression_loss: 1.1523 - classification_loss: 9330/10000 [==========================>...] - ETA: 5:15 - loss: 1.4916 - regression_loss: 1.1523 - classification_loss: 9331/10000 [==========================>...] - ETA: 5:15 - loss: 1.4915 - regression_loss: 1.1523 - classification_loss: 9332/10000 [==========================>...] - ETA: 5:15 - loss: 1.4916 - regression_loss: 1.1523 - classification_loss: 9333/10000 [==========================>...] - ETA: 5:14 - loss: 1.4916 - regression_loss: 1.1523 - classification_loss: 9334/10000 [===========================>..] - ETA: 5:14 - loss: 1.4916 - regression_loss: 1.1523 - classification_loss: 9335/10000 [===========================>..] - ETA: 5:13 - loss: 1.4916 - regression_loss: 1.1523 - classification_loss: 9336/10000 [===========================>..] - ETA: 5:13 - loss: 1.4916 - regression_loss: 1.1523 - classification_loss: 9337/10000 [===========================>..] - ETA: 5:12 - loss: 1.4916 - regression_loss: 1.1523 - classification_loss: 9338/10000 [===========================>..] - ETA: 5:12 - loss: 1.4916 - regression_loss: 1.1523 - classification_loss: 9339/10000 [===========================>..] - ETA: 5:11 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9340/10000 [===========================>..] - ETA: 5:11 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9341/10000 [===========================>..] - ETA: 5:10 - loss: 1.4914 - regression_loss: 1.1522 - classification_loss: 9342/10000 [===========================>..] - ETA: 5:10 - loss: 1.4914 - regression_loss: 1.1522 - classification_loss: 9343/10000 [===========================>..] - ETA: 5:09 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9344/10000 [===========================>..] - ETA: 5:09 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9345/10000 [===========================>..] - ETA: 5:08 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9346/10000 [===========================>..] - ETA: 5:08 - loss: 1.4914 - regression_loss: 1.1521 - classification_loss: 9347/10000 [===========================>..] - ETA: 5:07 - loss: 1.4914 - regression_loss: 1.1521 - classification_loss: 9348/10000 [===========================>..] - ETA: 5:07 - loss: 1.4914 - regression_loss: 1.1522 - classification_loss: 9349/10000 [===========================>..] - ETA: 5:07 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9350/10000 [===========================>..] - ETA: 5:06 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9351/10000 [===========================>..] - ETA: 5:06 - loss: 1.4914 - regression_loss: 1.1522 - classification_loss: 9352/10000 [===========================>..] - ETA: 5:05 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9353/10000 [===========================>..] - ETA: 5:05 - loss: 1.4916 - regression_loss: 1.1523 - classification_loss: 9354/10000 [===========================>..] - ETA: 5:04 - loss: 1.4917 - regression_loss: 1.1524 - classification_loss: 9355/10000 [===========================>..] - ETA: 5:04 - loss: 1.4916 - regression_loss: 1.1523 - classification_loss: 9356/10000 [===========================>..] - ETA: 5:03 - loss: 1.4916 - regression_loss: 1.1523 - classification_loss: 9357/10000 [===========================>..] - ETA: 5:03 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9358/10000 [===========================>..] - ETA: 5:02 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9359/10000 [===========================>..] - ETA: 5:02 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9360/10000 [===========================>..] - ETA: 5:01 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9361/10000 [===========================>..] - ETA: 5:01 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9362/10000 [===========================>..] - ETA: 5:00 - loss: 1.4916 - regression_loss: 1.1523 - classification_loss: 9363/10000 [===========================>..] - ETA: 5:00 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9364/10000 [===========================>..] - ETA: 4:59 - loss: 1.4916 - regression_loss: 1.1523 - classification_loss: 9365/10000 [===========================>..] - ETA: 4:59 - loss: 1.4916 - regression_loss: 1.1523 - classification_loss: 9366/10000 [===========================>..] - ETA: 4:58 - loss: 1.4916 - regression_loss: 1.1523 - classification_loss: 9367/10000 [===========================>..] - ETA: 4:58 - loss: 1.4916 - regression_loss: 1.1523 - classification_loss: 9368/10000 [===========================>..] - ETA: 4:58 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9369/10000 [===========================>..] - ETA: 4:57 - loss: 1.4914 - regression_loss: 1.1522 - classification_loss: 9370/10000 [===========================>..] - ETA: 4:57 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9371/10000 [===========================>..] - ETA: 4:56 - loss: 1.4915 - regression_loss: 1.1522 - classification_loss: 9372/10000 [===========================>..] - ETA: 4:56 - loss: 1.4913 - regression_loss: 1.1521 - classification_loss: 9373/10000 [===========================>..] - ETA: 4:55 - loss: 1.4914 - regression_loss: 1.1521 - classification_loss: 9374/10000 [===========================>..] - ETA: 4:55 - loss: 1.4913 - regression_loss: 1.1520 - classification_loss: 9375/10000 [===========================>..] - ETA: 4:54 - loss: 1.4912 - regression_loss: 1.1520 - classification_loss: 9376/10000 [===========================>..] - ETA: 4:54 - loss: 1.4912 - regression_loss: 1.1520 - classification_loss: 9377/10000 [===========================>..] - ETA: 4:53 - loss: 1.4912 - regression_loss: 1.1520 - classification_loss: 9378/10000 [===========================>..] - ETA: 4:53 - loss: 1.4912 - regression_loss: 1.1520 - classification_loss: 9379/10000 [===========================>..] - ETA: 4:52 - loss: 1.4913 - regression_loss: 1.1520 - classification_loss: 9380/10000 [===========================>..] - ETA: 4:52 - loss: 1.4913 - regression_loss: 1.1520 - classification_loss: 9381/10000 [===========================>..] - ETA: 4:51 - loss: 1.4913 - regression_loss: 1.1520 - classification_loss: 9382/10000 [===========================>..] - ETA: 4:51 - loss: 1.4913 - regression_loss: 1.1520 - classification_loss: 9383/10000 [===========================>..] - ETA: 4:50 - loss: 1.4914 - regression_loss: 1.1521 - classification_loss: 9384/10000 [===========================>..] - ETA: 4:50 - loss: 1.4913 - regression_loss: 1.1521 - classification_loss: 9385/10000 [===========================>..] - ETA: 4:50 - loss: 1.4913 - regression_loss: 1.1520 - classification_loss: 9386/10000 [===========================>..] - ETA: 4:49 - loss: 1.4914 - regression_loss: 1.1521 - classification_loss: 9387/10000 [===========================>..] - ETA: 4:49 - loss: 1.4913 - regression_loss: 1.1520 - classification_loss: 9388/10000 [===========================>..] - ETA: 4:48 - loss: 1.4913 - regression_loss: 1.1520 - classification_loss: 9389/10000 [===========================>..] - ETA: 4:48 - loss: 1.4912 - regression_loss: 1.1520 - classification_loss: 9390/10000 [===========================>..] - ETA: 4:47 - loss: 1.4912 - regression_loss: 1.1519 - classification_loss: 9391/10000 [===========================>..] - ETA: 4:47 - loss: 1.4911 - regression_loss: 1.1519 - classification_loss: 9392/10000 [===========================>..] - ETA: 4:46 - loss: 1.4911 - regression_loss: 1.1519 - classification_loss: 9393/10000 [===========================>..] - ETA: 4:46 - loss: 1.4910 - regression_loss: 1.1518 - classification_loss: 9394/10000 [===========================>..] - ETA: 4:45 - loss: 1.4910 - regression_loss: 1.1519 - classification_loss: 9395/10000 [===========================>..] - ETA: 4:45 - loss: 1.4910 - regression_loss: 1.1518 - classification_loss: 9396/10000 [===========================>..] - ETA: 4:44 - loss: 1.4909 - regression_loss: 1.1518 - classification_loss: 9397/10000 [===========================>..] - ETA: 4:44 - loss: 1.4909 - regression_loss: 1.1518 - classification_loss: 9398/10000 [===========================>..] - ETA: 4:43 - loss: 1.4909 - regression_loss: 1.1518 - classification_loss: 9399/10000 [===========================>..] - ETA: 4:43 - loss: 1.4909 - regression_loss: 1.1518 - classification_loss: 9400/10000 [===========================>..] - ETA: 4:42 - loss: 1.4909 - regression_loss: 1.1518 - classification_loss: 9401/10000 [===========================>..] - ETA: 4:42 - loss: 1.4910 - regression_loss: 1.1519 - classification_loss: 9402/10000 [===========================>..] - ETA: 4:41 - loss: 1.4909 - regression_loss: 1.1518 - classification_loss: 9403/10000 [===========================>..] - ETA: 4:41 - loss: 1.4908 - regression_loss: 1.1518 - classification_loss: 9404/10000 [===========================>..] - ETA: 4:41 - loss: 1.4909 - regression_loss: 1.1518 - classification_loss: 9405/10000 [===========================>..] - ETA: 4:40 - loss: 1.4910 - regression_loss: 1.1519 - classification_loss: 9406/10000 [===========================>..] - ETA: 4:40 - loss: 1.4910 - regression_loss: 1.1519 - classification_loss: 9407/10000 [===========================>..] - ETA: 4:39 - loss: 1.4911 - regression_loss: 1.1520 - classification_loss: 9408/10000 [===========================>..] - ETA: 4:39 - loss: 1.4911 - regression_loss: 1.1519 - classification_loss: 9409/10000 [===========================>..] - ETA: 4:38 - loss: 1.4911 - regression_loss: 1.1519 - classification_loss: 9410/10000 [===========================>..] - ETA: 4:38 - loss: 1.4911 - regression_loss: 1.1519 - classification_loss: 9411/10000 [===========================>..] - ETA: 4:37 - loss: 1.4911 - regression_loss: 1.1519 - classification_loss: 9412/10000 [===========================>..] - ETA: 4:37 - loss: 1.4911 - regression_loss: 1.1519 - classification_loss: 9413/10000 [===========================>..] - ETA: 4:36 - loss: 1.4912 - regression_loss: 1.1520 - classification_loss: 9414/10000 [===========================>..] - ETA: 4:36 - loss: 1.4911 - regression_loss: 1.1519 - classification_loss: 9415/10000 [===========================>..] - ETA: 4:35 - loss: 1.4910 - regression_loss: 1.1519 - classification_loss: 9416/10000 [===========================>..] - ETA: 4:35 - loss: 1.4911 - regression_loss: 1.1519 - classification_loss: 9417/10000 [===========================>..] - ETA: 4:34 - loss: 1.4910 - regression_loss: 1.1518 - classification_loss: 9418/10000 [===========================>..] - ETA: 4:34 - loss: 1.4910 - regression_loss: 1.1519 - classification_loss: 9419/10000 [===========================>..] - ETA: 4:33 - loss: 1.4910 - regression_loss: 1.1519 - classification_loss: 9420/10000 [===========================>..] - ETA: 4:33 - loss: 1.4910 - regression_loss: 1.1519 - classification_loss: 9421/10000 [===========================>..] - ETA: 4:33 - loss: 1.4909 - regression_loss: 1.1518 - classification_loss: 9422/10000 [===========================>..] - ETA: 4:32 - loss: 1.4908 - regression_loss: 1.1518 - classification_loss: 9423/10000 [===========================>..] - ETA: 4:32 - loss: 1.4908 - regression_loss: 1.1517 - classification_loss: 9424/10000 [===========================>..] - ETA: 4:31 - loss: 1.4908 - regression_loss: 1.1517 - classification_loss: 9425/10000 [===========================>..] - ETA: 4:31 - loss: 1.4908 - regression_loss: 1.1517 - classification_loss: 9426/10000 [===========================>..] - ETA: 4:30 - loss: 1.4910 - regression_loss: 1.1519 - classification_loss: 9427/10000 [===========================>..] - ETA: 4:30 - loss: 1.4909 - regression_loss: 1.1519 - classification_loss: 9428/10000 [===========================>..] - ETA: 4:29 - loss: 1.4909 - regression_loss: 1.1518 - classification_loss: 9429/10000 [===========================>..] - ETA: 4:29 - loss: 1.4908 - regression_loss: 1.1517 - classification_loss: 9430/10000 [===========================>..] - ETA: 4:28 - loss: 1.4907 - regression_loss: 1.1517 - classification_loss: 9431/10000 [===========================>..] - ETA: 4:28 - loss: 1.4906 - regression_loss: 1.1516 - classification_loss: 9432/10000 [===========================>..] - ETA: 4:27 - loss: 1.4905 - regression_loss: 1.1515 - classification_loss: 9433/10000 [===========================>..] - ETA: 4:27 - loss: 1.4905 - regression_loss: 1.1515 - classification_loss: 9434/10000 [===========================>..] - ETA: 4:26 - loss: 1.4906 - regression_loss: 1.1516 - classification_loss: 9435/10000 [===========================>..] - ETA: 4:26 - loss: 1.4906 - regression_loss: 1.1516 - classification_loss: 9436/10000 [===========================>..] - ETA: 4:25 - loss: 1.4906 - regression_loss: 1.1516 - classification_loss: 9437/10000 [===========================>..] - ETA: 4:25 - loss: 1.4906 - regression_loss: 1.1516 - classification_loss: 9438/10000 [===========================>..] - ETA: 4:24 - loss: 1.4906 - regression_loss: 1.1516 - classification_loss: 9439/10000 [===========================>..] - ETA: 4:24 - loss: 1.4906 - regression_loss: 1.1516 - classification_loss: 9440/10000 [===========================>..] - ETA: 4:24 - loss: 1.4906 - regression_loss: 1.1516 - classification_loss: 9441/10000 [===========================>..] - ETA: 4:23 - loss: 1.4906 - regression_loss: 1.1516 - classification_loss: 9442/10000 [===========================>..] - ETA: 4:23 - loss: 1.4905 - regression_loss: 1.1515 - classification_loss: 9443/10000 [===========================>..] - ETA: 4:22 - loss: 1.4905 - regression_loss: 1.1515 - classification_loss: 9444/10000 [===========================>..] - ETA: 4:22 - loss: 1.4905 - regression_loss: 1.1515 - classification_loss: 9445/10000 [===========================>..] - ETA: 4:21 - loss: 1.4906 - regression_loss: 1.1516 - classification_loss: 9446/10000 [===========================>..] - ETA: 4:21 - loss: 1.4906 - regression_loss: 1.1516 - classification_loss: 9447/10000 [===========================>..] - ETA: 4:20 - loss: 1.4907 - regression_loss: 1.1516 - classification_loss: 9448/10000 [===========================>..] - ETA: 4:20 - loss: 1.4906 - regression_loss: 1.1516 - classification_loss: 9449/10000 [===========================>..] - ETA: 4:19 - loss: 1.4906 - regression_loss: 1.1516 - classification_loss: 9450/10000 [===========================>..] - ETA: 4:19 - loss: 1.4906 - regression_loss: 1.1516 - classification_loss: 9451/10000 [===========================>..] - ETA: 4:18 - loss: 1.4906 - regression_loss: 1.1516 - classification_loss: 9452/10000 [===========================>..] - ETA: 4:18 - loss: 1.4906 - regression_loss: 1.1516 - classification_loss: 9453/10000 [===========================>..] - ETA: 4:17 - loss: 1.4907 - regression_loss: 1.1517 - classification_loss: 9454/10000 [===========================>..] - ETA: 4:17 - loss: 1.4907 - regression_loss: 1.1517 - classification_loss: 9455/10000 [===========================>..] - ETA: 4:16 - loss: 1.4907 - regression_loss: 1.1517 - classification_loss: 9456/10000 [===========================>..] - ETA: 4:16 - loss: 1.4907 - regression_loss: 1.1517 - classification_loss: 9457/10000 [===========================>..] - ETA: 4:16 - loss: 1.4907 - regression_loss: 1.1517 - classification_loss: 9458/10000 [===========================>..] - ETA: 4:15 - loss: 1.4908 - regression_loss: 1.1517 - classification_loss: 9459/10000 [===========================>..] - ETA: 4:15 - loss: 1.4909 - regression_loss: 1.1518 - classification_loss: 9460/10000 [===========================>..] - ETA: 4:14 - loss: 1.4909 - regression_loss: 1.1518 - classification_loss: 9461/10000 [===========================>..] - ETA: 4:14 - loss: 1.4907 - regression_loss: 1.1517 - classification_loss: 9462/10000 [===========================>..] - ETA: 4:13 - loss: 1.4907 - regression_loss: 1.1517 - classification_loss: 9463/10000 [===========================>..] - ETA: 4:13 - loss: 1.4906 - regression_loss: 1.1516 - classification_loss: 9464/10000 [===========================>..] - ETA: 4:12 - loss: 1.4907 - regression_loss: 1.1517 - classification_loss: 9465/10000 [===========================>..] - ETA: 4:12 - loss: 1.4907 - regression_loss: 1.1517 - classification_loss: 9466/10000 [===========================>..] - ETA: 4:12 - loss: 1.4907 - regression_loss: 1.1517 - classification_loss: 9467/10000 [===========================>..] - ETA: 4:11 - loss: 1.4908 - regression_loss: 1.1518 - classification_loss: 9468/10000 [===========================>..] - ETA: 4:11 - loss: 1.4907 - regression_loss: 1.1518 - classification_loss: 9469/10000 [===========================>..] - ETA: 4:10 - loss: 1.4908 - regression_loss: 1.1518 - classification_loss: 9470/10000 [===========================>..] - ETA: 4:10 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9471/10000 [===========================>..] - ETA: 4:09 - loss: 1.4909 - regression_loss: 1.1519 - classification_loss: 9472/10000 [===========================>..] - ETA: 4:09 - loss: 1.4909 - regression_loss: 1.1519 - classification_loss: 9473/10000 [===========================>..] - ETA: 4:08 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9474/10000 [===========================>..] - ETA: 4:08 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9475/10000 [===========================>..] - ETA: 4:07 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9476/10000 [===========================>..] - ETA: 4:07 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9477/10000 [===========================>..] - ETA: 4:06 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9478/10000 [===========================>..] - ETA: 4:06 - loss: 1.4907 - regression_loss: 1.1518 - classification_loss: 9479/10000 [===========================>..] - ETA: 4:05 - loss: 1.4907 - regression_loss: 1.1518 - classification_loss: 9480/10000 [===========================>..] - ETA: 4:05 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9481/10000 [===========================>..] - ETA: 4:04 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9482/10000 [===========================>..] - ETA: 4:04 - loss: 1.4907 - regression_loss: 1.1518 - classification_loss: 9483/10000 [===========================>..] - ETA: 4:03 - loss: 1.4907 - regression_loss: 1.1518 - classification_loss: 9484/10000 [===========================>..] - ETA: 4:03 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9485/10000 [===========================>..] - ETA: 4:03 - loss: 1.4907 - regression_loss: 1.1518 - classification_loss: 9486/10000 [===========================>..] - ETA: 4:02 - loss: 1.4907 - regression_loss: 1.1518 - classification_loss: 9487/10000 [===========================>..] - ETA: 4:02 - loss: 1.4907 - regression_loss: 1.1518 - classification_loss: 9488/10000 [===========================>..] - ETA: 4:01 - loss: 1.4909 - regression_loss: 1.1519 - classification_loss: 9489/10000 [===========================>..] - ETA: 4:01 - loss: 1.4909 - regression_loss: 1.1520 - classification_loss: 9490/10000 [===========================>..] - ETA: 4:00 - loss: 1.4909 - regression_loss: 1.1520 - classification_loss: 9491/10000 [===========================>..] - ETA: 4:00 - loss: 1.4909 - regression_loss: 1.1520 - classification_loss: 9492/10000 [===========================>..] - ETA: 3:59 - loss: 1.4910 - regression_loss: 1.1521 - classification_loss: 9493/10000 [===========================>..] - ETA: 3:59 - loss: 1.4910 - regression_loss: 1.1521 - classification_loss: 9494/10000 [===========================>..] - ETA: 3:58 - loss: 1.4910 - regression_loss: 1.1520 - classification_loss: 9495/10000 [===========================>..] - ETA: 3:58 - loss: 1.4909 - regression_loss: 1.1520 - classification_loss: 9496/10000 [===========================>..] - ETA: 3:57 - loss: 1.4909 - regression_loss: 1.1520 - classification_loss: 9497/10000 [===========================>..] - ETA: 3:57 - loss: 1.4911 - regression_loss: 1.1521 - classification_loss: 9498/10000 [===========================>..] - ETA: 3:56 - loss: 1.4910 - regression_loss: 1.1521 - classification_loss: 9499/10000 [===========================>..] - ETA: 3:56 - loss: 1.4911 - regression_loss: 1.1521 - classification_loss: 9500/10000 [===========================>..] - ETA: 3:55 - loss: 1.4912 - regression_loss: 1.1522 - classification_loss: 9501/10000 [===========================>..] - ETA: 3:55 - loss: 1.4912 - regression_loss: 1.1522 - classification_loss: 9502/10000 [===========================>..] - ETA: 3:54 - loss: 1.4911 - regression_loss: 1.1521 - classification_loss: 9503/10000 [===========================>..] - ETA: 3:54 - loss: 1.4911 - regression_loss: 1.1521 - classification_loss: 9504/10000 [===========================>..] - ETA: 3:54 - loss: 1.4910 - regression_loss: 1.1520 - classification_loss: 9505/10000 [===========================>..] - ETA: 3:53 - loss: 1.4909 - regression_loss: 1.1519 - classification_loss: 9506/10000 [===========================>..] - ETA: 3:53 - loss: 1.4908 - regression_loss: 1.1518 - classification_loss: 9507/10000 [===========================>..] - ETA: 3:52 - loss: 1.4907 - regression_loss: 1.1518 - classification_loss: 9508/10000 [===========================>..] - ETA: 3:52 - loss: 1.4908 - regression_loss: 1.1518 - classification_loss: 9509/10000 [===========================>..] - ETA: 3:51 - loss: 1.4909 - regression_loss: 1.1519 - classification_loss: 9510/10000 [===========================>..] - ETA: 3:51 - loss: 1.4909 - regression_loss: 1.1520 - classification_loss: 9511/10000 [===========================>..] - ETA: 3:50 - loss: 1.4909 - regression_loss: 1.1520 - classification_loss: 9512/10000 [===========================>..] - ETA: 3:50 - loss: 1.4910 - regression_loss: 1.1520 - classification_loss: 9513/10000 [===========================>..] - ETA: 3:49 - loss: 1.4909 - regression_loss: 1.1520 - classification_loss: 9514/10000 [===========================>..] - ETA: 3:49 - loss: 1.4909 - regression_loss: 1.1519 - classification_loss: 9515/10000 [===========================>..] - ETA: 3:48 - loss: 1.4909 - regression_loss: 1.1519 - classification_loss: 9516/10000 [===========================>..] - ETA: 3:48 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9517/10000 [===========================>..] - ETA: 3:47 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9518/10000 [===========================>..] - ETA: 3:47 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9519/10000 [===========================>..] - ETA: 3:46 - loss: 1.4909 - regression_loss: 1.1519 - classification_loss: 9520/10000 [===========================>..] - ETA: 3:46 - loss: 1.4909 - regression_loss: 1.1519 - classification_loss: 9521/10000 [===========================>..] - ETA: 3:46 - loss: 1.4909 - regression_loss: 1.1520 - classification_loss: 9522/10000 [===========================>..] - ETA: 3:45 - loss: 1.4910 - regression_loss: 1.1520 - classification_loss: 9523/10000 [===========================>..] - ETA: 3:45 - loss: 1.4909 - regression_loss: 1.1519 - classification_loss: 9524/10000 [===========================>..] - ETA: 3:44 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9525/10000 [===========================>..] - ETA: 3:44 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9526/10000 [===========================>..] - ETA: 3:43 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9527/10000 [===========================>..] - ETA: 3:43 - loss: 1.4909 - regression_loss: 1.1519 - classification_loss: 9528/10000 [===========================>..] - ETA: 3:42 - loss: 1.4909 - regression_loss: 1.1520 - classification_loss: 9529/10000 [===========================>..] - ETA: 3:42 - loss: 1.4910 - regression_loss: 1.1521 - classification_loss: 9530/10000 [===========================>..] - ETA: 3:41 - loss: 1.4911 - regression_loss: 1.1522 - classification_loss: 9531/10000 [===========================>..] - ETA: 3:41 - loss: 1.4911 - regression_loss: 1.1522 - classification_loss: 9532/10000 [===========================>..] - ETA: 3:40 - loss: 1.4910 - regression_loss: 1.1521 - classification_loss: 9533/10000 [===========================>..] - ETA: 3:40 - loss: 1.4909 - regression_loss: 1.1520 - classification_loss: 9534/10000 [===========================>..] - ETA: 3:39 - loss: 1.4908 - regression_loss: 1.1520 - classification_loss: 9535/10000 [===========================>..] - ETA: 3:39 - loss: 1.4907 - regression_loss: 1.1519 - classification_loss: 9536/10000 [===========================>..] - ETA: 3:38 - loss: 1.4907 - regression_loss: 1.1519 - classification_loss: 9537/10000 [===========================>..] - ETA: 3:38 - loss: 1.4907 - regression_loss: 1.1519 - classification_loss: 9538/10000 [===========================>..] - ETA: 3:37 - loss: 1.4907 - regression_loss: 1.1519 - classification_loss: 9539/10000 [===========================>..] - ETA: 3:37 - loss: 1.4908 - regression_loss: 1.1520 - classification_loss: 9540/10000 [===========================>..] - ETA: 3:37 - loss: 1.4907 - regression_loss: 1.1519 - classification_loss: 9541/10000 [===========================>..] - ETA: 3:36 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9542/10000 [===========================>..] - ETA: 3:36 - loss: 1.4907 - regression_loss: 1.1519 - classification_loss: 9543/10000 [===========================>..] - ETA: 3:35 - loss: 1.4907 - regression_loss: 1.1519 - classification_loss: 9544/10000 [===========================>..] - ETA: 3:35 - loss: 1.4906 - regression_loss: 1.1518 - classification_loss: 9545/10000 [===========================>..] - ETA: 3:34 - loss: 1.4906 - regression_loss: 1.1517 - classification_loss: 9546/10000 [===========================>..] - ETA: 3:34 - loss: 1.4907 - regression_loss: 1.1518 - classification_loss: 9547/10000 [===========================>..] - ETA: 3:33 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9548/10000 [===========================>..] - ETA: 3:33 - loss: 1.4907 - regression_loss: 1.1518 - classification_loss: 9549/10000 [===========================>..] - ETA: 3:32 - loss: 1.4906 - regression_loss: 1.1517 - classification_loss: 9550/10000 [===========================>..] - ETA: 3:32 - loss: 1.4906 - regression_loss: 1.1517 - classification_loss: 9551/10000 [===========================>..] - ETA: 3:31 - loss: 1.4906 - regression_loss: 1.1517 - classification_loss: 9552/10000 [===========================>..] - ETA: 3:31 - loss: 1.4906 - regression_loss: 1.1517 - classification_loss: 9553/10000 [===========================>..] - ETA: 3:30 - loss: 1.4905 - regression_loss: 1.1516 - classification_loss: 9554/10000 [===========================>..] - ETA: 3:30 - loss: 1.4904 - regression_loss: 1.1516 - classification_loss: 9555/10000 [===========================>..] - ETA: 3:29 - loss: 1.4905 - regression_loss: 1.1516 - classification_loss: 9556/10000 [===========================>..] - ETA: 3:29 - loss: 1.4905 - regression_loss: 1.1517 - classification_loss: 9557/10000 [===========================>..] - ETA: 3:29 - loss: 1.4906 - regression_loss: 1.1518 - classification_loss: 9558/10000 [===========================>..] - ETA: 3:28 - loss: 1.4907 - regression_loss: 1.1518 - classification_loss: 9559/10000 [===========================>..] - ETA: 3:28 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9560/10000 [===========================>..] - ETA: 3:27 - loss: 1.4907 - regression_loss: 1.1518 - classification_loss: 9561/10000 [===========================>..] - ETA: 3:27 - loss: 1.4907 - regression_loss: 1.1518 - classification_loss: 9562/10000 [===========================>..] - ETA: 3:26 - loss: 1.4908 - regression_loss: 1.1518 - classification_loss: 9563/10000 [===========================>..] - ETA: 3:26 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9564/10000 [===========================>..] - ETA: 3:25 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9565/10000 [===========================>..] - ETA: 3:25 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9566/10000 [===========================>..] - ETA: 3:24 - loss: 1.4908 - regression_loss: 1.1518 - classification_loss: 9567/10000 [===========================>..] - ETA: 3:24 - loss: 1.4908 - regression_loss: 1.1518 - classification_loss: 9568/10000 [===========================>..] - ETA: 3:23 - loss: 1.4908 - regression_loss: 1.1518 - classification_loss: 9569/10000 [===========================>..] - ETA: 3:23 - loss: 1.4909 - regression_loss: 1.1519 - classification_loss: 9570/10000 [===========================>..] - ETA: 3:22 - loss: 1.4909 - regression_loss: 1.1519 - classification_loss: 9571/10000 [===========================>..] - ETA: 3:22 - loss: 1.4910 - regression_loss: 1.1520 - classification_loss: 9572/10000 [===========================>..] - ETA: 3:21 - loss: 1.4910 - regression_loss: 1.1520 - classification_loss: 9573/10000 [===========================>..] - ETA: 3:21 - loss: 1.4909 - regression_loss: 1.1519 - classification_loss: 9574/10000 [===========================>..] - ETA: 3:20 - loss: 1.4909 - regression_loss: 1.1519 - classification_loss: 9575/10000 [===========================>..] - ETA: 3:20 - loss: 1.4909 - regression_loss: 1.1519 - classification_loss: 9576/10000 [===========================>..] - ETA: 3:20 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9577/10000 [===========================>..] - ETA: 3:19 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9578/10000 [===========================>..] - ETA: 3:19 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9579/10000 [===========================>..] - ETA: 3:18 - loss: 1.4909 - regression_loss: 1.1519 - classification_loss: 9580/10000 [===========================>..] - ETA: 3:18 - loss: 1.4909 - regression_loss: 1.1519 - classification_loss: 9581/10000 [===========================>..] - ETA: 3:17 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9582/10000 [===========================>..] - ETA: 3:17 - loss: 1.4909 - regression_loss: 1.1520 - classification_loss: 9583/10000 [===========================>..] - ETA: 3:16 - loss: 1.4909 - regression_loss: 1.1520 - classification_loss: 9584/10000 [===========================>..] - ETA: 3:16 - loss: 1.4909 - regression_loss: 1.1520 - classification_loss: 9585/10000 [===========================>..] - ETA: 3:15 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9586/10000 [===========================>..] - ETA: 3:15 - loss: 1.4907 - regression_loss: 1.1518 - classification_loss: 9587/10000 [===========================>..] - ETA: 3:14 - loss: 1.4906 - regression_loss: 1.1517 - classification_loss: 9588/10000 [===========================>..] - ETA: 3:14 - loss: 1.4905 - regression_loss: 1.1517 - classification_loss: 9589/10000 [===========================>..] - ETA: 3:13 - loss: 1.4905 - regression_loss: 1.1517 - classification_loss: 9590/10000 [===========================>..] - ETA: 3:13 - loss: 1.4905 - regression_loss: 1.1516 - classification_loss: 9591/10000 [===========================>..] - ETA: 3:12 - loss: 1.4907 - regression_loss: 1.1518 - classification_loss: 9592/10000 [===========================>..] - ETA: 3:12 - loss: 1.4906 - regression_loss: 1.1517 - classification_loss: 9593/10000 [===========================>..] - ETA: 3:12 - loss: 1.4906 - regression_loss: 1.1517 - classification_loss: 9594/10000 [===========================>..] - ETA: 3:11 - loss: 1.4905 - regression_loss: 1.1516 - classification_loss: 9595/10000 [===========================>..] - ETA: 3:11 - loss: 1.4905 - regression_loss: 1.1516 - classification_loss: 9596/10000 [===========================>..] - ETA: 3:10 - loss: 1.4905 - regression_loss: 1.1517 - classification_loss: 9597/10000 [===========================>..] - ETA: 3:10 - loss: 1.4905 - regression_loss: 1.1516 - classification_loss: 9598/10000 [===========================>..] - ETA: 3:09 - loss: 1.4904 - regression_loss: 1.1515 - classification_loss: 9599/10000 [===========================>..] - ETA: 3:09 - loss: 1.4905 - regression_loss: 1.1516 - classification_loss: 9600/10000 [===========================>..] - ETA: 3:08 - loss: 1.4905 - regression_loss: 1.1516 - classification_loss: 9601/10000 [===========================>..] - ETA: 3:08 - loss: 1.4905 - regression_loss: 1.1517 - classification_loss: 9602/10000 [===========================>..] - ETA: 3:07 - loss: 1.4905 - regression_loss: 1.1516 - classification_loss: 9603/10000 [===========================>..] - ETA: 3:07 - loss: 1.4905 - regression_loss: 1.1516 - classification_loss: 9604/10000 [===========================>..] - ETA: 3:06 - loss: 1.4906 - regression_loss: 1.1517 - classification_loss: 9605/10000 [===========================>..] - ETA: 3:06 - loss: 1.4906 - regression_loss: 1.1517 - classification_loss: 9606/10000 [===========================>..] - ETA: 3:05 - loss: 1.4906 - regression_loss: 1.1517 - classification_loss: 9607/10000 [===========================>..] - ETA: 3:05 - loss: 1.4906 - regression_loss: 1.1518 - classification_loss: 9608/10000 [===========================>..] - ETA: 3:04 - loss: 1.4907 - regression_loss: 1.1518 - classification_loss: 9609/10000 [===========================>..] - ETA: 3:04 - loss: 1.4906 - regression_loss: 1.1518 - classification_loss: 9610/10000 [===========================>..] - ETA: 3:03 - loss: 1.4909 - regression_loss: 1.1519 - classification_loss: 9611/10000 [===========================>..] - ETA: 3:03 - loss: 1.4909 - regression_loss: 1.1520 - classification_loss: 9612/10000 [===========================>..] - ETA: 3:03 - loss: 1.4909 - regression_loss: 1.1520 - classification_loss: 9613/10000 [===========================>..] - ETA: 3:02 - loss: 1.4909 - regression_loss: 1.1520 - classification_loss: 9614/10000 [===========================>..] - ETA: 3:02 - loss: 1.4909 - regression_loss: 1.1520 - classification_loss: 9615/10000 [===========================>..] - ETA: 3:01 - loss: 1.4909 - regression_loss: 1.1520 - classification_loss: 9616/10000 [===========================>..] - ETA: 3:01 - loss: 1.4909 - regression_loss: 1.1520 - classification_loss: 9617/10000 [===========================>..] - ETA: 3:00 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9618/10000 [===========================>..] - ETA: 3:00 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9619/10000 [===========================>..] - ETA: 2:59 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9620/10000 [===========================>..] - ETA: 2:59 - loss: 1.4908 - regression_loss: 1.1519 - classification_loss: 9621/10000 [===========================>..] - ETA: 2:58 - loss: 1.4907 - regression_loss: 1.1519 - classification_loss: 9622/10000 [===========================>..] - ETA: 2:58 - loss: 1.4907 - regression_loss: 1.1519 - classification_loss: 9623/10000 [===========================>..] - ETA: 2:57 - loss: 1.4907 - regression_loss: 1.1518 - classification_loss: 9624/10000 [===========================>..] - ETA: 2:57 - loss: 1.4907 - regression_loss: 1.1518 - classification_loss: 9625/10000 [===========================>..] - ETA: 2:56 - loss: 1.4907 - regression_loss: 1.1518 - classification_loss: 9626/10000 [===========================>..] - ETA: 2:56 - loss: 1.4907 - regression_loss: 1.1518 - classification_loss: 9627/10000 [===========================>..] - ETA: 2:55 - loss: 1.4907 - regression_loss: 1.1518 - classification_loss: 9628/10000 [===========================>..] - ETA: 2:55 - loss: 1.4906 - regression_loss: 1.1518 - classification_loss: 9629/10000 [===========================>..] - ETA: 2:55 - loss: 1.4906 - regression_loss: 1.1517 - classification_loss: 9630/10000 [===========================>..] - ETA: 2:54 - loss: 1.4905 - regression_loss: 1.1517 - classification_loss: 9631/10000 [===========================>..] - ETA: 2:54 - loss: 1.4905 - regression_loss: 1.1516 - classification_loss: 9632/10000 [===========================>..] - ETA: 2:53 - loss: 1.4904 - regression_loss: 1.1516 - classification_loss: 9633/10000 [===========================>..] - ETA: 2:53 - loss: 1.4904 - regression_loss: 1.1516 - classification_loss: 9634/10000 [===========================>..] - ETA: 2:52 - loss: 1.4905 - regression_loss: 1.1516 - classification_loss: 9635/10000 [===========================>..] - ETA: 2:52 - loss: 1.4905 - regression_loss: 1.1516 - classification_loss: 9636/10000 [===========================>..] - ETA: 2:51 - loss: 1.4905 - regression_loss: 1.1517 - classification_loss: 9637/10000 [===========================>..] - ETA: 2:51 - loss: 1.4905 - regression_loss: 1.1517 - classification_loss: 9638/10000 [===========================>..] - ETA: 2:50 - loss: 1.4905 - regression_loss: 1.1517 - classification_loss: 9639/10000 [===========================>..] - ETA: 2:50 - loss: 1.4905 - regression_loss: 1.1517 - classification_loss: 9640/10000 [===========================>..] - ETA: 2:49 - loss: 1.4905 - regression_loss: 1.1517 - classification_loss: 9641/10000 [===========================>..] - ETA: 2:49 - loss: 1.4906 - regression_loss: 1.1518 - classification_loss: 9642/10000 [===========================>..] - ETA: 2:48 - loss: 1.4905 - regression_loss: 1.1517 - classification_loss: 9643/10000 [===========================>..] - ETA: 2:48 - loss: 1.4905 - regression_loss: 1.1517 - classification_loss: 9644/10000 [===========================>..] - ETA: 2:47 - loss: 1.4904 - regression_loss: 1.1516 - classification_loss: 9645/10000 [===========================>..] - ETA: 2:47 - loss: 1.4904 - regression_loss: 1.1516 - classification_loss: 9646/10000 [===========================>..] - ETA: 2:46 - loss: 1.4903 - regression_loss: 1.1515 - classification_loss: 9647/10000 [===========================>..] - ETA: 2:46 - loss: 1.4903 - regression_loss: 1.1515 - classification_loss: 9648/10000 [===========================>..] - ETA: 2:46 - loss: 1.4902 - regression_loss: 1.1514 - classification_loss: 9649/10000 [===========================>..] - ETA: 2:45 - loss: 1.4902 - regression_loss: 1.1515 - classification_loss: 9650/10000 [===========================>..] - ETA: 2:45 - loss: 1.4903 - regression_loss: 1.1515 - classification_loss: 9651/10000 [===========================>..] - ETA: 2:44 - loss: 1.4902 - regression_loss: 1.1515 - classification_loss: 9652/10000 [===========================>..] - ETA: 2:44 - loss: 1.4904 - regression_loss: 1.1516 - classification_loss: 9653/10000 [===========================>..] - ETA: 2:43 - loss: 1.4903 - regression_loss: 1.1515 - classification_loss: 9654/10000 [===========================>..] - ETA: 2:43 - loss: 1.4904 - regression_loss: 1.1516 - classification_loss: 9655/10000 [===========================>..] - ETA: 2:42 - loss: 1.4905 - regression_loss: 1.1516 - classification_loss: 9656/10000 [===========================>..] - ETA: 2:42 - loss: 1.4905 - regression_loss: 1.1516 - classification_loss: 9657/10000 [===========================>..] - ETA: 2:41 - loss: 1.4905 - regression_loss: 1.1517 - classification_loss: 9658/10000 [===========================>..] - ETA: 2:41 - loss: 1.4905 - regression_loss: 1.1517 - classification_loss: 9659/10000 [===========================>..] - ETA: 2:40 - loss: 1.4904 - regression_loss: 1.1516 - classification_loss: 9660/10000 [===========================>..] - ETA: 2:40 - loss: 1.4904 - regression_loss: 1.1516 - classification_loss: 9661/10000 [===========================>..] - ETA: 2:39 - loss: 1.4903 - regression_loss: 1.1516 - classification_loss: 9662/10000 [===========================>..] - ETA: 2:39 - loss: 1.4903 - regression_loss: 1.1515 - classification_loss: 9663/10000 [===========================>..] - ETA: 2:38 - loss: 1.4903 - regression_loss: 1.1515 - classification_loss: 9664/10000 [===========================>..] - ETA: 2:38 - loss: 1.4902 - regression_loss: 1.1515 - classification_loss: 9665/10000 [===========================>..] - ETA: 2:37 - loss: 1.4902 - regression_loss: 1.1515 - classification_loss: 9666/10000 [===========================>..] - ETA: 2:37 - loss: 1.4901 - regression_loss: 1.1514 - classification_loss: 9667/10000 [============================>.] - ETA: 2:37 - loss: 1.4901 - regression_loss: 1.1514 - classification_loss: 9668/10000 [============================>.] - ETA: 2:36 - loss: 1.4901 - regression_loss: 1.1514 - classification_loss: 9669/10000 [============================>.] - ETA: 2:36 - loss: 1.4900 - regression_loss: 1.1513 - classification_loss: 9670/10000 [============================>.] - ETA: 2:35 - loss: 1.4899 - regression_loss: 1.1513 - classification_loss: 9671/10000 [============================>.] - ETA: 2:35 - loss: 1.4900 - regression_loss: 1.1513 - classification_loss: 9672/10000 [============================>.] - ETA: 2:34 - loss: 1.4899 - regression_loss: 1.1512 - classification_loss: 9673/10000 [============================>.] - ETA: 2:34 - loss: 1.4898 - regression_loss: 1.1512 - classification_loss: 9674/10000 [============================>.] - ETA: 2:33 - loss: 1.4899 - regression_loss: 1.1512 - classification_loss: 9675/10000 [============================>.] - ETA: 2:33 - loss: 1.4899 - regression_loss: 1.1512 - classification_loss: 9676/10000 [============================>.] - ETA: 2:32 - loss: 1.4899 - regression_loss: 1.1512 - classification_loss: 9677/10000 [============================>.] - ETA: 2:32 - loss: 1.4898 - regression_loss: 1.1512 - classification_loss: 9678/10000 [============================>.] - ETA: 2:31 - loss: 1.4899 - regression_loss: 1.1513 - classification_loss: 9679/10000 [============================>.] - ETA: 2:31 - loss: 1.4899 - regression_loss: 1.1513 - classification_loss: 9680/10000 [============================>.] - ETA: 2:30 - loss: 1.4898 - regression_loss: 1.1512 - classification_loss: 9681/10000 [============================>.] - ETA: 2:30 - loss: 1.4899 - regression_loss: 1.1512 - classification_loss: 9682/10000 [============================>.] - ETA: 2:29 - loss: 1.4898 - regression_loss: 1.1512 - classification_loss: 9683/10000 [============================>.] - ETA: 2:29 - loss: 1.4899 - regression_loss: 1.1512 - classification_loss: 9684/10000 [============================>.] - ETA: 2:29 - loss: 1.4898 - regression_loss: 1.1512 - classification_loss: 9685/10000 [============================>.] - ETA: 2:28 - loss: 1.4898 - regression_loss: 1.1512 - classification_loss: 9686/10000 [============================>.] - ETA: 2:28 - loss: 1.4898 - regression_loss: 1.1512 - classification_loss: 9687/10000 [============================>.] - ETA: 2:27 - loss: 1.4897 - regression_loss: 1.1511 - classification_loss: 9688/10000 [============================>.] - ETA: 2:27 - loss: 1.4897 - regression_loss: 1.1511 - classification_loss: 9689/10000 [============================>.] - ETA: 2:26 - loss: 1.4897 - regression_loss: 1.1511 - classification_loss: 9690/10000 [============================>.] - ETA: 2:26 - loss: 1.4897 - regression_loss: 1.1511 - classification_loss: 9691/10000 [============================>.] - ETA: 2:25 - loss: 1.4897 - regression_loss: 1.1511 - classification_loss: 9692/10000 [============================>.] - ETA: 2:25 - loss: 1.4896 - regression_loss: 1.1510 - classification_loss: 9693/10000 [============================>.] - ETA: 2:24 - loss: 1.4897 - regression_loss: 1.1511 - classification_loss: 9694/10000 [============================>.] - ETA: 2:24 - loss: 1.4897 - regression_loss: 1.1511 - classification_loss: 9695/10000 [============================>.] - ETA: 2:23 - loss: 1.4898 - regression_loss: 1.1512 - classification_loss: 9696/10000 [============================>.] - ETA: 2:23 - loss: 1.4899 - regression_loss: 1.1513 - classification_loss: 9697/10000 [============================>.] - ETA: 2:22 - loss: 1.4899 - regression_loss: 1.1513 - classification_loss: 9698/10000 [============================>.] - ETA: 2:22 - loss: 1.4899 - regression_loss: 1.1513 - classification_loss: 9699/10000 [============================>.] - ETA: 2:21 - loss: 1.4899 - regression_loss: 1.1512 - classification_loss: 9700/10000 [============================>.] - ETA: 2:21 - loss: 1.4899 - regression_loss: 1.1513 - classification_loss: 9701/10000 [============================>.] - ETA: 2:21 - loss: 1.4899 - regression_loss: 1.1513 - classification_loss: 9702/10000 [============================>.] - ETA: 2:20 - loss: 1.4901 - regression_loss: 1.1514 - classification_loss: 9703/10000 [============================>.] - ETA: 2:20 - loss: 1.4902 - regression_loss: 1.1516 - classification_loss: 9704/10000 [============================>.] - ETA: 2:19 - loss: 1.4902 - regression_loss: 1.1516 - classification_loss: 9705/10000 [============================>.] - ETA: 2:19 - loss: 1.4902 - regression_loss: 1.1516 - classification_loss: 9706/10000 [============================>.] - ETA: 2:18 - loss: 1.4903 - regression_loss: 1.1516 - classification_loss: 9707/10000 [============================>.] - ETA: 2:18 - loss: 1.4902 - regression_loss: 1.1516 - classification_loss: 9708/10000 [============================>.] - ETA: 2:17 - loss: 1.4902 - regression_loss: 1.1516 - classification_loss: 9709/10000 [============================>.] - ETA: 2:17 - loss: 1.4902 - regression_loss: 1.1516 - classification_loss: 9710/10000 [============================>.] - ETA: 2:16 - loss: 1.4902 - regression_loss: 1.1516 - classification_loss: 9711/10000 [============================>.] - ETA: 2:16 - loss: 1.4901 - regression_loss: 1.1515 - classification_loss: 9712/10000 [============================>.] - ETA: 2:15 - loss: 1.4902 - regression_loss: 1.1516 - classification_loss: 9713/10000 [============================>.] - ETA: 2:15 - loss: 1.4903 - regression_loss: 1.1516 - classification_loss: 9714/10000 [============================>.] - ETA: 2:14 - loss: 1.4904 - regression_loss: 1.1517 - classification_loss: 9715/10000 [============================>.] - ETA: 2:14 - loss: 1.4904 - regression_loss: 1.1517 - classification_loss: 9716/10000 [============================>.] - ETA: 2:13 - loss: 1.4905 - regression_loss: 1.1518 - classification_loss: 9717/10000 [============================>.] - ETA: 2:13 - loss: 1.4906 - regression_loss: 1.1519 - classification_loss: 9718/10000 [============================>.] - ETA: 2:12 - loss: 1.4906 - regression_loss: 1.1519 - classification_loss: 9719/10000 [============================>.] - ETA: 2:12 - loss: 1.4907 - regression_loss: 1.1520 - classification_loss: 9720/10000 [============================>.] - ETA: 2:12 - loss: 1.4906 - regression_loss: 1.1519 - classification_loss: 9721/10000 [============================>.] - ETA: 2:11 - loss: 1.4907 - regression_loss: 1.1520 - classification_loss: 9722/10000 [============================>.] - ETA: 2:11 - loss: 1.4907 - regression_loss: 1.1520 - classification_loss: 9723/10000 [============================>.] - ETA: 2:10 - loss: 1.4906 - regression_loss: 1.1519 - classification_loss: 9724/10000 [============================>.] - ETA: 2:10 - loss: 1.4906 - regression_loss: 1.1519 - classification_loss: 9725/10000 [============================>.] - ETA: 2:09 - loss: 1.4906 - regression_loss: 1.1519 - classification_loss: 9726/10000 [============================>.] - ETA: 2:09 - loss: 1.4905 - regression_loss: 1.1518 - classification_loss: 9727/10000 [============================>.] - ETA: 2:08 - loss: 1.4904 - regression_loss: 1.1518 - classification_loss: 9728/10000 [============================>.] - ETA: 2:08 - loss: 1.4905 - regression_loss: 1.1518 - classification_loss: 9729/10000 [============================>.] - ETA: 2:07 - loss: 1.4905 - regression_loss: 1.1518 - classification_loss: 9730/10000 [============================>.] - ETA: 2:07 - loss: 1.4905 - regression_loss: 1.1518 - classification_loss: 9731/10000 [============================>.] - ETA: 2:06 - loss: 1.4906 - regression_loss: 1.1519 - classification_loss: 9732/10000 [============================>.] - ETA: 2:06 - loss: 1.4905 - regression_loss: 1.1518 - classification_loss: 9733/10000 [============================>.] - ETA: 2:05 - loss: 1.4904 - regression_loss: 1.1517 - classification_loss: 9734/10000 [============================>.] - ETA: 2:05 - loss: 1.4904 - regression_loss: 1.1517 - classification_loss: 9735/10000 [============================>.] - ETA: 2:04 - loss: 1.4904 - regression_loss: 1.1517 - classification_loss: 9736/10000 [============================>.] - ETA: 2:04 - loss: 1.4904 - regression_loss: 1.1518 - classification_loss: 9737/10000 [============================>.] - ETA: 2:04 - loss: 1.4904 - regression_loss: 1.1518 - classification_loss: 9738/10000 [============================>.] - ETA: 2:03 - loss: 1.4905 - regression_loss: 1.1518 - classification_loss: 9739/10000 [============================>.] - ETA: 2:03 - loss: 1.4905 - regression_loss: 1.1519 - classification_loss: 9740/10000 [============================>.] - ETA: 2:02 - loss: 1.4905 - regression_loss: 1.1518 - classification_loss: 9741/10000 [============================>.] - ETA: 2:02 - loss: 1.4905 - regression_loss: 1.1518 - classification_loss: 9742/10000 [============================>.] - ETA: 2:01 - loss: 1.4904 - regression_loss: 1.1518 - classification_loss: 9743/10000 [============================>.] - ETA: 2:01 - loss: 1.4904 - regression_loss: 1.1518 - classification_loss: 9744/10000 [============================>.] - ETA: 2:00 - loss: 1.4905 - regression_loss: 1.1518 - classification_loss: 9745/10000 [============================>.] - ETA: 2:00 - loss: 1.4905 - regression_loss: 1.1519 - classification_loss: 9746/10000 [============================>.] - ETA: 1:59 - loss: 1.4906 - regression_loss: 1.1519 - classification_loss: 9747/10000 [============================>.] - ETA: 1:59 - loss: 1.4906 - regression_loss: 1.1520 - classification_loss: 9748/10000 [============================>.] - ETA: 1:58 - loss: 1.4907 - regression_loss: 1.1520 - classification_loss: 9749/10000 [============================>.] - ETA: 1:58 - loss: 1.4907 - regression_loss: 1.1520 - classification_loss: 9750/10000 [============================>.] - ETA: 1:57 - loss: 1.4907 - regression_loss: 1.1520 - classification_loss: 9751/10000 [============================>.] - ETA: 1:57 - loss: 1.4906 - regression_loss: 1.1519 - classification_loss: 9752/10000 [============================>.] - ETA: 1:56 - loss: 1.4907 - regression_loss: 1.1520 - classification_loss: 9753/10000 [============================>.] - ETA: 1:56 - loss: 1.4908 - regression_loss: 1.1521 - classification_loss: 9754/10000 [============================>.] - ETA: 1:55 - loss: 1.4908 - regression_loss: 1.1521 - classification_loss: 9755/10000 [============================>.] - ETA: 1:55 - loss: 1.4907 - regression_loss: 1.1520 - classification_loss: 9756/10000 [============================>.] - ETA: 1:55 - loss: 1.4907 - regression_loss: 1.1520 - classification_loss: 9757/10000 [============================>.] - ETA: 1:54 - loss: 1.4907 - regression_loss: 1.1520 - classification_loss: 9758/10000 [============================>.] - ETA: 1:54 - loss: 1.4907 - regression_loss: 1.1520 - classification_loss: 9759/10000 [============================>.] - ETA: 1:53 - loss: 1.4906 - regression_loss: 1.1519 - classification_loss: 9760/10000 [============================>.] - ETA: 1:53 - loss: 1.4905 - regression_loss: 1.1519 - classification_loss: 9761/10000 [============================>.] - ETA: 1:52 - loss: 1.4905 - regression_loss: 1.1519 - classification_loss: 9762/10000 [============================>.] - ETA: 1:52 - loss: 1.4906 - regression_loss: 1.1519 - classification_loss: 9763/10000 [============================>.] - ETA: 1:51 - loss: 1.4906 - regression_loss: 1.1520 - classification_loss: 9764/10000 [============================>.] - ETA: 1:51 - loss: 1.4907 - regression_loss: 1.1520 - classification_loss: 9765/10000 [============================>.] - ETA: 1:50 - loss: 1.4907 - regression_loss: 1.1520 - classification_loss: 9766/10000 [============================>.] - ETA: 1:50 - loss: 1.4907 - regression_loss: 1.1521 - classification_loss: 9767/10000 [============================>.] - ETA: 1:49 - loss: 1.4907 - regression_loss: 1.1520 - classification_loss: 9768/10000 [============================>.] - ETA: 1:49 - loss: 1.4908 - regression_loss: 1.1521 - classification_loss: 9769/10000 [============================>.] - ETA: 1:48 - loss: 1.4908 - regression_loss: 1.1521 - classification_loss: 9770/10000 [============================>.] - ETA: 1:48 - loss: 1.4907 - regression_loss: 1.1521 - classification_loss: 9771/10000 [============================>.] - ETA: 1:47 - loss: 1.4907 - regression_loss: 1.1520 - classification_loss: 9772/10000 [============================>.] - ETA: 1:47 - loss: 1.4907 - regression_loss: 1.1521 - classification_loss: 9773/10000 [============================>.] - ETA: 1:47 - loss: 1.4907 - regression_loss: 1.1520 - classification_loss: 9774/10000 [============================>.] - ETA: 1:46 - loss: 1.4907 - regression_loss: 1.1520 - classification_loss: 9775/10000 [============================>.] - ETA: 1:46 - loss: 1.4906 - regression_loss: 1.1520 - classification_loss: 9776/10000 [============================>.] - ETA: 1:45 - loss: 1.4906 - regression_loss: 1.1520 - classification_loss: 9777/10000 [============================>.] - ETA: 1:45 - loss: 1.4907 - regression_loss: 1.1520 - classification_loss: 9778/10000 [============================>.] - ETA: 1:44 - loss: 1.4906 - regression_loss: 1.1520 - classification_loss: 9779/10000 [============================>.] - ETA: 1:44 - loss: 1.4905 - regression_loss: 1.1520 - classification_loss: 9780/10000 [============================>.] - ETA: 1:43 - loss: 1.4906 - regression_loss: 1.1520 - classification_loss: 9781/10000 [============================>.] - ETA: 1:43 - loss: 1.4906 - regression_loss: 1.1521 - classification_loss: 9782/10000 [============================>.] - ETA: 1:42 - loss: 1.4906 - regression_loss: 1.1521 - classification_loss: 9783/10000 [============================>.] - ETA: 1:42 - loss: 1.4906 - regression_loss: 1.1520 - classification_loss: 9784/10000 [============================>.] - ETA: 1:41 - loss: 1.4905 - regression_loss: 1.1520 - classification_loss: 9785/10000 [============================>.] - ETA: 1:41 - loss: 1.4906 - regression_loss: 1.1521 - classification_loss: 9786/10000 [============================>.] - ETA: 1:40 - loss: 1.4906 - regression_loss: 1.1521 - classification_loss: 9787/10000 [============================>.] - ETA: 1:40 - loss: 1.4906 - regression_loss: 1.1520 - classification_loss: 9788/10000 [============================>.] - ETA: 1:39 - loss: 1.4905 - regression_loss: 1.1520 - classification_loss: 9789/10000 [============================>.] - ETA: 1:39 - loss: 1.4904 - regression_loss: 1.1519 - classification_loss: 9790/10000 [============================>.] - ETA: 1:39 - loss: 1.4904 - regression_loss: 1.1519 - classification_loss: 9791/10000 [============================>.] - ETA: 1:38 - loss: 1.4904 - regression_loss: 1.1519 - classification_loss: 9792/10000 [============================>.] - ETA: 1:38 - loss: 1.4904 - regression_loss: 1.1519 - classification_loss: 9793/10000 [============================>.] - ETA: 1:37 - loss: 1.4904 - regression_loss: 1.1519 - classification_loss: 9794/10000 [============================>.] - ETA: 1:37 - loss: 1.4904 - regression_loss: 1.1519 - classification_loss: 9795/10000 [============================>.] - ETA: 1:36 - loss: 1.4905 - regression_loss: 1.1520 - classification_loss: 9796/10000 [============================>.] - ETA: 1:36 - loss: 1.4904 - regression_loss: 1.1519 - classification_loss: 9797/10000 [============================>.] - ETA: 1:35 - loss: 1.4904 - regression_loss: 1.1519 - classification_loss: 9798/10000 [============================>.] - ETA: 1:35 - loss: 1.4904 - regression_loss: 1.1519 - classification_loss: 9799/10000 [============================>.] - ETA: 1:34 - loss: 1.4904 - regression_loss: 1.1518 - classification_loss: 9800/10000 [============================>.] - ETA: 1:34 - loss: 1.4904 - regression_loss: 1.1519 - classification_loss: 9801/10000 [============================>.] - ETA: 1:33 - loss: 1.4904 - regression_loss: 1.1519 - classification_loss: 9802/10000 [============================>.] - ETA: 1:33 - loss: 1.4904 - regression_loss: 1.1519 - classification_loss: 9803/10000 [============================>.] - ETA: 1:32 - loss: 1.4904 - regression_loss: 1.1519 - classification_loss: 9804/10000 [============================>.] - ETA: 1:32 - loss: 1.4904 - regression_loss: 1.1518 - classification_loss: 9805/10000 [============================>.] - ETA: 1:31 - loss: 1.4904 - regression_loss: 1.1518 - classification_loss: 9806/10000 [============================>.] - ETA: 1:31 - loss: 1.4904 - regression_loss: 1.1518 - classification_loss: 9807/10000 [============================>.] - ETA: 1:30 - loss: 1.4903 - regression_loss: 1.1518 - classification_loss: 9808/10000 [============================>.] - ETA: 1:30 - loss: 1.4903 - regression_loss: 1.1518 - classification_loss: 9809/10000 [============================>.] - ETA: 1:30 - loss: 1.4904 - regression_loss: 1.1518 - classification_loss: 9810/10000 [============================>.] - ETA: 1:29 - loss: 1.4904 - regression_loss: 1.1519 - classification_loss: 9811/10000 [============================>.] - ETA: 1:29 - loss: 1.4904 - regression_loss: 1.1519 - classification_loss: 9812/10000 [============================>.] - ETA: 1:28 - loss: 1.4904 - regression_loss: 1.1518 - classification_loss: 9813/10000 [============================>.] - ETA: 1:28 - loss: 1.4903 - regression_loss: 1.1518 - classification_loss: 9814/10000 [============================>.] - ETA: 1:27 - loss: 1.4903 - regression_loss: 1.1518 - classification_loss: 9815/10000 [============================>.] - ETA: 1:27 - loss: 1.4903 - regression_loss: 1.1518 - classification_loss: 9816/10000 [============================>.] - ETA: 1:26 - loss: 1.4904 - regression_loss: 1.1519 - classification_loss: 9817/10000 [============================>.] - ETA: 1:26 - loss: 1.4903 - regression_loss: 1.1518 - classification_loss: 9818/10000 [============================>.] - ETA: 1:25 - loss: 1.4902 - regression_loss: 1.1518 - classification_loss: 9819/10000 [============================>.] - ETA: 1:25 - loss: 1.4903 - regression_loss: 1.1518 - classification_loss: 9820/10000 [============================>.] - ETA: 1:24 - loss: 1.4904 - regression_loss: 1.1519 - classification_loss: 9821/10000 [============================>.] - ETA: 1:24 - loss: 1.4904 - regression_loss: 1.1519 - classification_loss: 9822/10000 [============================>.] - ETA: 1:23 - loss: 1.4905 - regression_loss: 1.1520 - classification_loss: 9823/10000 [============================>.] - ETA: 1:23 - loss: 1.4904 - regression_loss: 1.1519 - classification_loss: 9824/10000 [============================>.] - ETA: 1:22 - loss: 1.4905 - regression_loss: 1.1520 - classification_loss: 9825/10000 [============================>.] - ETA: 1:22 - loss: 1.4904 - regression_loss: 1.1520 - classification_loss: 9826/10000 [============================>.] - ETA: 1:22 - loss: 1.4903 - regression_loss: 1.1519 - classification_loss: 9827/10000 [============================>.] - ETA: 1:21 - loss: 1.4903 - regression_loss: 1.1518 - classification_loss: 9828/10000 [============================>.] - ETA: 1:21 - loss: 1.4903 - regression_loss: 1.1519 - classification_loss: 9829/10000 [============================>.] - ETA: 1:20 - loss: 1.4903 - regression_loss: 1.1519 - classification_loss: 9830/10000 [============================>.] - ETA: 1:20 - loss: 1.4903 - regression_loss: 1.1519 - classification_loss: 9831/10000 [============================>.] - ETA: 1:19 - loss: 1.4904 - regression_loss: 1.1519 - classification_loss: 9832/10000 [============================>.] - ETA: 1:19 - loss: 1.4903 - regression_loss: 1.1519 - classification_loss: 9833/10000 [============================>.] - ETA: 1:18 - loss: 1.4903 - regression_loss: 1.1518 - classification_loss: 9834/10000 [============================>.] - ETA: 1:18 - loss: 1.4903 - regression_loss: 1.1519 - classification_loss: 9835/10000 [============================>.] - ETA: 1:17 - loss: 1.4904 - regression_loss: 1.1519 - classification_loss: 9836/10000 [============================>.] - ETA: 1:17 - loss: 1.4903 - regression_loss: 1.1519 - classification_loss: 9837/10000 [============================>.] - ETA: 1:16 - loss: 1.4903 - regression_loss: 1.1519 - classification_loss: 9838/10000 [============================>.] - ETA: 1:16 - loss: 1.4903 - regression_loss: 1.1519 - classification_loss: 9839/10000 [============================>.] - ETA: 1:15 - loss: 1.4903 - regression_loss: 1.1519 - classification_loss: 9840/10000 [============================>.] - ETA: 1:15 - loss: 1.4903 - regression_loss: 1.1519 - classification_loss: 9841/10000 [============================>.] - ETA: 1:14 - loss: 1.4904 - regression_loss: 1.1520 - classification_loss: 9842/10000 [============================>.] - ETA: 1:14 - loss: 1.4904 - regression_loss: 1.1520 - classification_loss: 9843/10000 [============================>.] - ETA: 1:14 - loss: 1.4904 - regression_loss: 1.1520 - classification_loss: 9844/10000 [============================>.] - ETA: 1:13 - loss: 1.4904 - regression_loss: 1.1520 - classification_loss: 9845/10000 [============================>.] - ETA: 1:13 - loss: 1.4903 - regression_loss: 1.1519 - classification_loss: 9846/10000 [============================>.] - ETA: 1:12 - loss: 1.4902 - regression_loss: 1.1518 - classification_loss: 9847/10000 [============================>.] - ETA: 1:12 - loss: 1.4902 - regression_loss: 1.1519 - classification_loss: 9848/10000 [============================>.] - ETA: 1:11 - loss: 1.4902 - regression_loss: 1.1519 - classification_loss: 9849/10000 [============================>.] - ETA: 1:11 - loss: 1.4903 - regression_loss: 1.1519 - classification_loss: 9850/10000 [============================>.] - ETA: 1:10 - loss: 1.4903 - regression_loss: 1.1519 - classification_loss: 9851/10000 [============================>.] - ETA: 1:10 - loss: 1.4903 - regression_loss: 1.1520 - classification_loss: 9852/10000 [============================>.] - ETA: 1:09 - loss: 1.4903 - regression_loss: 1.1519 - classification_loss: 9853/10000 [============================>.] - ETA: 1:09 - loss: 1.4902 - regression_loss: 1.1519 - classification_loss: 9854/10000 [============================>.] - ETA: 1:08 - loss: 1.4902 - regression_loss: 1.1519 - classification_loss: 9855/10000 [============================>.] - ETA: 1:08 - loss: 1.4903 - regression_loss: 1.1519 - classification_loss: 9856/10000 [============================>.] - ETA: 1:07 - loss: 1.4903 - regression_loss: 1.1520 - classification_loss: 9857/10000 [============================>.] - ETA: 1:07 - loss: 1.4903 - regression_loss: 1.1519 - classification_loss: 9858/10000 [============================>.] - ETA: 1:06 - loss: 1.4903 - regression_loss: 1.1519 - classification_loss: 9859/10000 [============================>.] - ETA: 1:06 - loss: 1.4903 - regression_loss: 1.1519 - classification_loss: 9860/10000 [============================>.] - ETA: 1:05 - loss: 1.4903 - regression_loss: 1.1519 - classification_loss: 9861/10000 [============================>.] - ETA: 1:05 - loss: 1.4902 - regression_loss: 1.1519 - classification_loss: 9862/10000 [============================>.] - ETA: 1:05 - loss: 1.4901 - regression_loss: 1.1518 - classification_loss: 9863/10000 [============================>.] - ETA: 1:04 - loss: 1.4901 - regression_loss: 1.1517 - classification_loss: 9864/10000 [============================>.] - ETA: 1:04 - loss: 1.4901 - regression_loss: 1.1517 - classification_loss: 9865/10000 [============================>.] - ETA: 1:03 - loss: 1.4901 - regression_loss: 1.1517 - classification_loss: 9866/10000 [============================>.] - ETA: 1:03 - loss: 1.4900 - regression_loss: 1.1516 - classification_loss: 9867/10000 [============================>.] - ETA: 1:02 - loss: 1.4900 - regression_loss: 1.1516 - classification_loss: 9868/10000 [============================>.] - ETA: 1:02 - loss: 1.4899 - regression_loss: 1.1515 - classification_loss: 9869/10000 [============================>.] - ETA: 1:01 - loss: 1.4900 - regression_loss: 1.1516 - classification_loss: 9870/10000 [============================>.] - ETA: 1:01 - loss: 1.4900 - regression_loss: 1.1516 - classification_loss: 9871/10000 [============================>.] - ETA: 1:00 - loss: 1.4899 - regression_loss: 1.1516 - classification_loss: 9872/10000 [============================>.] - ETA: 1:00 - loss: 1.4899 - regression_loss: 1.1515 - classification_loss: 9873/10000 [============================>.] - ETA: 59s - loss: 1.4898 - regression_loss: 1.1514 - classification_loss:  9874/10000 [============================>.] - ETA: 59s - loss: 1.4897 - regression_loss: 1.1514 - classification_loss:  9875/10000 [============================>.] - ETA: 58s - loss: 1.4897 - regression_loss: 1.1513 - classification_loss:  9876/10000 [============================>.] - ETA: 58s - loss: 1.4896 - regression_loss: 1.1513 - classification_loss:  9877/10000 [============================>.] - ETA: 57s - loss: 1.4897 - regression_loss: 1.1514 - classification_loss:  9878/10000 [============================>.] - ETA: 57s - loss: 1.4897 - regression_loss: 1.1514 - classification_loss:  9879/10000 [============================>.] - ETA: 57s - loss: 1.4897 - regression_loss: 1.1514 - classification_loss:  9880/10000 [============================>.] - ETA: 56s - loss: 1.4897 - regression_loss: 1.1514 - classification_loss:  9881/10000 [============================>.] - ETA: 56s - loss: 1.4897 - regression_loss: 1.1514 - classification_loss:  9882/10000 [============================>.] - ETA: 55s - loss: 1.4896 - regression_loss: 1.1514 - classification_loss:  9883/10000 [============================>.] - ETA: 55s - loss: 1.4897 - regression_loss: 1.1514 - classification_loss:  9884/10000 [============================>.] - ETA: 54s - loss: 1.4897 - regression_loss: 1.1515 - classification_loss:  9885/10000 [============================>.] - ETA: 54s - loss: 1.4897 - regression_loss: 1.1514 - classification_loss:  9886/10000 [============================>.] - ETA: 53s - loss: 1.4898 - regression_loss: 1.1515 - classification_loss:  9887/10000 [============================>.] - ETA: 53s - loss: 1.4897 - regression_loss: 1.1515 - classification_loss:  9888/10000 [============================>.] - ETA: 52s - loss: 1.4897 - regression_loss: 1.1514 - classification_loss:  9889/10000 [============================>.] - ETA: 52s - loss: 1.4897 - regression_loss: 1.1514 - classification_loss:  9890/10000 [============================>.] - ETA: 51s - loss: 1.4897 - regression_loss: 1.1514 - classification_loss:  9891/10000 [============================>.] - ETA: 51s - loss: 1.4896 - regression_loss: 1.1513 - classification_loss:  9892/10000 [============================>.] - ETA: 50s - loss: 1.4896 - regression_loss: 1.1512 - classification_loss:  9893/10000 [============================>.] - ETA: 50s - loss: 1.4896 - regression_loss: 1.1513 - classification_loss:  9894/10000 [============================>.] - ETA: 49s - loss: 1.4896 - regression_loss: 1.1513 - classification_loss:  9895/10000 [============================>.] - ETA: 49s - loss: 1.4896 - regression_loss: 1.1513 - classification_loss:  9896/10000 [============================>.] - ETA: 49s - loss: 1.4895 - regression_loss: 1.1512 - classification_loss:  9897/10000 [============================>.] - ETA: 48s - loss: 1.4894 - regression_loss: 1.1512 - classification_loss:  9898/10000 [============================>.] - ETA: 48s - loss: 1.4894 - regression_loss: 1.1511 - classification_loss:  9899/10000 [============================>.] - ETA: 47s - loss: 1.4894 - regression_loss: 1.1511 - classification_loss:  9900/10000 [============================>.] - ETA: 47s - loss: 1.4893 - regression_loss: 1.1510 - classification_loss:  9901/10000 [============================>.] - ETA: 46s - loss: 1.4893 - regression_loss: 1.1511 - classification_loss:  9902/10000 [============================>.] - ETA: 46s - loss: 1.4893 - regression_loss: 1.1510 - classification_loss:  9903/10000 [============================>.] - ETA: 45s - loss: 1.4893 - regression_loss: 1.1510 - classification_loss:  9904/10000 [============================>.] - ETA: 45s - loss: 1.4892 - regression_loss: 1.1510 - classification_loss:  9905/10000 [============================>.] - ETA: 44s - loss: 1.4893 - regression_loss: 1.1510 - classification_loss:  9906/10000 [============================>.] - ETA: 44s - loss: 1.4892 - regression_loss: 1.1510 - classification_loss:  9907/10000 [============================>.] - ETA: 43s - loss: 1.4892 - regression_loss: 1.1510 - classification_loss:  9908/10000 [============================>.] - ETA: 43s - loss: 1.4892 - regression_loss: 1.1510 - classification_loss:  9909/10000 [============================>.] - ETA: 42s - loss: 1.4891 - regression_loss: 1.1509 - classification_loss:  9910/10000 [============================>.] - ETA: 42s - loss: 1.4891 - regression_loss: 1.1509 - classification_loss:  9911/10000 [============================>.] - ETA: 41s - loss: 1.4891 - regression_loss: 1.1508 - classification_loss:  9912/10000 [============================>.] - ETA: 41s - loss: 1.4890 - regression_loss: 1.1508 - classification_loss:  9913/10000 [============================>.] - ETA: 41s - loss: 1.4890 - regression_loss: 1.1508 - classification_loss:  9914/10000 [============================>.] - ETA: 40s - loss: 1.4890 - regression_loss: 1.1508 - classification_loss:  9915/10000 [============================>.] - ETA: 40s - loss: 1.4889 - regression_loss: 1.1507 - classification_loss:  9916/10000 [============================>.] - ETA: 39s - loss: 1.4888 - regression_loss: 1.1506 - classification_loss:  9917/10000 [============================>.] - ETA: 39s - loss: 1.4889 - regression_loss: 1.1507 - classification_loss:  9918/10000 [============================>.] - ETA: 38s - loss: 1.4888 - regression_loss: 1.1506 - classification_loss:  9919/10000 [============================>.] - ETA: 38s - loss: 1.4888 - regression_loss: 1.1506 - classification_loss:  9920/10000 [============================>.] - ETA: 37s - loss: 1.4889 - regression_loss: 1.1507 - classification_loss:  9921/10000 [============================>.] - ETA: 37s - loss: 1.4889 - regression_loss: 1.1507 - classification_loss:  9922/10000 [============================>.] - ETA: 36s - loss: 1.4890 - regression_loss: 1.1508 - classification_loss:  9923/10000 [============================>.] - ETA: 36s - loss: 1.4890 - regression_loss: 1.1508 - classification_loss:  9924/10000 [============================>.] - ETA: 35s - loss: 1.4888 - regression_loss: 1.1507 - classification_loss:  9925/10000 [============================>.] - ETA: 35s - loss: 1.4888 - regression_loss: 1.1506 - classification_loss:  9926/10000 [============================>.] - ETA: 34s - loss: 1.4889 - regression_loss: 1.1507 - classification_loss:  9927/10000 [============================>.] - ETA: 34s - loss: 1.4889 - regression_loss: 1.1507 - classification_loss:  9928/10000 [============================>.] - ETA: 33s - loss: 1.4889 - regression_loss: 1.1507 - classification_loss:  9929/10000 [============================>.] - ETA: 33s - loss: 1.4889 - regression_loss: 1.1507 - classification_loss:  9930/10000 [============================>.] - ETA: 32s - loss: 1.4889 - regression_loss: 1.1507 - classification_loss:  9931/10000 [============================>.] - ETA: 32s - loss: 1.4888 - regression_loss: 1.1506 - classification_loss:  9932/10000 [============================>.] - ETA: 32s - loss: 1.4887 - regression_loss: 1.1506 - classification_loss:  9933/10000 [============================>.] - ETA: 31s - loss: 1.4887 - regression_loss: 1.1506 - classification_loss:  9934/10000 [============================>.] - ETA: 31s - loss: 1.4888 - regression_loss: 1.1506 - classification_loss:  9935/10000 [============================>.] - ETA: 30s - loss: 1.4888 - regression_loss: 1.1506 - classification_loss:  9936/10000 [============================>.] - ETA: 30s - loss: 1.4887 - regression_loss: 1.1506 - classification_loss:  9937/10000 [============================>.] - ETA: 29s - loss: 1.4889 - regression_loss: 1.1507 - classification_loss:  9938/10000 [============================>.] - ETA: 29s - loss: 1.4889 - regression_loss: 1.1507 - classification_loss:  9939/10000 [============================>.] - ETA: 28s - loss: 1.4889 - regression_loss: 1.1506 - classification_loss:  9940/10000 [============================>.] - ETA: 28s - loss: 1.4889 - regression_loss: 1.1507 - classification_loss:  9941/10000 [============================>.] - ETA: 27s - loss: 1.4889 - regression_loss: 1.1507 - classification_loss:  9942/10000 [============================>.] - ETA: 27s - loss: 1.4890 - regression_loss: 1.1507 - classification_loss:  9943/10000 [============================>.] - ETA: 26s - loss: 1.4889 - regression_loss: 1.1507 - classification_loss:  9944/10000 [============================>.] - ETA: 26s - loss: 1.4888 - regression_loss: 1.1506 - classification_loss:  9945/10000 [============================>.] - ETA: 25s - loss: 1.4887 - regression_loss: 1.1505 - classification_loss:  9946/10000 [============================>.] - ETA: 25s - loss: 1.4887 - regression_loss: 1.1505 - classification_loss:  9947/10000 [============================>.] - ETA: 24s - loss: 1.4886 - regression_loss: 1.1504 - classification_loss:  9948/10000 [============================>.] - ETA: 24s - loss: 1.4886 - regression_loss: 1.1505 - classification_loss:  9949/10000 [============================>.] - ETA: 24s - loss: 1.4886 - regression_loss: 1.1505 - classification_loss:  9950/10000 [============================>.] - ETA: 23s - loss: 1.4886 - regression_loss: 1.1505 - classification_loss:  9951/10000 [============================>.] - ETA: 23s - loss: 1.4887 - regression_loss: 1.1505 - classification_loss:  9952/10000 [============================>.] - ETA: 22s - loss: 1.4887 - regression_loss: 1.1506 - classification_loss:  9953/10000 [============================>.] - ETA: 22s - loss: 1.4887 - regression_loss: 1.1506 - classification_loss:  9954/10000 [============================>.] - ETA: 21s - loss: 1.4887 - regression_loss: 1.1505 - classification_loss:  9955/10000 [============================>.] - ETA: 21s - loss: 1.4886 - regression_loss: 1.1505 - classification_loss:  9956/10000 [============================>.] - ETA: 20s - loss: 1.4885 - regression_loss: 1.1504 - classification_loss:  9957/10000 [============================>.] - ETA: 20s - loss: 1.4885 - regression_loss: 1.1504 - classification_loss:  9958/10000 [============================>.] - ETA: 19s - loss: 1.4885 - regression_loss: 1.1504 - classification_loss:  9959/10000 [============================>.] - ETA: 19s - loss: 1.4885 - regression_loss: 1.1504 - classification_loss:  9960/10000 [============================>.] - ETA: 18s - loss: 1.4884 - regression_loss: 1.1503 - classification_loss:  9961/10000 [============================>.] - ETA: 18s - loss: 1.4884 - regression_loss: 1.1503 - classification_loss:  9962/10000 [============================>.] - ETA: 17s - loss: 1.4884 - regression_loss: 1.1503 - classification_loss:  9963/10000 [============================>.] - ETA: 17s - loss: 1.4883 - regression_loss: 1.1503 - classification_loss:  9964/10000 [============================>.] - ETA: 16s - loss: 1.4883 - regression_loss: 1.1502 - classification_loss:  9965/10000 [============================>.] - ETA: 16s - loss: 1.4882 - regression_loss: 1.1502 - classification_loss:  9966/10000 [============================>.] - ETA: 16s - loss: 1.4882 - regression_loss: 1.1502 - classification_loss:  9967/10000 [============================>.] - ETA: 15s - loss: 1.4882 - regression_loss: 1.1502 - classification_loss:  9968/10000 [============================>.] - ETA: 15s - loss: 1.4882 - regression_loss: 1.1502 - classification_loss:  9969/10000 [============================>.] - ETA: 14s - loss: 1.4882 - regression_loss: 1.1502 - classification_loss:  9970/10000 [============================>.] - ETA: 14s - loss: 1.4884 - regression_loss: 1.1503 - classification_loss:  9971/10000 [============================>.] - ETA: 13s - loss: 1.4883 - regression_loss: 1.1503 - classification_loss:  9972/10000 [============================>.] - ETA: 13s - loss: 1.4882 - regression_loss: 1.1502 - classification_loss:  9973/10000 [============================>.] - ETA: 12s - loss: 1.4882 - regression_loss: 1.1501 - classification_loss:  9974/10000 [============================>.] - ETA: 12s - loss: 1.4882 - regression_loss: 1.1502 - classification_loss:  9975/10000 [============================>.] - ETA: 11s - loss: 1.4882 - regression_loss: 1.1502 - classification_loss:  9976/10000 [============================>.] - ETA: 11s - loss: 1.4882 - regression_loss: 1.1502 - classification_loss:  9977/10000 [============================>.] - ETA: 10s - loss: 1.4882 - regression_loss: 1.1502 - classification_loss:  9978/10000 [============================>.] - ETA: 10s - loss: 1.4882 - regression_loss: 1.1502 - classification_loss:  9979/10000 [============================>.] - ETA: 9s - loss: 1.4882 - regression_loss: 1.1501 - classification_loss: 0 9980/10000 [============================>.] - ETA: 9s - loss: 1.4881 - regression_loss: 1.1501 - classification_loss: 0 9981/10000 [============================>.] - ETA: 8s - loss: 1.4881 - regression_loss: 1.1501 - classification_loss: 0 9982/10000 [============================>.] - ETA: 8s - loss: 1.4882 - regression_loss: 1.1502 - classification_loss: 0 9983/10000 [============================>.] - ETA: 8s - loss: 1.4882 - regression_loss: 1.1503 - classification_loss: 0 9984/10000 [============================>.] - ETA: 7s - loss: 1.4882 - regression_loss: 1.1502 - classification_loss: 0 9985/10000 [============================>.] - ETA: 7s - loss: 1.4881 - regression_loss: 1.1502 - classification_loss: 0 9986/10000 [============================>.] - ETA: 6s - loss: 1.4882 - regression_loss: 1.1502 - classification_loss: 0 9987/10000 [============================>.] - ETA: 6s - loss: 1.4881 - regression_loss: 1.1502 - classification_loss: 0 9988/10000 [============================>.] - ETA: 5s - loss: 1.4881 - regression_loss: 1.1502 - classification_loss: 0 9989/10000 [============================>.] - ETA: 5s - loss: 1.4880 - regression_loss: 1.1501 - classification_loss: 0 9990/10000 [============================>.] - ETA: 4s - loss: 1.4879 - regression_loss: 1.1500 - classification_loss: 0 9991/10000 [============================>.] - ETA: 4s - loss: 1.4878 - regression_loss: 1.1499 - classification_loss: 0 9992/10000 [============================>.] - ETA: 3s - loss: 1.4878 - regression_loss: 1.1499 - classification_loss: 0 9993/10000 [============================>.] - ETA: 3s - loss: 1.4878 - regression_loss: 1.1500 - classification_loss: 0 9994/10000 [============================>.] - ETA: 2s - loss: 1.4877 - regression_loss: 1.1499 - classification_loss: 0 9995/10000 [============================>.] - ETA: 2s - loss: 1.4877 - regression_loss: 1.1498 - classification_loss: 0 9996/10000 [============================>.] - ETA: 1s - loss: 1.4877 - regression_loss: 1.1499 - classification_loss: 0 9997/10000 [============================>.] - ETA: 1s - loss: 1.4877 - regression_loss: 1.1499 - classification_loss: 0 9998/10000 [============================>.] - ETA: 0s - loss: 1.4877 - regression_loss: 1.1499 - classification_loss: 0 9999/10000 [============================>.] - ETA: 0s - loss: 1.4876 - regression_loss: 1.1498 - classification_loss: 010000/10000 [==============================] - 4712s 471ms/step - loss: 1.4876 - regression_loss: 1.1498 - classification_loss: 0.3378
aeroplane 0.4924
bicycle 0.5339
bird 0.3801
boat 0.2233
bottle 0.3037
bus 0.4218
car 0.6774
cat 0.6358
chair 0.2993
cow 0.2261
diningtable 0.3435
dog 0.3621
horse 0.5159
motorbike 0.4725
person 0.7085
pottedplant 0.2486
sheep 0.2400
sofa 0.4055
train 0.4135
tvmonitor 0.5165
mAP: 0.4210

Epoch 00001: saving model to ./snapshots\resnet50_pascal_01.h5

D:\JupyterWorkSpace\keras-retinanet>pause
. . .