Creating model, this may take a second...
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, None, None, 3 0                                            
__________________________________________________________________________________________________
padding_conv1 (ZeroPadding2D)   (None, None, None, 3 0           input_1[0][0]                    
__________________________________________________________________________________________________
conv1 (Conv2D)                  (None, None, None, 6 9408        padding_conv1[0][0]              
__________________________________________________________________________________________________
bn_conv1 (BatchNormalization)   (None, None, None, 6 256         conv1[0][0]                      
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, None, None, 6 0           bn_conv1[0][0]                   
__________________________________________________________________________________________________
pool1 (MaxPooling2D)            (None, None, None, 6 0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
res2a_branch2a (Conv2D)         (None, None, None, 6 4096        pool1[0][0]                      
__________________________________________________________________________________________________
bn2a_branch2a (BatchNormalizati (None, None, None, 6 256         res2a_branch2a[0][0]             
__________________________________________________________________________________________________
res2a_branch2a_relu (Activation (None, None, None, 6 0           bn2a_branch2a[0][0]              
__________________________________________________________________________________________________
padding2a_branch2b (ZeroPadding (None, None, None, 6 0           res2a_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res2a_branch2b (Conv2D)         (None, None, None, 6 36864       padding2a_branch2b[0][0]         
__________________________________________________________________________________________________
bn2a_branch2b (BatchNormalizati (None, None, None, 6 256         res2a_branch2b[0][0]             
__________________________________________________________________________________________________
res2a_branch2b_relu (Activation (None, None, None, 6 0           bn2a_branch2b[0][0]              
__________________________________________________________________________________________________
res2a_branch2c (Conv2D)         (None, None, None, 2 16384       res2a_branch2b_relu[0][0]        
__________________________________________________________________________________________________
res2a_branch1 (Conv2D)          (None, None, None, 2 16384       pool1[0][0]                      
__________________________________________________________________________________________________
bn2a_branch2c (BatchNormalizati (None, None, None, 2 1024        res2a_branch2c[0][0]             
__________________________________________________________________________________________________
bn2a_branch1 (BatchNormalizatio (None, None, None, 2 1024        res2a_branch1[0][0]              
__________________________________________________________________________________________________
res2a (Add)                     (None, None, None, 2 0           bn2a_branch2c[0][0]              
                                                                 bn2a_branch1[0][0]               
__________________________________________________________________________________________________
res2a_relu (Activation)         (None, None, None, 2 0           res2a[0][0]                      
__________________________________________________________________________________________________
res2b_branch2a (Conv2D)         (None, None, None, 6 16384       res2a_relu[0][0]                 
__________________________________________________________________________________________________
bn2b_branch2a (BatchNormalizati (None, None, None, 6 256         res2b_branch2a[0][0]             
__________________________________________________________________________________________________
res2b_branch2a_relu (Activation (None, None, None, 6 0           bn2b_branch2a[0][0]              
__________________________________________________________________________________________________
padding2b_branch2b (ZeroPadding (None, None, None, 6 0           res2b_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res2b_branch2b (Conv2D)         (None, None, None, 6 36864       padding2b_branch2b[0][0]         
__________________________________________________________________________________________________
bn2b_branch2b (BatchNormalizati (None, None, None, 6 256         res2b_branch2b[0][0]             
__________________________________________________________________________________________________
res2b_branch2b_relu (Activation (None, None, None, 6 0           bn2b_branch2b[0][0]              
__________________________________________________________________________________________________
res2b_branch2c (Conv2D)         (None, None, None, 2 16384       res2b_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn2b_branch2c (BatchNormalizati (None, None, None, 2 1024        res2b_branch2c[0][0]             
__________________________________________________________________________________________________
res2b (Add)                     (None, None, None, 2 0           bn2b_branch2c[0][0]              
                                                                 res2a_relu[0][0]                 
__________________________________________________________________________________________________
res2b_relu (Activation)         (None, None, None, 2 0           res2b[0][0]                      
__________________________________________________________________________________________________
res2c_branch2a (Conv2D)         (None, None, None, 6 16384       res2b_relu[0][0]                 
__________________________________________________________________________________________________
bn2c_branch2a (BatchNormalizati (None, None, None, 6 256         res2c_branch2a[0][0]             
__________________________________________________________________________________________________
res2c_branch2a_relu (Activation (None, None, None, 6 0           bn2c_branch2a[0][0]              
__________________________________________________________________________________________________
padding2c_branch2b (ZeroPadding (None, None, None, 6 0           res2c_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res2c_branch2b (Conv2D)         (None, None, None, 6 36864       padding2c_branch2b[0][0]         
__________________________________________________________________________________________________
bn2c_branch2b (BatchNormalizati (None, None, None, 6 256         res2c_branch2b[0][0]             
__________________________________________________________________________________________________
res2c_branch2b_relu (Activation (None, None, None, 6 0           bn2c_branch2b[0][0]              
__________________________________________________________________________________________________
res2c_branch2c (Conv2D)         (None, None, None, 2 16384       res2c_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn2c_branch2c (BatchNormalizati (None, None, None, 2 1024        res2c_branch2c[0][0]             
__________________________________________________________________________________________________
res2c (Add)                     (None, None, None, 2 0           bn2c_branch2c[0][0]              
                                                                 res2b_relu[0][0]                 
__________________________________________________________________________________________________
res2c_relu (Activation)         (None, None, None, 2 0           res2c[0][0]                      
__________________________________________________________________________________________________
res3a_branch2a (Conv2D)         (None, None, None, 1 32768       res2c_relu[0][0]                 
__________________________________________________________________________________________________
bn3a_branch2a (BatchNormalizati (None, None, None, 1 512         res3a_branch2a[0][0]             
__________________________________________________________________________________________________
res3a_branch2a_relu (Activation (None, None, None, 1 0           bn3a_branch2a[0][0]              
__________________________________________________________________________________________________
padding3a_branch2b (ZeroPadding (None, None, None, 1 0           res3a_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res3a_branch2b (Conv2D)         (None, None, None, 1 147456      padding3a_branch2b[0][0]         
__________________________________________________________________________________________________
bn3a_branch2b (BatchNormalizati (None, None, None, 1 512         res3a_branch2b[0][0]             
__________________________________________________________________________________________________
res3a_branch2b_relu (Activation (None, None, None, 1 0           bn3a_branch2b[0][0]              
__________________________________________________________________________________________________
res3a_branch2c (Conv2D)         (None, None, None, 5 65536       res3a_branch2b_relu[0][0]        
__________________________________________________________________________________________________
res3a_branch1 (Conv2D)          (None, None, None, 5 131072      res2c_relu[0][0]                 
__________________________________________________________________________________________________
bn3a_branch2c (BatchNormalizati (None, None, None, 5 2048        res3a_branch2c[0][0]             
__________________________________________________________________________________________________
bn3a_branch1 (BatchNormalizatio (None, None, None, 5 2048        res3a_branch1[0][0]              
__________________________________________________________________________________________________
res3a (Add)                     (None, None, None, 5 0           bn3a_branch2c[0][0]              
                                                                 bn3a_branch1[0][0]               
__________________________________________________________________________________________________
res3a_relu (Activation)         (None, None, None, 5 0           res3a[0][0]                      
__________________________________________________________________________________________________
res3b_branch2a (Conv2D)         (None, None, None, 1 65536       res3a_relu[0][0]                 
__________________________________________________________________________________________________
bn3b_branch2a (BatchNormalizati (None, None, None, 1 512         res3b_branch2a[0][0]             
__________________________________________________________________________________________________
res3b_branch2a_relu (Activation (None, None, None, 1 0           bn3b_branch2a[0][0]              
__________________________________________________________________________________________________
padding3b_branch2b (ZeroPadding (None, None, None, 1 0           res3b_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res3b_branch2b (Conv2D)         (None, None, None, 1 147456      padding3b_branch2b[0][0]         
__________________________________________________________________________________________________
bn3b_branch2b (BatchNormalizati (None, None, None, 1 512         res3b_branch2b[0][0]             
__________________________________________________________________________________________________
res3b_branch2b_relu (Activation (None, None, None, 1 0           bn3b_branch2b[0][0]              
__________________________________________________________________________________________________
res3b_branch2c (Conv2D)         (None, None, None, 5 65536       res3b_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn3b_branch2c (BatchNormalizati (None, None, None, 5 2048        res3b_branch2c[0][0]             
__________________________________________________________________________________________________
res3b (Add)                     (None, None, None, 5 0           bn3b_branch2c[0][0]              
                                                                 res3a_relu[0][0]                 
__________________________________________________________________________________________________
res3b_relu (Activation)         (None, None, None, 5 0           res3b[0][0]                      
__________________________________________________________________________________________________
res3c_branch2a (Conv2D)         (None, None, None, 1 65536       res3b_relu[0][0]                 
__________________________________________________________________________________________________
bn3c_branch2a (BatchNormalizati (None, None, None, 1 512         res3c_branch2a[0][0]             
__________________________________________________________________________________________________
res3c_branch2a_relu (Activation (None, None, None, 1 0           bn3c_branch2a[0][0]              
__________________________________________________________________________________________________
padding3c_branch2b (ZeroPadding (None, None, None, 1 0           res3c_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res3c_branch2b (Conv2D)         (None, None, None, 1 147456      padding3c_branch2b[0][0]         
__________________________________________________________________________________________________
bn3c_branch2b (BatchNormalizati (None, None, None, 1 512         res3c_branch2b[0][0]             
__________________________________________________________________________________________________
res3c_branch2b_relu (Activation (None, None, None, 1 0           bn3c_branch2b[0][0]              
__________________________________________________________________________________________________
res3c_branch2c (Conv2D)         (None, None, None, 5 65536       res3c_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn3c_branch2c (BatchNormalizati (None, None, None, 5 2048        res3c_branch2c[0][0]             
__________________________________________________________________________________________________
res3c (Add)                     (None, None, None, 5 0           bn3c_branch2c[0][0]              
                                                                 res3b_relu[0][0]                 
__________________________________________________________________________________________________
res3c_relu (Activation)         (None, None, None, 5 0           res3c[0][0]                      
__________________________________________________________________________________________________
res3d_branch2a (Conv2D)         (None, None, None, 1 65536       res3c_relu[0][0]                 
__________________________________________________________________________________________________
bn3d_branch2a (BatchNormalizati (None, None, None, 1 512         res3d_branch2a[0][0]             
__________________________________________________________________________________________________
res3d_branch2a_relu (Activation (None, None, None, 1 0           bn3d_branch2a[0][0]              
__________________________________________________________________________________________________
padding3d_branch2b (ZeroPadding (None, None, None, 1 0           res3d_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res3d_branch2b (Conv2D)         (None, None, None, 1 147456      padding3d_branch2b[0][0]         
__________________________________________________________________________________________________
bn3d_branch2b (BatchNormalizati (None, None, None, 1 512         res3d_branch2b[0][0]             
__________________________________________________________________________________________________
res3d_branch2b_relu (Activation (None, None, None, 1 0           bn3d_branch2b[0][0]              
__________________________________________________________________________________________________
res3d_branch2c (Conv2D)         (None, None, None, 5 65536       res3d_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn3d_branch2c (BatchNormalizati (None, None, None, 5 2048        res3d_branch2c[0][0]             
__________________________________________________________________________________________________
res3d (Add)                     (None, None, None, 5 0           bn3d_branch2c[0][0]              
                                                                 res3c_relu[0][0]                 
__________________________________________________________________________________________________
res3d_relu (Activation)         (None, None, None, 5 0           res3d[0][0]                      
__________________________________________________________________________________________________
res4a_branch2a (Conv2D)         (None, None, None, 2 131072      res3d_relu[0][0]                 
__________________________________________________________________________________________________
bn4a_branch2a (BatchNormalizati (None, None, None, 2 1024        res4a_branch2a[0][0]             
__________________________________________________________________________________________________
res4a_branch2a_relu (Activation (None, None, None, 2 0           bn4a_branch2a[0][0]              
__________________________________________________________________________________________________
padding4a_branch2b (ZeroPadding (None, None, None, 2 0           res4a_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res4a_branch2b (Conv2D)         (None, None, None, 2 589824      padding4a_branch2b[0][0]         
__________________________________________________________________________________________________
bn4a_branch2b (BatchNormalizati (None, None, None, 2 1024        res4a_branch2b[0][0]             
__________________________________________________________________________________________________
res4a_branch2b_relu (Activation (None, None, None, 2 0           bn4a_branch2b[0][0]              
__________________________________________________________________________________________________
res4a_branch2c (Conv2D)         (None, None, None, 1 262144      res4a_branch2b_relu[0][0]        
__________________________________________________________________________________________________
res4a_branch1 (Conv2D)          (None, None, None, 1 524288      res3d_relu[0][0]                 
__________________________________________________________________________________________________
bn4a_branch2c (BatchNormalizati (None, None, None, 1 4096        res4a_branch2c[0][0]             
__________________________________________________________________________________________________
bn4a_branch1 (BatchNormalizatio (None, None, None, 1 4096        res4a_branch1[0][0]              
__________________________________________________________________________________________________
res4a (Add)                     (None, None, None, 1 0           bn4a_branch2c[0][0]              
                                                                 bn4a_branch1[0][0]               
__________________________________________________________________________________________________
res4a_relu (Activation)         (None, None, None, 1 0           res4a[0][0]                      
__________________________________________________________________________________________________
res4b_branch2a (Conv2D)         (None, None, None, 2 262144      res4a_relu[0][0]                 
__________________________________________________________________________________________________
bn4b_branch2a (BatchNormalizati (None, None, None, 2 1024        res4b_branch2a[0][0]             
__________________________________________________________________________________________________
res4b_branch2a_relu (Activation (None, None, None, 2 0           bn4b_branch2a[0][0]              
__________________________________________________________________________________________________
padding4b_branch2b (ZeroPadding (None, None, None, 2 0           res4b_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res4b_branch2b (Conv2D)         (None, None, None, 2 589824      padding4b_branch2b[0][0]         
__________________________________________________________________________________________________
bn4b_branch2b (BatchNormalizati (None, None, None, 2 1024        res4b_branch2b[0][0]             
__________________________________________________________________________________________________
res4b_branch2b_relu (Activation (None, None, None, 2 0           bn4b_branch2b[0][0]              
__________________________________________________________________________________________________
res4b_branch2c (Conv2D)         (None, None, None, 1 262144      res4b_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn4b_branch2c (BatchNormalizati (None, None, None, 1 4096        res4b_branch2c[0][0]             
__________________________________________________________________________________________________
res4b (Add)                     (None, None, None, 1 0           bn4b_branch2c[0][0]              
                                                                 res4a_relu[0][0]                 
__________________________________________________________________________________________________
res4b_relu (Activation)         (None, None, None, 1 0           res4b[0][0]                      
__________________________________________________________________________________________________
res4c_branch2a (Conv2D)         (None, None, None, 2 262144      res4b_relu[0][0]                 
__________________________________________________________________________________________________
bn4c_branch2a (BatchNormalizati (None, None, None, 2 1024        res4c_branch2a[0][0]             
__________________________________________________________________________________________________
res4c_branch2a_relu (Activation (None, None, None, 2 0           bn4c_branch2a[0][0]              
__________________________________________________________________________________________________
padding4c_branch2b (ZeroPadding (None, None, None, 2 0           res4c_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res4c_branch2b (Conv2D)         (None, None, None, 2 589824      padding4c_branch2b[0][0]         
__________________________________________________________________________________________________
bn4c_branch2b (BatchNormalizati (None, None, None, 2 1024        res4c_branch2b[0][0]             
__________________________________________________________________________________________________
res4c_branch2b_relu (Activation (None, None, None, 2 0           bn4c_branch2b[0][0]              
__________________________________________________________________________________________________
res4c_branch2c (Conv2D)         (None, None, None, 1 262144      res4c_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn4c_branch2c (BatchNormalizati (None, None, None, 1 4096        res4c_branch2c[0][0]             
__________________________________________________________________________________________________
res4c (Add)                     (None, None, None, 1 0           bn4c_branch2c[0][0]              
                                                                 res4b_relu[0][0]                 
__________________________________________________________________________________________________
res4c_relu (Activation)         (None, None, None, 1 0           res4c[0][0]                      
__________________________________________________________________________________________________
res4d_branch2a (Conv2D)         (None, None, None, 2 262144      res4c_relu[0][0]                 
__________________________________________________________________________________________________
bn4d_branch2a (BatchNormalizati (None, None, None, 2 1024        res4d_branch2a[0][0]             
__________________________________________________________________________________________________
res4d_branch2a_relu (Activation (None, None, None, 2 0           bn4d_branch2a[0][0]              
__________________________________________________________________________________________________
padding4d_branch2b (ZeroPadding (None, None, None, 2 0           res4d_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res4d_branch2b (Conv2D)         (None, None, None, 2 589824      padding4d_branch2b[0][0]         
__________________________________________________________________________________________________
bn4d_branch2b (BatchNormalizati (None, None, None, 2 1024        res4d_branch2b[0][0]             
__________________________________________________________________________________________________
res4d_branch2b_relu (Activation (None, None, None, 2 0           bn4d_branch2b[0][0]              
__________________________________________________________________________________________________
res4d_branch2c (Conv2D)         (None, None, None, 1 262144      res4d_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn4d_branch2c (BatchNormalizati (None, None, None, 1 4096        res4d_branch2c[0][0]             
__________________________________________________________________________________________________
res4d (Add)                     (None, None, None, 1 0           bn4d_branch2c[0][0]              
                                                                 res4c_relu[0][0]                 
__________________________________________________________________________________________________
res4d_relu (Activation)         (None, None, None, 1 0           res4d[0][0]                      
__________________________________________________________________________________________________
res4e_branch2a (Conv2D)         (None, None, None, 2 262144      res4d_relu[0][0]                 
__________________________________________________________________________________________________
bn4e_branch2a (BatchNormalizati (None, None, None, 2 1024        res4e_branch2a[0][0]             
__________________________________________________________________________________________________
res4e_branch2a_relu (Activation (None, None, None, 2 0           bn4e_branch2a[0][0]              
__________________________________________________________________________________________________
padding4e_branch2b (ZeroPadding (None, None, None, 2 0           res4e_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res4e_branch2b (Conv2D)         (None, None, None, 2 589824      padding4e_branch2b[0][0]         
__________________________________________________________________________________________________
bn4e_branch2b (BatchNormalizati (None, None, None, 2 1024        res4e_branch2b[0][0]             
__________________________________________________________________________________________________
res4e_branch2b_relu (Activation (None, None, None, 2 0           bn4e_branch2b[0][0]              
__________________________________________________________________________________________________
res4e_branch2c (Conv2D)         (None, None, None, 1 262144      res4e_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn4e_branch2c (BatchNormalizati (None, None, None, 1 4096        res4e_branch2c[0][0]             
__________________________________________________________________________________________________
res4e (Add)                     (None, None, None, 1 0           bn4e_branch2c[0][0]              
                                                                 res4d_relu[0][0]                 
__________________________________________________________________________________________________
res4e_relu (Activation)         (None, None, None, 1 0           res4e[0][0]                      
__________________________________________________________________________________________________
res4f_branch2a (Conv2D)         (None, None, None, 2 262144      res4e_relu[0][0]                 
__________________________________________________________________________________________________
bn4f_branch2a (BatchNormalizati (None, None, None, 2 1024        res4f_branch2a[0][0]             
__________________________________________________________________________________________________
res4f_branch2a_relu (Activation (None, None, None, 2 0           bn4f_branch2a[0][0]              
__________________________________________________________________________________________________
padding4f_branch2b (ZeroPadding (None, None, None, 2 0           res4f_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res4f_branch2b (Conv2D)         (None, None, None, 2 589824      padding4f_branch2b[0][0]         
__________________________________________________________________________________________________
bn4f_branch2b (BatchNormalizati (None, None, None, 2 1024        res4f_branch2b[0][0]             
__________________________________________________________________________________________________
res4f_branch2b_relu (Activation (None, None, None, 2 0           bn4f_branch2b[0][0]              
__________________________________________________________________________________________________
res4f_branch2c (Conv2D)         (None, None, None, 1 262144      res4f_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn4f_branch2c (BatchNormalizati (None, None, None, 1 4096        res4f_branch2c[0][0]             
__________________________________________________________________________________________________
res4f (Add)                     (None, None, None, 1 0           bn4f_branch2c[0][0]              
                                                                 res4e_relu[0][0]                 
__________________________________________________________________________________________________
res4f_relu (Activation)         (None, None, None, 1 0           res4f[0][0]                      
__________________________________________________________________________________________________
res5a_branch2a (Conv2D)         (None, None, None, 5 524288      res4f_relu[0][0]                 
__________________________________________________________________________________________________
bn5a_branch2a (BatchNormalizati (None, None, None, 5 2048        res5a_branch2a[0][0]             
__________________________________________________________________________________________________
res5a_branch2a_relu (Activation (None, None, None, 5 0           bn5a_branch2a[0][0]              
__________________________________________________________________________________________________
padding5a_branch2b (ZeroPadding (None, None, None, 5 0           res5a_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res5a_branch2b (Conv2D)         (None, None, None, 5 2359296     padding5a_branch2b[0][0]         
__________________________________________________________________________________________________
bn5a_branch2b (BatchNormalizati (None, None, None, 5 2048        res5a_branch2b[0][0]             
__________________________________________________________________________________________________
res5a_branch2b_relu (Activation (None, None, None, 5 0           bn5a_branch2b[0][0]              
__________________________________________________________________________________________________
res5a_branch2c (Conv2D)         (None, None, None, 2 1048576     res5a_branch2b_relu[0][0]        
__________________________________________________________________________________________________
res5a_branch1 (Conv2D)          (None, None, None, 2 2097152     res4f_relu[0][0]                 
__________________________________________________________________________________________________
bn5a_branch2c (BatchNormalizati (None, None, None, 2 8192        res5a_branch2c[0][0]             
__________________________________________________________________________________________________
bn5a_branch1 (BatchNormalizatio (None, None, None, 2 8192        res5a_branch1[0][0]              
__________________________________________________________________________________________________
res5a (Add)                     (None, None, None, 2 0           bn5a_branch2c[0][0]              
                                                                 bn5a_branch1[0][0]               
__________________________________________________________________________________________________
res5a_relu (Activation)         (None, None, None, 2 0           res5a[0][0]                      
__________________________________________________________________________________________________
res5b_branch2a (Conv2D)         (None, None, None, 5 1048576     res5a_relu[0][0]                 
__________________________________________________________________________________________________
bn5b_branch2a (BatchNormalizati (None, None, None, 5 2048        res5b_branch2a[0][0]             
__________________________________________________________________________________________________
res5b_branch2a_relu (Activation (None, None, None, 5 0           bn5b_branch2a[0][0]              
__________________________________________________________________________________________________
padding5b_branch2b (ZeroPadding (None, None, None, 5 0           res5b_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res5b_branch2b (Conv2D)         (None, None, None, 5 2359296     padding5b_branch2b[0][0]         
__________________________________________________________________________________________________
bn5b_branch2b (BatchNormalizati (None, None, None, 5 2048        res5b_branch2b[0][0]             
__________________________________________________________________________________________________
res5b_branch2b_relu (Activation (None, None, None, 5 0           bn5b_branch2b[0][0]              
__________________________________________________________________________________________________
res5b_branch2c (Conv2D)         (None, None, None, 2 1048576     res5b_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn5b_branch2c (BatchNormalizati (None, None, None, 2 8192        res5b_branch2c[0][0]             
__________________________________________________________________________________________________
res5b (Add)                     (None, None, None, 2 0           bn5b_branch2c[0][0]              
                                                                 res5a_relu[0][0]                 
__________________________________________________________________________________________________
res5b_relu (Activation)         (None, None, None, 2 0           res5b[0][0]                      
__________________________________________________________________________________________________
res5c_branch2a (Conv2D)         (None, None, None, 5 1048576     res5b_relu[0][0]                 
__________________________________________________________________________________________________
bn5c_branch2a (BatchNormalizati (None, None, None, 5 2048        res5c_branch2a[0][0]             
__________________________________________________________________________________________________
res5c_branch2a_relu (Activation (None, None, None, 5 0           bn5c_branch2a[0][0]              
__________________________________________________________________________________________________
padding5c_branch2b (ZeroPadding (None, None, None, 5 0           res5c_branch2a_relu[0][0]        
__________________________________________________________________________________________________
res5c_branch2b (Conv2D)         (None, None, None, 5 2359296     padding5c_branch2b[0][0]         
__________________________________________________________________________________________________
bn5c_branch2b (BatchNormalizati (None, None, None, 5 2048        res5c_branch2b[0][0]             
__________________________________________________________________________________________________
res5c_branch2b_relu (Activation (None, None, None, 5 0           bn5c_branch2b[0][0]              
__________________________________________________________________________________________________
res5c_branch2c (Conv2D)         (None, None, None, 2 1048576     res5c_branch2b_relu[0][0]        
__________________________________________________________________________________________________
bn5c_branch2c (BatchNormalizati (None, None, None, 2 8192        res5c_branch2c[0][0]             
__________________________________________________________________________________________________
res5c (Add)                     (None, None, None, 2 0           bn5c_branch2c[0][0]              
                                                                 res5b_relu[0][0]                 
__________________________________________________________________________________________________
res5c_relu (Activation)         (None, None, None, 2 0           res5c[0][0]                      
__________________________________________________________________________________________________
C5_reduced (Conv2D)             (None, None, None, 2 524544      res5c_relu[0][0]                 
__________________________________________________________________________________________________
P5_upsampled (UpsampleLike)     (None, None, None, 2 0           C5_reduced[0][0]                 
                                                                 res4f_relu[0][0]                 
__________________________________________________________________________________________________
C4_reduced (Conv2D)             (None, None, None, 2 262400      res4f_relu[0][0]                 
__________________________________________________________________________________________________
P4_merged (Add)                 (None, None, None, 2 0           P5_upsampled[0][0]               
                                                                 C4_reduced[0][0]                 
__________________________________________________________________________________________________
P4_upsampled (UpsampleLike)     (None, None, None, 2 0           P4_merged[0][0]                  
                                                                 res3d_relu[0][0]                 
__________________________________________________________________________________________________
C3_reduced (Conv2D)             (None, None, None, 2 131328      res3d_relu[0][0]                 
__________________________________________________________________________________________________
P6 (Conv2D)                     (None, None, None, 2 4718848     res5c_relu[0][0]                 
__________________________________________________________________________________________________
P3_merged (Add)                 (None, None, None, 2 0           P4_upsampled[0][0]               
                                                                 C3_reduced[0][0]                 
__________________________________________________________________________________________________
C6_relu (Activation)            (None, None, None, 2 0           P6[0][0]                         
__________________________________________________________________________________________________
P3 (Conv2D)                     (None, None, None, 2 590080      P3_merged[0][0]                  
__________________________________________________________________________________________________
P4 (Conv2D)                     (None, None, None, 2 590080      P4_merged[0][0]                  
__________________________________________________________________________________________________
P5 (Conv2D)                     (None, None, None, 2 590080      C5_reduced[0][0]                 
__________________________________________________________________________________________________
P7 (Conv2D)                     (None, None, None, 2 590080      C6_relu[0][0]                    
__________________________________________________________________________________________________
regression_submodel (Model)     (None, None, 4)      2443300     P3[0][0]                         
                                                                 P4[0][0]                         
                                                                 P5[0][0]                         
                                                                 P6[0][0]                         
                                                                 P7[0][0]                         
__________________________________________________________________________________________________
classification_submodel (Model) (None, None, 20)     2775220     P3[0][0]                         
                                                                 P4[0][0]                         
                                                                 P5[0][0]                         
                                                                 P6[0][0]                         
                                                                 P7[0][0]                         
__________________________________________________________________________________________________
regression (Concatenate)        (None, None, 4)      0           regression_submodel[1][0]        
                                                                 regression_submodel[2][0]        
                                                                 regression_submodel[3][0]        
                                                                 regression_submodel[4][0]        
                                                                 regression_submodel[5][0]        
__________________________________________________________________________________________________
classification (Concatenate)    (None, None, 20)     0           classification_submodel[1][0]    
                                                                 classification_submodel[2][0]    
                                                                 classification_submodel[3][0]    
                                                                 classification_submodel[4][0]    
                                                                 classification_submodel[5][0]    
==================================================================================================
Total params: 36,777,112
Trainable params: 36,670,872
Non-trainable params: 106,240
__________________________________________________________________________________________________
None
Epoch 1/2

    1/10000 [..............................] - ETA: 13:00:30 - loss: 0.3797 - regression_loss: 0.2250 - classification_loss: 0.1547
    2/10000 [..............................] - ETA: 7:31:46 - loss: 0.6348 - regression_loss: 0.5078 - classification_loss: 0.1270 
    3/10000 [..............................] - ETA: 6:08:42 - loss: 1.0142 - regression_loss: 0.8276 - classification_loss: 0.1866
    4/10000 [..............................] - ETA: 4:56:08 - loss: 0.9340 - regression_loss: 0.7759 - classification_loss: 0.1581
    5/10000 [..............................] - ETA: 4:11:28 - loss: 0.8331 - regression_loss: 0.6751 - classification_loss: 0.1580
    6/10000 [..............................] - ETA: 4:02:03 - loss: 0.7735 - regression_loss: 0.6284 - classification_loss: 0.1451
    7/10000 [..............................] - ETA: 3:38:14 - loss: 0.8147 - regression_loss: 0.6526 - classification_loss: 0.1622
    8/10000 [..............................] - ETA: 3:33:41 - loss: 0.8059 - regression_loss: 0.6499 - classification_loss: 0.1560
    9/10000 [..............................] - ETA: 3:18:14 - loss: 0.8750 - regression_loss: 0.6913 - classification_loss: 0.1838
   10/10000 [..............................] - ETA: 3:05:47 - loss: 0.8294 - regression_loss: 0.6452 - classification_loss: 0.1842
   11/10000 [..............................] - ETA: 2:55:44 - loss: 0.8393 - regression_loss: 0.6536 - classification_loss: 0.1857
   12/10000 [..............................] - ETA: 2:47:17 - loss: 0.8426 - regression_loss: 0.6616 - classification_loss: 0.1810
   13/10000 [..............................] - ETA: 2:40:01 - loss: 0.8071 - regression_loss: 0.6288 - classification_loss: 0.1783
   14/10000 [..............................] - ETA: 2:34:00 - loss: 0.7896 - regression_loss: 0.6154 - classification_loss: 0.1743
   15/10000 [..............................] - ETA: 2:28:41 - loss: 0.7540 - regression_loss: 0.5836 - classification_loss: 0.1704
   16/10000 [..............................] - ETA: 2:28:56 - loss: 0.7626 - regression_loss: 0.5965 - classification_loss: 0.1662
   17/10000 [..............................] - ETA: 2:24:35 - loss: 0.7336 - regression_loss: 0.5749 - classification_loss: 0.1587
   18/10000 [..............................] - ETA: 2:26:52 - loss: 0.7104 - regression_loss: 0.5491 - classification_loss: 0.1612
   19/10000 [..............................] - ETA: 2:23:04 - loss: 0.7463 - regression_loss: 0.5813 - classification_loss: 0.1650
   20/10000 [..............................] - ETA: 2:19:39 - loss: 0.7627 - regression_loss: 0.5952 - classification_loss: 0.1675
   21/10000 [..............................] - ETA: 2:16:55 - loss: 0.7592 - regression_loss: 0.5946 - classification_loss: 0.1646
   22/10000 [..............................] - ETA: 2:14:10 - loss: 0.7478 - regression_loss: 0.5830 - classification_loss: 0.1648
   23/10000 [..............................] - ETA: 2:12:17 - loss: 0.7223 - regression_loss: 0.5630 - classification_loss: 0.1592
   24/10000 [..............................] - ETA: 2:09:53 - loss: 0.7261 - regression_loss: 0.5643 - classification_loss: 0.1618
   25/10000 [..............................] - ETA: 2:12:44 - loss: 0.7202 - regression_loss: 0.5577 - classification_loss: 0.1625
   26/10000 [..............................] - ETA: 2:10:30 - loss: 0.7501 - regression_loss: 0.5838 - classification_loss: 0.1664
   27/10000 [..............................] - ETA: 2:08:21 - loss: 0.7725 - regression_loss: 0.6022 - classification_loss: 0.1703
   28/10000 [..............................] - ETA: 2:09:43 - loss: 0.7895 - regression_loss: 0.6173 - classification_loss: 0.1721
   29/10000 [..............................] - ETA: 2:08:00 - loss: 0.7956 - regression_loss: 0.6236 - classification_loss: 0.1719
   30/10000 [..............................] - ETA: 2:06:14 - loss: 0.7903 - regression_loss: 0.6215 - classification_loss: 0.1688
   31/10000 [..............................] - ETA: 2:04:33 - loss: 0.7711 - regression_loss: 0.6065 - classification_loss: 0.1646
   32/10000 [..............................] - ETA: 2:02:58 - loss: 0.7632 - regression_loss: 0.6000 - classification_loss: 0.1632
   33/10000 [..............................] - ETA: 2:05:12 - loss: 0.7695 - regression_loss: 0.5995 - classification_loss: 0.1700
   34/10000 [..............................] - ETA: 2:03:45 - loss: 0.7531 - regression_loss: 0.5857 - classification_loss: 0.1674
   35/10000 [..............................] - ETA: 2:02:14 - loss: 0.7629 - regression_loss: 0.5970 - classification_loss: 0.1659
   36/10000 [..............................] - ETA: 2:00:54 - loss: 0.7486 - regression_loss: 0.5862 - classification_loss: 0.1624
   37/10000 [..............................] - ETA: 1:59:39 - loss: 0.7537 - regression_loss: 0.5896 - classification_loss: 0.1642
   38/10000 [..............................] - ETA: 1:58:27 - loss: 0.7461 - regression_loss: 0.5837 - classification_loss: 0.1624
   39/10000 [..............................] - ETA: 1:57:19 - loss: 0.7559 - regression_loss: 0.5912 - classification_loss: 0.1647
   40/10000 [..............................] - ETA: 1:56:11 - loss: 0.7610 - regression_loss: 0.5973 - classification_loss: 0.1637
   41/10000 [..............................] - ETA: 1:55:10 - loss: 0.7619 - regression_loss: 0.5968 - classification_loss: 0.1651
   42/10000 [..............................] - ETA: 1:54:14 - loss: 0.7528 - regression_loss: 0.5899 - classification_loss: 0.1628
   43/10000 [..............................] - ETA: 1:57:02 - loss: 0.7465 - regression_loss: 0.5860 - classification_loss: 0.1605
   44/10000 [..............................] - ETA: 1:56:11 - loss: 0.7406 - regression_loss: 0.5822 - classification_loss: 0.1584
   45/10000 [..............................] - ETA: 1:55:15 - loss: 0.7435 - regression_loss: 0.5851 - classification_loss: 0.1584
   46/10000 [..............................] - ETA: 1:56:55 - loss: 0.7430 - regression_loss: 0.5861 - classification_loss: 0.1568
   47/10000 [..............................] - ETA: 1:58:14 - loss: 0.7452 - regression_loss: 0.5893 - classification_loss: 0.1559
   48/10000 [..............................] - ETA: 1:57:17 - loss: 0.7381 - regression_loss: 0.5847 - classification_loss: 0.1535
   49/10000 [..............................] - ETA: 1:57:29 - loss: 0.7377 - regression_loss: 0.5845 - classification_loss: 0.1532
   50/10000 [..............................] - ETA: 1:59:35 - loss: 0.7342 - regression_loss: 0.5831 - classification_loss: 0.1511
   51/10000 [..............................] - ETA: 1:59:39 - loss: 0.7301 - regression_loss: 0.5745 - classification_loss: 0.1556
   52/10000 [..............................] - ETA: 1:58:48 - loss: 0.7228 - regression_loss: 0.5682 - classification_loss: 0.1546
   53/10000 [..............................] - ETA: 1:58:04 - loss: 0.7256 - regression_loss: 0.5712 - classification_loss: 0.1544
   54/10000 [..............................] - ETA: 1:58:45 - loss: 0.7214 - regression_loss: 0.5682 - classification_loss: 0.1531
   55/10000 [..............................] - ETA: 1:59:26 - loss: 0.7110 - regression_loss: 0.5600 - classification_loss: 0.1510
   56/10000 [..............................] - ETA: 1:58:36 - loss: 0.7086 - regression_loss: 0.5586 - classification_loss: 0.1500
   57/10000 [..............................] - ETA: 1:57:50 - loss: 0.7002 - regression_loss: 0.5525 - classification_loss: 0.1477
   58/10000 [..............................] - ETA: 1:57:04 - loss: 0.6973 - regression_loss: 0.5512 - classification_loss: 0.1461
   59/10000 [..............................] - ETA: 1:56:21 - loss: 0.6976 - regression_loss: 0.5512 - classification_loss: 0.1464
   60/10000 [..............................] - ETA: 1:55:48 - loss: 0.7164 - regression_loss: 0.5678 - classification_loss: 0.1485
   61/10000 [..............................] - ETA: 1:55:07 - loss: 0.7059 - regression_loss: 0.5592 - classification_loss: 0.1467
   62/10000 [..............................] - ETA: 1:54:45 - loss: 0.7133 - regression_loss: 0.5659 - classification_loss: 0.1474
   63/10000 [..............................] - ETA: 1:54:12 - loss: 0.7315 - regression_loss: 0.5791 - classification_loss: 0.1524
   64/10000 [..............................] - ETA: 1:53:39 - loss: 0.7250 - regression_loss: 0.5731 - classification_loss: 0.1519
   65/10000 [..............................] - ETA: 1:53:00 - loss: 0.7208 - regression_loss: 0.5690 - classification_loss: 0.1518
   66/10000 [..............................] - ETA: 1:53:43 - loss: 0.7244 - regression_loss: 0.5741 - classification_loss: 0.1503
   67/10000 [..............................] - ETA: 1:53:12 - loss: 0.7198 - regression_loss: 0.5699 - classification_loss: 0.1499
   68/10000 [..............................] - ETA: 1:52:38 - loss: 0.7113 - regression_loss: 0.5629 - classification_loss: 0.1483
   69/10000 [..............................] - ETA: 1:52:09 - loss: 0.7072 - regression_loss: 0.5598 - classification_loss: 0.1473
   70/10000 [..............................] - ETA: 1:51:36 - loss: 0.7035 - regression_loss: 0.5576 - classification_loss: 0.1459
   71/10000 [..............................] - ETA: 1:51:03 - loss: 0.7041 - regression_loss: 0.5583 - classification_loss: 0.1458
   72/10000 [..............................] - ETA: 1:50:32 - loss: 0.7006 - regression_loss: 0.5562 - classification_loss: 0.1444
   73/10000 [..............................] - ETA: 1:50:00 - loss: 0.7045 - regression_loss: 0.5591 - classification_loss: 0.1453
   74/10000 [..............................] - ETA: 1:49:31 - loss: 0.7053 - regression_loss: 0.5605 - classification_loss: 0.1448
   75/10000 [..............................] - ETA: 1:49:07 - loss: 0.7017 - regression_loss: 0.5568 - classification_loss: 0.1449
   76/10000 [..............................] - ETA: 1:48:37 - loss: 0.6959 - regression_loss: 0.5525 - classification_loss: 0.1434
   77/10000 [..............................] - ETA: 1:48:30 - loss: 0.6930 - regression_loss: 0.5505 - classification_loss: 0.1425
   78/10000 [..............................] - ETA: 1:48:08 - loss: 0.6952 - regression_loss: 0.5525 - classification_loss: 0.1427
   79/10000 [..............................] - ETA: 1:47:44 - loss: 0.6923 - regression_loss: 0.5506 - classification_loss: 0.1417
   80/10000 [..............................] - ETA: 1:47:17 - loss: 0.6901 - regression_loss: 0.5493 - classification_loss: 0.1408
   81/10000 [..............................] - ETA: 1:46:52 - loss: 0.6898 - regression_loss: 0.5478 - classification_loss: 0.1420
   82/10000 [..............................] - ETA: 1:46:32 - loss: 0.6850 - regression_loss: 0.5444 - classification_loss: 0.1406
   83/10000 [..............................] - ETA: 1:46:12 - loss: 0.6823 - regression_loss: 0.5425 - classification_loss: 0.1398
   84/10000 [..............................] - ETA: 1:46:44 - loss: 0.6755 - regression_loss: 0.5371 - classification_loss: 0.1384
   85/10000 [..............................] - ETA: 1:46:19 - loss: 0.6786 - regression_loss: 0.5382 - classification_loss: 0.1403
   86/10000 [..............................] - ETA: 1:45:57 - loss: 0.6763 - regression_loss: 0.5372 - classification_loss: 0.1392
   87/10000 [..............................] - ETA: 1:45:40 - loss: 0.6698 - regression_loss: 0.5319 - classification_loss: 0.1379
   88/10000 [..............................] - ETA: 1:45:22 - loss: 0.6660 - regression_loss: 0.5290 - classification_loss: 0.1370
   89/10000 [..............................] - ETA: 1:46:24 - loss: 0.6660 - regression_loss: 0.5300 - classification_loss: 0.1360
   90/10000 [..............................] - ETA: 1:46:06 - loss: 0.6697 - regression_loss: 0.5326 - classification_loss: 0.1371
   91/10000 [..............................] - ETA: 1:45:43 - loss: 0.6663 - regression_loss: 0.5296 - classification_loss: 0.1367
   92/10000 [..............................] - ETA: 1:46:58 - loss: 0.6638 - regression_loss: 0.5278 - classification_loss: 0.1359
   93/10000 [..............................] - ETA: 1:46:35 - loss: 0.6589 - regression_loss: 0.5239 - classification_loss: 0.1349
   94/10000 [..............................] - ETA: 1:46:14 - loss: 0.6574 - regression_loss: 0.5230 - classification_loss: 0.1344
   95/10000 [..............................] - ETA: 1:45:52 - loss: 0.6540 - regression_loss: 0.5200 - classification_loss: 0.1340
   96/10000 [..............................] - ETA: 1:45:32 - loss: 0.6518 - regression_loss: 0.5181 - classification_loss: 0.1337
   97/10000 [..............................] - ETA: 1:46:02 - loss: 0.6490 - regression_loss: 0.5156 - classification_loss: 0.1334
   98/10000 [..............................] - ETA: 1:45:42 - loss: 0.6505 - regression_loss: 0.5167 - classification_loss: 0.1338
   99/10000 [..............................] - ETA: 1:45:21 - loss: 0.6475 - regression_loss: 0.5146 - classification_loss: 0.1329
  100/10000 [..............................] - ETA: 1:45:02 - loss: 0.6476 - regression_loss: 0.5140 - classification_loss: 0.1337
  101/10000 [..............................] - ETA: 1:44:46 - loss: 0.6505 - regression_loss: 0.5165 - classification_loss: 0.1339
  102/10000 [..............................] - ETA: 1:44:28 - loss: 0.6476 - regression_loss: 0.5144 - classification_loss: 0.1331
  103/10000 [..............................] - ETA: 1:44:09 - loss: 0.6486 - regression_loss: 0.5146 - classification_loss: 0.1340
  104/10000 [..............................] - ETA: 1:44:54 - loss: 0.6490 - regression_loss: 0.5151 - classification_loss: 0.1339
  105/10000 [..............................] - ETA: 1:44:34 - loss: 0.6450 - regression_loss: 0.5123 - classification_loss: 0.1327
  106/10000 [..............................] - ETA: 1:44:20 - loss: 0.6407 - regression_loss: 0.5086 - classification_loss: 0.1321
  107/10000 [..............................] - ETA: 1:44:02 - loss: 0.6547 - regression_loss: 0.5186 - classification_loss: 0.1361
  108/10000 [..............................] - ETA: 1:44:56 - loss: 0.6526 - regression_loss: 0.5174 - classification_loss: 0.1353
  109/10000 [..............................] - ETA: 1:44:38 - loss: 0.6485 - regression_loss: 0.5142 - classification_loss: 0.1343
  110/10000 [..............................] - ETA: 1:44:21 - loss: 0.6453 - regression_loss: 0.5120 - classification_loss: 0.1333
  111/10000 [..............................] - ETA: 1:44:03 - loss: 0.6405 - regression_loss: 0.5081 - classification_loss: 0.1324
  112/10000 [..............................] - ETA: 1:43:47 - loss: 0.6386 - regression_loss: 0.5070 - classification_loss: 0.1316
  113/10000 [..............................] - ETA: 1:43:31 - loss: 0.6370 - regression_loss: 0.5051 - classification_loss: 0.1319
  114/10000 [..............................] - ETA: 1:43:14 - loss: 0.6404 - regression_loss: 0.5059 - classification_loss: 0.1345
  115/10000 [..............................] - ETA: 1:42:57 - loss: 0.6447 - regression_loss: 0.5096 - classification_loss: 0.1351
  116/10000 [..............................] - ETA: 1:44:04 - loss: 0.6460 - regression_loss: 0.5108 - classification_loss: 0.1352
  117/10000 [..............................] - ETA: 1:43:48 - loss: 0.6483 - regression_loss: 0.5132 - classification_loss: 0.1351
  118/10000 [..............................] - ETA: 1:43:35 - loss: 0.6510 - regression_loss: 0.5162 - classification_loss: 0.1348
  119/10000 [..............................] - ETA: 1:43:20 - loss: 0.6501 - regression_loss: 0.5147 - classification_loss: 0.1354
  120/10000 [..............................] - ETA: 1:43:09 - loss: 0.6536 - regression_loss: 0.5186 - classification_loss: 0.1350
  121/10000 [..............................] - ETA: 1:42:54 - loss: 0.6535 - regression_loss: 0.5190 - classification_loss: 0.1345
  122/10000 [..............................] - ETA: 1:42:38 - loss: 0.6619 - regression_loss: 0.5262 - classification_loss: 0.1357
  123/10000 [..............................] - ETA: 1:42:27 - loss: 0.6592 - regression_loss: 0.5240 - classification_loss: 0.1352
  124/10000 [..............................] - ETA: 1:42:13 - loss: 0.6565 - regression_loss: 0.5215 - classification_loss: 0.1350
  125/10000 [..............................] - ETA: 1:42:01 - loss: 0.6536 - regression_loss: 0.5192 - classification_loss: 0.1344
  126/10000 [..............................] - ETA: 1:41:47 - loss: 0.6585 - regression_loss: 0.5240 - classification_loss: 0.1345
  127/10000 [..............................] - ETA: 1:42:07 - loss: 0.6576 - regression_loss: 0.5228 - classification_loss: 0.1348
  128/10000 [..............................] - ETA: 1:41:53 - loss: 0.6555 - regression_loss: 0.5215 - classification_loss: 0.1341
  129/10000 [..............................] - ETA: 1:41:37 - loss: 0.6619 - regression_loss: 0.5265 - classification_loss: 0.1354
  130/10000 [..............................] - ETA: 1:41:27 - loss: 0.6574 - regression_loss: 0.5229 - classification_loss: 0.1345
  131/10000 [..............................] - ETA: 1:41:16 - loss: 0.6647 - regression_loss: 0.5289 - classification_loss: 0.1358
  132/10000 [..............................] - ETA: 1:41:06 - loss: 0.6641 - regression_loss: 0.5289 - classification_loss: 0.1352
  133/10000 [..............................] - ETA: 1:41:55 - loss: 0.6611 - regression_loss: 0.5266 - classification_loss: 0.1345
  134/10000 [..............................] - ETA: 1:42:07 - loss: 0.6622 - regression_loss: 0.5280 - classification_loss: 0.1342
  135/10000 [..............................] - ETA: 1:42:32 - loss: 0.6632 - regression_loss: 0.5295 - classification_loss: 0.1337
  136/10000 [..............................] - ETA: 1:42:59 - loss: 0.6595 - regression_loss: 0.5266 - classification_loss: 0.1330
  137/10000 [..............................] - ETA: 1:42:52 - loss: 0.6570 - regression_loss: 0.5248 - classification_loss: 0.1322
  138/10000 [..............................] - ETA: 1:42:39 - loss: 0.6553 - regression_loss: 0.5234 - classification_loss: 0.1319
  139/10000 [..............................] - ETA: 1:42:55 - loss: 0.6565 - regression_loss: 0.5239 - classification_loss: 0.1327
  140/10000 [..............................] - ETA: 1:42:45 - loss: 0.6580 - regression_loss: 0.5253 - classification_loss: 0.1327
  141/10000 [..............................] - ETA: 1:42:36 - loss: 0.6600 - regression_loss: 0.5272 - classification_loss: 0.1328
  142/10000 [..............................] - ETA: 1:42:24 - loss: 0.6599 - regression_loss: 0.5275 - classification_loss: 0.1325
  143/10000 [..............................] - ETA: 1:42:20 - loss: 0.6621 - regression_loss: 0.5300 - classification_loss: 0.1321
  144/10000 [..............................] - ETA: 1:42:08 - loss: 0.6621 - regression_loss: 0.5305 - classification_loss: 0.1316
  145/10000 [..............................] - ETA: 1:41:55 - loss: 0.6608 - regression_loss: 0.5296 - classification_loss: 0.1312
  146/10000 [..............................] - ETA: 1:41:43 - loss: 0.6610 - regression_loss: 0.5300 - classification_loss: 0.1311
  147/10000 [..............................] - ETA: 1:41:31 - loss: 0.6658 - regression_loss: 0.5344 - classification_loss: 0.1314
  148/10000 [..............................] - ETA: 1:41:19 - loss: 0.6639 - regression_loss: 0.5331 - classification_loss: 0.1308
  149/10000 [..............................] - ETA: 1:41:10 - loss: 0.6630 - regression_loss: 0.5326 - classification_loss: 0.1304
  150/10000 [..............................] - ETA: 1:41:01 - loss: 0.6653 - regression_loss: 0.5349 - classification_loss: 0.1304
  151/10000 [..............................] - ETA: 1:40:50 - loss: 0.6686 - regression_loss: 0.5369 - classification_loss: 0.1317
  152/10000 [..............................] - ETA: 1:40:39 - loss: 0.6675 - regression_loss: 0.5358 - classification_loss: 0.1318
  153/10000 [..............................] - ETA: 1:40:29 - loss: 0.6645 - regression_loss: 0.5334 - classification_loss: 0.1311
  154/10000 [..............................] - ETA: 1:40:17 - loss: 0.6617 - regression_loss: 0.5312 - classification_loss: 0.1305
  155/10000 [..............................] - ETA: 1:40:11 - loss: 0.6631 - regression_loss: 0.5322 - classification_loss: 0.1309
  156/10000 [..............................] - ETA: 1:40:01 - loss: 0.6632 - regression_loss: 0.5326 - classification_loss: 0.1306
  157/10000 [..............................] - ETA: 1:39:51 - loss: 0.6632 - regression_loss: 0.5331 - classification_loss: 0.1301
  158/10000 [..............................] - ETA: 1:40:06 - loss: 0.6663 - regression_loss: 0.5350 - classification_loss: 0.1312
  159/10000 [..............................] - ETA: 1:39:59 - loss: 0.6646 - regression_loss: 0.5334 - classification_loss: 0.1312
  160/10000 [..............................] - ETA: 1:39:47 - loss: 0.6615 - regression_loss: 0.5308 - classification_loss: 0.1306
  161/10000 [..............................] - ETA: 1:39:39 - loss: 0.6595 - regression_loss: 0.5293 - classification_loss: 0.1302
  162/10000 [..............................] - ETA: 1:39:29 - loss: 0.6569 - regression_loss: 0.5271 - classification_loss: 0.1297
  163/10000 [..............................] - ETA: 1:39:20 - loss: 0.6552 - regression_loss: 0.5260 - classification_loss: 0.1292
  164/10000 [..............................] - ETA: 1:39:11 - loss: 0.6559 - regression_loss: 0.5259 - classification_loss: 0.1300
  165/10000 [..............................] - ETA: 1:39:01 - loss: 0.6571 - regression_loss: 0.5267 - classification_loss: 0.1304
  166/10000 [..............................] - ETA: 1:38:53 - loss: 0.6583 - regression_loss: 0.5276 - classification_loss: 0.1307
  167/10000 [..............................] - ETA: 1:38:44 - loss: 0.6558 - regression_loss: 0.5255 - classification_loss: 0.1303
  168/10000 [..............................] - ETA: 1:38:34 - loss: 0.6553 - regression_loss: 0.5254 - classification_loss: 0.1299
  169/10000 [..............................] - ETA: 1:38:24 - loss: 0.6533 - regression_loss: 0.5238 - classification_loss: 0.1295
  170/10000 [..............................] - ETA: 1:38:40 - loss: 0.6512 - regression_loss: 0.5221 - classification_loss: 0.1291
  171/10000 [..............................] - ETA: 1:38:29 - loss: 0.6518 - regression_loss: 0.5227 - classification_loss: 0.1291
  172/10000 [..............................] - ETA: 1:38:20 - loss: 0.6512 - regression_loss: 0.5226 - classification_loss: 0.1287
  173/10000 [..............................] - ETA: 1:38:11 - loss: 0.6511 - regression_loss: 0.5223 - classification_loss: 0.1289
  174/10000 [..............................] - ETA: 1:38:03 - loss: 0.6487 - regression_loss: 0.5203 - classification_loss: 0.1284
  175/10000 [..............................] - ETA: 1:37:49 - loss: 0.6501 - regression_loss: 0.5215 - classification_loss: 0.1286
  176/10000 [..............................] - ETA: 1:37:56 - loss: 0.6508 - regression_loss: 0.5222 - classification_loss: 0.1286
  177/10000 [..............................] - ETA: 1:38:40 - loss: 0.6499 - regression_loss: 0.5217 - classification_loss: 0.1282
  178/10000 [..............................] - ETA: 1:38:31 - loss: 0.6490 - regression_loss: 0.5212 - classification_loss: 0.1278
  179/10000 [..............................] - ETA: 1:38:21 - loss: 0.6516 - regression_loss: 0.5238 - classification_loss: 0.1278
  180/10000 [..............................] - ETA: 1:38:13 - loss: 0.6498 - regression_loss: 0.5222 - classification_loss: 0.1275
  181/10000 [..............................] - ETA: 1:38:06 - loss: 0.6470 - regression_loss: 0.5201 - classification_loss: 0.1270
  182/10000 [..............................] - ETA: 1:37:56 - loss: 0.6494 - regression_loss: 0.5223 - classification_loss: 0.1271
  183/10000 [..............................] - ETA: 1:37:47 - loss: 0.6498 - regression_loss: 0.5225 - classification_loss: 0.1273
  184/10000 [..............................] - ETA: 1:37:39 - loss: 0.6470 - regression_loss: 0.5202 - classification_loss: 0.1268
  185/10000 [..............................] - ETA: 1:37:31 - loss: 0.6472 - regression_loss: 0.5207 - classification_loss: 0.1265
  186/10000 [..............................] - ETA: 1:37:22 - loss: 0.6461 - regression_loss: 0.5200 - classification_loss: 0.1261
  187/10000 [..............................] - ETA: 1:37:13 - loss: 0.6436 - regression_loss: 0.5181 - classification_loss: 0.1255
  188/10000 [..............................] - ETA: 1:37:11 - loss: 0.6464 - regression_loss: 0.5210 - classification_loss: 0.1253
  189/10000 [..............................] - ETA: 1:37:25 - loss: 0.6469 - regression_loss: 0.5217 - classification_loss: 0.1252
  190/10000 [..............................] - ETA: 1:37:17 - loss: 0.6446 - regression_loss: 0.5193 - classification_loss: 0.1254
  191/10000 [..............................] - ETA: 1:37:10 - loss: 0.6446 - regression_loss: 0.5183 - classification_loss: 0.1263
  192/10000 [..............................] - ETA: 1:37:03 - loss: 0.6447 - regression_loss: 0.5184 - classification_loss: 0.1263
  193/10000 [..............................] - ETA: 1:36:55 - loss: 0.6445 - regression_loss: 0.5175 - classification_loss: 0.1269
  194/10000 [..............................] - ETA: 1:37:05 - loss: 0.6455 - regression_loss: 0.5185 - classification_loss: 0.1270
  195/10000 [..............................] - ETA: 1:36:58 - loss: 0.6451 - regression_loss: 0.5181 - classification_loss: 0.1269
  196/10000 [..............................] - ETA: 1:36:49 - loss: 0.6432 - regression_loss: 0.5165 - classification_loss: 0.1268
  197/10000 [..............................] - ETA: 1:36:42 - loss: 0.6463 - regression_loss: 0.5194 - classification_loss: 0.1269
  198/10000 [..............................] - ETA: 1:36:34 - loss: 0.6462 - regression_loss: 0.5196 - classification_loss: 0.1266
  199/10000 [..............................] - ETA: 1:36:28 - loss: 0.6453 - regression_loss: 0.5184 - classification_loss: 0.1269
  200/10000 [..............................] - ETA: 1:36:20 - loss: 0.6467 - regression_loss: 0.5199 - classification_loss: 0.1268
  201/10000 [..............................] - ETA: 1:36:13 - loss: 0.6453 - regression_loss: 0.5190 - classification_loss: 0.1263
  202/10000 [..............................] - ETA: 1:36:06 - loss: 0.6467 - regression_loss: 0.5200 - classification_loss: 0.1267
  203/10000 [..............................] - ETA: 1:35:59 - loss: 0.6486 - regression_loss: 0.5214 - classification_loss: 0.1272
  204/10000 [..............................] - ETA: 1:35:51 - loss: 0.6468 - regression_loss: 0.5198 - classification_loss: 0.1270
  205/10000 [..............................] - ETA: 1:35:45 - loss: 0.6462 - regression_loss: 0.5190 - classification_loss: 0.1272
  206/10000 [..............................] - ETA: 1:35:38 - loss: 0.6439 - regression_loss: 0.5171 - classification_loss: 0.1268
  207/10000 [..............................] - ETA: 1:35:31 - loss: 0.6422 - regression_loss: 0.5152 - classification_loss: 0.1270
  208/10000 [..............................] - ETA: 1:35:25 - loss: 0.6434 - regression_loss: 0.5166 - classification_loss: 0.1267
  209/10000 [..............................] - ETA: 1:35:19 - loss: 0.6464 - regression_loss: 0.5194 - classification_loss: 0.1270
  210/10000 [..............................] - ETA: 1:35:12 - loss: 0.6466 - regression_loss: 0.5195 - classification_loss: 0.1270
  211/10000 [..............................] - ETA: 1:35:05 - loss: 0.6445 - regression_loss: 0.5179 - classification_loss: 0.1266
  212/10000 [..............................] - ETA: 1:34:59 - loss: 0.6447 - regression_loss: 0.5181 - classification_loss: 0.1266
  213/10000 [..............................] - ETA: 1:34:54 - loss: 0.6430 - regression_loss: 0.5167 - classification_loss: 0.1263
  214/10000 [..............................] - ETA: 1:34:47 - loss: 0.6420 - regression_loss: 0.5158 - classification_loss: 0.1262
  215/10000 [..............................] - ETA: 1:34:39 - loss: 0.6396 - regression_loss: 0.5139 - classification_loss: 0.1257
  216/10000 [..............................] - ETA: 1:34:33 - loss: 0.6380 - regression_loss: 0.5125 - classification_loss: 0.1256
  217/10000 [..............................] - ETA: 1:34:28 - loss: 0.6386 - regression_loss: 0.5130 - classification_loss: 0.1256
  218/10000 [..............................] - ETA: 1:34:21 - loss: 0.6380 - regression_loss: 0.5124 - classification_loss: 0.1256
  219/10000 [..............................] - ETA: 1:34:15 - loss: 0.6367 - regression_loss: 0.5106 - classification_loss: 0.1261
  220/10000 [..............................] - ETA: 1:34:11 - loss: 0.6375 - regression_loss: 0.5113 - classification_loss: 0.1262
  221/10000 [..............................] - ETA: 1:34:04 - loss: 0.6358 - regression_loss: 0.5095 - classification_loss: 0.1262
  222/10000 [..............................] - ETA: 1:33:58 - loss: 0.6381 - regression_loss: 0.5114 - classification_loss: 0.1267
  223/10000 [..............................] - ETA: 1:33:54 - loss: 0.6378 - regression_loss: 0.5113 - classification_loss: 0.1265
  224/10000 [..............................] - ETA: 1:33:48 - loss: 0.6360 - regression_loss: 0.5099 - classification_loss: 0.1261
  225/10000 [..............................] - ETA: 1:33:42 - loss: 0.6347 - regression_loss: 0.5090 - classification_loss: 0.1257
  226/10000 [..............................] - ETA: 1:33:36 - loss: 0.6324 - regression_loss: 0.5072 - classification_loss: 0.1252
  227/10000 [..............................] - ETA: 1:33:30 - loss: 0.6356 - regression_loss: 0.5100 - classification_loss: 0.1256
  228/10000 [..............................] - ETA: 1:33:24 - loss: 0.6399 - regression_loss: 0.5132 - classification_loss: 0.1266
  229/10000 [..............................] - ETA: 1:33:20 - loss: 0.6384 - regression_loss: 0.5114 - classification_loss: 0.1270
  230/10000 [..............................] - ETA: 1:33:28 - loss: 0.6402 - regression_loss: 0.5134 - classification_loss: 0.1268
  231/10000 [..............................] - ETA: 1:33:21 - loss: 0.6421 - regression_loss: 0.5150 - classification_loss: 0.1271
  232/10000 [..............................] - ETA: 1:33:16 - loss: 0.6434 - regression_loss: 0.5162 - classification_loss: 0.1272
  233/10000 [..............................] - ETA: 1:33:11 - loss: 0.6450 - regression_loss: 0.5179 - classification_loss: 0.1271
  234/10000 [..............................] - ETA: 1:33:05 - loss: 0.6437 - regression_loss: 0.5166 - classification_loss: 0.1271
  235/10000 [..............................] - ETA: 1:33:01 - loss: 0.6488 - regression_loss: 0.5204 - classification_loss: 0.1284
  236/10000 [..............................] - ETA: 1:32:55 - loss: 0.6479 - regression_loss: 0.5199 - classification_loss: 0.1280
  237/10000 [..............................] - ETA: 1:32:51 - loss: 0.6478 - regression_loss: 0.5200 - classification_loss: 0.1278
  238/10000 [..............................] - ETA: 1:32:45 - loss: 0.6463 - regression_loss: 0.5187 - classification_loss: 0.1276
  239/10000 [..............................] - ETA: 1:32:40 - loss: 0.6470 - regression_loss: 0.5196 - classification_loss: 0.1274
  240/10000 [..............................] - ETA: 1:32:36 - loss: 0.6450 - regression_loss: 0.5180 - classification_loss: 0.1270
  241/10000 [..............................] - ETA: 1:32:30 - loss: 0.6433 - regression_loss: 0.5167 - classification_loss: 0.1266
  242/10000 [..............................] - ETA: 1:32:27 - loss: 0.6417 - regression_loss: 0.5156 - classification_loss: 0.1261
  243/10000 [..............................] - ETA: 1:32:23 - loss: 0.6422 - regression_loss: 0.5161 - classification_loss: 0.1261
  244/10000 [..............................] - ETA: 1:32:17 - loss: 0.6401 - regression_loss: 0.5143 - classification_loss: 0.1258
  245/10000 [..............................] - ETA: 1:32:12 - loss: 0.6392 - regression_loss: 0.5135 - classification_loss: 0.1257
  246/10000 [..............................] - ETA: 1:32:07 - loss: 0.6380 - regression_loss: 0.5126 - classification_loss: 0.1254
  247/10000 [..............................] - ETA: 1:32:40 - loss: 0.6399 - regression_loss: 0.5146 - classification_loss: 0.1253
  248/10000 [..............................] - ETA: 1:32:35 - loss: 0.6384 - regression_loss: 0.5135 - classification_loss: 0.1249
  249/10000 [..............................] - ETA: 1:32:30 - loss: 0.6422 - regression_loss: 0.5165 - classification_loss: 0.1257
  250/10000 [..............................] - ETA: 1:32:26 - loss: 0.6410 - regression_loss: 0.5155 - classification_loss: 0.1254
  251/10000 [..............................] - ETA: 1:32:21 - loss: 0.6388 - regression_loss: 0.5136 - classification_loss: 0.1251
  252/10000 [..............................] - ETA: 1:32:16 - loss: 0.6367 - regression_loss: 0.5119 - classification_loss: 0.1248
  253/10000 [..............................] - ETA: 1:32:12 - loss: 0.6396 - regression_loss: 0.5143 - classification_loss: 0.1253
  254/10000 [..............................] - ETA: 1:32:08 - loss: 0.6382 - regression_loss: 0.5132 - classification_loss: 0.1250
  255/10000 [..............................] - ETA: 1:32:04 - loss: 0.6382 - regression_loss: 0.5131 - classification_loss: 0.1251
  256/10000 [..............................] - ETA: 1:31:59 - loss: 0.6367 - regression_loss: 0.5119 - classification_loss: 0.1248
  257/10000 [..............................] - ETA: 1:31:55 - loss: 0.6368 - regression_loss: 0.5118 - classification_loss: 0.1250
  258/10000 [..............................] - ETA: 1:31:51 - loss: 0.6390 - regression_loss: 0.5139 - classification_loss: 0.1251
  259/10000 [..............................] - ETA: 1:31:47 - loss: 0.6373 - regression_loss: 0.5124 - classification_loss: 0.1249
  260/10000 [..............................] - ETA: 1:32:15 - loss: 0.6396 - regression_loss: 0.5145 - classification_loss: 0.1251
  261/10000 [..............................] - ETA: 1:32:12 - loss: 0.6422 - regression_loss: 0.5168 - classification_loss: 0.1255
  262/10000 [..............................] - ETA: 1:32:06 - loss: 0.6441 - regression_loss: 0.5186 - classification_loss: 0.1255
  263/10000 [..............................] - ETA: 1:32:02 - loss: 0.6432 - regression_loss: 0.5180 - classification_loss: 0.1252
  264/10000 [..............................] - ETA: 1:31:58 - loss: 0.6408 - regression_loss: 0.5161 - classification_loss: 0.1247
  265/10000 [..............................] - ETA: 1:31:54 - loss: 0.6390 - regression_loss: 0.5145 - classification_loss: 0.1245
  266/10000 [..............................] - ETA: 1:31:49 - loss: 0.6394 - regression_loss: 0.5152 - classification_loss: 0.1243
  267/10000 [..............................] - ETA: 1:32:12 - loss: 0.6379 - regression_loss: 0.5140 - classification_loss: 0.1239
  268/10000 [..............................] - ETA: 1:32:07 - loss: 0.6364 - regression_loss: 0.5127 - classification_loss: 0.1238
  269/10000 [..............................] - ETA: 1:32:02 - loss: 0.6367 - regression_loss: 0.5131 - classification_loss: 0.1237
  270/10000 [..............................] - ETA: 1:31:57 - loss: 0.6388 - regression_loss: 0.5147 - classification_loss: 0.1242
  271/10000 [..............................] - ETA: 1:31:53 - loss: 0.6378 - regression_loss: 0.5138 - classification_loss: 0.1240
  272/10000 [..............................] - ETA: 1:31:49 - loss: 0.6369 - regression_loss: 0.5127 - classification_loss: 0.1242
  273/10000 [..............................] - ETA: 1:31:45 - loss: 0.6368 - regression_loss: 0.5124 - classification_loss: 0.1244
  274/10000 [..............................] - ETA: 1:31:41 - loss: 0.6362 - regression_loss: 0.5117 - classification_loss: 0.1245
  275/10000 [..............................] - ETA: 1:31:38 - loss: 0.6392 - regression_loss: 0.5145 - classification_loss: 0.1246
  276/10000 [..............................] - ETA: 1:31:42 - loss: 0.6382 - regression_loss: 0.5140 - classification_loss: 0.1243
  277/10000 [..............................] - ETA: 1:31:37 - loss: 0.6385 - regression_loss: 0.5138 - classification_loss: 0.1247
  278/10000 [..............................] - ETA: 1:31:32 - loss: 0.6376 - regression_loss: 0.5132 - classification_loss: 0.1244
  279/10000 [..............................] - ETA: 1:31:29 - loss: 0.6390 - regression_loss: 0.5144 - classification_loss: 0.1245
  280/10000 [..............................] - ETA: 1:31:50 - loss: 0.6391 - regression_loss: 0.5146 - classification_loss: 0.1245
  281/10000 [..............................] - ETA: 1:31:47 - loss: 0.6397 - regression_loss: 0.5152 - classification_loss: 0.1245
  282/10000 [..............................] - ETA: 1:31:44 - loss: 0.6424 - regression_loss: 0.5176 - classification_loss: 0.1248
  283/10000 [..............................] - ETA: 1:31:39 - loss: 0.6421 - regression_loss: 0.5168 - classification_loss: 0.1253
  284/10000 [..............................] - ETA: 1:31:34 - loss: 0.6413 - regression_loss: 0.5160 - classification_loss: 0.1253
  285/10000 [..............................] - ETA: 1:31:30 - loss: 0.6426 - regression_loss: 0.5170 - classification_loss: 0.1256
  286/10000 [..............................] - ETA: 1:31:25 - loss: 0.6419 - regression_loss: 0.5162 - classification_loss: 0.1257
  287/10000 [..............................] - ETA: 1:31:22 - loss: 0.6397 - regression_loss: 0.5144 - classification_loss: 0.1253
  288/10000 [..............................] - ETA: 1:31:17 - loss: 0.6410 - regression_loss: 0.5153 - classification_loss: 0.1257
  289/10000 [..............................] - ETA: 1:31:13 - loss: 0.6416 - regression_loss: 0.5155 - classification_loss: 0.1262
  290/10000 [..............................] - ETA: 1:31:10 - loss: 0.6409 - regression_loss: 0.5149 - classification_loss: 0.1259
  291/10000 [..............................] - ETA: 1:31:05 - loss: 0.6428 - regression_loss: 0.5164 - classification_loss: 0.1264
  292/10000 [..............................] - ETA: 1:31:32 - loss: 0.6437 - regression_loss: 0.5174 - classification_loss: 0.1263
  293/10000 [..............................] - ETA: 1:31:29 - loss: 0.6454 - regression_loss: 0.5191 - classification_loss: 0.1263
  294/10000 [..............................] - ETA: 1:31:47 - loss: 0.6453 - regression_loss: 0.5190 - classification_loss: 0.1263
  295/10000 [..............................] - ETA: 1:32:00 - loss: 0.6452 - regression_loss: 0.5185 - classification_loss: 0.1267
  296/10000 [..............................] - ETA: 1:31:56 - loss: 0.6446 - regression_loss: 0.5182 - classification_loss: 0.1265
  297/10000 [..............................] - ETA: 1:31:51 - loss: 0.6445 - regression_loss: 0.5179 - classification_loss: 0.1267
  298/10000 [..............................] - ETA: 1:31:47 - loss: 0.6436 - regression_loss: 0.5171 - classification_loss: 0.1265
  299/10000 [..............................] - ETA: 1:31:42 - loss: 0.6442 - regression_loss: 0.5176 - classification_loss: 0.1266
  300/10000 [..............................] - ETA: 1:31:37 - loss: 0.6470 - regression_loss: 0.5191 - classification_loss: 0.1279
  301/10000 [..............................] - ETA: 1:31:33 - loss: 0.6467 - regression_loss: 0.5189 - classification_loss: 0.1278
  302/10000 [..............................] - ETA: 1:31:28 - loss: 0.6466 - regression_loss: 0.5186 - classification_loss: 0.1280
  303/10000 [..............................] - ETA: 1:31:25 - loss: 0.6448 - regression_loss: 0.5171 - classification_loss: 0.1276
  304/10000 [..............................] - ETA: 1:31:20 - loss: 0.6433 - regression_loss: 0.5160 - classification_loss: 0.1273
  305/10000 [..............................] - ETA: 1:31:16 - loss: 0.6416 - regression_loss: 0.5145 - classification_loss: 0.1270
  306/10000 [..............................] - ETA: 1:31:11 - loss: 0.6409 - regression_loss: 0.5141 - classification_loss: 0.1268
  307/10000 [..............................] - ETA: 1:31:07 - loss: 0.6414 - regression_loss: 0.5136 - classification_loss: 0.1279
  308/10000 [..............................] - ETA: 1:31:02 - loss: 0.6410 - regression_loss: 0.5132 - classification_loss: 0.1278
  309/10000 [..............................] - ETA: 1:30:58 - loss: 0.6400 - regression_loss: 0.5125 - classification_loss: 0.1275
  310/10000 [..............................] - ETA: 1:30:54 - loss: 0.6460 - regression_loss: 0.5165 - classification_loss: 0.1295
  311/10000 [..............................] - ETA: 1:30:50 - loss: 0.6450 - regression_loss: 0.5153 - classification_loss: 0.1297
  312/10000 [..............................] - ETA: 1:30:46 - loss: 0.6436 - regression_loss: 0.5140 - classification_loss: 0.1297
  313/10000 [..............................] - ETA: 1:31:03 - loss: 0.6437 - regression_loss: 0.5133 - classification_loss: 0.1303
  314/10000 [..............................] - ETA: 1:31:12 - loss: 0.6435 - regression_loss: 0.5133 - classification_loss: 0.1302
  315/10000 [..............................] - ETA: 1:31:28 - loss: 0.6430 - regression_loss: 0.5131 - classification_loss: 0.1299
  316/10000 [..............................] - ETA: 1:31:23 - loss: 0.6446 - regression_loss: 0.5148 - classification_loss: 0.1298
  317/10000 [..............................] - ETA: 1:31:20 - loss: 0.6447 - regression_loss: 0.5146 - classification_loss: 0.1300
  318/10000 [..............................] - ETA: 1:31:15 - loss: 0.6452 - regression_loss: 0.5153 - classification_loss: 0.1299
  319/10000 [..............................] - ETA: 1:31:11 - loss: 0.6439 - regression_loss: 0.5140 - classification_loss: 0.1298
  320/10000 [..............................] - ETA: 1:31:07 - loss: 0.6462 - regression_loss: 0.5159 - classification_loss: 0.1302
  321/10000 [..............................] - ETA: 1:31:03 - loss: 0.6459 - regression_loss: 0.5154 - classification_loss: 0.1305
  322/10000 [..............................] - ETA: 1:31:00 - loss: 0.6458 - regression_loss: 0.5152 - classification_loss: 0.1306
  323/10000 [..............................] - ETA: 1:30:55 - loss: 0.6451 - regression_loss: 0.5145 - classification_loss: 0.1306
  324/10000 [..............................] - ETA: 1:30:51 - loss: 0.6444 - regression_loss: 0.5138 - classification_loss: 0.1307
  325/10000 [..............................] - ETA: 1:30:47 - loss: 0.6442 - regression_loss: 0.5134 - classification_loss: 0.1309
  326/10000 [..............................] - ETA: 1:30:43 - loss: 0.6466 - regression_loss: 0.5152 - classification_loss: 0.1313
  327/10000 [..............................] - ETA: 1:31:02 - loss: 0.6459 - regression_loss: 0.5143 - classification_loss: 0.1316
  328/10000 [..............................] - ETA: 1:30:58 - loss: 0.6453 - regression_loss: 0.5136 - classification_loss: 0.1317
  329/10000 [..............................] - ETA: 1:30:55 - loss: 0.6451 - regression_loss: 0.5136 - classification_loss: 0.1316
  330/10000 [..............................] - ETA: 1:30:51 - loss: 0.6459 - regression_loss: 0.5141 - classification_loss: 0.1318
  331/10000 [..............................] - ETA: 1:30:47 - loss: 0.6479 - regression_loss: 0.5156 - classification_loss: 0.1323
  332/10000 [..............................] - ETA: 1:30:43 - loss: 0.6468 - regression_loss: 0.5148 - classification_loss: 0.1321
  333/10000 [..............................] - ETA: 1:30:40 - loss: 0.6459 - regression_loss: 0.5142 - classification_loss: 0.1318
  334/10000 [>.............................] - ETA: 1:30:37 - loss: 0.6452 - regression_loss: 0.5137 - classification_loss: 0.1314
  335/10000 [>.............................] - ETA: 1:30:58 - loss: 0.6437 - regression_loss: 0.5125 - classification_loss: 0.1312
  336/10000 [>.............................] - ETA: 1:30:54 - loss: 0.6447 - regression_loss: 0.5135 - classification_loss: 0.1312
  337/10000 [>.............................] - ETA: 1:30:50 - loss: 0.6439 - regression_loss: 0.5128 - classification_loss: 0.1311
  338/10000 [>.............................] - ETA: 1:30:46 - loss: 0.6460 - regression_loss: 0.5147 - classification_loss: 0.1313
  339/10000 [>.............................] - ETA: 1:30:42 - loss: 0.6450 - regression_loss: 0.5139 - classification_loss: 0.1311
  340/10000 [>.............................] - ETA: 1:30:39 - loss: 0.6458 - regression_loss: 0.5146 - classification_loss: 0.1312
  341/10000 [>.............................] - ETA: 1:30:36 - loss: 0.6459 - regression_loss: 0.5148 - classification_loss: 0.1310
  342/10000 [>.............................] - ETA: 1:30:45 - loss: 0.6481 - regression_loss: 0.5170 - classification_loss: 0.1312
  343/10000 [>.............................] - ETA: 1:30:42 - loss: 0.6469 - regression_loss: 0.5160 - classification_loss: 0.1309
  344/10000 [>.............................] - ETA: 1:30:46 - loss: 0.6459 - regression_loss: 0.5153 - classification_loss: 0.1306
  345/10000 [>.............................] - ETA: 1:30:42 - loss: 0.6475 - regression_loss: 0.5164 - classification_loss: 0.1311
  346/10000 [>.............................] - ETA: 1:30:39 - loss: 0.6492 - regression_loss: 0.5178 - classification_loss: 0.1315
  347/10000 [>.............................] - ETA: 1:30:35 - loss: 0.6481 - regression_loss: 0.5168 - classification_loss: 0.1313
  348/10000 [>.............................] - ETA: 1:30:31 - loss: 0.6480 - regression_loss: 0.5167 - classification_loss: 0.1314
  349/10000 [>.............................] - ETA: 1:30:28 - loss: 0.6506 - regression_loss: 0.5185 - classification_loss: 0.1320
  350/10000 [>.............................] - ETA: 1:30:23 - loss: 0.6504 - regression_loss: 0.5184 - classification_loss: 0.1320
  351/10000 [>.............................] - ETA: 1:30:21 - loss: 0.6495 - regression_loss: 0.5178 - classification_loss: 0.1317
  352/10000 [>.............................] - ETA: 1:30:19 - loss: 0.6500 - regression_loss: 0.5183 - classification_loss: 0.1317
  353/10000 [>.............................] - ETA: 1:30:16 - loss: 0.6489 - regression_loss: 0.5174 - classification_loss: 0.1316
  354/10000 [>.............................] - ETA: 1:30:29 - loss: 0.6480 - regression_loss: 0.5168 - classification_loss: 0.1312
  355/10000 [>.............................] - ETA: 1:30:46 - loss: 0.6494 - regression_loss: 0.5182 - classification_loss: 0.1311
  356/10000 [>.............................] - ETA: 1:30:43 - loss: 0.6484 - regression_loss: 0.5175 - classification_loss: 0.1309
  357/10000 [>.............................] - ETA: 1:30:39 - loss: 0.6470 - regression_loss: 0.5164 - classification_loss: 0.1306
  358/10000 [>.............................] - ETA: 1:30:45 - loss: 0.6510 - regression_loss: 0.5197 - classification_loss: 0.1313
  359/10000 [>.............................] - ETA: 1:30:43 - loss: 0.6497 - regression_loss: 0.5187 - classification_loss: 0.1310
  360/10000 [>.............................] - ETA: 1:30:39 - loss: 0.6489 - regression_loss: 0.5181 - classification_loss: 0.1308
  361/10000 [>.............................] - ETA: 1:30:36 - loss: 0.6489 - regression_loss: 0.5182 - classification_loss: 0.1307
  362/10000 [>.............................] - ETA: 1:30:39 - loss: 0.6482 - regression_loss: 0.5177 - classification_loss: 0.1304
  363/10000 [>.............................] - ETA: 1:30:37 - loss: 0.6499 - regression_loss: 0.5195 - classification_loss: 0.1304
  364/10000 [>.............................] - ETA: 1:30:44 - loss: 0.6486 - regression_loss: 0.5185 - classification_loss: 0.1301
  365/10000 [>.............................] - ETA: 1:30:41 - loss: 0.6476 - regression_loss: 0.5177 - classification_loss: 0.1299
  366/10000 [>.............................] - ETA: 1:30:38 - loss: 0.6473 - regression_loss: 0.5177 - classification_loss: 0.1296
  367/10000 [>.............................] - ETA: 1:30:34 - loss: 0.6483 - regression_loss: 0.5188 - classification_loss: 0.1296
  368/10000 [>.............................] - ETA: 1:30:30 - loss: 0.6492 - regression_loss: 0.5197 - classification_loss: 0.1296
  369/10000 [>.............................] - ETA: 1:30:27 - loss: 0.6486 - regression_loss: 0.5192 - classification_loss: 0.1293
  370/10000 [>.............................] - ETA: 1:30:23 - loss: 0.6498 - regression_loss: 0.5204 - classification_loss: 0.1294
  371/10000 [>.............................] - ETA: 1:30:20 - loss: 0.6490 - regression_loss: 0.5198 - classification_loss: 0.1291
  372/10000 [>.............................] - ETA: 1:30:43 - loss: 0.6513 - regression_loss: 0.5220 - classification_loss: 0.1293
  373/10000 [>.............................] - ETA: 1:30:41 - loss: 0.6509 - regression_loss: 0.5217 - classification_loss: 0.1291
  374/10000 [>.............................] - ETA: 1:30:46 - loss: 0.6524 - regression_loss: 0.5232 - classification_loss: 0.1292
  375/10000 [>.............................] - ETA: 1:30:42 - loss: 0.6518 - regression_loss: 0.5227 - classification_loss: 0.1290
  376/10000 [>.............................] - ETA: 1:30:40 - loss: 0.6515 - regression_loss: 0.5222 - classification_loss: 0.1293
  377/10000 [>.............................] - ETA: 1:30:36 - loss: 0.6521 - regression_loss: 0.5229 - classification_loss: 0.1292
  378/10000 [>.............................] - ETA: 1:30:34 - loss: 0.6527 - regression_loss: 0.5231 - classification_loss: 0.1297
  379/10000 [>.............................] - ETA: 1:30:30 - loss: 0.6518 - regression_loss: 0.5224 - classification_loss: 0.1294
  380/10000 [>.............................] - ETA: 1:30:26 - loss: 0.6511 - regression_loss: 0.5218 - classification_loss: 0.1292
  381/10000 [>.............................] - ETA: 1:30:23 - loss: 0.6513 - regression_loss: 0.5221 - classification_loss: 0.1291
  382/10000 [>.............................] - ETA: 1:30:19 - loss: 0.6503 - regression_loss: 0.5213 - classification_loss: 0.1289
  383/10000 [>.............................] - ETA: 1:30:16 - loss: 0.6497 - regression_loss: 0.5209 - classification_loss: 0.1288
  384/10000 [>.............................] - ETA: 1:30:12 - loss: 0.6495 - regression_loss: 0.5204 - classification_loss: 0.1291
  385/10000 [>.............................] - ETA: 1:30:11 - loss: 0.6502 - regression_loss: 0.5208 - classification_loss: 0.1294
  386/10000 [>.............................] - ETA: 1:30:09 - loss: 0.6522 - regression_loss: 0.5226 - classification_loss: 0.1296
  387/10000 [>.............................] - ETA: 1:30:18 - loss: 0.6529 - regression_loss: 0.5231 - classification_loss: 0.1298
  388/10000 [>.............................] - ETA: 1:30:15 - loss: 0.6519 - regression_loss: 0.5224 - classification_loss: 0.1295
  389/10000 [>.............................] - ETA: 1:30:12 - loss: 0.6509 - regression_loss: 0.5217 - classification_loss: 0.1293
  390/10000 [>.............................] - ETA: 1:30:09 - loss: 0.6513 - regression_loss: 0.5216 - classification_loss: 0.1298
  391/10000 [>.............................] - ETA: 1:30:10 - loss: 0.6513 - regression_loss: 0.5214 - classification_loss: 0.1298
  392/10000 [>.............................] - ETA: 1:30:07 - loss: 0.6520 - regression_loss: 0.5222 - classification_loss: 0.1299
  393/10000 [>.............................] - ETA: 1:30:04 - loss: 0.6511 - regression_loss: 0.5214 - classification_loss: 0.1297
  394/10000 [>.............................] - ETA: 1:30:01 - loss: 0.6510 - regression_loss: 0.5215 - classification_loss: 0.1295
  395/10000 [>.............................] - ETA: 1:29:58 - loss: 0.6511 - regression_loss: 0.5217 - classification_loss: 0.1294
  396/10000 [>.............................] - ETA: 1:29:55 - loss: 0.6497 - regression_loss: 0.5206 - classification_loss: 0.1292
  397/10000 [>.............................] - ETA: 1:29:52 - loss: 0.6502 - regression_loss: 0.5210 - classification_loss: 0.1292
  398/10000 [>.............................] - ETA: 1:29:50 - loss: 0.6496 - regression_loss: 0.5205 - classification_loss: 0.1291
  399/10000 [>.............................] - ETA: 1:29:48 - loss: 0.6493 - regression_loss: 0.5203 - classification_loss: 0.1290
  400/10000 [>.............................] - ETA: 1:29:44 - loss: 0.6502 - regression_loss: 0.5212 - classification_loss: 0.1291
  401/10000 [>.............................] - ETA: 1:29:41 - loss: 0.6507 - regression_loss: 0.5215 - classification_loss: 0.1292
  402/10000 [>.............................] - ETA: 1:29:40 - loss: 0.6502 - regression_loss: 0.5211 - classification_loss: 0.1291
  403/10000 [>.............................] - ETA: 1:29:36 - loss: 0.6509 - regression_loss: 0.5219 - classification_loss: 0.1290
  404/10000 [>.............................] - ETA: 1:29:34 - loss: 0.6502 - regression_loss: 0.5213 - classification_loss: 0.1289
  405/10000 [>.............................] - ETA: 1:29:31 - loss: 0.6506 - regression_loss: 0.5217 - classification_loss: 0.1289
  406/10000 [>.............................] - ETA: 1:29:28 - loss: 0.6517 - regression_loss: 0.5228 - classification_loss: 0.1289
  407/10000 [>.............................] - ETA: 1:29:25 - loss: 0.6527 - regression_loss: 0.5237 - classification_loss: 0.1289
  408/10000 [>.............................] - ETA: 1:29:22 - loss: 0.6528 - regression_loss: 0.5240 - classification_loss: 0.1288
  409/10000 [>.............................] - ETA: 1:29:19 - loss: 0.6525 - regression_loss: 0.5237 - classification_loss: 0.1287
  410/10000 [>.............................] - ETA: 1:29:17 - loss: 0.6519 - regression_loss: 0.5234 - classification_loss: 0.1285
  411/10000 [>.............................] - ETA: 1:29:14 - loss: 0.6515 - regression_loss: 0.5230 - classification_loss: 0.1285
  412/10000 [>.............................] - ETA: 1:29:11 - loss: 0.6518 - regression_loss: 0.5235 - classification_loss: 0.1283
  413/10000 [>.............................] - ETA: 1:29:09 - loss: 0.6506 - regression_loss: 0.5224 - classification_loss: 0.1282
  414/10000 [>.............................] - ETA: 1:29:18 - loss: 0.6506 - regression_loss: 0.5226 - classification_loss: 0.1280
  415/10000 [>.............................] - ETA: 1:29:16 - loss: 0.6524 - regression_loss: 0.5242 - classification_loss: 0.1282
  416/10000 [>.............................] - ETA: 1:29:14 - loss: 0.6513 - regression_loss: 0.5233 - classification_loss: 0.1280
  417/10000 [>.............................] - ETA: 1:29:11 - loss: 0.6525 - regression_loss: 0.5245 - classification_loss: 0.1280
  418/10000 [>.............................] - ETA: 1:29:08 - loss: 0.6518 - regression_loss: 0.5240 - classification_loss: 0.1278
  419/10000 [>.............................] - ETA: 1:29:06 - loss: 0.6510 - regression_loss: 0.5234 - classification_loss: 0.1276
  420/10000 [>.............................] - ETA: 1:29:03 - loss: 0.6527 - regression_loss: 0.5251 - classification_loss: 0.1276
  421/10000 [>.............................] - ETA: 1:28:59 - loss: 0.6530 - regression_loss: 0.5253 - classification_loss: 0.1277
  422/10000 [>.............................] - ETA: 1:28:56 - loss: 0.6523 - regression_loss: 0.5244 - classification_loss: 0.1279
  423/10000 [>.............................] - ETA: 1:28:54 - loss: 0.6515 - regression_loss: 0.5237 - classification_loss: 0.1277
  424/10000 [>.............................] - ETA: 1:28:52 - loss: 0.6531 - regression_loss: 0.5251 - classification_loss: 0.1280
  425/10000 [>.............................] - ETA: 1:28:51 - loss: 0.6548 - regression_loss: 0.5268 - classification_loss: 0.1281
  426/10000 [>.............................] - ETA: 1:28:48 - loss: 0.6543 - regression_loss: 0.5264 - classification_loss: 0.1279
  427/10000 [>.............................] - ETA: 1:28:45 - loss: 0.6534 - regression_loss: 0.5256 - classification_loss: 0.1278
  428/10000 [>.............................] - ETA: 1:28:42 - loss: 0.6540 - regression_loss: 0.5261 - classification_loss: 0.1279
  429/10000 [>.............................] - ETA: 1:28:56 - loss: 0.6567 - regression_loss: 0.5277 - classification_loss: 0.1290
  430/10000 [>.............................] - ETA: 1:28:53 - loss: 0.6561 - regression_loss: 0.5273 - classification_loss: 0.1289
  431/10000 [>.............................] - ETA: 1:29:01 - loss: 0.6553 - regression_loss: 0.5266 - classification_loss: 0.1288
  432/10000 [>.............................] - ETA: 1:28:57 - loss: 0.6544 - regression_loss: 0.5259 - classification_loss: 0.1285
  433/10000 [>.............................] - ETA: 1:29:05 - loss: 0.6539 - regression_loss: 0.5256 - classification_loss: 0.1283
  434/10000 [>.............................] - ETA: 1:29:02 - loss: 0.6530 - regression_loss: 0.5248 - classification_loss: 0.1281
  435/10000 [>.............................] - ETA: 1:29:08 - loss: 0.6518 - regression_loss: 0.5239 - classification_loss: 0.1279
  436/10000 [>.............................] - ETA: 1:29:05 - loss: 0.6526 - regression_loss: 0.5246 - classification_loss: 0.1280
  437/10000 [>.............................] - ETA: 1:29:02 - loss: 0.6532 - regression_loss: 0.5252 - classification_loss: 0.1280
  438/10000 [>.............................] - ETA: 1:28:59 - loss: 0.6561 - regression_loss: 0.5278 - classification_loss: 0.1283
  439/10000 [>.............................] - ETA: 1:28:56 - loss: 0.6552 - regression_loss: 0.5271 - classification_loss: 0.1281
  440/10000 [>.............................] - ETA: 1:28:53 - loss: 0.6540 - regression_loss: 0.5262 - classification_loss: 0.1279
  441/10000 [>.............................] - ETA: 1:28:50 - loss: 0.6533 - regression_loss: 0.5257 - classification_loss: 0.1276
  442/10000 [>.............................] - ETA: 1:28:47 - loss: 0.6546 - regression_loss: 0.5264 - classification_loss: 0.1281
  443/10000 [>.............................] - ETA: 1:28:44 - loss: 0.6557 - regression_loss: 0.5274 - classification_loss: 0.1283
  444/10000 [>.............................] - ETA: 1:28:41 - loss: 0.6559 - regression_loss: 0.5277 - classification_loss: 0.1282
  445/10000 [>.............................] - ETA: 1:28:38 - loss: 0.6558 - regression_loss: 0.5277 - classification_loss: 0.1281
  446/10000 [>.............................] - ETA: 1:28:36 - loss: 0.6551 - regression_loss: 0.5271 - classification_loss: 0.1281
  447/10000 [>.............................] - ETA: 1:28:34 - loss: 0.6548 - regression_loss: 0.5269 - classification_loss: 0.1279
  448/10000 [>.............................] - ETA: 1:28:30 - loss: 0.6542 - regression_loss: 0.5264 - classification_loss: 0.1278
  449/10000 [>.............................] - ETA: 1:28:39 - loss: 0.6544 - regression_loss: 0.5265 - classification_loss: 0.1280
  450/10000 [>.............................] - ETA: 1:28:36 - loss: 0.6562 - regression_loss: 0.5276 - classification_loss: 0.1286
  451/10000 [>.............................] - ETA: 1:28:40 - loss: 0.6553 - regression_loss: 0.5269 - classification_loss: 0.1284
  452/10000 [>.............................] - ETA: 1:28:47 - loss: 0.6542 - regression_loss: 0.5260 - classification_loss: 0.1282
  453/10000 [>.............................] - ETA: 1:28:44 - loss: 0.6534 - regression_loss: 0.5254 - classification_loss: 0.1280
  454/10000 [>.............................] - ETA: 1:28:58 - loss: 0.6530 - regression_loss: 0.5252 - classification_loss: 0.1278
  455/10000 [>.............................] - ETA: 1:28:55 - loss: 0.6520 - regression_loss: 0.5243 - classification_loss: 0.1277
  456/10000 [>.............................] - ETA: 1:28:52 - loss: 0.6546 - regression_loss: 0.5263 - classification_loss: 0.1283
  457/10000 [>.............................] - ETA: 1:28:49 - loss: 0.6550 - regression_loss: 0.5268 - classification_loss: 0.1283
  458/10000 [>.............................] - ETA: 1:28:46 - loss: 0.6548 - regression_loss: 0.5267 - classification_loss: 0.1281
  459/10000 [>.............................] - ETA: 1:28:44 - loss: 0.6551 - regression_loss: 0.5270 - classification_loss: 0.1281
  460/10000 [>.............................] - ETA: 1:28:41 - loss: 0.6553 - regression_loss: 0.5264 - classification_loss: 0.1289
  461/10000 [>.............................] - ETA: 1:28:38 - loss: 0.6552 - regression_loss: 0.5264 - classification_loss: 0.1288
  462/10000 [>.............................] - ETA: 1:28:38 - loss: 0.6551 - regression_loss: 0.5264 - classification_loss: 0.1287
  463/10000 [>.............................] - ETA: 1:28:35 - loss: 0.6551 - regression_loss: 0.5264 - classification_loss: 0.1287
  464/10000 [>.............................] - ETA: 1:28:32 - loss: 0.6555 - regression_loss: 0.5268 - classification_loss: 0.1288
  465/10000 [>.............................] - ETA: 1:28:29 - loss: 0.6567 - regression_loss: 0.5280 - classification_loss: 0.1287
  466/10000 [>.............................] - ETA: 1:28:27 - loss: 0.6561 - regression_loss: 0.5276 - classification_loss: 0.1286
  467/10000 [>.............................] - ETA: 1:28:40 - loss: 0.6560 - regression_loss: 0.5276 - classification_loss: 0.1285
  468/10000 [>.............................] - ETA: 1:28:56 - loss: 0.6577 - regression_loss: 0.5290 - classification_loss: 0.1287
  469/10000 [>.............................] - ETA: 1:28:54 - loss: 0.6577 - regression_loss: 0.5290 - classification_loss: 0.1286
  470/10000 [>.............................] - ETA: 1:28:51 - loss: 0.6575 - regression_loss: 0.5290 - classification_loss: 0.1285
  471/10000 [>.............................] - ETA: 1:28:48 - loss: 0.6571 - regression_loss: 0.5286 - classification_loss: 0.1285
  472/10000 [>.............................] - ETA: 1:28:46 - loss: 0.6572 - regression_loss: 0.5288 - classification_loss: 0.1284
  473/10000 [>.............................] - ETA: 1:29:00 - loss: 0.6569 - regression_loss: 0.5285 - classification_loss: 0.1284
  474/10000 [>.............................] - ETA: 1:28:57 - loss: 0.6561 - regression_loss: 0.5279 - classification_loss: 0.1282
  475/10000 [>.............................] - ETA: 1:28:54 - loss: 0.6558 - regression_loss: 0.5277 - classification_loss: 0.1282
  476/10000 [>.............................] - ETA: 1:28:51 - loss: 0.6556 - regression_loss: 0.5275 - classification_loss: 0.1280
  477/10000 [>.............................] - ETA: 1:28:49 - loss: 0.6555 - regression_loss: 0.5277 - classification_loss: 0.1279
  478/10000 [>.............................] - ETA: 1:28:47 - loss: 0.6549 - regression_loss: 0.5271 - classification_loss: 0.1278
  479/10000 [>.............................] - ETA: 1:28:45 - loss: 0.6555 - regression_loss: 0.5275 - classification_loss: 0.1281
  480/10000 [>.............................] - ETA: 1:28:43 - loss: 0.6567 - regression_loss: 0.5285 - classification_loss: 0.1281
  481/10000 [>.............................] - ETA: 1:28:41 - loss: 0.6563 - regression_loss: 0.5283 - classification_loss: 0.1280
  482/10000 [>.............................] - ETA: 1:28:41 - loss: 0.6568 - regression_loss: 0.5285 - classification_loss: 0.1283
  483/10000 [>.............................] - ETA: 1:28:38 - loss: 0.6578 - regression_loss: 0.5294 - classification_loss: 0.1284
  484/10000 [>.............................] - ETA: 1:28:37 - loss: 0.6572 - regression_loss: 0.5289 - classification_loss: 0.1283
  485/10000 [>.............................] - ETA: 1:28:34 - loss: 0.6573 - regression_loss: 0.5290 - classification_loss: 0.1283
  486/10000 [>.............................] - ETA: 1:28:31 - loss: 0.6587 - regression_loss: 0.5301 - classification_loss: 0.1286
  487/10000 [>.............................] - ETA: 1:28:29 - loss: 0.6588 - regression_loss: 0.5302 - classification_loss: 0.1286
  488/10000 [>.............................] - ETA: 1:28:28 - loss: 0.6582 - regression_loss: 0.5298 - classification_loss: 0.1283
  489/10000 [>.............................] - ETA: 1:28:36 - loss: 0.6588 - regression_loss: 0.5305 - classification_loss: 0.1283
  490/10000 [>.............................] - ETA: 1:28:34 - loss: 0.6596 - regression_loss: 0.5313 - classification_loss: 0.1283
  491/10000 [>.............................] - ETA: 1:28:31 - loss: 0.6605 - regression_loss: 0.5320 - classification_loss: 0.1285
  492/10000 [>.............................] - ETA: 1:28:35 - loss: 0.6606 - regression_loss: 0.5321 - classification_loss: 0.1285
  493/10000 [>.............................] - ETA: 1:28:33 - loss: 0.6608 - regression_loss: 0.5321 - classification_loss: 0.1287
  494/10000 [>.............................] - ETA: 1:28:30 - loss: 0.6606 - regression_loss: 0.5320 - classification_loss: 0.1286
  495/10000 [>.............................] - ETA: 1:28:27 - loss: 0.6601 - regression_loss: 0.5317 - classification_loss: 0.1284
  496/10000 [>.............................] - ETA: 1:28:25 - loss: 0.6610 - regression_loss: 0.5323 - classification_loss: 0.1288
  497/10000 [>.............................] - ETA: 1:28:21 - loss: 0.6601 - regression_loss: 0.5314 - classification_loss: 0.1287
  498/10000 [>.............................] - ETA: 1:28:19 - loss: 0.6623 - regression_loss: 0.5331 - classification_loss: 0.1292
  499/10000 [>.............................] - ETA: 1:28:16 - loss: 0.6631 - regression_loss: 0.5337 - classification_loss: 0.1294
  500/10000 [>.............................] - ETA: 1:28:14 - loss: 0.6630 - regression_loss: 0.5336 - classification_loss: 0.1294
  501/10000 [>.............................] - ETA: 1:28:11 - loss: 0.6619 - regression_loss: 0.5327 - classification_loss: 0.1292
  502/10000 [>.............................] - ETA: 1:28:08 - loss: 0.6612 - regression_loss: 0.5321 - classification_loss: 0.1291
  503/10000 [>.............................] - ETA: 1:28:06 - loss: 0.6607 - regression_loss: 0.5318 - classification_loss: 0.1289
  504/10000 [>.............................] - ETA: 1:28:04 - loss: 0.6611 - regression_loss: 0.5322 - classification_loss: 0.1289
  505/10000 [>.............................] - ETA: 1:28:02 - loss: 0.6602 - regression_loss: 0.5315 - classification_loss: 0.1287
  506/10000 [>.............................] - ETA: 1:28:00 - loss: 0.6601 - regression_loss: 0.5315 - classification_loss: 0.1286
  507/10000 [>.............................] - ETA: 1:27:58 - loss: 0.6606 - regression_loss: 0.5320 - classification_loss: 0.1286
  508/10000 [>.............................] - ETA: 1:27:56 - loss: 0.6604 - regression_loss: 0.5319 - classification_loss: 0.1285
  509/10000 [>.............................] - ETA: 1:27:53 - loss: 0.6604 - regression_loss: 0.5316 - classification_loss: 0.1288
  510/10000 [>.............................] - ETA: 1:27:49 - loss: 0.6605 - regression_loss: 0.5318 - classification_loss: 0.1287
  511/10000 [>.............................] - ETA: 1:27:48 - loss: 0.6606 - regression_loss: 0.5320 - classification_loss: 0.1285
  512/10000 [>.............................] - ETA: 1:27:45 - loss: 0.6596 - regression_loss: 0.5312 - classification_loss: 0.1284
  513/10000 [>.............................] - ETA: 1:27:43 - loss: 0.6591 - regression_loss: 0.5309 - classification_loss: 0.1283
  514/10000 [>.............................] - ETA: 1:27:41 - loss: 0.6594 - regression_loss: 0.5309 - classification_loss: 0.1284
  515/10000 [>.............................] - ETA: 1:27:54 - loss: 0.6585 - regression_loss: 0.5303 - classification_loss: 0.1282
  516/10000 [>.............................] - ETA: 1:27:51 - loss: 0.6588 - regression_loss: 0.5305 - classification_loss: 0.1283
  517/10000 [>.............................] - ETA: 1:27:49 - loss: 0.6586 - regression_loss: 0.5305 - classification_loss: 0.1281
  518/10000 [>.............................] - ETA: 1:27:46 - loss: 0.6586 - regression_loss: 0.5304 - classification_loss: 0.1282
  519/10000 [>.............................] - ETA: 1:27:44 - loss: 0.6595 - regression_loss: 0.5311 - classification_loss: 0.1284
  520/10000 [>.............................] - ETA: 1:27:41 - loss: 0.6600 - regression_loss: 0.5315 - classification_loss: 0.1284
  521/10000 [>.............................] - ETA: 1:27:38 - loss: 0.6589 - regression_loss: 0.5307 - classification_loss: 0.1282
  522/10000 [>.............................] - ETA: 1:27:36 - loss: 0.6601 - regression_loss: 0.5316 - classification_loss: 0.1285
  523/10000 [>.............................] - ETA: 1:27:34 - loss: 0.6594 - regression_loss: 0.5311 - classification_loss: 0.1284
  524/10000 [>.............................] - ETA: 1:27:31 - loss: 0.6592 - regression_loss: 0.5309 - classification_loss: 0.1284
  525/10000 [>.............................] - ETA: 1:27:29 - loss: 0.6586 - regression_loss: 0.5303 - classification_loss: 0.1282
  526/10000 [>.............................] - ETA: 1:27:27 - loss: 0.6595 - regression_loss: 0.5313 - classification_loss: 0.1282
  527/10000 [>.............................] - ETA: 1:27:25 - loss: 0.6589 - regression_loss: 0.5306 - classification_loss: 0.1283
  528/10000 [>.............................] - ETA: 1:27:24 - loss: 0.6585 - regression_loss: 0.5301 - classification_loss: 0.1284
  529/10000 [>.............................] - ETA: 1:27:22 - loss: 0.6604 - regression_loss: 0.5318 - classification_loss: 0.1285
  530/10000 [>.............................] - ETA: 1:27:20 - loss: 0.6595 - regression_loss: 0.5311 - classification_loss: 0.1283
  531/10000 [>.............................] - ETA: 1:27:17 - loss: 0.6591 - regression_loss: 0.5309 - classification_loss: 0.1282
  532/10000 [>.............................] - ETA: 1:27:15 - loss: 0.6595 - regression_loss: 0.5310 - classification_loss: 0.1285
  533/10000 [>.............................] - ETA: 1:27:13 - loss: 0.6597 - regression_loss: 0.5312 - classification_loss: 0.1285
  534/10000 [>.............................] - ETA: 1:27:11 - loss: 0.6603 - regression_loss: 0.5317 - classification_loss: 0.1286
  535/10000 [>.............................] - ETA: 1:27:09 - loss: 0.6599 - regression_loss: 0.5314 - classification_loss: 0.1285
  536/10000 [>.............................] - ETA: 1:27:07 - loss: 0.6593 - regression_loss: 0.5310 - classification_loss: 0.1283
  537/10000 [>.............................] - ETA: 1:27:05 - loss: 0.6586 - regression_loss: 0.5305 - classification_loss: 0.1282
  538/10000 [>.............................] - ETA: 1:27:02 - loss: 0.6582 - regression_loss: 0.5301 - classification_loss: 0.1281
  539/10000 [>.............................] - ETA: 1:27:07 - loss: 0.6588 - regression_loss: 0.5308 - classification_loss: 0.1280
  540/10000 [>.............................] - ETA: 1:27:19 - loss: 0.6582 - regression_loss: 0.5304 - classification_loss: 0.1279
  541/10000 [>.............................] - ETA: 1:27:17 - loss: 0.6574 - regression_loss: 0.5297 - classification_loss: 0.1277
  542/10000 [>.............................] - ETA: 1:27:15 - loss: 0.6571 - regression_loss: 0.5296 - classification_loss: 0.1276
  543/10000 [>.............................] - ETA: 1:27:13 - loss: 0.6561 - regression_loss: 0.5288 - classification_loss: 0.1273
  544/10000 [>.............................] - ETA: 1:27:10 - loss: 0.6559 - regression_loss: 0.5283 - classification_loss: 0.1276
  545/10000 [>.............................] - ETA: 1:27:08 - loss: 0.6552 - regression_loss: 0.5277 - classification_loss: 0.1274
  546/10000 [>.............................] - ETA: 1:27:07 - loss: 0.6544 - regression_loss: 0.5271 - classification_loss: 0.1273
  547/10000 [>.............................] - ETA: 1:27:04 - loss: 0.6545 - regression_loss: 0.5271 - classification_loss: 0.1274
  548/10000 [>.............................] - ETA: 1:27:03 - loss: 0.6539 - regression_loss: 0.5266 - classification_loss: 0.1273
  549/10000 [>.............................] - ETA: 1:27:02 - loss: 0.6530 - regression_loss: 0.5259 - classification_loss: 0.1271
  550/10000 [>.............................] - ETA: 1:27:00 - loss: 0.6522 - regression_loss: 0.5252 - classification_loss: 0.1270
  551/10000 [>.............................] - ETA: 1:26:58 - loss: 0.6514 - regression_loss: 0.5246 - classification_loss: 0.1268
  552/10000 [>.............................] - ETA: 1:26:55 - loss: 0.6518 - regression_loss: 0.5248 - classification_loss: 0.1269
  553/10000 [>.............................] - ETA: 1:26:53 - loss: 0.6516 - regression_loss: 0.5248 - classification_loss: 0.1268
  554/10000 [>.............................] - ETA: 1:26:50 - loss: 0.6517 - regression_loss: 0.5246 - classification_loss: 0.1270
  555/10000 [>.............................] - ETA: 1:26:49 - loss: 0.6511 - regression_loss: 0.5241 - classification_loss: 0.1269
  556/10000 [>.............................] - ETA: 1:26:47 - loss: 0.6507 - regression_loss: 0.5238 - classification_loss: 0.1269
  557/10000 [>.............................] - ETA: 1:26:44 - loss: 0.6501 - regression_loss: 0.5234 - classification_loss: 0.1267
  558/10000 [>.............................] - ETA: 1:26:44 - loss: 0.6498 - regression_loss: 0.5232 - classification_loss: 0.1266
  559/10000 [>.............................] - ETA: 1:26:53 - loss: 0.6498 - regression_loss: 0.5233 - classification_loss: 0.1265
  560/10000 [>.............................] - ETA: 1:26:51 - loss: 0.6497 - regression_loss: 0.5233 - classification_loss: 0.1264
  561/10000 [>.............................] - ETA: 1:26:49 - loss: 0.6492 - regression_loss: 0.5228 - classification_loss: 0.1264
  562/10000 [>.............................] - ETA: 1:26:47 - loss: 0.6488 - regression_loss: 0.5225 - classification_loss: 0.1263
  563/10000 [>.............................] - ETA: 1:26:45 - loss: 0.6481 - regression_loss: 0.5219 - classification_loss: 0.1261
  564/10000 [>.............................] - ETA: 1:26:44 - loss: 0.6489 - regression_loss: 0.5227 - classification_loss: 0.1262
  565/10000 [>.............................] - ETA: 1:26:42 - loss: 0.6496 - regression_loss: 0.5231 - classification_loss: 0.1265
  566/10000 [>.............................] - ETA: 1:26:40 - loss: 0.6493 - regression_loss: 0.5229 - classification_loss: 0.1264
  567/10000 [>.............................] - ETA: 1:26:37 - loss: 0.6484 - regression_loss: 0.5222 - classification_loss: 0.1262
  568/10000 [>.............................] - ETA: 1:26:35 - loss: 0.6485 - regression_loss: 0.5223 - classification_loss: 0.1263
  569/10000 [>.............................] - ETA: 1:26:34 - loss: 0.6482 - regression_loss: 0.5219 - classification_loss: 0.1263
  570/10000 [>.............................] - ETA: 1:26:32 - loss: 0.6484 - regression_loss: 0.5222 - classification_loss: 0.1262
  571/10000 [>.............................] - ETA: 1:26:30 - loss: 0.6478 - regression_loss: 0.5217 - classification_loss: 0.1261
  572/10000 [>.............................] - ETA: 1:26:27 - loss: 0.6478 - regression_loss: 0.5217 - classification_loss: 0.1261
  573/10000 [>.............................] - ETA: 1:26:25 - loss: 0.6476 - regression_loss: 0.5216 - classification_loss: 0.1261
  574/10000 [>.............................] - ETA: 1:26:28 - loss: 0.6487 - regression_loss: 0.5226 - classification_loss: 0.1261
  575/10000 [>.............................] - ETA: 1:26:26 - loss: 0.6481 - regression_loss: 0.5221 - classification_loss: 0.1261
  576/10000 [>.............................] - ETA: 1:26:24 - loss: 0.6486 - regression_loss: 0.5226 - classification_loss: 0.1261
  577/10000 [>.............................] - ETA: 1:26:22 - loss: 0.6492 - regression_loss: 0.5230 - classification_loss: 0.1262
  578/10000 [>.............................] - ETA: 1:26:20 - loss: 0.6487 - regression_loss: 0.5226 - classification_loss: 0.1261
  579/10000 [>.............................] - ETA: 1:26:18 - loss: 0.6478 - regression_loss: 0.5218 - classification_loss: 0.1260
  580/10000 [>.............................] - ETA: 1:26:15 - loss: 0.6488 - regression_loss: 0.5227 - classification_loss: 0.1261
  581/10000 [>.............................] - ETA: 1:26:14 - loss: 0.6482 - regression_loss: 0.5221 - classification_loss: 0.1261
  582/10000 [>.............................] - ETA: 1:26:12 - loss: 0.6478 - regression_loss: 0.5219 - classification_loss: 0.1259
  583/10000 [>.............................] - ETA: 1:26:11 - loss: 0.6475 - regression_loss: 0.5217 - classification_loss: 0.1258
  584/10000 [>.............................] - ETA: 1:26:10 - loss: 0.6480 - regression_loss: 0.5221 - classification_loss: 0.1259
  585/10000 [>.............................] - ETA: 1:26:07 - loss: 0.6471 - regression_loss: 0.5214 - classification_loss: 0.1258
  586/10000 [>.............................] - ETA: 1:26:06 - loss: 0.6471 - regression_loss: 0.5214 - classification_loss: 0.1257
  587/10000 [>.............................] - ETA: 1:26:04 - loss: 0.6473 - regression_loss: 0.5216 - classification_loss: 0.1257
  588/10000 [>.............................] - ETA: 1:26:02 - loss: 0.6480 - regression_loss: 0.5223 - classification_loss: 0.1257
  589/10000 [>.............................] - ETA: 1:26:07 - loss: 0.6474 - regression_loss: 0.5219 - classification_loss: 0.1256
  590/10000 [>.............................] - ETA: 1:26:05 - loss: 0.6473 - regression_loss: 0.5218 - classification_loss: 0.1255
  591/10000 [>.............................] - ETA: 1:26:03 - loss: 0.6469 - regression_loss: 0.5214 - classification_loss: 0.1255
  592/10000 [>.............................] - ETA: 1:26:01 - loss: 0.6463 - regression_loss: 0.5209 - classification_loss: 0.1254
  593/10000 [>.............................] - ETA: 1:25:59 - loss: 0.6455 - regression_loss: 0.5203 - classification_loss: 0.1252
  594/10000 [>.............................] - ETA: 1:25:57 - loss: 0.6454 - regression_loss: 0.5202 - classification_loss: 0.1252
  595/10000 [>.............................] - ETA: 1:25:55 - loss: 0.6448 - regression_loss: 0.5198 - classification_loss: 0.1250
  596/10000 [>.............................] - ETA: 1:25:53 - loss: 0.6458 - regression_loss: 0.5206 - classification_loss: 0.1253
  597/10000 [>.............................] - ETA: 1:25:51 - loss: 0.6457 - regression_loss: 0.5204 - classification_loss: 0.1253
  598/10000 [>.............................] - ETA: 1:25:51 - loss: 0.6448 - regression_loss: 0.5197 - classification_loss: 0.1251
  599/10000 [>.............................] - ETA: 1:25:50 - loss: 0.6456 - regression_loss: 0.5204 - classification_loss: 0.1252
  600/10000 [>.............................] - ETA: 1:25:48 - loss: 0.6452 - regression_loss: 0.5200 - classification_loss: 0.1251
  601/10000 [>.............................] - ETA: 1:25:54 - loss: 0.6458 - regression_loss: 0.5206 - classification_loss: 0.1252
  602/10000 [>.............................] - ETA: 1:25:53 - loss: 0.6473 - regression_loss: 0.5216 - classification_loss: 0.1257
  603/10000 [>.............................] - ETA: 1:25:51 - loss: 0.6476 - regression_loss: 0.5216 - classification_loss: 0.1259
  604/10000 [>.............................] - ETA: 1:25:53 - loss: 0.6476 - regression_loss: 0.5217 - classification_loss: 0.1260
  605/10000 [>.............................] - ETA: 1:25:52 - loss: 0.6480 - regression_loss: 0.5221 - classification_loss: 0.1259
  606/10000 [>.............................] - ETA: 1:25:50 - loss: 0.6474 - regression_loss: 0.5217 - classification_loss: 0.1257
  607/10000 [>.............................] - ETA: 1:25:48 - loss: 0.6468 - regression_loss: 0.5211 - classification_loss: 0.1256
  608/10000 [>.............................] - ETA: 1:25:48 - loss: 0.6479 - regression_loss: 0.5222 - classification_loss: 0.1257
  609/10000 [>.............................] - ETA: 1:25:46 - loss: 0.6489 - regression_loss: 0.5231 - classification_loss: 0.1258
  610/10000 [>.............................] - ETA: 1:25:44 - loss: 0.6485 - regression_loss: 0.5228 - classification_loss: 0.1257
  611/10000 [>.............................] - ETA: 1:25:42 - loss: 0.6484 - regression_loss: 0.5227 - classification_loss: 0.1257
  612/10000 [>.............................] - ETA: 1:25:40 - loss: 0.6490 - regression_loss: 0.5232 - classification_loss: 0.1258
  613/10000 [>.............................] - ETA: 1:25:50 - loss: 0.6490 - regression_loss: 0.5230 - classification_loss: 0.1260
  614/10000 [>.............................] - ETA: 1:25:48 - loss: 0.6499 - regression_loss: 0.5238 - classification_loss: 0.1261
  615/10000 [>.............................] - ETA: 1:25:46 - loss: 0.6499 - regression_loss: 0.5238 - classification_loss: 0.1261
  616/10000 [>.............................] - ETA: 1:25:43 - loss: 0.6497 - regression_loss: 0.5236 - classification_loss: 0.1260
  617/10000 [>.............................] - ETA: 1:25:42 - loss: 0.6491 - regression_loss: 0.5232 - classification_loss: 0.1260
  618/10000 [>.............................] - ETA: 1:25:39 - loss: 0.6484 - regression_loss: 0.5226 - classification_loss: 0.1258
  619/10000 [>.............................] - ETA: 1:25:37 - loss: 0.6485 - regression_loss: 0.5227 - classification_loss: 0.1258
  620/10000 [>.............................] - ETA: 1:25:36 - loss: 0.6484 - regression_loss: 0.5227 - classification_loss: 0.1257
  621/10000 [>.............................] - ETA: 1:25:34 - loss: 0.6482 - regression_loss: 0.5225 - classification_loss: 0.1257
  622/10000 [>.............................] - ETA: 1:25:32 - loss: 0.6475 - regression_loss: 0.5220 - classification_loss: 0.1256
  623/10000 [>.............................] - ETA: 1:25:38 - loss: 0.6468 - regression_loss: 0.5214 - classification_loss: 0.1254
  624/10000 [>.............................] - ETA: 1:25:36 - loss: 0.6466 - regression_loss: 0.5212 - classification_loss: 0.1254
  625/10000 [>.............................] - ETA: 1:25:34 - loss: 0.6460 - regression_loss: 0.5208 - classification_loss: 0.1252
  626/10000 [>.............................] - ETA: 1:25:32 - loss: 0.6461 - regression_loss: 0.5208 - classification_loss: 0.1253
  627/10000 [>.............................] - ETA: 1:25:32 - loss: 0.6470 - regression_loss: 0.5215 - classification_loss: 0.1255
  628/10000 [>.............................] - ETA: 1:25:29 - loss: 0.6470 - regression_loss: 0.5213 - classification_loss: 0.1256
  629/10000 [>.............................] - ETA: 1:25:28 - loss: 0.6480 - regression_loss: 0.5223 - classification_loss: 0.1257
  630/10000 [>.............................] - ETA: 1:25:26 - loss: 0.6477 - regression_loss: 0.5221 - classification_loss: 0.1256
  631/10000 [>.............................] - ETA: 1:25:24 - loss: 0.6471 - regression_loss: 0.5216 - classification_loss: 0.1255
  632/10000 [>.............................] - ETA: 1:25:31 - loss: 0.6476 - regression_loss: 0.5220 - classification_loss: 0.1256
  633/10000 [>.............................] - ETA: 1:25:29 - loss: 0.6471 - regression_loss: 0.5216 - classification_loss: 0.1255
  634/10000 [>.............................] - ETA: 1:25:27 - loss: 0.6474 - regression_loss: 0.5220 - classification_loss: 0.1255
  635/10000 [>.............................] - ETA: 1:25:26 - loss: 0.6482 - regression_loss: 0.5227 - classification_loss: 0.1255
  636/10000 [>.............................] - ETA: 1:25:24 - loss: 0.6475 - regression_loss: 0.5221 - classification_loss: 0.1254
  637/10000 [>.............................] - ETA: 1:25:23 - loss: 0.6495 - regression_loss: 0.5237 - classification_loss: 0.1258
  638/10000 [>.............................] - ETA: 1:25:21 - loss: 0.6501 - regression_loss: 0.5242 - classification_loss: 0.1259
  639/10000 [>.............................] - ETA: 1:25:20 - loss: 0.6495 - regression_loss: 0.5237 - classification_loss: 0.1258
  640/10000 [>.............................] - ETA: 1:25:18 - loss: 0.6490 - regression_loss: 0.5233 - classification_loss: 0.1257
  641/10000 [>.............................] - ETA: 1:25:16 - loss: 0.6482 - regression_loss: 0.5226 - classification_loss: 0.1256
  642/10000 [>.............................] - ETA: 1:25:14 - loss: 0.6481 - regression_loss: 0.5225 - classification_loss: 0.1256
  643/10000 [>.............................] - ETA: 1:25:12 - loss: 0.6486 - regression_loss: 0.5229 - classification_loss: 0.1257
  644/10000 [>.............................] - ETA: 1:25:10 - loss: 0.6478 - regression_loss: 0.5223 - classification_loss: 0.1255
  645/10000 [>.............................] - ETA: 1:25:09 - loss: 0.6473 - regression_loss: 0.5218 - classification_loss: 0.1254
  646/10000 [>.............................] - ETA: 1:25:07 - loss: 0.6469 - regression_loss: 0.5215 - classification_loss: 0.1254
  647/10000 [>.............................] - ETA: 1:25:05 - loss: 0.6464 - regression_loss: 0.5211 - classification_loss: 0.1253
  648/10000 [>.............................] - ETA: 1:25:04 - loss: 0.6461 - regression_loss: 0.5209 - classification_loss: 0.1251
  649/10000 [>.............................] - ETA: 1:25:11 - loss: 0.6464 - regression_loss: 0.5212 - classification_loss: 0.1252
  650/10000 [>.............................] - ETA: 1:25:17 - loss: 0.6460 - regression_loss: 0.5206 - classification_loss: 0.1253
  651/10000 [>.............................] - ETA: 1:25:15 - loss: 0.6459 - regression_loss: 0.5206 - classification_loss: 0.1253
  652/10000 [>.............................] - ETA: 1:25:14 - loss: 0.6459 - regression_loss: 0.5207 - classification_loss: 0.1252
  653/10000 [>.............................] - ETA: 1:25:12 - loss: 0.6476 - regression_loss: 0.5222 - classification_loss: 0.1254
  654/10000 [>.............................] - ETA: 1:25:10 - loss: 0.6470 - regression_loss: 0.5217 - classification_loss: 0.1253
  655/10000 [>.............................] - ETA: 1:25:08 - loss: 0.6487 - regression_loss: 0.5231 - classification_loss: 0.1256
  656/10000 [>.............................] - ETA: 1:25:14 - loss: 0.6501 - regression_loss: 0.5240 - classification_loss: 0.1261
  657/10000 [>.............................] - ETA: 1:25:12 - loss: 0.6501 - regression_loss: 0.5240 - classification_loss: 0.1261
  658/10000 [>.............................] - ETA: 1:25:11 - loss: 0.6501 - regression_loss: 0.5240 - classification_loss: 0.1261
  659/10000 [>.............................] - ETA: 1:25:11 - loss: 0.6503 - regression_loss: 0.5240 - classification_loss: 0.1263
  660/10000 [>.............................] - ETA: 1:25:08 - loss: 0.6498 - regression_loss: 0.5236 - classification_loss: 0.1263
  661/10000 [>.............................] - ETA: 1:25:06 - loss: 0.6505 - regression_loss: 0.5243 - classification_loss: 0.1263
  662/10000 [>.............................] - ETA: 1:25:04 - loss: 0.6502 - regression_loss: 0.5241 - classification_loss: 0.1261
  663/10000 [>.............................] - ETA: 1:25:03 - loss: 0.6504 - regression_loss: 0.5244 - classification_loss: 0.1261
  664/10000 [>.............................] - ETA: 1:25:01 - loss: 0.6500 - regression_loss: 0.5240 - classification_loss: 0.1260
  665/10000 [>.............................] - ETA: 1:24:59 - loss: 0.6500 - regression_loss: 0.5241 - classification_loss: 0.1260
  666/10000 [>.............................] - ETA: 1:24:57 - loss: 0.6496 - regression_loss: 0.5237 - classification_loss: 0.1259
  667/10000 [=>............................] - ETA: 1:24:55 - loss: 0.6504 - regression_loss: 0.5243 - classification_loss: 0.1260
  668/10000 [=>............................] - ETA: 1:24:53 - loss: 0.6501 - regression_loss: 0.5241 - classification_loss: 0.1260
  669/10000 [=>............................] - ETA: 1:24:52 - loss: 0.6491 - regression_loss: 0.5233 - classification_loss: 0.1259
  670/10000 [=>............................] - ETA: 1:24:50 - loss: 0.6486 - regression_loss: 0.5228 - classification_loss: 0.1259
  671/10000 [=>............................] - ETA: 1:24:48 - loss: 0.6479 - regression_loss: 0.5222 - classification_loss: 0.1257
  672/10000 [=>............................] - ETA: 1:24:46 - loss: 0.6476 - regression_loss: 0.5219 - classification_loss: 0.1256
  673/10000 [=>............................] - ETA: 1:24:43 - loss: 0.6475 - regression_loss: 0.5219 - classification_loss: 0.1255
  674/10000 [=>............................] - ETA: 1:24:43 - loss: 0.6474 - regression_loss: 0.5219 - classification_loss: 0.1256
  675/10000 [=>............................] - ETA: 1:24:41 - loss: 0.6484 - regression_loss: 0.5227 - classification_loss: 0.1257
  676/10000 [=>............................] - ETA: 1:24:40 - loss: 0.6487 - regression_loss: 0.5230 - classification_loss: 0.1257
  677/10000 [=>............................] - ETA: 1:24:38 - loss: 0.6503 - regression_loss: 0.5243 - classification_loss: 0.1260
  678/10000 [=>............................] - ETA: 1:24:37 - loss: 0.6499 - regression_loss: 0.5240 - classification_loss: 0.1259
  679/10000 [=>............................] - ETA: 1:24:35 - loss: 0.6495 - regression_loss: 0.5237 - classification_loss: 0.1258
  680/10000 [=>............................] - ETA: 1:24:33 - loss: 0.6489 - regression_loss: 0.5232 - classification_loss: 0.1257
  681/10000 [=>............................] - ETA: 1:24:31 - loss: 0.6482 - regression_loss: 0.5226 - classification_loss: 0.1256
  682/10000 [=>............................] - ETA: 1:24:29 - loss: 0.6487 - regression_loss: 0.5231 - classification_loss: 0.1256
  683/10000 [=>............................] - ETA: 1:24:28 - loss: 0.6480 - regression_loss: 0.5225 - classification_loss: 0.1255
  684/10000 [=>............................] - ETA: 1:24:27 - loss: 0.6476 - regression_loss: 0.5221 - classification_loss: 0.1254
  685/10000 [=>............................] - ETA: 1:24:25 - loss: 0.6471 - regression_loss: 0.5216 - classification_loss: 0.1256
  686/10000 [=>............................] - ETA: 1:24:23 - loss: 0.6479 - regression_loss: 0.5220 - classification_loss: 0.1259
  687/10000 [=>............................] - ETA: 1:24:21 - loss: 0.6481 - regression_loss: 0.5220 - classification_loss: 0.1262
  688/10000 [=>............................] - ETA: 1:24:20 - loss: 0.6485 - regression_loss: 0.5221 - classification_loss: 0.1264
  689/10000 [=>............................] - ETA: 1:24:18 - loss: 0.6483 - regression_loss: 0.5220 - classification_loss: 0.1263
  690/10000 [=>............................] - ETA: 1:24:17 - loss: 0.6478 - regression_loss: 0.5217 - classification_loss: 0.1262
  691/10000 [=>............................] - ETA: 1:24:15 - loss: 0.6489 - regression_loss: 0.5225 - classification_loss: 0.1264
  692/10000 [=>............................] - ETA: 1:24:14 - loss: 0.6482 - regression_loss: 0.5220 - classification_loss: 0.1262
  693/10000 [=>............................] - ETA: 1:24:12 - loss: 0.6483 - regression_loss: 0.5222 - classification_loss: 0.1262
  694/10000 [=>............................] - ETA: 1:24:11 - loss: 0.6483 - regression_loss: 0.5223 - classification_loss: 0.1261
  695/10000 [=>............................] - ETA: 1:24:09 - loss: 0.6478 - regression_loss: 0.5218 - classification_loss: 0.1259
  696/10000 [=>............................] - ETA: 1:24:08 - loss: 0.6471 - regression_loss: 0.5213 - classification_loss: 0.1258
  697/10000 [=>............................] - ETA: 1:24:06 - loss: 0.6468 - regression_loss: 0.5211 - classification_loss: 0.1258
  698/10000 [=>............................] - ETA: 1:24:04 - loss: 0.6459 - regression_loss: 0.5203 - classification_loss: 0.1256
  699/10000 [=>............................] - ETA: 1:24:03 - loss: 0.6455 - regression_loss: 0.5200 - classification_loss: 0.1255
  700/10000 [=>............................] - ETA: 1:24:01 - loss: 0.6447 - regression_loss: 0.5194 - classification_loss: 0.1254
  701/10000 [=>............................] - ETA: 1:23:59 - loss: 0.6454 - regression_loss: 0.5200 - classification_loss: 0.1254
  702/10000 [=>............................] - ETA: 1:23:58 - loss: 0.6449 - regression_loss: 0.5196 - classification_loss: 0.1253
  703/10000 [=>............................] - ETA: 1:23:57 - loss: 0.6447 - regression_loss: 0.5194 - classification_loss: 0.1253
  704/10000 [=>............................] - ETA: 1:24:01 - loss: 0.6450 - regression_loss: 0.5194 - classification_loss: 0.1256
  705/10000 [=>............................] - ETA: 1:23:59 - loss: 0.6456 - regression_loss: 0.5201 - classification_loss: 0.1255
  706/10000 [=>............................] - ETA: 1:23:57 - loss: 0.6455 - regression_loss: 0.5200 - classification_loss: 0.1255
  707/10000 [=>............................] - ETA: 1:23:56 - loss: 0.6457 - regression_loss: 0.5202 - classification_loss: 0.1255
  708/10000 [=>............................] - ETA: 1:23:54 - loss: 0.6463 - regression_loss: 0.5207 - classification_loss: 0.1256
  709/10000 [=>............................] - ETA: 1:23:52 - loss: 0.6454 - regression_loss: 0.5200 - classification_loss: 0.1254
  710/10000 [=>............................] - ETA: 1:23:51 - loss: 0.6453 - regression_loss: 0.5196 - classification_loss: 0.1257
  711/10000 [=>............................] - ETA: 1:23:50 - loss: 0.6451 - regression_loss: 0.5194 - classification_loss: 0.1257
  712/10000 [=>............................] - ETA: 1:23:48 - loss: 0.6444 - regression_loss: 0.5188 - classification_loss: 0.1257
  713/10000 [=>............................] - ETA: 1:23:47 - loss: 0.6445 - regression_loss: 0.5189 - classification_loss: 0.1256
  714/10000 [=>............................] - ETA: 1:23:45 - loss: 0.6440 - regression_loss: 0.5184 - classification_loss: 0.1256
  715/10000 [=>............................] - ETA: 1:23:45 - loss: 0.6439 - regression_loss: 0.5185 - classification_loss: 0.1255
  716/10000 [=>............................] - ETA: 1:23:44 - loss: 0.6435 - regression_loss: 0.5180 - classification_loss: 0.1255
  717/10000 [=>............................] - ETA: 1:23:42 - loss: 0.6430 - regression_loss: 0.5177 - classification_loss: 0.1253
  718/10000 [=>............................] - ETA: 1:23:41 - loss: 0.6431 - regression_loss: 0.5178 - classification_loss: 0.1253
  719/10000 [=>............................] - ETA: 1:23:39 - loss: 0.6426 - regression_loss: 0.5174 - classification_loss: 0.1252
  720/10000 [=>............................] - ETA: 1:23:37 - loss: 0.6435 - regression_loss: 0.5182 - classification_loss: 0.1253
  721/10000 [=>............................] - ETA: 1:23:36 - loss: 0.6429 - regression_loss: 0.5177 - classification_loss: 0.1251
  722/10000 [=>............................] - ETA: 1:23:34 - loss: 0.6425 - regression_loss: 0.5174 - classification_loss: 0.1252
  723/10000 [=>............................] - ETA: 1:23:33 - loss: 0.6417 - regression_loss: 0.5167 - classification_loss: 0.1250
  724/10000 [=>............................] - ETA: 1:23:32 - loss: 0.6417 - regression_loss: 0.5166 - classification_loss: 0.1251
  725/10000 [=>............................] - ETA: 1:23:30 - loss: 0.6417 - regression_loss: 0.5166 - classification_loss: 0.1251
  726/10000 [=>............................] - ETA: 1:23:28 - loss: 0.6415 - regression_loss: 0.5163 - classification_loss: 0.1252
  727/10000 [=>............................] - ETA: 1:23:27 - loss: 0.6409 - regression_loss: 0.5159 - classification_loss: 0.1251
  728/10000 [=>............................] - ETA: 1:23:25 - loss: 0.6407 - regression_loss: 0.5156 - classification_loss: 0.1251
  729/10000 [=>............................] - ETA: 1:23:24 - loss: 0.6409 - regression_loss: 0.5159 - classification_loss: 0.1251
  730/10000 [=>............................] - ETA: 1:23:22 - loss: 0.6411 - regression_loss: 0.5161 - classification_loss: 0.1251
  731/10000 [=>............................] - ETA: 1:23:21 - loss: 0.6412 - regression_loss: 0.5161 - classification_loss: 0.1251
  732/10000 [=>............................] - ETA: 1:23:19 - loss: 0.6406 - regression_loss: 0.5157 - classification_loss: 0.1249
  733/10000 [=>............................] - ETA: 1:23:17 - loss: 0.6399 - regression_loss: 0.5151 - classification_loss: 0.1248
  734/10000 [=>............................] - ETA: 1:23:16 - loss: 0.6398 - regression_loss: 0.5149 - classification_loss: 0.1249
  735/10000 [=>............................] - ETA: 1:23:14 - loss: 0.6396 - regression_loss: 0.5147 - classification_loss: 0.1249
  736/10000 [=>............................] - ETA: 1:23:13 - loss: 0.6411 - regression_loss: 0.5157 - classification_loss: 0.1254
  737/10000 [=>............................] - ETA: 1:23:11 - loss: 0.6420 - regression_loss: 0.5161 - classification_loss: 0.1260
  738/10000 [=>............................] - ETA: 1:23:09 - loss: 0.6428 - regression_loss: 0.5167 - classification_loss: 0.1262
  739/10000 [=>............................] - ETA: 1:23:08 - loss: 0.6425 - regression_loss: 0.5164 - classification_loss: 0.1261
  740/10000 [=>............................] - ETA: 1:23:06 - loss: 0.6424 - regression_loss: 0.5164 - classification_loss: 0.1260
  741/10000 [=>............................] - ETA: 1:23:05 - loss: 0.6427 - regression_loss: 0.5166 - classification_loss: 0.1262
  742/10000 [=>............................] - ETA: 1:23:03 - loss: 0.6425 - regression_loss: 0.5164 - classification_loss: 0.1261
  743/10000 [=>............................] - ETA: 1:23:02 - loss: 0.6419 - regression_loss: 0.5159 - classification_loss: 0.1260
  744/10000 [=>............................] - ETA: 1:23:00 - loss: 0.6423 - regression_loss: 0.5161 - classification_loss: 0.1262
  745/10000 [=>............................] - ETA: 1:22:59 - loss: 0.6416 - regression_loss: 0.5156 - classification_loss: 0.1260
  746/10000 [=>............................] - ETA: 1:22:57 - loss: 0.6414 - regression_loss: 0.5154 - classification_loss: 0.1260
  747/10000 [=>............................] - ETA: 1:22:55 - loss: 0.6420 - regression_loss: 0.5157 - classification_loss: 0.1262
  748/10000 [=>............................] - ETA: 1:22:54 - loss: 0.6418 - regression_loss: 0.5155 - classification_loss: 0.1263
  749/10000 [=>............................] - ETA: 1:22:53 - loss: 0.6425 - regression_loss: 0.5160 - classification_loss: 0.1264
  750/10000 [=>............................] - ETA: 1:22:50 - loss: 0.6426 - regression_loss: 0.5162 - classification_loss: 0.1264
  751/10000 [=>............................] - ETA: 1:22:49 - loss: 0.6433 - regression_loss: 0.5169 - classification_loss: 0.1264
  752/10000 [=>............................] - ETA: 1:22:47 - loss: 0.6434 - regression_loss: 0.5170 - classification_loss: 0.1264
  753/10000 [=>............................] - ETA: 1:22:46 - loss: 0.6431 - regression_loss: 0.5167 - classification_loss: 0.1264
  754/10000 [=>............................] - ETA: 1:22:43 - loss: 0.6435 - regression_loss: 0.5169 - classification_loss: 0.1266
  755/10000 [=>............................] - ETA: 1:22:42 - loss: 0.6430 - regression_loss: 0.5165 - classification_loss: 0.1265
  756/10000 [=>............................] - ETA: 1:22:40 - loss: 0.6429 - regression_loss: 0.5165 - classification_loss: 0.1264
  757/10000 [=>............................] - ETA: 1:22:38 - loss: 0.6425 - regression_loss: 0.5161 - classification_loss: 0.1264
  758/10000 [=>............................] - ETA: 1:22:37 - loss: 0.6428 - regression_loss: 0.5164 - classification_loss: 0.1263
  759/10000 [=>............................] - ETA: 1:22:35 - loss: 0.6437 - regression_loss: 0.5174 - classification_loss: 0.1263
  760/10000 [=>............................] - ETA: 1:22:34 - loss: 0.6435 - regression_loss: 0.5173 - classification_loss: 0.1263
  761/10000 [=>............................] - ETA: 1:22:43 - loss: 0.6429 - regression_loss: 0.5167 - classification_loss: 0.1261
  762/10000 [=>............................] - ETA: 1:22:47 - loss: 0.6428 - regression_loss: 0.5167 - classification_loss: 0.1261
  763/10000 [=>............................] - ETA: 1:22:45 - loss: 0.6434 - regression_loss: 0.5173 - classification_loss: 0.1262
  764/10000 [=>............................] - ETA: 1:22:51 - loss: 0.6429 - regression_loss: 0.5169 - classification_loss: 0.1260
  765/10000 [=>............................] - ETA: 1:22:49 - loss: 0.6430 - regression_loss: 0.5171 - classification_loss: 0.1260
  766/10000 [=>............................] - ETA: 1:22:48 - loss: 0.6436 - regression_loss: 0.5176 - classification_loss: 0.1260
  767/10000 [=>............................] - ETA: 1:22:46 - loss: 0.6440 - regression_loss: 0.5178 - classification_loss: 0.1261
  768/10000 [=>............................] - ETA: 1:22:45 - loss: 0.6447 - regression_loss: 0.5184 - classification_loss: 0.1263
  769/10000 [=>............................] - ETA: 1:22:43 - loss: 0.6443 - regression_loss: 0.5181 - classification_loss: 0.1262
  770/10000 [=>............................] - ETA: 1:22:41 - loss: 0.6437 - regression_loss: 0.5176 - classification_loss: 0.1260
  771/10000 [=>............................] - ETA: 1:22:40 - loss: 0.6435 - regression_loss: 0.5175 - classification_loss: 0.1260
  772/10000 [=>............................] - ETA: 1:22:38 - loss: 0.6444 - regression_loss: 0.5183 - classification_loss: 0.1261
  773/10000 [=>............................] - ETA: 1:22:36 - loss: 0.6440 - regression_loss: 0.5180 - classification_loss: 0.1260
  774/10000 [=>............................] - ETA: 1:22:35 - loss: 0.6449 - regression_loss: 0.5188 - classification_loss: 0.1261
  775/10000 [=>............................] - ETA: 1:22:34 - loss: 0.6456 - regression_loss: 0.5196 - classification_loss: 0.1260
  776/10000 [=>............................] - ETA: 1:22:33 - loss: 0.6451 - regression_loss: 0.5193 - classification_loss: 0.1259
  777/10000 [=>............................] - ETA: 1:22:32 - loss: 0.6449 - regression_loss: 0.5190 - classification_loss: 0.1259
  778/10000 [=>............................] - ETA: 1:22:30 - loss: 0.6444 - regression_loss: 0.5186 - classification_loss: 0.1258
  779/10000 [=>............................] - ETA: 1:22:28 - loss: 0.6445 - regression_loss: 0.5185 - classification_loss: 0.1259
  780/10000 [=>............................] - ETA: 1:22:27 - loss: 0.6439 - regression_loss: 0.5180 - classification_loss: 0.1259
  781/10000 [=>............................] - ETA: 1:22:25 - loss: 0.6435 - regression_loss: 0.5177 - classification_loss: 0.1258
  782/10000 [=>............................] - ETA: 1:22:24 - loss: 0.6432 - regression_loss: 0.5174 - classification_loss: 0.1258
  783/10000 [=>............................] - ETA: 1:22:22 - loss: 0.6439 - regression_loss: 0.5180 - classification_loss: 0.1259
  784/10000 [=>............................] - ETA: 1:22:25 - loss: 0.6450 - regression_loss: 0.5189 - classification_loss: 0.1261
  785/10000 [=>............................] - ETA: 1:22:24 - loss: 0.6452 - regression_loss: 0.5190 - classification_loss: 0.1261
  786/10000 [=>............................] - ETA: 1:22:22 - loss: 0.6451 - regression_loss: 0.5189 - classification_loss: 0.1261
  787/10000 [=>............................] - ETA: 1:22:21 - loss: 0.6460 - regression_loss: 0.5196 - classification_loss: 0.1263
  788/10000 [=>............................] - ETA: 1:22:20 - loss: 0.6463 - regression_loss: 0.5200 - classification_loss: 0.1263
  789/10000 [=>............................] - ETA: 1:22:19 - loss: 0.6469 - regression_loss: 0.5205 - classification_loss: 0.1264
  790/10000 [=>............................] - ETA: 1:22:18 - loss: 0.6465 - regression_loss: 0.5202 - classification_loss: 0.1263
  791/10000 [=>............................] - ETA: 1:22:16 - loss: 0.6466 - regression_loss: 0.5204 - classification_loss: 0.1262
  792/10000 [=>............................] - ETA: 1:22:15 - loss: 0.6471 - regression_loss: 0.5209 - classification_loss: 0.1261
  793/10000 [=>............................] - ETA: 1:22:13 - loss: 0.6464 - regression_loss: 0.5204 - classification_loss: 0.1260
  794/10000 [=>............................] - ETA: 1:22:13 - loss: 0.6462 - regression_loss: 0.5202 - classification_loss: 0.1260
  795/10000 [=>............................] - ETA: 1:22:16 - loss: 0.6464 - regression_loss: 0.5205 - classification_loss: 0.1260
  796/10000 [=>............................] - ETA: 1:22:19 - loss: 0.6461 - regression_loss: 0.5202 - classification_loss: 0.1259
  797/10000 [=>............................] - ETA: 1:22:17 - loss: 0.6470 - regression_loss: 0.5211 - classification_loss: 0.1259
  798/10000 [=>............................] - ETA: 1:22:16 - loss: 0.6471 - regression_loss: 0.5211 - classification_loss: 0.1260
  799/10000 [=>............................] - ETA: 1:22:14 - loss: 0.6469 - regression_loss: 0.5208 - classification_loss: 0.1261
  800/10000 [=>............................] - ETA: 1:22:13 - loss: 0.6475 - regression_loss: 0.5213 - classification_loss: 0.1261
  801/10000 [=>............................] - ETA: 1:22:11 - loss: 0.6480 - regression_loss: 0.5218 - classification_loss: 0.1262
  802/10000 [=>............................] - ETA: 1:22:10 - loss: 0.6482 - regression_loss: 0.5219 - classification_loss: 0.1262
  803/10000 [=>............................] - ETA: 1:22:08 - loss: 0.6480 - regression_loss: 0.5218 - classification_loss: 0.1262
  804/10000 [=>............................] - ETA: 1:22:07 - loss: 0.6481 - regression_loss: 0.5220 - classification_loss: 0.1262
  805/10000 [=>............................] - ETA: 1:22:06 - loss: 0.6479 - regression_loss: 0.5217 - classification_loss: 0.1261
  806/10000 [=>............................] - ETA: 1:22:04 - loss: 0.6475 - regression_loss: 0.5214 - classification_loss: 0.1260
  807/10000 [=>............................] - ETA: 1:22:03 - loss: 0.6477 - regression_loss: 0.5215 - classification_loss: 0.1262
  808/10000 [=>............................] - ETA: 1:22:02 - loss: 0.6486 - regression_loss: 0.5224 - classification_loss: 0.1262
  809/10000 [=>............................] - ETA: 1:22:00 - loss: 0.6484 - regression_loss: 0.5221 - classification_loss: 0.1262
  810/10000 [=>............................] - ETA: 1:21:59 - loss: 0.6478 - regression_loss: 0.5218 - classification_loss: 0.1261
  811/10000 [=>............................] - ETA: 1:21:58 - loss: 0.6474 - regression_loss: 0.5214 - classification_loss: 0.1260
  812/10000 [=>............................] - ETA: 1:21:57 - loss: 0.6466 - regression_loss: 0.5208 - classification_loss: 0.1258
  813/10000 [=>............................] - ETA: 1:21:56 - loss: 0.6467 - regression_loss: 0.5210 - classification_loss: 0.1257
  814/10000 [=>............................] - ETA: 1:21:54 - loss: 0.6467 - regression_loss: 0.5210 - classification_loss: 0.1257
  815/10000 [=>............................] - ETA: 1:21:53 - loss: 0.6462 - regression_loss: 0.5205 - classification_loss: 0.1256
  816/10000 [=>............................] - ETA: 1:21:52 - loss: 0.6473 - regression_loss: 0.5215 - classification_loss: 0.1258
  817/10000 [=>............................] - ETA: 1:21:50 - loss: 0.6468 - regression_loss: 0.5212 - classification_loss: 0.1257
  818/10000 [=>............................] - ETA: 1:21:49 - loss: 0.6469 - regression_loss: 0.5212 - classification_loss: 0.1257
  819/10000 [=>............................] - ETA: 1:21:47 - loss: 0.6467 - regression_loss: 0.5210 - classification_loss: 0.1257
  820/10000 [=>............................] - ETA: 1:21:47 - loss: 0.6464 - regression_loss: 0.5207 - classification_loss: 0.1257
  821/10000 [=>............................] - ETA: 1:21:46 - loss: 0.6463 - regression_loss: 0.5206 - classification_loss: 0.1257
  822/10000 [=>............................] - ETA: 1:21:52 - loss: 0.6465 - regression_loss: 0.5207 - classification_loss: 0.1257
  823/10000 [=>............................] - ETA: 1:21:51 - loss: 0.6463 - regression_loss: 0.5206 - classification_loss: 0.1256
  824/10000 [=>............................] - ETA: 1:21:50 - loss: 0.6463 - regression_loss: 0.5207 - classification_loss: 0.1256
  825/10000 [=>............................] - ETA: 1:21:49 - loss: 0.6479 - regression_loss: 0.5221 - classification_loss: 0.1258
  826/10000 [=>............................] - ETA: 1:21:47 - loss: 0.6477 - regression_loss: 0.5220 - classification_loss: 0.1257
  827/10000 [=>............................] - ETA: 1:21:46 - loss: 0.6472 - regression_loss: 0.5217 - classification_loss: 0.1256
  828/10000 [=>............................] - ETA: 1:21:45 - loss: 0.6474 - regression_loss: 0.5219 - classification_loss: 0.1255
  829/10000 [=>............................] - ETA: 1:21:43 - loss: 0.6478 - regression_loss: 0.5223 - classification_loss: 0.1255
  830/10000 [=>............................] - ETA: 1:21:42 - loss: 0.6480 - regression_loss: 0.5223 - classification_loss: 0.1257
  831/10000 [=>............................] - ETA: 1:21:41 - loss: 0.6483 - regression_loss: 0.5225 - classification_loss: 0.1258
  832/10000 [=>............................] - ETA: 1:21:39 - loss: 0.6487 - regression_loss: 0.5229 - classification_loss: 0.1259
  833/10000 [=>............................] - ETA: 1:21:38 - loss: 0.6500 - regression_loss: 0.5240 - classification_loss: 0.1259
  834/10000 [=>............................] - ETA: 1:21:37 - loss: 0.6499 - regression_loss: 0.5241 - classification_loss: 0.1259
  835/10000 [=>............................] - ETA: 1:21:38 - loss: 0.6493 - regression_loss: 0.5236 - classification_loss: 0.1258
  836/10000 [=>............................] - ETA: 1:21:37 - loss: 0.6493 - regression_loss: 0.5236 - classification_loss: 0.1257
  837/10000 [=>............................] - ETA: 1:21:35 - loss: 0.6495 - regression_loss: 0.5238 - classification_loss: 0.1256
  838/10000 [=>............................] - ETA: 1:21:34 - loss: 0.6491 - regression_loss: 0.5235 - classification_loss: 0.1255
  839/10000 [=>............................] - ETA: 1:21:32 - loss: 0.6487 - regression_loss: 0.5232 - classification_loss: 0.1255
  840/10000 [=>............................] - ETA: 1:21:36 - loss: 0.6492 - regression_loss: 0.5236 - classification_loss: 0.1256
  841/10000 [=>............................] - ETA: 1:21:34 - loss: 0.6494 - regression_loss: 0.5238 - classification_loss: 0.1256
  842/10000 [=>............................] - ETA: 1:21:32 - loss: 0.6497 - regression_loss: 0.5242 - classification_loss: 0.1256
  843/10000 [=>............................] - ETA: 1:21:31 - loss: 0.6494 - regression_loss: 0.5239 - classification_loss: 0.1255
  844/10000 [=>............................] - ETA: 1:21:29 - loss: 0.6491 - regression_loss: 0.5236 - classification_loss: 0.1255
  845/10000 [=>............................] - ETA: 1:21:28 - loss: 0.6498 - regression_loss: 0.5242 - classification_loss: 0.1256
  846/10000 [=>............................] - ETA: 1:21:26 - loss: 0.6495 - regression_loss: 0.5240 - classification_loss: 0.1255
  847/10000 [=>............................] - ETA: 1:21:34 - loss: 0.6494 - regression_loss: 0.5239 - classification_loss: 0.1255
  848/10000 [=>............................] - ETA: 1:21:32 - loss: 0.6491 - regression_loss: 0.5237 - classification_loss: 0.1254
  849/10000 [=>............................] - ETA: 1:21:31 - loss: 0.6490 - regression_loss: 0.5236 - classification_loss: 0.1255
  850/10000 [=>............................] - ETA: 1:21:29 - loss: 0.6486 - regression_loss: 0.5232 - classification_loss: 0.1254
  851/10000 [=>............................] - ETA: 1:21:28 - loss: 0.6488 - regression_loss: 0.5234 - classification_loss: 0.1254
  852/10000 [=>............................] - ETA: 1:21:27 - loss: 0.6488 - regression_loss: 0.5234 - classification_loss: 0.1254
  853/10000 [=>............................] - ETA: 1:21:25 - loss: 0.6494 - regression_loss: 0.5239 - classification_loss: 0.1254
  854/10000 [=>............................] - ETA: 1:21:24 - loss: 0.6491 - regression_loss: 0.5238 - classification_loss: 0.1254
  855/10000 [=>............................] - ETA: 1:21:22 - loss: 0.6487 - regression_loss: 0.5234 - classification_loss: 0.1253
  856/10000 [=>............................] - ETA: 1:21:21 - loss: 0.6482 - regression_loss: 0.5231 - classification_loss: 0.1252
  857/10000 [=>............................] - ETA: 1:21:20 - loss: 0.6485 - regression_loss: 0.5233 - classification_loss: 0.1252
  858/10000 [=>............................] - ETA: 1:21:18 - loss: 0.6479 - regression_loss: 0.5229 - classification_loss: 0.1251
  859/10000 [=>............................] - ETA: 1:21:17 - loss: 0.6475 - regression_loss: 0.5224 - classification_loss: 0.1251
  860/10000 [=>............................] - ETA: 1:21:15 - loss: 0.6482 - regression_loss: 0.5230 - classification_loss: 0.1252
  861/10000 [=>............................] - ETA: 1:21:14 - loss: 0.6487 - regression_loss: 0.5234 - classification_loss: 0.1253
  862/10000 [=>............................] - ETA: 1:21:21 - loss: 0.6497 - regression_loss: 0.5241 - classification_loss: 0.1255
  863/10000 [=>............................] - ETA: 1:21:20 - loss: 0.6497 - regression_loss: 0.5242 - classification_loss: 0.1256
  864/10000 [=>............................] - ETA: 1:21:19 - loss: 0.6493 - regression_loss: 0.5237 - classification_loss: 0.1256
  865/10000 [=>............................] - ETA: 1:21:18 - loss: 0.6496 - regression_loss: 0.5240 - classification_loss: 0.1256
  866/10000 [=>............................] - ETA: 1:21:17 - loss: 0.6493 - regression_loss: 0.5238 - classification_loss: 0.1255
  867/10000 [=>............................] - ETA: 1:21:16 - loss: 0.6487 - regression_loss: 0.5234 - classification_loss: 0.1254
  868/10000 [=>............................] - ETA: 1:21:14 - loss: 0.6488 - regression_loss: 0.5235 - classification_loss: 0.1253
  869/10000 [=>............................] - ETA: 1:21:13 - loss: 0.6486 - regression_loss: 0.5233 - classification_loss: 0.1254
  870/10000 [=>............................] - ETA: 1:21:12 - loss: 0.6492 - regression_loss: 0.5238 - classification_loss: 0.1255
  871/10000 [=>............................] - ETA: 1:21:12 - loss: 0.6489 - regression_loss: 0.5235 - classification_loss: 0.1254
  872/10000 [=>............................] - ETA: 1:21:11 - loss: 0.6487 - regression_loss: 0.5233 - classification_loss: 0.1254
  873/10000 [=>............................] - ETA: 1:21:10 - loss: 0.6487 - regression_loss: 0.5233 - classification_loss: 0.1254
  874/10000 [=>............................] - ETA: 1:21:08 - loss: 0.6492 - regression_loss: 0.5238 - classification_loss: 0.1255
  875/10000 [=>............................] - ETA: 1:21:07 - loss: 0.6500 - regression_loss: 0.5244 - classification_loss: 0.1255
  876/10000 [=>............................] - ETA: 1:21:05 - loss: 0.6495 - regression_loss: 0.5241 - classification_loss: 0.1254
  877/10000 [=>............................] - ETA: 1:21:04 - loss: 0.6492 - regression_loss: 0.5238 - classification_loss: 0.1255
  878/10000 [=>............................] - ETA: 1:21:11 - loss: 0.6504 - regression_loss: 0.5248 - classification_loss: 0.1256
  879/10000 [=>............................] - ETA: 1:21:09 - loss: 0.6501 - regression_loss: 0.5246 - classification_loss: 0.1255
  880/10000 [=>............................] - ETA: 1:21:08 - loss: 0.6498 - regression_loss: 0.5244 - classification_loss: 0.1255
  881/10000 [=>............................] - ETA: 1:21:14 - loss: 0.6499 - regression_loss: 0.5245 - classification_loss: 0.1254
  882/10000 [=>............................] - ETA: 1:21:13 - loss: 0.6503 - regression_loss: 0.5249 - classification_loss: 0.1254
  883/10000 [=>............................] - ETA: 1:21:11 - loss: 0.6501 - regression_loss: 0.5247 - classification_loss: 0.1254
  884/10000 [=>............................] - ETA: 1:21:10 - loss: 0.6510 - regression_loss: 0.5256 - classification_loss: 0.1254
  885/10000 [=>............................] - ETA: 1:21:08 - loss: 0.6511 - regression_loss: 0.5257 - classification_loss: 0.1254
  886/10000 [=>............................] - ETA: 1:21:07 - loss: 0.6519 - regression_loss: 0.5265 - classification_loss: 0.1254
  887/10000 [=>............................] - ETA: 1:21:06 - loss: 0.6516 - regression_loss: 0.5262 - classification_loss: 0.1253
  888/10000 [=>............................] - ETA: 1:21:05 - loss: 0.6519 - regression_loss: 0.5266 - classification_loss: 0.1253
  889/10000 [=>............................] - ETA: 1:21:03 - loss: 0.6518 - regression_loss: 0.5262 - classification_loss: 0.1255
  890/10000 [=>............................] - ETA: 1:21:02 - loss: 0.6520 - regression_loss: 0.5264 - classification_loss: 0.1256
  891/10000 [=>............................] - ETA: 1:21:01 - loss: 0.6522 - regression_loss: 0.5266 - classification_loss: 0.1256
  892/10000 [=>............................] - ETA: 1:21:00 - loss: 0.6523 - regression_loss: 0.5267 - classification_loss: 0.1256
  893/10000 [=>............................] - ETA: 1:20:59 - loss: 0.6520 - regression_loss: 0.5263 - classification_loss: 0.1257
  894/10000 [=>............................] - ETA: 1:20:57 - loss: 0.6516 - regression_loss: 0.5260 - classification_loss: 0.1256
  895/10000 [=>............................] - ETA: 1:20:56 - loss: 0.6524 - regression_loss: 0.5268 - classification_loss: 0.1256
  896/10000 [=>............................] - ETA: 1:20:55 - loss: 0.6521 - regression_loss: 0.5266 - classification_loss: 0.1256
  897/10000 [=>............................] - ETA: 1:20:54 - loss: 0.6520 - regression_loss: 0.5264 - classification_loss: 0.1256
  898/10000 [=>............................] - ETA: 1:20:53 - loss: 0.6518 - regression_loss: 0.5262 - classification_loss: 0.1255
  899/10000 [=>............................] - ETA: 1:20:52 - loss: 0.6522 - regression_loss: 0.5266 - classification_loss: 0.1255
  900/10000 [=>............................] - ETA: 1:20:51 - loss: 0.6526 - regression_loss: 0.5267 - classification_loss: 0.1259
  901/10000 [=>............................] - ETA: 1:20:49 - loss: 0.6524 - regression_loss: 0.5265 - classification_loss: 0.1259
  902/10000 [=>............................] - ETA: 1:20:48 - loss: 0.6526 - regression_loss: 0.5266 - classification_loss: 0.1260
  903/10000 [=>............................] - ETA: 1:20:47 - loss: 0.6525 - regression_loss: 0.5266 - classification_loss: 0.1259
  904/10000 [=>............................] - ETA: 1:20:45 - loss: 0.6522 - regression_loss: 0.5264 - classification_loss: 0.1258
  905/10000 [=>............................] - ETA: 1:20:44 - loss: 0.6517 - regression_loss: 0.5260 - classification_loss: 0.1257
  906/10000 [=>............................] - ETA: 1:20:43 - loss: 0.6517 - regression_loss: 0.5261 - classification_loss: 0.1257
  907/10000 [=>............................] - ETA: 1:20:41 - loss: 0.6515 - regression_loss: 0.5259 - classification_loss: 0.1256
  908/10000 [=>............................] - ETA: 1:20:40 - loss: 0.6511 - regression_loss: 0.5256 - classification_loss: 0.1255
  909/10000 [=>............................] - ETA: 1:20:39 - loss: 0.6508 - regression_loss: 0.5252 - classification_loss: 0.1256
  910/10000 [=>............................] - ETA: 1:20:37 - loss: 0.6504 - regression_loss: 0.5249 - classification_loss: 0.1255
  911/10000 [=>............................] - ETA: 1:20:36 - loss: 0.6498 - regression_loss: 0.5244 - classification_loss: 0.1254
  912/10000 [=>............................] - ETA: 1:20:35 - loss: 0.6497 - regression_loss: 0.5243 - classification_loss: 0.1254
  913/10000 [=>............................] - ETA: 1:20:33 - loss: 0.6492 - regression_loss: 0.5239 - classification_loss: 0.1254
  914/10000 [=>............................] - ETA: 1:20:32 - loss: 0.6492 - regression_loss: 0.5239 - classification_loss: 0.1253
  915/10000 [=>............................] - ETA: 1:20:31 - loss: 0.6495 - regression_loss: 0.5242 - classification_loss: 0.1253
  916/10000 [=>............................] - ETA: 1:20:29 - loss: 0.6491 - regression_loss: 0.5238 - classification_loss: 0.1254
  917/10000 [=>............................] - ETA: 1:20:28 - loss: 0.6486 - regression_loss: 0.5233 - classification_loss: 0.1253
  918/10000 [=>............................] - ETA: 1:20:27 - loss: 0.6491 - regression_loss: 0.5234 - classification_loss: 0.1256
  919/10000 [=>............................] - ETA: 1:20:26 - loss: 0.6486 - regression_loss: 0.5230 - classification_loss: 0.1255
  920/10000 [=>............................] - ETA: 1:20:24 - loss: 0.6482 - regression_loss: 0.5227 - classification_loss: 0.1254
  921/10000 [=>............................] - ETA: 1:20:23 - loss: 0.6486 - regression_loss: 0.5230 - classification_loss: 0.1256
  922/10000 [=>............................] - ETA: 1:20:21 - loss: 0.6489 - regression_loss: 0.5232 - classification_loss: 0.1257
  923/10000 [=>............................] - ETA: 1:20:20 - loss: 0.6489 - regression_loss: 0.5231 - classification_loss: 0.1258
  924/10000 [=>............................] - ETA: 1:20:19 - loss: 0.6489 - regression_loss: 0.5231 - classification_loss: 0.1258
  925/10000 [=>............................] - ETA: 1:20:18 - loss: 0.6494 - regression_loss: 0.5236 - classification_loss: 0.1258
  926/10000 [=>............................] - ETA: 1:20:17 - loss: 0.6493 - regression_loss: 0.5236 - classification_loss: 0.1257
  927/10000 [=>............................] - ETA: 1:20:16 - loss: 0.6492 - regression_loss: 0.5233 - classification_loss: 0.1259
  928/10000 [=>............................] - ETA: 1:20:15 - loss: 0.6493 - regression_loss: 0.5233 - classification_loss: 0.1260
  929/10000 [=>............................] - ETA: 1:20:13 - loss: 0.6502 - regression_loss: 0.5240 - classification_loss: 0.1261
  930/10000 [=>............................] - ETA: 1:20:12 - loss: 0.6506 - regression_loss: 0.5244 - classification_loss: 0.1262
  931/10000 [=>............................] - ETA: 1:20:11 - loss: 0.6513 - regression_loss: 0.5249 - classification_loss: 0.1264
  932/10000 [=>............................] - ETA: 1:20:10 - loss: 0.6509 - regression_loss: 0.5246 - classification_loss: 0.1263
  933/10000 [=>............................] - ETA: 1:20:08 - loss: 0.6516 - regression_loss: 0.5252 - classification_loss: 0.1264
  934/10000 [=>............................] - ETA: 1:20:07 - loss: 0.6521 - regression_loss: 0.5257 - classification_loss: 0.1264
  935/10000 [=>............................] - ETA: 1:20:06 - loss: 0.6520 - regression_loss: 0.5257 - classification_loss: 0.1263
  936/10000 [=>............................] - ETA: 1:20:05 - loss: 0.6522 - regression_loss: 0.5258 - classification_loss: 0.1264
  937/10000 [=>............................] - ETA: 1:20:04 - loss: 0.6518 - regression_loss: 0.5256 - classification_loss: 0.1263
  938/10000 [=>............................] - ETA: 1:20:02 - loss: 0.6516 - regression_loss: 0.5254 - classification_loss: 0.1262
  939/10000 [=>............................] - ETA: 1:20:01 - loss: 0.6515 - regression_loss: 0.5253 - classification_loss: 0.1262
  940/10000 [=>............................] - ETA: 1:20:00 - loss: 0.6514 - regression_loss: 0.5252 - classification_loss: 0.1262
  941/10000 [=>............................] - ETA: 1:19:59 - loss: 0.6515 - regression_loss: 0.5254 - classification_loss: 0.1261
  942/10000 [=>............................] - ETA: 1:19:57 - loss: 0.6525 - regression_loss: 0.5262 - classification_loss: 0.1263
  943/10000 [=>............................] - ETA: 1:19:56 - loss: 0.6528 - regression_loss: 0.5263 - classification_loss: 0.1264
  944/10000 [=>............................] - ETA: 1:19:55 - loss: 0.6537 - regression_loss: 0.5270 - classification_loss: 0.1266
  945/10000 [=>............................] - ETA: 1:19:54 - loss: 0.6539 - regression_loss: 0.5270 - classification_loss: 0.1269
  946/10000 [=>............................] - ETA: 1:19:53 - loss: 0.6536 - regression_loss: 0.5268 - classification_loss: 0.1268
  947/10000 [=>............................] - ETA: 1:19:51 - loss: 0.6533 - regression_loss: 0.5266 - classification_loss: 0.1268
  948/10000 [=>............................] - ETA: 1:19:50 - loss: 0.6532 - regression_loss: 0.5264 - classification_loss: 0.1267
  949/10000 [=>............................] - ETA: 1:19:49 - loss: 0.6530 - regression_loss: 0.5262 - classification_loss: 0.1268
  950/10000 [=>............................] - ETA: 1:19:55 - loss: 0.6532 - regression_loss: 0.5265 - classification_loss: 0.1267
  951/10000 [=>............................] - ETA: 1:19:54 - loss: 0.6528 - regression_loss: 0.5262 - classification_loss: 0.1267
  952/10000 [=>............................] - ETA: 1:19:53 - loss: 0.6530 - regression_loss: 0.5263 - classification_loss: 0.1267
  953/10000 [=>............................] - ETA: 1:19:51 - loss: 0.6536 - regression_loss: 0.5267 - classification_loss: 0.1268
  954/10000 [=>............................] - ETA: 1:19:50 - loss: 0.6529 - regression_loss: 0.5262 - classification_loss: 0.1267
  955/10000 [=>............................] - ETA: 1:19:48 - loss: 0.6528 - regression_loss: 0.5259 - classification_loss: 0.1269
  956/10000 [=>............................] - ETA: 1:19:47 - loss: 0.6535 - regression_loss: 0.5264 - classification_loss: 0.1271
  957/10000 [=>............................] - ETA: 1:19:46 - loss: 0.6534 - regression_loss: 0.5263 - classification_loss: 0.1271
  958/10000 [=>............................] - ETA: 1:19:45 - loss: 0.6531 - regression_loss: 0.5261 - classification_loss: 0.1270
  959/10000 [=>............................] - ETA: 1:19:43 - loss: 0.6535 - regression_loss: 0.5266 - classification_loss: 0.1269
  960/10000 [=>............................] - ETA: 1:19:42 - loss: 0.6537 - regression_loss: 0.5266 - classification_loss: 0.1271
  961/10000 [=>............................] - ETA: 1:19:41 - loss: 0.6545 - regression_loss: 0.5273 - classification_loss: 0.1272
  962/10000 [=>............................] - ETA: 1:19:39 - loss: 0.6542 - regression_loss: 0.5270 - classification_loss: 0.1272
  963/10000 [=>............................] - ETA: 1:19:38 - loss: 0.6548 - regression_loss: 0.5277 - classification_loss: 0.1271
  964/10000 [=>............................] - ETA: 1:19:37 - loss: 0.6544 - regression_loss: 0.5273 - classification_loss: 0.1271
  965/10000 [=>............................] - ETA: 1:19:36 - loss: 0.6546 - regression_loss: 0.5274 - classification_loss: 0.1272
  966/10000 [=>............................] - ETA: 1:19:34 - loss: 0.6542 - regression_loss: 0.5271 - classification_loss: 0.1271
  967/10000 [=>............................] - ETA: 1:19:33 - loss: 0.6550 - regression_loss: 0.5277 - classification_loss: 0.1273
  968/10000 [=>............................] - ETA: 1:19:32 - loss: 0.6556 - regression_loss: 0.5282 - classification_loss: 0.1274
  969/10000 [=>............................] - ETA: 1:19:31 - loss: 0.6556 - regression_loss: 0.5283 - classification_loss: 0.1273
  970/10000 [=>............................] - ETA: 1:19:30 - loss: 0.6565 - regression_loss: 0.5289 - classification_loss: 0.1275
  971/10000 [=>............................] - ETA: 1:19:28 - loss: 0.6560 - regression_loss: 0.5285 - classification_loss: 0.1275
  972/10000 [=>............................] - ETA: 1:19:27 - loss: 0.6561 - regression_loss: 0.5286 - classification_loss: 0.1275
  973/10000 [=>............................] - ETA: 1:19:26 - loss: 0.6566 - regression_loss: 0.5291 - classification_loss: 0.1276
  974/10000 [=>............................] - ETA: 1:19:25 - loss: 0.6571 - regression_loss: 0.5294 - classification_loss: 0.1277
  975/10000 [=>............................] - ETA: 1:19:23 - loss: 0.6575 - regression_loss: 0.5297 - classification_loss: 0.1278
  976/10000 [=>............................] - ETA: 1:19:22 - loss: 0.6577 - regression_loss: 0.5300 - classification_loss: 0.1277
  977/10000 [=>............................] - ETA: 1:19:21 - loss: 0.6577 - regression_loss: 0.5300 - classification_loss: 0.1277
  978/10000 [=>............................] - ETA: 1:19:19 - loss: 0.6578 - regression_loss: 0.5301 - classification_loss: 0.1277
  979/10000 [=>............................] - ETA: 1:19:19 - loss: 0.6575 - regression_loss: 0.5299 - classification_loss: 0.1276
  980/10000 [=>............................] - ETA: 1:19:23 - loss: 0.6575 - regression_loss: 0.5299 - classification_loss: 0.1276
  981/10000 [=>............................] - ETA: 1:19:22 - loss: 0.6571 - regression_loss: 0.5296 - classification_loss: 0.1275
  982/10000 [=>............................] - ETA: 1:19:21 - loss: 0.6571 - regression_loss: 0.5296 - classification_loss: 0.1275
  983/10000 [=>............................] - ETA: 1:19:20 - loss: 0.6578 - regression_loss: 0.5303 - classification_loss: 0.1275
  984/10000 [=>............................] - ETA: 1:19:19 - loss: 0.6583 - regression_loss: 0.5307 - classification_loss: 0.1276
  985/10000 [=>............................] - ETA: 1:19:18 - loss: 0.6580 - regression_loss: 0.5305 - classification_loss: 0.1276
  986/10000 [=>............................] - ETA: 1:19:17 - loss: 0.6579 - regression_loss: 0.5304 - classification_loss: 0.1275
  987/10000 [=>............................] - ETA: 1:19:15 - loss: 0.6576 - regression_loss: 0.5302 - classification_loss: 0.1274
  988/10000 [=>............................] - ETA: 1:19:14 - loss: 0.6576 - regression_loss: 0.5302 - classification_loss: 0.1274
  989/10000 [=>............................] - ETA: 1:19:13 - loss: 0.6577 - regression_loss: 0.5303 - classification_loss: 0.1274
  990/10000 [=>............................] - ETA: 1:19:12 - loss: 0.6587 - regression_loss: 0.5312 - classification_loss: 0.1275
  991/10000 [=>............................] - ETA: 1:19:11 - loss: 0.6590 - regression_loss: 0.5314 - classification_loss: 0.1275
  992/10000 [=>............................] - ETA: 1:19:09 - loss: 0.6595 - regression_loss: 0.5317 - classification_loss: 0.1278
  993/10000 [=>............................] - ETA: 1:19:09 - loss: 0.6596 - regression_loss: 0.5318 - classification_loss: 0.1278
  994/10000 [=>............................] - ETA: 1:19:08 - loss: 0.6594 - regression_loss: 0.5316 - classification_loss: 0.1278
  995/10000 [=>............................] - ETA: 1:19:06 - loss: 0.6591 - regression_loss: 0.5313 - classification_loss: 0.1278
  996/10000 [=>............................] - ETA: 1:19:05 - loss: 0.6590 - regression_loss: 0.5312 - classification_loss: 0.1278
  997/10000 [=>............................] - ETA: 1:19:04 - loss: 0.6587 - regression_loss: 0.5310 - classification_loss: 0.1277
  998/10000 [=>............................] - ETA: 1:19:03 - loss: 0.6589 - regression_loss: 0.5312 - classification_loss: 0.1277
  999/10000 [=>............................] - ETA: 1:19:01 - loss: 0.6595 - regression_loss: 0.5317 - classification_loss: 0.1279
 1000/10000 [==>...........................] - ETA: 1:19:00 - loss: 0.6594 - regression_loss: 0.5314 - classification_loss: 0.1280
 1001/10000 [==>...........................] - ETA: 1:18:59 - loss: 0.6591 - regression_loss: 0.5312 - classification_loss: 0.1279
 1002/10000 [==>...........................] - ETA: 1:18:59 - loss: 0.6590 - regression_loss: 0.5312 - classification_loss: 0.1278
 1003/10000 [==>...........................] - ETA: 1:18:57 - loss: 0.6587 - regression_loss: 0.5310 - classification_loss: 0.1278
 1004/10000 [==>...........................] - ETA: 1:18:56 - loss: 0.6586 - regression_loss: 0.5308 - classification_loss: 0.1277
 1005/10000 [==>...........................] - ETA: 1:18:55 - loss: 0.6581 - regression_loss: 0.5305 - classification_loss: 0.1276
 1006/10000 [==>...........................] - ETA: 1:18:54 - loss: 0.6578 - regression_loss: 0.5302 - classification_loss: 0.1276
 1007/10000 [==>...........................] - ETA: 1:18:53 - loss: 0.6583 - regression_loss: 0.5306 - classification_loss: 0.1277
 1008/10000 [==>...........................] - ETA: 1:18:52 - loss: 0.6580 - regression_loss: 0.5303 - classification_loss: 0.1277
 1009/10000 [==>...........................] - ETA: 1:18:51 - loss: 0.6580 - regression_loss: 0.5302 - classification_loss: 0.1278
 1010/10000 [==>...........................] - ETA: 1:18:50 - loss: 0.6578 - regression_loss: 0.5300 - classification_loss: 0.1278
 1011/10000 [==>...........................] - ETA: 1:18:48 - loss: 0.6578 - regression_loss: 0.5300 - classification_loss: 0.1278
 1012/10000 [==>...........................] - ETA: 1:18:48 - loss: 0.6574 - regression_loss: 0.5297 - classification_loss: 0.1277
 1013/10000 [==>...........................] - ETA: 1:18:46 - loss: 0.6580 - regression_loss: 0.5303 - classification_loss: 0.1277
 1014/10000 [==>...........................] - ETA: 1:18:45 - loss: 0.6576 - regression_loss: 0.5299 - classification_loss: 0.1276
 1015/10000 [==>...........................] - ETA: 1:18:44 - loss: 0.6571 - regression_loss: 0.5296 - classification_loss: 0.1275
 1016/10000 [==>...........................] - ETA: 1:18:43 - loss: 0.6573 - regression_loss: 0.5298 - classification_loss: 0.1275
 1017/10000 [==>...........................] - ETA: 1:18:42 - loss: 0.6574 - regression_loss: 0.5299 - classification_loss: 0.1275
 1018/10000 [==>...........................] - ETA: 1:18:41 - loss: 0.6570 - regression_loss: 0.5296 - classification_loss: 0.1274
 1019/10000 [==>...........................] - ETA: 1:18:43 - loss: 0.6569 - regression_loss: 0.5296 - classification_loss: 0.1274
 1020/10000 [==>...........................] - ETA: 1:18:42 - loss: 0.6570 - regression_loss: 0.5297 - classification_loss: 0.1273
 1021/10000 [==>...........................] - ETA: 1:18:41 - loss: 0.6566 - regression_loss: 0.5293 - classification_loss: 0.1272
 1022/10000 [==>...........................] - ETA: 1:18:40 - loss: 0.6574 - regression_loss: 0.5301 - classification_loss: 0.1273
 1023/10000 [==>...........................] - ETA: 1:18:39 - loss: 0.6575 - regression_loss: 0.5302 - classification_loss: 0.1272
 1024/10000 [==>...........................] - ETA: 1:18:38 - loss: 0.6587 - regression_loss: 0.5313 - classification_loss: 0.1275
 1025/10000 [==>...........................] - ETA: 1:18:37 - loss: 0.6590 - regression_loss: 0.5315 - classification_loss: 0.1275
 1026/10000 [==>...........................] - ETA: 1:18:36 - loss: 0.6586 - regression_loss: 0.5312 - classification_loss: 0.1274
 1027/10000 [==>...........................] - ETA: 1:18:35 - loss: 0.6600 - regression_loss: 0.5323 - classification_loss: 0.1277
 1028/10000 [==>...........................] - ETA: 1:18:34 - loss: 0.6600 - regression_loss: 0.5324 - classification_loss: 0.1277
 1029/10000 [==>...........................] - ETA: 1:18:33 - loss: 0.6597 - regression_loss: 0.5321 - classification_loss: 0.1276
 1030/10000 [==>...........................] - ETA: 1:18:32 - loss: 0.6595 - regression_loss: 0.5319 - classification_loss: 0.1276
 1031/10000 [==>...........................] - ETA: 1:18:31 - loss: 0.6604 - regression_loss: 0.5326 - classification_loss: 0.1278
 1032/10000 [==>...........................] - ETA: 1:18:30 - loss: 0.6601 - regression_loss: 0.5323 - classification_loss: 0.1278
 1033/10000 [==>...........................] - ETA: 1:18:29 - loss: 0.6607 - regression_loss: 0.5328 - classification_loss: 0.1279
 1034/10000 [==>...........................] - ETA: 1:18:28 - loss: 0.6605 - regression_loss: 0.5326 - classification_loss: 0.1279
 1035/10000 [==>...........................] - ETA: 1:18:27 - loss: 0.6607 - regression_loss: 0.5328 - classification_loss: 0.1279
 1036/10000 [==>...........................] - ETA: 1:18:25 - loss: 0.6608 - regression_loss: 0.5329 - classification_loss: 0.1279
 1037/10000 [==>...........................] - ETA: 1:18:24 - loss: 0.6625 - regression_loss: 0.5343 - classification_loss: 0.1283
 1038/10000 [==>...........................] - ETA: 1:18:23 - loss: 0.6632 - regression_loss: 0.5347 - classification_loss: 0.1284
 1039/10000 [==>...........................] - ETA: 1:18:22 - loss: 0.6628 - regression_loss: 0.5344 - classification_loss: 0.1283
 1040/10000 [==>...........................] - ETA: 1:18:21 - loss: 0.6623 - regression_loss: 0.5341 - classification_loss: 0.1283
 1041/10000 [==>...........................] - ETA: 1:18:19 - loss: 0.6629 - regression_loss: 0.5344 - classification_loss: 0.1285
 1042/10000 [==>...........................] - ETA: 1:18:18 - loss: 0.6627 - regression_loss: 0.5343 - classification_loss: 0.1284
 1043/10000 [==>...........................] - ETA: 1:18:17 - loss: 0.6625 - regression_loss: 0.5341 - classification_loss: 0.1284
 1044/10000 [==>...........................] - ETA: 1:18:16 - loss: 0.6623 - regression_loss: 0.5340 - classification_loss: 0.1284
 1045/10000 [==>...........................] - ETA: 1:18:15 - loss: 0.6623 - regression_loss: 0.5339 - classification_loss: 0.1283
 1046/10000 [==>...........................] - ETA: 1:18:14 - loss: 0.6619 - regression_loss: 0.5336 - classification_loss: 0.1283
 1047/10000 [==>...........................] - ETA: 1:18:13 - loss: 0.6618 - regression_loss: 0.5335 - classification_loss: 0.1283
 1048/10000 [==>...........................] - ETA: 1:18:13 - loss: 0.6616 - regression_loss: 0.5334 - classification_loss: 0.1282
 1049/10000 [==>...........................] - ETA: 1:18:12 - loss: 0.6617 - regression_loss: 0.5334 - classification_loss: 0.1283
 1050/10000 [==>...........................] - ETA: 1:18:10 - loss: 0.6616 - regression_loss: 0.5332 - classification_loss: 0.1283
 1051/10000 [==>...........................] - ETA: 1:18:10 - loss: 0.6612 - regression_loss: 0.5329 - classification_loss: 0.1283
 1052/10000 [==>...........................] - ETA: 1:18:09 - loss: 0.6607 - regression_loss: 0.5325 - classification_loss: 0.1282
 1053/10000 [==>...........................] - ETA: 1:18:08 - loss: 0.6602 - regression_loss: 0.5321 - classification_loss: 0.1281
 1054/10000 [==>...........................] - ETA: 1:18:07 - loss: 0.6598 - regression_loss: 0.5318 - classification_loss: 0.1280
 1055/10000 [==>...........................] - ETA: 1:18:07 - loss: 0.6598 - regression_loss: 0.5319 - classification_loss: 0.1279
 1056/10000 [==>...........................] - ETA: 1:18:05 - loss: 0.6596 - regression_loss: 0.5317 - classification_loss: 0.1279
 1057/10000 [==>...........................] - ETA: 1:18:04 - loss: 0.6593 - regression_loss: 0.5314 - classification_loss: 0.1279
 1058/10000 [==>...........................] - ETA: 1:18:04 - loss: 0.6600 - regression_loss: 0.5321 - classification_loss: 0.1279
 1059/10000 [==>...........................] - ETA: 1:18:03 - loss: 0.6600 - regression_loss: 0.5321 - classification_loss: 0.1279
 1060/10000 [==>...........................] - ETA: 1:18:01 - loss: 0.6598 - regression_loss: 0.5320 - classification_loss: 0.1278
 1061/10000 [==>...........................] - ETA: 1:18:00 - loss: 0.6599 - regression_loss: 0.5320 - classification_loss: 0.1278
 1062/10000 [==>...........................] - ETA: 1:17:59 - loss: 0.6608 - regression_loss: 0.5329 - classification_loss: 0.1279
 1063/10000 [==>...........................] - ETA: 1:17:58 - loss: 0.6607 - regression_loss: 0.5328 - classification_loss: 0.1279
 1064/10000 [==>...........................] - ETA: 1:17:57 - loss: 0.6607 - regression_loss: 0.5329 - classification_loss: 0.1279
 1065/10000 [==>...........................] - ETA: 1:17:56 - loss: 0.6606 - regression_loss: 0.5328 - classification_loss: 0.1278
 1066/10000 [==>...........................] - ETA: 1:17:55 - loss: 0.6607 - regression_loss: 0.5329 - classification_loss: 0.1278
 1067/10000 [==>...........................] - ETA: 1:17:54 - loss: 0.6609 - regression_loss: 0.5331 - classification_loss: 0.1278
 1068/10000 [==>...........................] - ETA: 1:17:53 - loss: 0.6611 - regression_loss: 0.5333 - classification_loss: 0.1278
 1069/10000 [==>...........................] - ETA: 1:17:52 - loss: 0.6606 - regression_loss: 0.5329 - classification_loss: 0.1277
 1070/10000 [==>...........................] - ETA: 1:17:51 - loss: 0.6602 - regression_loss: 0.5326 - classification_loss: 0.1276
 1071/10000 [==>...........................] - ETA: 1:17:49 - loss: 0.6607 - regression_loss: 0.5331 - classification_loss: 0.1276
 1072/10000 [==>...........................] - ETA: 1:17:48 - loss: 0.6613 - regression_loss: 0.5335 - classification_loss: 0.1279
 1073/10000 [==>...........................] - ETA: 1:17:47 - loss: 0.6612 - regression_loss: 0.5334 - classification_loss: 0.1278
 1074/10000 [==>...........................] - ETA: 1:17:46 - loss: 0.6610 - regression_loss: 0.5332 - classification_loss: 0.1278
 1075/10000 [==>...........................] - ETA: 1:17:45 - loss: 0.6605 - regression_loss: 0.5328 - classification_loss: 0.1277
 1076/10000 [==>...........................] - ETA: 1:17:44 - loss: 0.6608 - regression_loss: 0.5330 - classification_loss: 0.1278
 1077/10000 [==>...........................] - ETA: 1:17:43 - loss: 0.6607 - regression_loss: 0.5328 - classification_loss: 0.1279
 1078/10000 [==>...........................] - ETA: 1:17:42 - loss: 0.6608 - regression_loss: 0.5329 - classification_loss: 0.1279
 1079/10000 [==>...........................] - ETA: 1:17:41 - loss: 0.6607 - regression_loss: 0.5327 - classification_loss: 0.1280
 1080/10000 [==>...........................] - ETA: 1:17:40 - loss: 0.6607 - regression_loss: 0.5326 - classification_loss: 0.1281
 1081/10000 [==>...........................] - ETA: 1:17:39 - loss: 0.6603 - regression_loss: 0.5323 - classification_loss: 0.1280
 1082/10000 [==>...........................] - ETA: 1:17:38 - loss: 0.6607 - regression_loss: 0.5327 - classification_loss: 0.1280
 1083/10000 [==>...........................] - ETA: 1:17:36 - loss: 0.6608 - regression_loss: 0.5329 - classification_loss: 0.1280
 1084/10000 [==>...........................] - ETA: 1:17:35 - loss: 0.6604 - regression_loss: 0.5325 - classification_loss: 0.1279
 1085/10000 [==>...........................] - ETA: 1:17:34 - loss: 0.6609 - regression_loss: 0.5328 - classification_loss: 0.1281
 1086/10000 [==>...........................] - ETA: 1:17:33 - loss: 0.6607 - regression_loss: 0.5326 - classification_loss: 0.1281
 1087/10000 [==>...........................] - ETA: 1:17:32 - loss: 0.6613 - regression_loss: 0.5332 - classification_loss: 0.1281
 1088/10000 [==>...........................] - ETA: 1:17:31 - loss: 0.6612 - regression_loss: 0.5331 - classification_loss: 0.1281
 1089/10000 [==>...........................] - ETA: 1:17:30 - loss: 0.6614 - regression_loss: 0.5332 - classification_loss: 0.1282
 1090/10000 [==>...........................] - ETA: 1:17:29 - loss: 0.6611 - regression_loss: 0.5330 - classification_loss: 0.1281
 1091/10000 [==>...........................] - ETA: 1:17:28 - loss: 0.6617 - regression_loss: 0.5335 - classification_loss: 0.1281
 1092/10000 [==>...........................] - ETA: 1:17:27 - loss: 0.6617 - regression_loss: 0.5336 - classification_loss: 0.1281
 1093/10000 [==>...........................] - ETA: 1:17:26 - loss: 0.6614 - regression_loss: 0.5333 - classification_loss: 0.1281
 1094/10000 [==>...........................] - ETA: 1:17:25 - loss: 0.6611 - regression_loss: 0.5330 - classification_loss: 0.1281
 1095/10000 [==>...........................] - ETA: 1:17:24 - loss: 0.6608 - regression_loss: 0.5328 - classification_loss: 0.1280
 1096/10000 [==>...........................] - ETA: 1:17:23 - loss: 0.6608 - regression_loss: 0.5328 - classification_loss: 0.1280
 1097/10000 [==>...........................] - ETA: 1:17:22 - loss: 0.6607 - regression_loss: 0.5327 - classification_loss: 0.1279
 1098/10000 [==>...........................] - ETA: 1:17:21 - loss: 0.6603 - regression_loss: 0.5324 - classification_loss: 0.1279
 1099/10000 [==>...........................] - ETA: 1:17:20 - loss: 0.6605 - regression_loss: 0.5326 - classification_loss: 0.1279
 1100/10000 [==>...........................] - ETA: 1:17:18 - loss: 0.6602 - regression_loss: 0.5324 - classification_loss: 0.1278
 1101/10000 [==>...........................] - ETA: 1:17:17 - loss: 0.6598 - regression_loss: 0.5321 - classification_loss: 0.1278
 1102/10000 [==>...........................] - ETA: 1:17:16 - loss: 0.6596 - regression_loss: 0.5319 - classification_loss: 0.1277
 1103/10000 [==>...........................] - ETA: 1:17:15 - loss: 0.6592 - regression_loss: 0.5315 - classification_loss: 0.1277
 1104/10000 [==>...........................] - ETA: 1:17:14 - loss: 0.6591 - regression_loss: 0.5315 - classification_loss: 0.1277
 1105/10000 [==>...........................] - ETA: 1:17:13 - loss: 0.6593 - regression_loss: 0.5316 - classification_loss: 0.1277
 1106/10000 [==>...........................] - ETA: 1:17:12 - loss: 0.6590 - regression_loss: 0.5314 - classification_loss: 0.1277
 1107/10000 [==>...........................] - ETA: 1:17:11 - loss: 0.6593 - regression_loss: 0.5316 - classification_loss: 0.1277
 1108/10000 [==>...........................] - ETA: 1:17:10 - loss: 0.6589 - regression_loss: 0.5313 - classification_loss: 0.1276
 1109/10000 [==>...........................] - ETA: 1:17:09 - loss: 0.6596 - regression_loss: 0.5319 - classification_loss: 0.1277
 1110/10000 [==>...........................] - ETA: 1:17:08 - loss: 0.6601 - regression_loss: 0.5324 - classification_loss: 0.1277
 1111/10000 [==>...........................] - ETA: 1:17:06 - loss: 0.6603 - regression_loss: 0.5326 - classification_loss: 0.1278
 1112/10000 [==>...........................] - ETA: 1:17:05 - loss: 0.6599 - regression_loss: 0.5322 - classification_loss: 0.1277
 1113/10000 [==>...........................] - ETA: 1:17:05 - loss: 0.6602 - regression_loss: 0.5325 - classification_loss: 0.1277
 1114/10000 [==>...........................] - ETA: 1:17:04 - loss: 0.6602 - regression_loss: 0.5326 - classification_loss: 0.1277
 1115/10000 [==>...........................] - ETA: 1:17:03 - loss: 0.6610 - regression_loss: 0.5332 - classification_loss: 0.1278
 1116/10000 [==>...........................] - ETA: 1:17:04 - loss: 0.6611 - regression_loss: 0.5334 - classification_loss: 0.1278
 1117/10000 [==>...........................] - ETA: 1:17:03 - loss: 0.6620 - regression_loss: 0.5342 - classification_loss: 0.1279
 1118/10000 [==>...........................] - ETA: 1:17:02 - loss: 0.6617 - regression_loss: 0.5339 - classification_loss: 0.1278
 1119/10000 [==>...........................] - ETA: 1:17:01 - loss: 0.6620 - regression_loss: 0.5341 - classification_loss: 0.1279
 1120/10000 [==>...........................] - ETA: 1:17:01 - loss: 0.6627 - regression_loss: 0.5346 - classification_loss: 0.1281
 1121/10000 [==>...........................] - ETA: 1:16:59 - loss: 0.6629 - regression_loss: 0.5348 - classification_loss: 0.1281
 1122/10000 [==>...........................] - ETA: 1:16:58 - loss: 0.6632 - regression_loss: 0.5350 - classification_loss: 0.1282
 1123/10000 [==>...........................] - ETA: 1:16:58 - loss: 0.6631 - regression_loss: 0.5349 - classification_loss: 0.1281
 1124/10000 [==>...........................] - ETA: 1:16:57 - loss: 0.6633 - regression_loss: 0.5351 - classification_loss: 0.1282
 1125/10000 [==>...........................] - ETA: 1:16:56 - loss: 0.6635 - regression_loss: 0.5353 - classification_loss: 0.1282
 1126/10000 [==>...........................] - ETA: 1:16:55 - loss: 0.6635 - regression_loss: 0.5352 - classification_loss: 0.1283
 1127/10000 [==>...........................] - ETA: 1:16:54 - loss: 0.6635 - regression_loss: 0.5352 - classification_loss: 0.1283
 1128/10000 [==>...........................] - ETA: 1:16:53 - loss: 0.6630 - regression_loss: 0.5348 - classification_loss: 0.1282
 1129/10000 [==>...........................] - ETA: 1:16:53 - loss: 0.6628 - regression_loss: 0.5347 - classification_loss: 0.1281
 1130/10000 [==>...........................] - ETA: 1:16:52 - loss: 0.6628 - regression_loss: 0.5347 - classification_loss: 0.1281
 1131/10000 [==>...........................] - ETA: 1:16:51 - loss: 0.6629 - regression_loss: 0.5348 - classification_loss: 0.1280
 1132/10000 [==>...........................] - ETA: 1:16:50 - loss: 0.6624 - regression_loss: 0.5344 - classification_loss: 0.1279
 1133/10000 [==>...........................] - ETA: 1:16:49 - loss: 0.6626 - regression_loss: 0.5347 - classification_loss: 0.1279
 1134/10000 [==>...........................] - ETA: 1:16:49 - loss: 0.6626 - regression_loss: 0.5348 - classification_loss: 0.1278
 1135/10000 [==>...........................] - ETA: 1:16:48 - loss: 0.6622 - regression_loss: 0.5345 - classification_loss: 0.1278
 1136/10000 [==>...........................] - ETA: 1:16:47 - loss: 0.6629 - regression_loss: 0.5351 - classification_loss: 0.1278
 1137/10000 [==>...........................] - ETA: 1:16:46 - loss: 0.6626 - regression_loss: 0.5348 - classification_loss: 0.1277
 1138/10000 [==>...........................] - ETA: 1:16:45 - loss: 0.6623 - regression_loss: 0.5345 - classification_loss: 0.1278
 1139/10000 [==>...........................] - ETA: 1:16:44 - loss: 0.6620 - regression_loss: 0.5343 - classification_loss: 0.1278
 1140/10000 [==>...........................] - ETA: 1:16:43 - loss: 0.6620 - regression_loss: 0.5343 - classification_loss: 0.1277
 1141/10000 [==>...........................] - ETA: 1:16:42 - loss: 0.6618 - regression_loss: 0.5341 - classification_loss: 0.1276
 1142/10000 [==>...........................] - ETA: 1:16:41 - loss: 0.6613 - regression_loss: 0.5338 - classification_loss: 0.1275
 1143/10000 [==>...........................] - ETA: 1:16:47 - loss: 0.6617 - regression_loss: 0.5341 - classification_loss: 0.1275
 1144/10000 [==>...........................] - ETA: 1:16:46 - loss: 0.6615 - regression_loss: 0.5340 - classification_loss: 0.1275
 1145/10000 [==>...........................] - ETA: 1:16:48 - loss: 0.6618 - regression_loss: 0.5344 - classification_loss: 0.1275
 1146/10000 [==>...........................] - ETA: 1:16:47 - loss: 0.6622 - regression_loss: 0.5347 - classification_loss: 0.1275
 1147/10000 [==>...........................] - ETA: 1:16:46 - loss: 0.6622 - regression_loss: 0.5347 - classification_loss: 0.1275
 1148/10000 [==>...........................] - ETA: 1:16:45 - loss: 0.6623 - regression_loss: 0.5347 - classification_loss: 0.1276
 1149/10000 [==>...........................] - ETA: 1:16:49 - loss: 0.6619 - regression_loss: 0.5345 - classification_loss: 0.1275
 1150/10000 [==>...........................] - ETA: 1:16:48 - loss: 0.6621 - regression_loss: 0.5346 - classification_loss: 0.1275
 1151/10000 [==>...........................] - ETA: 1:16:47 - loss: 0.6629 - regression_loss: 0.5353 - classification_loss: 0.1276
 1152/10000 [==>...........................] - ETA: 1:16:46 - loss: 0.6631 - regression_loss: 0.5355 - classification_loss: 0.1276
 1153/10000 [==>...........................] - ETA: 1:16:49 - loss: 0.6632 - regression_loss: 0.5355 - classification_loss: 0.1276
 1154/10000 [==>...........................] - ETA: 1:16:48 - loss: 0.6632 - regression_loss: 0.5355 - classification_loss: 0.1277
 1155/10000 [==>...........................] - ETA: 1:16:47 - loss: 0.6632 - regression_loss: 0.5355 - classification_loss: 0.1277
 1156/10000 [==>...........................] - ETA: 1:16:46 - loss: 0.6634 - regression_loss: 0.5357 - classification_loss: 0.1277
 1157/10000 [==>...........................] - ETA: 1:16:45 - loss: 0.6631 - regression_loss: 0.5354 - classification_loss: 0.1277
 1158/10000 [==>...........................] - ETA: 1:16:45 - loss: 0.6633 - regression_loss: 0.5356 - classification_loss: 0.1277
 1159/10000 [==>...........................] - ETA: 1:16:44 - loss: 0.6635 - regression_loss: 0.5358 - classification_loss: 0.1277
 1160/10000 [==>...........................] - ETA: 1:16:43 - loss: 0.6637 - regression_loss: 0.5360 - classification_loss: 0.1278
 1161/10000 [==>...........................] - ETA: 1:16:42 - loss: 0.6643 - regression_loss: 0.5361 - classification_loss: 0.1281
 1162/10000 [==>...........................] - ETA: 1:16:41 - loss: 0.6648 - regression_loss: 0.5366 - classification_loss: 0.1281
 1163/10000 [==>...........................] - ETA: 1:16:40 - loss: 0.6649 - regression_loss: 0.5367 - classification_loss: 0.1282
 1164/10000 [==>...........................] - ETA: 1:16:39 - loss: 0.6651 - regression_loss: 0.5369 - classification_loss: 0.1282
 1165/10000 [==>...........................] - ETA: 1:16:38 - loss: 0.6654 - regression_loss: 0.5373 - classification_loss: 0.1282
 1166/10000 [==>...........................] - ETA: 1:16:38 - loss: 0.6661 - regression_loss: 0.5379 - classification_loss: 0.1282
 1167/10000 [==>...........................] - ETA: 1:16:37 - loss: 0.6660 - regression_loss: 0.5378 - classification_loss: 0.1281
 1168/10000 [==>...........................] - ETA: 1:16:36 - loss: 0.6659 - regression_loss: 0.5376 - classification_loss: 0.1283
 1169/10000 [==>...........................] - ETA: 1:16:36 - loss: 0.6659 - regression_loss: 0.5377 - classification_loss: 0.1282
 1170/10000 [==>...........................] - ETA: 1:16:35 - loss: 0.6655 - regression_loss: 0.5373 - classification_loss: 0.1282
 1171/10000 [==>...........................] - ETA: 1:16:34 - loss: 0.6650 - regression_loss: 0.5369 - classification_loss: 0.1281
 1172/10000 [==>...........................] - ETA: 1:16:33 - loss: 0.6654 - regression_loss: 0.5372 - classification_loss: 0.1282
 1173/10000 [==>...........................] - ETA: 1:16:32 - loss: 0.6651 - regression_loss: 0.5370 - classification_loss: 0.1281
 1174/10000 [==>...........................] - ETA: 1:16:31 - loss: 0.6654 - regression_loss: 0.5372 - classification_loss: 0.1282
 1175/10000 [==>...........................] - ETA: 1:16:30 - loss: 0.6657 - regression_loss: 0.5375 - classification_loss: 0.1282
 1176/10000 [==>...........................] - ETA: 1:16:29 - loss: 0.6656 - regression_loss: 0.5373 - classification_loss: 0.1283
 1177/10000 [==>...........................] - ETA: 1:16:29 - loss: 0.6654 - regression_loss: 0.5372 - classification_loss: 0.1282
 1178/10000 [==>...........................] - ETA: 1:16:28 - loss: 0.6654 - regression_loss: 0.5371 - classification_loss: 0.1283
 1179/10000 [==>...........................] - ETA: 1:16:27 - loss: 0.6651 - regression_loss: 0.5369 - classification_loss: 0.1282
 1180/10000 [==>...........................] - ETA: 1:16:26 - loss: 0.6651 - regression_loss: 0.5369 - classification_loss: 0.1282
 1181/10000 [==>...........................] - ETA: 1:16:25 - loss: 0.6647 - regression_loss: 0.5366 - classification_loss: 0.1282
 1182/10000 [==>...........................] - ETA: 1:16:24 - loss: 0.6650 - regression_loss: 0.5365 - classification_loss: 0.1285
 1183/10000 [==>...........................] - ETA: 1:16:23 - loss: 0.6651 - regression_loss: 0.5366 - classification_loss: 0.1285
 1184/10000 [==>...........................] - ETA: 1:16:22 - loss: 0.6654 - regression_loss: 0.5369 - classification_loss: 0.1286
 1185/10000 [==>...........................] - ETA: 1:16:24 - loss: 0.6655 - regression_loss: 0.5369 - classification_loss: 0.1286
 1186/10000 [==>...........................] - ETA: 1:16:24 - loss: 0.6658 - regression_loss: 0.5372 - classification_loss: 0.1286
 1187/10000 [==>...........................] - ETA: 1:16:23 - loss: 0.6656 - regression_loss: 0.5371 - classification_loss: 0.1286
 1188/10000 [==>...........................] - ETA: 1:16:22 - loss: 0.6656 - regression_loss: 0.5370 - classification_loss: 0.1286
 1189/10000 [==>...........................] - ETA: 1:16:21 - loss: 0.6656 - regression_loss: 0.5370 - classification_loss: 0.1286
 1190/10000 [==>...........................] - ETA: 1:16:20 - loss: 0.6664 - regression_loss: 0.5377 - classification_loss: 0.1287
 1191/10000 [==>...........................] - ETA: 1:16:19 - loss: 0.6661 - regression_loss: 0.5374 - classification_loss: 0.1286
 1192/10000 [==>...........................] - ETA: 1:16:18 - loss: 0.6657 - regression_loss: 0.5371 - classification_loss: 0.1286
 1193/10000 [==>...........................] - ETA: 1:16:17 - loss: 0.6661 - regression_loss: 0.5373 - classification_loss: 0.1288
 1194/10000 [==>...........................] - ETA: 1:16:16 - loss: 0.6659 - regression_loss: 0.5371 - classification_loss: 0.1288
 1195/10000 [==>...........................] - ETA: 1:16:15 - loss: 0.6658 - regression_loss: 0.5370 - classification_loss: 0.1288
 1196/10000 [==>...........................] - ETA: 1:16:15 - loss: 0.6658 - regression_loss: 0.5370 - classification_loss: 0.1288
 1197/10000 [==>...........................] - ETA: 1:16:14 - loss: 0.6660 - regression_loss: 0.5372 - classification_loss: 0.1288
 1198/10000 [==>...........................] - ETA: 1:16:13 - loss: 0.6663 - regression_loss: 0.5373 - classification_loss: 0.1290
 1199/10000 [==>...........................] - ETA: 1:16:12 - loss: 0.6660 - regression_loss: 0.5370 - classification_loss: 0.1289
 1200/10000 [==>...........................] - ETA: 1:16:11 - loss: 0.6662 - regression_loss: 0.5373 - classification_loss: 0.1289
 1201/10000 [==>...........................] - ETA: 1:16:10 - loss: 0.6664 - regression_loss: 0.5374 - classification_loss: 0.1290
 1202/10000 [==>...........................] - ETA: 1:16:09 - loss: 0.6663 - regression_loss: 0.5373 - classification_loss: 0.1290
 1203/10000 [==>...........................] - ETA: 1:16:08 - loss: 0.6663 - regression_loss: 0.5374 - classification_loss: 0.1289
 1204/10000 [==>...........................] - ETA: 1:16:07 - loss: 0.6663 - regression_loss: 0.5374 - classification_loss: 0.1289
 1205/10000 [==>...........................] - ETA: 1:16:06 - loss: 0.6659 - regression_loss: 0.5371 - classification_loss: 0.1288
 1206/10000 [==>...........................] - ETA: 1:16:05 - loss: 0.6661 - regression_loss: 0.5372 - classification_loss: 0.1288
 1207/10000 [==>...........................] - ETA: 1:16:04 - loss: 0.6657 - regression_loss: 0.5369 - classification_loss: 0.1288
 1208/10000 [==>...........................] - ETA: 1:16:03 - loss: 0.6657 - regression_loss: 0.5370 - classification_loss: 0.1287
 1209/10000 [==>...........................] - ETA: 1:16:03 - loss: 0.6653 - regression_loss: 0.5367 - classification_loss: 0.1287
 1210/10000 [==>...........................] - ETA: 1:16:02 - loss: 0.6650 - regression_loss: 0.5364 - classification_loss: 0.1286
 1211/10000 [==>...........................] - ETA: 1:16:01 - loss: 0.6650 - regression_loss: 0.5363 - classification_loss: 0.1287
 1212/10000 [==>...........................] - ETA: 1:16:01 - loss: 0.6654 - regression_loss: 0.5367 - classification_loss: 0.1288
 1213/10000 [==>...........................] - ETA: 1:16:00 - loss: 0.6662 - regression_loss: 0.5373 - classification_loss: 0.1289
 1214/10000 [==>...........................] - ETA: 1:15:58 - loss: 0.6662 - regression_loss: 0.5372 - classification_loss: 0.1289
 1215/10000 [==>...........................] - ETA: 1:15:57 - loss: 0.6658 - regression_loss: 0.5369 - classification_loss: 0.1289
 1216/10000 [==>...........................] - ETA: 1:15:56 - loss: 0.6658 - regression_loss: 0.5369 - classification_loss: 0.1289
 1217/10000 [==>...........................] - ETA: 1:15:55 - loss: 0.6662 - regression_loss: 0.5371 - classification_loss: 0.1290
 1218/10000 [==>...........................] - ETA: 1:15:54 - loss: 0.6662 - regression_loss: 0.5371 - classification_loss: 0.1290
 1219/10000 [==>...........................] - ETA: 1:15:54 - loss: 0.6658 - regression_loss: 0.5369 - classification_loss: 0.1290
 1220/10000 [==>...........................] - ETA: 1:15:53 - loss: 0.6658 - regression_loss: 0.5369 - classification_loss: 0.1289
 1221/10000 [==>...........................] - ETA: 1:15:52 - loss: 0.6654 - regression_loss: 0.5365 - classification_loss: 0.1289
 1222/10000 [==>...........................] - ETA: 1:15:51 - loss: 0.6652 - regression_loss: 0.5364 - classification_loss: 0.1289
 1223/10000 [==>...........................] - ETA: 1:15:50 - loss: 0.6654 - regression_loss: 0.5365 - classification_loss: 0.1289
 1224/10000 [==>...........................] - ETA: 1:15:49 - loss: 0.6650 - regression_loss: 0.5362 - classification_loss: 0.1288
 1225/10000 [==>...........................] - ETA: 1:15:48 - loss: 0.6648 - regression_loss: 0.5360 - classification_loss: 0.1287
 1226/10000 [==>...........................] - ETA: 1:15:47 - loss: 0.6650 - regression_loss: 0.5362 - classification_loss: 0.1287
 1227/10000 [==>...........................] - ETA: 1:15:46 - loss: 0.6656 - regression_loss: 0.5368 - classification_loss: 0.1288
 1228/10000 [==>...........................] - ETA: 1:15:45 - loss: 0.6657 - regression_loss: 0.5369 - classification_loss: 0.1288
 1229/10000 [==>...........................] - ETA: 1:15:44 - loss: 0.6655 - regression_loss: 0.5367 - classification_loss: 0.1288
 1230/10000 [==>...........................] - ETA: 1:15:43 - loss: 0.6654 - regression_loss: 0.5366 - classification_loss: 0.1287
 1231/10000 [==>...........................] - ETA: 1:15:42 - loss: 0.6656 - regression_loss: 0.5368 - classification_loss: 0.1288
 1232/10000 [==>...........................] - ETA: 1:15:42 - loss: 0.6657 - regression_loss: 0.5370 - classification_loss: 0.1288
 1233/10000 [==>...........................] - ETA: 1:15:41 - loss: 0.6655 - regression_loss: 0.5367 - classification_loss: 0.1288
 1234/10000 [==>...........................] - ETA: 1:15:40 - loss: 0.6656 - regression_loss: 0.5369 - classification_loss: 0.1287
 1235/10000 [==>...........................] - ETA: 1:15:39 - loss: 0.6655 - regression_loss: 0.5367 - classification_loss: 0.1287
 1236/10000 [==>...........................] - ETA: 1:15:38 - loss: 0.6658 - regression_loss: 0.5370 - classification_loss: 0.1287
 1237/10000 [==>...........................] - ETA: 1:15:37 - loss: 0.6658 - regression_loss: 0.5370 - classification_loss: 0.1288
 1238/10000 [==>...........................] - ETA: 1:15:36 - loss: 0.6654 - regression_loss: 0.5366 - classification_loss: 0.1288
 1239/10000 [==>...........................] - ETA: 1:15:35 - loss: 0.6653 - regression_loss: 0.5366 - classification_loss: 0.1288
 1240/10000 [==>...........................] - ETA: 1:15:34 - loss: 0.6653 - regression_loss: 0.5366 - classification_loss: 0.1287
 1241/10000 [==>...........................] - ETA: 1:15:33 - loss: 0.6658 - regression_loss: 0.5370 - classification_loss: 0.1288
 1242/10000 [==>...........................] - ETA: 1:15:33 - loss: 0.6653 - regression_loss: 0.5366 - classification_loss: 0.1287
 1243/10000 [==>...........................] - ETA: 1:15:32 - loss: 0.6655 - regression_loss: 0.5366 - classification_loss: 0.1288
 1244/10000 [==>...........................] - ETA: 1:15:31 - loss: 0.6657 - regression_loss: 0.5369 - classification_loss: 0.1288
 1245/10000 [==>...........................] - ETA: 1:15:34 - loss: 0.6655 - regression_loss: 0.5368 - classification_loss: 0.1287
 1246/10000 [==>...........................] - ETA: 1:15:33 - loss: 0.6655 - regression_loss: 0.5368 - classification_loss: 0.1287
 1247/10000 [==>...........................] - ETA: 1:15:32 - loss: 0.6653 - regression_loss: 0.5366 - classification_loss: 0.1287
 1248/10000 [==>...........................] - ETA: 1:15:35 - loss: 0.6654 - regression_loss: 0.5367 - classification_loss: 0.1287
 1249/10000 [==>...........................] - ETA: 1:15:34 - loss: 0.6653 - regression_loss: 0.5366 - classification_loss: 0.1287
 1250/10000 [==>...........................] - ETA: 1:15:33 - loss: 0.6657 - regression_loss: 0.5370 - classification_loss: 0.1287
 1251/10000 [==>...........................] - ETA: 1:15:32 - loss: 0.6656 - regression_loss: 0.5370 - classification_loss: 0.1286
 1252/10000 [==>...........................] - ETA: 1:15:31 - loss: 0.6655 - regression_loss: 0.5369 - classification_loss: 0.1286
 1253/10000 [==>...........................] - ETA: 1:15:33 - loss: 0.6653 - regression_loss: 0.5367 - classification_loss: 0.1286
 1254/10000 [==>...........................] - ETA: 1:15:32 - loss: 0.6653 - regression_loss: 0.5367 - classification_loss: 0.1286
 1255/10000 [==>...........................] - ETA: 1:15:31 - loss: 0.6653 - regression_loss: 0.5367 - classification_loss: 0.1286
 1256/10000 [==>...........................] - ETA: 1:15:30 - loss: 0.6653 - regression_loss: 0.5367 - classification_loss: 0.1285
 1257/10000 [==>...........................] - ETA: 1:15:30 - loss: 0.6655 - regression_loss: 0.5370 - classification_loss: 0.1285
 1258/10000 [==>...........................] - ETA: 1:15:29 - loss: 0.6652 - regression_loss: 0.5367 - classification_loss: 0.1285
 1259/10000 [==>...........................] - ETA: 1:15:28 - loss: 0.6649 - regression_loss: 0.5364 - classification_loss: 0.1284
 1260/10000 [==>...........................] - ETA: 1:15:27 - loss: 0.6651 - regression_loss: 0.5367 - classification_loss: 0.1284
 1261/10000 [==>...........................] - ETA: 1:15:26 - loss: 0.6650 - regression_loss: 0.5367 - classification_loss: 0.1283
 1262/10000 [==>...........................] - ETA: 1:15:25 - loss: 0.6647 - regression_loss: 0.5364 - classification_loss: 0.1283
 1263/10000 [==>...........................] - ETA: 1:15:24 - loss: 0.6654 - regression_loss: 0.5370 - classification_loss: 0.1285
 1264/10000 [==>...........................] - ETA: 1:15:23 - loss: 0.6657 - regression_loss: 0.5372 - classification_loss: 0.1285
 1265/10000 [==>...........................] - ETA: 1:15:22 - loss: 0.6654 - regression_loss: 0.5369 - classification_loss: 0.1284
 1266/10000 [==>...........................] - ETA: 1:15:21 - loss: 0.6652 - regression_loss: 0.5368 - classification_loss: 0.1284
 1267/10000 [==>...........................] - ETA: 1:15:20 - loss: 0.6653 - regression_loss: 0.5367 - classification_loss: 0.1286
 1268/10000 [==>...........................] - ETA: 1:15:19 - loss: 0.6653 - regression_loss: 0.5367 - classification_loss: 0.1285
 1269/10000 [==>...........................] - ETA: 1:15:18 - loss: 0.6659 - regression_loss: 0.5372 - classification_loss: 0.1287
 1270/10000 [==>...........................] - ETA: 1:15:17 - loss: 0.6658 - regression_loss: 0.5372 - classification_loss: 0.1286
 1271/10000 [==>...........................] - ETA: 1:15:16 - loss: 0.6661 - regression_loss: 0.5374 - classification_loss: 0.1287
 1272/10000 [==>...........................] - ETA: 1:15:15 - loss: 0.6660 - regression_loss: 0.5374 - classification_loss: 0.1287
 1273/10000 [==>...........................] - ETA: 1:15:14 - loss: 0.6665 - regression_loss: 0.5377 - classification_loss: 0.1288
 1274/10000 [==>...........................] - ETA: 1:15:13 - loss: 0.6667 - regression_loss: 0.5379 - classification_loss: 0.1288
 1275/10000 [==>...........................] - ETA: 1:15:12 - loss: 0.6667 - regression_loss: 0.5380 - classification_loss: 0.1287
 1276/10000 [==>...........................] - ETA: 1:15:11 - loss: 0.6668 - regression_loss: 0.5382 - classification_loss: 0.1287
 1277/10000 [==>...........................] - ETA: 1:15:10 - loss: 0.6673 - regression_loss: 0.5385 - classification_loss: 0.1288
 1278/10000 [==>...........................] - ETA: 1:15:09 - loss: 0.6676 - regression_loss: 0.5388 - classification_loss: 0.1288
 1279/10000 [==>...........................] - ETA: 1:15:08 - loss: 0.6673 - regression_loss: 0.5386 - classification_loss: 0.1288
 1280/10000 [==>...........................] - ETA: 1:15:07 - loss: 0.6673 - regression_loss: 0.5386 - classification_loss: 0.1287
 1281/10000 [==>...........................] - ETA: 1:15:06 - loss: 0.6670 - regression_loss: 0.5383 - classification_loss: 0.1287
 1282/10000 [==>...........................] - ETA: 1:15:05 - loss: 0.6666 - regression_loss: 0.5380 - classification_loss: 0.1286
 1283/10000 [==>...........................] - ETA: 1:15:05 - loss: 0.6667 - regression_loss: 0.5381 - classification_loss: 0.1286
 1284/10000 [==>...........................] - ETA: 1:15:04 - loss: 0.6663 - regression_loss: 0.5378 - classification_loss: 0.1285
 1285/10000 [==>...........................] - ETA: 1:15:03 - loss: 0.6670 - regression_loss: 0.5383 - classification_loss: 0.1286
 1286/10000 [==>...........................] - ETA: 1:15:02 - loss: 0.6666 - regression_loss: 0.5381 - classification_loss: 0.1286
 1287/10000 [==>...........................] - ETA: 1:15:02 - loss: 0.6664 - regression_loss: 0.5379 - classification_loss: 0.1285
 1288/10000 [==>...........................] - ETA: 1:15:01 - loss: 0.6669 - regression_loss: 0.5383 - classification_loss: 0.1286
 1289/10000 [==>...........................] - ETA: 1:15:00 - loss: 0.6667 - regression_loss: 0.5382 - classification_loss: 0.1285
 1290/10000 [==>...........................] - ETA: 1:14:59 - loss: 0.6669 - regression_loss: 0.5383 - classification_loss: 0.1285
 1291/10000 [==>...........................] - ETA: 1:14:59 - loss: 0.6665 - regression_loss: 0.5381 - classification_loss: 0.1285
 1292/10000 [==>...........................] - ETA: 1:14:58 - loss: 0.6663 - regression_loss: 0.5379 - classification_loss: 0.1284
 1293/10000 [==>...........................] - ETA: 1:14:57 - loss: 0.6663 - regression_loss: 0.5379 - classification_loss: 0.1284
 1294/10000 [==>...........................] - ETA: 1:14:56 - loss: 0.6664 - regression_loss: 0.5380 - classification_loss: 0.1284
 1295/10000 [==>...........................] - ETA: 1:14:55 - loss: 0.6663 - regression_loss: 0.5379 - classification_loss: 0.1284
 1296/10000 [==>...........................] - ETA: 1:14:54 - loss: 0.6669 - regression_loss: 0.5382 - classification_loss: 0.1287
 1297/10000 [==>...........................] - ETA: 1:14:53 - loss: 0.6667 - regression_loss: 0.5381 - classification_loss: 0.1286
 1298/10000 [==>...........................] - ETA: 1:14:52 - loss: 0.6664 - regression_loss: 0.5379 - classification_loss: 0.1286
 1299/10000 [==>...........................] - ETA: 1:14:51 - loss: 0.6669 - regression_loss: 0.5384 - classification_loss: 0.1286
 1300/10000 [==>...........................] - ETA: 1:14:50 - loss: 0.6667 - regression_loss: 0.5382 - classification_loss: 0.1285
 1301/10000 [==>...........................] - ETA: 1:14:50 - loss: 0.6669 - regression_loss: 0.5384 - classification_loss: 0.1285
 1302/10000 [==>...........................] - ETA: 1:14:51 - loss: 0.6664 - regression_loss: 0.5380 - classification_loss: 0.1284
 1303/10000 [==>...........................] - ETA: 1:14:51 - loss: 0.6661 - regression_loss: 0.5378 - classification_loss: 0.1283
 1304/10000 [==>...........................] - ETA: 1:14:50 - loss: 0.6666 - regression_loss: 0.5382 - classification_loss: 0.1283
 1305/10000 [==>...........................] - ETA: 1:14:49 - loss: 0.6664 - regression_loss: 0.5381 - classification_loss: 0.1284
 1306/10000 [==>...........................] - ETA: 1:14:48 - loss: 0.6667 - regression_loss: 0.5383 - classification_loss: 0.1284
 1307/10000 [==>...........................] - ETA: 1:14:47 - loss: 0.6670 - regression_loss: 0.5386 - classification_loss: 0.1284
 1308/10000 [==>...........................] - ETA: 1:14:46 - loss: 0.6671 - regression_loss: 0.5387 - classification_loss: 0.1284
 1309/10000 [==>...........................] - ETA: 1:14:45 - loss: 0.6670 - regression_loss: 0.5386 - classification_loss: 0.1284
 1310/10000 [==>...........................] - ETA: 1:14:44 - loss: 0.6666 - regression_loss: 0.5383 - classification_loss: 0.1283
 1311/10000 [==>...........................] - ETA: 1:14:43 - loss: 0.6663 - regression_loss: 0.5380 - classification_loss: 0.1283
 1312/10000 [==>...........................] - ETA: 1:14:42 - loss: 0.6660 - regression_loss: 0.5378 - classification_loss: 0.1282
 1313/10000 [==>...........................] - ETA: 1:14:42 - loss: 0.6666 - regression_loss: 0.5381 - classification_loss: 0.1285
 1314/10000 [==>...........................] - ETA: 1:14:41 - loss: 0.6671 - regression_loss: 0.5385 - classification_loss: 0.1286
 1315/10000 [==>...........................] - ETA: 1:14:40 - loss: 0.6668 - regression_loss: 0.5383 - classification_loss: 0.1285
 1316/10000 [==>...........................] - ETA: 1:14:39 - loss: 0.6669 - regression_loss: 0.5384 - classification_loss: 0.1285
 1317/10000 [==>...........................] - ETA: 1:14:39 - loss: 0.6667 - regression_loss: 0.5383 - classification_loss: 0.1284
 1318/10000 [==>...........................] - ETA: 1:14:38 - loss: 0.6664 - regression_loss: 0.5380 - classification_loss: 0.1283
 1319/10000 [==>...........................] - ETA: 1:14:37 - loss: 0.6667 - regression_loss: 0.5383 - classification_loss: 0.1284
 1320/10000 [==>...........................] - ETA: 1:14:36 - loss: 0.6666 - regression_loss: 0.5382 - classification_loss: 0.1284
 1321/10000 [==>...........................] - ETA: 1:14:36 - loss: 0.6669 - regression_loss: 0.5385 - classification_loss: 0.1284
 1322/10000 [==>...........................] - ETA: 1:14:35 - loss: 0.6672 - regression_loss: 0.5388 - classification_loss: 0.1284
 1323/10000 [==>...........................] - ETA: 1:14:34 - loss: 0.6675 - regression_loss: 0.5390 - classification_loss: 0.1285
 1324/10000 [==>...........................] - ETA: 1:14:33 - loss: 0.6675 - regression_loss: 0.5388 - classification_loss: 0.1286
 1325/10000 [==>...........................] - ETA: 1:14:32 - loss: 0.6680 - regression_loss: 0.5393 - classification_loss: 0.1287
 1326/10000 [==>...........................] - ETA: 1:14:32 - loss: 0.6678 - regression_loss: 0.5391 - classification_loss: 0.1287
 1327/10000 [==>...........................] - ETA: 1:14:31 - loss: 0.6677 - regression_loss: 0.5391 - classification_loss: 0.1286
 1328/10000 [==>...........................] - ETA: 1:14:30 - loss: 0.6676 - regression_loss: 0.5390 - classification_loss: 0.1286
 1329/10000 [==>...........................] - ETA: 1:14:29 - loss: 0.6675 - regression_loss: 0.5388 - classification_loss: 0.1287
 1330/10000 [==>...........................] - ETA: 1:14:28 - loss: 0.6673 - regression_loss: 0.5386 - classification_loss: 0.1287
 1331/10000 [==>...........................] - ETA: 1:14:27 - loss: 0.6671 - regression_loss: 0.5385 - classification_loss: 0.1286
 1332/10000 [==>...........................] - ETA: 1:14:30 - loss: 0.6675 - regression_loss: 0.5387 - classification_loss: 0.1287
 1333/10000 [==>...........................] - ETA: 1:14:29 - loss: 0.6673 - regression_loss: 0.5385 - classification_loss: 0.1287
 1334/10000 [===>..........................] - ETA: 1:14:28 - loss: 0.6676 - regression_loss: 0.5388 - classification_loss: 0.1288
 1335/10000 [===>..........................] - ETA: 1:14:31 - loss: 0.6678 - regression_loss: 0.5390 - classification_loss: 0.1288
 1336/10000 [===>..........................] - ETA: 1:14:30 - loss: 0.6684 - regression_loss: 0.5396 - classification_loss: 0.1288
 1337/10000 [===>..........................] - ETA: 1:14:30 - loss: 0.6681 - regression_loss: 0.5393 - classification_loss: 0.1287
 1338/10000 [===>..........................] - ETA: 1:14:29 - loss: 0.6680 - regression_loss: 0.5393 - classification_loss: 0.1287
 1339/10000 [===>..........................] - ETA: 1:14:28 - loss: 0.6690 - regression_loss: 0.5400 - classification_loss: 0.1290
 1340/10000 [===>..........................] - ETA: 1:14:27 - loss: 0.6691 - regression_loss: 0.5400 - classification_loss: 0.1291
 1341/10000 [===>..........................] - ETA: 1:14:27 - loss: 0.6693 - regression_loss: 0.5402 - classification_loss: 0.1291
 1342/10000 [===>..........................] - ETA: 1:14:26 - loss: 0.6693 - regression_loss: 0.5401 - classification_loss: 0.1292
 1343/10000 [===>..........................] - ETA: 1:14:24 - loss: 0.6692 - regression_loss: 0.5400 - classification_loss: 0.1292
 1344/10000 [===>..........................] - ETA: 1:14:23 - loss: 0.6694 - regression_loss: 0.5402 - classification_loss: 0.1292
 1345/10000 [===>..........................] - ETA: 1:14:23 - loss: 0.6695 - regression_loss: 0.5400 - classification_loss: 0.1294
 1346/10000 [===>..........................] - ETA: 1:14:22 - loss: 0.6693 - regression_loss: 0.5399 - classification_loss: 0.1294
 1347/10000 [===>..........................] - ETA: 1:14:21 - loss: 0.6696 - regression_loss: 0.5401 - classification_loss: 0.1295
 1348/10000 [===>..........................] - ETA: 1:14:20 - loss: 0.6704 - regression_loss: 0.5406 - classification_loss: 0.1298
 1349/10000 [===>..........................] - ETA: 1:14:19 - loss: 0.6705 - regression_loss: 0.5407 - classification_loss: 0.1298
 1350/10000 [===>..........................] - ETA: 1:14:18 - loss: 0.6712 - regression_loss: 0.5413 - classification_loss: 0.1299
 1351/10000 [===>..........................] - ETA: 1:14:17 - loss: 0.6712 - regression_loss: 0.5413 - classification_loss: 0.1299
 1352/10000 [===>..........................] - ETA: 1:14:16 - loss: 0.6709 - regression_loss: 0.5410 - classification_loss: 0.1299
 1353/10000 [===>..........................] - ETA: 1:14:15 - loss: 0.6709 - regression_loss: 0.5410 - classification_loss: 0.1299
 1354/10000 [===>..........................] - ETA: 1:14:14 - loss: 0.6710 - regression_loss: 0.5411 - classification_loss: 0.1299
 1355/10000 [===>..........................] - ETA: 1:14:13 - loss: 0.6709 - regression_loss: 0.5411 - classification_loss: 0.1299
 1356/10000 [===>..........................] - ETA: 1:14:12 - loss: 0.6717 - regression_loss: 0.5417 - classification_loss: 0.1300
 1357/10000 [===>..........................] - ETA: 1:14:12 - loss: 0.6718 - regression_loss: 0.5418 - classification_loss: 0.1299
 1358/10000 [===>..........................] - ETA: 1:14:11 - loss: 0.6715 - regression_loss: 0.5416 - classification_loss: 0.1299
 1359/10000 [===>..........................] - ETA: 1:14:11 - loss: 0.6713 - regression_loss: 0.5415 - classification_loss: 0.1299
 1360/10000 [===>..........................] - ETA: 1:14:10 - loss: 0.6709 - regression_loss: 0.5411 - classification_loss: 0.1298
 1361/10000 [===>..........................] - ETA: 1:14:09 - loss: 0.6705 - regression_loss: 0.5408 - classification_loss: 0.1297
 1362/10000 [===>..........................] - ETA: 1:14:12 - loss: 0.6702 - regression_loss: 0.5406 - classification_loss: 0.1296
 1363/10000 [===>..........................] - ETA: 1:14:11 - loss: 0.6707 - regression_loss: 0.5406 - classification_loss: 0.1301
 1364/10000 [===>..........................] - ETA: 1:14:10 - loss: 0.6706 - regression_loss: 0.5404 - classification_loss: 0.1301
 1365/10000 [===>..........................] - ETA: 1:14:09 - loss: 0.6706 - regression_loss: 0.5403 - classification_loss: 0.1303
 1366/10000 [===>..........................] - ETA: 1:14:08 - loss: 0.6706 - regression_loss: 0.5404 - classification_loss: 0.1303
 1367/10000 [===>..........................] - ETA: 1:14:08 - loss: 0.6703 - regression_loss: 0.5401 - classification_loss: 0.1302
 1368/10000 [===>..........................] - ETA: 1:14:07 - loss: 0.6702 - regression_loss: 0.5399 - classification_loss: 0.1303
 1369/10000 [===>..........................] - ETA: 1:14:06 - loss: 0.6697 - regression_loss: 0.5395 - classification_loss: 0.1302
 1370/10000 [===>..........................] - ETA: 1:14:05 - loss: 0.6696 - regression_loss: 0.5395 - classification_loss: 0.1301
 1371/10000 [===>..........................] - ETA: 1:14:04 - loss: 0.6699 - regression_loss: 0.5396 - classification_loss: 0.1303
 1372/10000 [===>..........................] - ETA: 1:14:04 - loss: 0.6701 - regression_loss: 0.5399 - classification_loss: 0.1303
 1373/10000 [===>..........................] - ETA: 1:14:03 - loss: 0.6704 - regression_loss: 0.5402 - classification_loss: 0.1302
 1374/10000 [===>..........................] - ETA: 1:14:02 - loss: 0.6705 - regression_loss: 0.5402 - classification_loss: 0.1303
 1375/10000 [===>..........................] - ETA: 1:14:00 - loss: 0.6713 - regression_loss: 0.5406 - classification_loss: 0.1307
 1376/10000 [===>..........................] - ETA: 1:13:59 - loss: 0.6713 - regression_loss: 0.5406 - classification_loss: 0.1306
 1377/10000 [===>..........................] - ETA: 1:13:59 - loss: 0.6719 - regression_loss: 0.5412 - classification_loss: 0.1307
 1378/10000 [===>..........................] - ETA: 1:13:58 - loss: 0.6716 - regression_loss: 0.5409 - classification_loss: 0.1307
 1379/10000 [===>..........................] - ETA: 1:13:57 - loss: 0.6716 - regression_loss: 0.5410 - classification_loss: 0.1307
 1380/10000 [===>..........................] - ETA: 1:13:56 - loss: 0.6714 - regression_loss: 0.5407 - classification_loss: 0.1307
 1381/10000 [===>..........................] - ETA: 1:13:55 - loss: 0.6717 - regression_loss: 0.5410 - classification_loss: 0.1307
 1382/10000 [===>..........................] - ETA: 1:13:54 - loss: 0.6714 - regression_loss: 0.5407 - classification_loss: 0.1307
 1383/10000 [===>..........................] - ETA: 1:13:53 - loss: 0.6719 - regression_loss: 0.5411 - classification_loss: 0.1308
 1384/10000 [===>..........................] - ETA: 1:13:52 - loss: 0.6720 - regression_loss: 0.5412 - classification_loss: 0.1308
 1385/10000 [===>..........................] - ETA: 1:13:51 - loss: 0.6724 - regression_loss: 0.5416 - classification_loss: 0.1308
 1386/10000 [===>..........................] - ETA: 1:13:51 - loss: 0.6722 - regression_loss: 0.5414 - classification_loss: 0.1308
 1387/10000 [===>..........................] - ETA: 1:13:50 - loss: 0.6719 - regression_loss: 0.5411 - classification_loss: 0.1307
 1388/10000 [===>..........................] - ETA: 1:13:49 - loss: 0.6719 - regression_loss: 0.5411 - classification_loss: 0.1308
 1389/10000 [===>..........................] - ETA: 1:13:48 - loss: 0.6721 - regression_loss: 0.5413 - classification_loss: 0.1308
 1390/10000 [===>..........................] - ETA: 1:13:47 - loss: 0.6724 - regression_loss: 0.5415 - classification_loss: 0.1309
 1391/10000 [===>..........................] - ETA: 1:13:46 - loss: 0.6723 - regression_loss: 0.5414 - classification_loss: 0.1309
 1392/10000 [===>..........................] - ETA: 1:13:46 - loss: 0.6720 - regression_loss: 0.5411 - classification_loss: 0.1309
 1393/10000 [===>..........................] - ETA: 1:13:45 - loss: 0.6720 - regression_loss: 0.5412 - classification_loss: 0.1308
 1394/10000 [===>..........................] - ETA: 1:13:44 - loss: 0.6719 - regression_loss: 0.5410 - classification_loss: 0.1308
 1395/10000 [===>..........................] - ETA: 1:13:43 - loss: 0.6717 - regression_loss: 0.5410 - classification_loss: 0.1308
 1396/10000 [===>..........................] - ETA: 1:13:42 - loss: 0.6717 - regression_loss: 0.5409 - classification_loss: 0.1307
 1397/10000 [===>..........................] - ETA: 1:13:41 - loss: 0.6715 - regression_loss: 0.5408 - classification_loss: 0.1307
 1398/10000 [===>..........................] - ETA: 1:13:40 - loss: 0.6714 - regression_loss: 0.5407 - classification_loss: 0.1307
 1399/10000 [===>..........................] - ETA: 1:13:39 - loss: 0.6714 - regression_loss: 0.5407 - classification_loss: 0.1307
 1400/10000 [===>..........................] - ETA: 1:13:38 - loss: 0.6714 - regression_loss: 0.5407 - classification_loss: 0.1308
 1401/10000 [===>..........................] - ETA: 1:13:37 - loss: 0.6715 - regression_loss: 0.5408 - classification_loss: 0.1308
 1402/10000 [===>..........................] - ETA: 1:13:36 - loss: 0.6720 - regression_loss: 0.5411 - classification_loss: 0.1309
 1403/10000 [===>..........................] - ETA: 1:13:37 - loss: 0.6717 - regression_loss: 0.5409 - classification_loss: 0.1308
 1404/10000 [===>..........................] - ETA: 1:13:37 - loss: 0.6718 - regression_loss: 0.5410 - classification_loss: 0.1308
 1405/10000 [===>..........................] - ETA: 1:13:36 - loss: 0.6718 - regression_loss: 0.5410 - classification_loss: 0.1308
 1406/10000 [===>..........................] - ETA: 1:13:35 - loss: 0.6723 - regression_loss: 0.5415 - classification_loss: 0.1308
 1407/10000 [===>..........................] - ETA: 1:13:35 - loss: 0.6721 - regression_loss: 0.5413 - classification_loss: 0.1307
 1408/10000 [===>..........................] - ETA: 1:13:34 - loss: 0.6719 - regression_loss: 0.5411 - classification_loss: 0.1308
 1409/10000 [===>..........................] - ETA: 1:13:33 - loss: 0.6716 - regression_loss: 0.5409 - classification_loss: 0.1307
 1410/10000 [===>..........................] - ETA: 1:13:32 - loss: 0.6718 - regression_loss: 0.5411 - classification_loss: 0.1307
 1411/10000 [===>..........................] - ETA: 1:13:32 - loss: 0.6718 - regression_loss: 0.5412 - classification_loss: 0.1307
 1412/10000 [===>..........................] - ETA: 1:13:31 - loss: 0.6718 - regression_loss: 0.5411 - classification_loss: 0.1306
 1413/10000 [===>..........................] - ETA: 1:13:30 - loss: 0.6715 - regression_loss: 0.5410 - classification_loss: 0.1306
 1414/10000 [===>..........................] - ETA: 1:13:29 - loss: 0.6714 - regression_loss: 0.5407 - classification_loss: 0.1306
 1415/10000 [===>..........................] - ETA: 1:13:28 - loss: 0.6712 - regression_loss: 0.5406 - classification_loss: 0.1306
 1416/10000 [===>..........................] - ETA: 1:13:27 - loss: 0.6712 - regression_loss: 0.5406 - classification_loss: 0.1306
 1417/10000 [===>..........................] - ETA: 1:13:27 - loss: 0.6712 - regression_loss: 0.5406 - classification_loss: 0.1306
 1418/10000 [===>..........................] - ETA: 1:13:26 - loss: 0.6709 - regression_loss: 0.5404 - classification_loss: 0.1305
 1419/10000 [===>..........................] - ETA: 1:13:25 - loss: 0.6710 - regression_loss: 0.5405 - classification_loss: 0.1305
 1420/10000 [===>..........................] - ETA: 1:13:24 - loss: 0.6713 - regression_loss: 0.5408 - classification_loss: 0.1305
 1421/10000 [===>..........................] - ETA: 1:13:24 - loss: 0.6712 - regression_loss: 0.5407 - classification_loss: 0.1304
 1422/10000 [===>..........................] - ETA: 1:13:23 - loss: 0.6712 - regression_loss: 0.5408 - classification_loss: 0.1304
 1423/10000 [===>..........................] - ETA: 1:13:22 - loss: 0.6709 - regression_loss: 0.5405 - classification_loss: 0.1304
 1424/10000 [===>..........................] - ETA: 1:13:21 - loss: 0.6708 - regression_loss: 0.5405 - classification_loss: 0.1304
 1425/10000 [===>..........................] - ETA: 1:13:20 - loss: 0.6705 - regression_loss: 0.5402 - classification_loss: 0.1303
 1426/10000 [===>..........................] - ETA: 1:13:19 - loss: 0.6707 - regression_loss: 0.5404 - classification_loss: 0.1304
 1427/10000 [===>..........................] - ETA: 1:13:19 - loss: 0.6709 - regression_loss: 0.5405 - classification_loss: 0.1304
 1428/10000 [===>..........................] - ETA: 1:13:18 - loss: 0.6711 - regression_loss: 0.5407 - classification_loss: 0.1304
 1429/10000 [===>..........................] - ETA: 1:13:18 - loss: 0.6711 - regression_loss: 0.5406 - classification_loss: 0.1305
 1430/10000 [===>..........................] - ETA: 1:13:17 - loss: 0.6709 - regression_loss: 0.5404 - classification_loss: 0.1305
 1431/10000 [===>..........................] - ETA: 1:13:19 - loss: 0.6712 - regression_loss: 0.5407 - classification_loss: 0.1304
 1432/10000 [===>..........................] - ETA: 1:13:18 - loss: 0.6714 - regression_loss: 0.5409 - classification_loss: 0.1305
 1433/10000 [===>..........................] - ETA: 1:13:17 - loss: 0.6716 - regression_loss: 0.5410 - classification_loss: 0.1306
 1434/10000 [===>..........................] - ETA: 1:13:17 - loss: 0.6715 - regression_loss: 0.5409 - classification_loss: 0.1306
 1435/10000 [===>..........................] - ETA: 1:13:16 - loss: 0.6712 - regression_loss: 0.5406 - classification_loss: 0.1306
 1436/10000 [===>..........................] - ETA: 1:13:15 - loss: 0.6713 - regression_loss: 0.5407 - classification_loss: 0.1305
 1437/10000 [===>..........................] - ETA: 1:13:14 - loss: 0.6717 - regression_loss: 0.5410 - classification_loss: 0.1307
 1438/10000 [===>..........................] - ETA: 1:13:13 - loss: 0.6716 - regression_loss: 0.5410 - classification_loss: 0.1306
 1439/10000 [===>..........................] - ETA: 1:13:13 - loss: 0.6717 - regression_loss: 0.5411 - classification_loss: 0.1306
 1440/10000 [===>..........................] - ETA: 1:13:12 - loss: 0.6722 - regression_loss: 0.5415 - classification_loss: 0.1307
 1441/10000 [===>..........................] - ETA: 1:13:11 - loss: 0.6720 - regression_loss: 0.5414 - classification_loss: 0.1307
 1442/10000 [===>..........................] - ETA: 1:13:10 - loss: 0.6719 - regression_loss: 0.5413 - classification_loss: 0.1306
 1443/10000 [===>..........................] - ETA: 1:13:09 - loss: 0.6724 - regression_loss: 0.5417 - classification_loss: 0.1307
 1444/10000 [===>..........................] - ETA: 1:13:09 - loss: 0.6723 - regression_loss: 0.5417 - classification_loss: 0.1306
 1445/10000 [===>..........................] - ETA: 1:13:11 - loss: 0.6722 - regression_loss: 0.5416 - classification_loss: 0.1306
 1446/10000 [===>..........................] - ETA: 1:13:10 - loss: 0.6725 - regression_loss: 0.5417 - classification_loss: 0.1308
 1447/10000 [===>..........................] - ETA: 1:13:10 - loss: 0.6724 - regression_loss: 0.5417 - classification_loss: 0.1308
 1448/10000 [===>..........................] - ETA: 1:13:09 - loss: 0.6723 - regression_loss: 0.5416 - classification_loss: 0.1307
 1449/10000 [===>..........................] - ETA: 1:13:08 - loss: 0.6730 - regression_loss: 0.5422 - classification_loss: 0.1308
 1450/10000 [===>..........................] - ETA: 1:13:07 - loss: 0.6729 - regression_loss: 0.5421 - classification_loss: 0.1308
 1451/10000 [===>..........................] - ETA: 1:13:07 - loss: 0.6732 - regression_loss: 0.5424 - classification_loss: 0.1308
 1452/10000 [===>..........................] - ETA: 1:13:06 - loss: 0.6731 - regression_loss: 0.5423 - classification_loss: 0.1308
 1453/10000 [===>..........................] - ETA: 1:13:05 - loss: 0.6728 - regression_loss: 0.5420 - classification_loss: 0.1308
 1454/10000 [===>..........................] - ETA: 1:13:04 - loss: 0.6727 - regression_loss: 0.5418 - classification_loss: 0.1309
 1455/10000 [===>..........................] - ETA: 1:13:03 - loss: 0.6724 - regression_loss: 0.5415 - classification_loss: 0.1308
 1456/10000 [===>..........................] - ETA: 1:13:03 - loss: 0.6721 - regression_loss: 0.5414 - classification_loss: 0.1308
 1457/10000 [===>..........................] - ETA: 1:13:02 - loss: 0.6722 - regression_loss: 0.5414 - classification_loss: 0.1309
 1458/10000 [===>..........................] - ETA: 1:13:01 - loss: 0.6726 - regression_loss: 0.5413 - classification_loss: 0.1313
 1459/10000 [===>..........................] - ETA: 1:13:00 - loss: 0.6730 - regression_loss: 0.5417 - classification_loss: 0.1313
 1460/10000 [===>..........................] - ETA: 1:12:59 - loss: 0.6735 - regression_loss: 0.5421 - classification_loss: 0.1314
 1461/10000 [===>..........................] - ETA: 1:12:58 - loss: 0.6734 - regression_loss: 0.5420 - classification_loss: 0.1314
 1462/10000 [===>..........................] - ETA: 1:12:57 - loss: 0.6730 - regression_loss: 0.5416 - classification_loss: 0.1313
 1463/10000 [===>..........................] - ETA: 1:12:56 - loss: 0.6727 - regression_loss: 0.5414 - classification_loss: 0.1313
 1464/10000 [===>..........................] - ETA: 1:12:55 - loss: 0.6726 - regression_loss: 0.5413 - classification_loss: 0.1312
 1465/10000 [===>..........................] - ETA: 1:12:56 - loss: 0.6735 - regression_loss: 0.5423 - classification_loss: 0.1312
 1466/10000 [===>..........................] - ETA: 1:12:56 - loss: 0.6733 - regression_loss: 0.5421 - classification_loss: 0.1312
 1467/10000 [===>..........................] - ETA: 1:12:55 - loss: 0.6737 - regression_loss: 0.5423 - classification_loss: 0.1314
 1468/10000 [===>..........................] - ETA: 1:12:54 - loss: 0.6737 - regression_loss: 0.5422 - classification_loss: 0.1314
 1469/10000 [===>..........................] - ETA: 1:12:53 - loss: 0.6735 - regression_loss: 0.5421 - classification_loss: 0.1314
 1470/10000 [===>..........................] - ETA: 1:12:52 - loss: 0.6734 - regression_loss: 0.5420 - classification_loss: 0.1314
 1471/10000 [===>..........................] - ETA: 1:12:51 - loss: 0.6740 - regression_loss: 0.5426 - classification_loss: 0.1314
 1472/10000 [===>..........................] - ETA: 1:12:51 - loss: 0.6737 - regression_loss: 0.5423 - classification_loss: 0.1314
 1473/10000 [===>..........................] - ETA: 1:12:50 - loss: 0.6735 - regression_loss: 0.5421 - classification_loss: 0.1313
 1474/10000 [===>..........................] - ETA: 1:12:49 - loss: 0.6734 - regression_loss: 0.5420 - classification_loss: 0.1313
 1475/10000 [===>..........................] - ETA: 1:12:48 - loss: 0.6731 - regression_loss: 0.5418 - classification_loss: 0.1313
 1476/10000 [===>..........................] - ETA: 1:12:47 - loss: 0.6730 - regression_loss: 0.5418 - classification_loss: 0.1313
 1477/10000 [===>..........................] - ETA: 1:12:47 - loss: 0.6729 - regression_loss: 0.5416 - classification_loss: 0.1312
 1478/10000 [===>..........................] - ETA: 1:12:46 - loss: 0.6727 - regression_loss: 0.5416 - classification_loss: 0.1312
 1479/10000 [===>..........................] - ETA: 1:12:45 - loss: 0.6727 - regression_loss: 0.5415 - classification_loss: 0.1312
 1480/10000 [===>..........................] - ETA: 1:12:44 - loss: 0.6730 - regression_loss: 0.5418 - classification_loss: 0.1312
 1481/10000 [===>..........................] - ETA: 1:12:43 - loss: 0.6727 - regression_loss: 0.5415 - classification_loss: 0.1312
 1482/10000 [===>..........................] - ETA: 1:12:43 - loss: 0.6725 - regression_loss: 0.5414 - classification_loss: 0.1311
 1483/10000 [===>..........................] - ETA: 1:12:42 - loss: 0.6723 - regression_loss: 0.5412 - classification_loss: 0.1311
 1484/10000 [===>..........................] - ETA: 1:12:41 - loss: 0.6722 - regression_loss: 0.5411 - classification_loss: 0.1312
 1485/10000 [===>..........................] - ETA: 1:12:40 - loss: 0.6719 - regression_loss: 0.5408 - classification_loss: 0.1311
 1486/10000 [===>..........................] - ETA: 1:12:40 - loss: 0.6716 - regression_loss: 0.5406 - classification_loss: 0.1310
 1487/10000 [===>..........................] - ETA: 1:12:39 - loss: 0.6714 - regression_loss: 0.5405 - classification_loss: 0.1310
 1488/10000 [===>..........................] - ETA: 1:12:38 - loss: 0.6714 - regression_loss: 0.5404 - classification_loss: 0.1310
 1489/10000 [===>..........................] - ETA: 1:12:37 - loss: 0.6715 - regression_loss: 0.5405 - classification_loss: 0.1310
 1490/10000 [===>..........................] - ETA: 1:12:36 - loss: 0.6716 - regression_loss: 0.5405 - classification_loss: 0.1311
 1491/10000 [===>..........................] - ETA: 1:12:35 - loss: 0.6718 - regression_loss: 0.5407 - classification_loss: 0.1311
 1492/10000 [===>..........................] - ETA: 1:12:34 - loss: 0.6720 - regression_loss: 0.5409 - classification_loss: 0.1311
 1493/10000 [===>..........................] - ETA: 1:12:33 - loss: 0.6725 - regression_loss: 0.5414 - classification_loss: 0.1312
 1494/10000 [===>..........................] - ETA: 1:12:32 - loss: 0.6729 - regression_loss: 0.5417 - classification_loss: 0.1312
 1495/10000 [===>..........................] - ETA: 1:12:31 - loss: 0.6726 - regression_loss: 0.5415 - classification_loss: 0.1311
 1496/10000 [===>..........................] - ETA: 1:12:30 - loss: 0.6724 - regression_loss: 0.5413 - classification_loss: 0.1311
 1497/10000 [===>..........................] - ETA: 1:12:29 - loss: 0.6724 - regression_loss: 0.5413 - classification_loss: 0.1310
 1498/10000 [===>..........................] - ETA: 1:12:28 - loss: 0.6723 - regression_loss: 0.5412 - classification_loss: 0.1311
 1499/10000 [===>..........................] - ETA: 1:12:28 - loss: 0.6721 - regression_loss: 0.5410 - classification_loss: 0.1310
 1500/10000 [===>..........................] - ETA: 1:12:27 - loss: 0.6727 - regression_loss: 0.5416 - classification_loss: 0.1311
 1501/10000 [===>..........................] - ETA: 1:12:26 - loss: 0.6726 - regression_loss: 0.5416 - classification_loss: 0.1310
 1502/10000 [===>..........................] - ETA: 1:12:25 - loss: 0.6724 - regression_loss: 0.5414 - classification_loss: 0.1310
 1503/10000 [===>..........................] - ETA: 1:12:24 - loss: 0.6723 - regression_loss: 0.5413 - classification_loss: 0.1310
 1504/10000 [===>..........................] - ETA: 1:12:23 - loss: 0.6721 - regression_loss: 0.5411 - classification_loss: 0.1310
 1505/10000 [===>..........................] - ETA: 1:12:23 - loss: 0.6724 - regression_loss: 0.5413 - classification_loss: 0.1312
 1506/10000 [===>..........................] - ETA: 1:12:22 - loss: 0.6727 - regression_loss: 0.5415 - classification_loss: 0.1312
 1507/10000 [===>..........................] - ETA: 1:12:21 - loss: 0.6724 - regression_loss: 0.5413 - classification_loss: 0.1311
 1508/10000 [===>..........................] - ETA: 1:12:20 - loss: 0.6726 - regression_loss: 0.5415 - classification_loss: 0.1311
 1509/10000 [===>..........................] - ETA: 1:12:19 - loss: 0.6729 - regression_loss: 0.5417 - classification_loss: 0.1311
 1510/10000 [===>..........................] - ETA: 1:12:19 - loss: 0.6727 - regression_loss: 0.5416 - classification_loss: 0.1311
 1511/10000 [===>..........................] - ETA: 1:12:21 - loss: 0.6723 - regression_loss: 0.5413 - classification_loss: 0.1310
 1512/10000 [===>..........................] - ETA: 1:12:20 - loss: 0.6724 - regression_loss: 0.5414 - classification_loss: 0.1310
 1513/10000 [===>..........................] - ETA: 1:12:19 - loss: 0.6727 - regression_loss: 0.5414 - classification_loss: 0.1313
 1514/10000 [===>..........................] - ETA: 1:12:18 - loss: 0.6724 - regression_loss: 0.5412 - classification_loss: 0.1312
 1515/10000 [===>..........................] - ETA: 1:12:18 - loss: 0.6727 - regression_loss: 0.5414 - classification_loss: 0.1313
 1516/10000 [===>..........................] - ETA: 1:12:17 - loss: 0.6727 - regression_loss: 0.5414 - classification_loss: 0.1313
 1517/10000 [===>..........................] - ETA: 1:12:16 - loss: 0.6725 - regression_loss: 0.5411 - classification_loss: 0.1314
 1518/10000 [===>..........................] - ETA: 1:12:15 - loss: 0.6724 - regression_loss: 0.5411 - classification_loss: 0.1313
 1519/10000 [===>..........................] - ETA: 1:12:15 - loss: 0.6724 - regression_loss: 0.5411 - classification_loss: 0.1313
 1520/10000 [===>..........................] - ETA: 1:12:13 - loss: 0.6721 - regression_loss: 0.5409 - classification_loss: 0.1313
 1521/10000 [===>..........................] - ETA: 1:12:12 - loss: 0.6727 - regression_loss: 0.5413 - classification_loss: 0.1313
 1522/10000 [===>..........................] - ETA: 1:12:12 - loss: 0.6730 - regression_loss: 0.5417 - classification_loss: 0.1313
 1523/10000 [===>..........................] - ETA: 1:12:11 - loss: 0.6735 - regression_loss: 0.5422 - classification_loss: 0.1313
 1524/10000 [===>..........................] - ETA: 1:12:10 - loss: 0.6734 - regression_loss: 0.5421 - classification_loss: 0.1313
 1525/10000 [===>..........................] - ETA: 1:12:10 - loss: 0.6734 - regression_loss: 0.5421 - classification_loss: 0.1313
 1526/10000 [===>..........................] - ETA: 1:12:09 - loss: 0.6732 - regression_loss: 0.5419 - classification_loss: 0.1313
 1527/10000 [===>..........................] - ETA: 1:12:08 - loss: 0.6730 - regression_loss: 0.5417 - classification_loss: 0.1312
 1528/10000 [===>..........................] - ETA: 1:12:07 - loss: 0.6730 - regression_loss: 0.5418 - classification_loss: 0.1312
 1529/10000 [===>..........................] - ETA: 1:12:06 - loss: 0.6733 - regression_loss: 0.5420 - classification_loss: 0.1313
 1530/10000 [===>..........................] - ETA: 1:12:05 - loss: 0.6731 - regression_loss: 0.5418 - classification_loss: 0.1312
 1531/10000 [===>..........................] - ETA: 1:12:04 - loss: 0.6731 - regression_loss: 0.5419 - classification_loss: 0.1312
 1532/10000 [===>..........................] - ETA: 1:12:04 - loss: 0.6736 - regression_loss: 0.5423 - classification_loss: 0.1313
 1533/10000 [===>..........................] - ETA: 1:12:03 - loss: 0.6738 - regression_loss: 0.5424 - classification_loss: 0.1313
 1534/10000 [===>..........................] - ETA: 1:12:02 - loss: 0.6740 - regression_loss: 0.5426 - classification_loss: 0.1314
 1535/10000 [===>..........................] - ETA: 1:12:01 - loss: 0.6740 - regression_loss: 0.5426 - classification_loss: 0.1314
 1536/10000 [===>..........................] - ETA: 1:12:01 - loss: 0.6743 - regression_loss: 0.5426 - classification_loss: 0.1317
 1537/10000 [===>..........................] - ETA: 1:12:00 - loss: 0.6742 - regression_loss: 0.5425 - classification_loss: 0.1317
 1538/10000 [===>..........................] - ETA: 1:12:00 - loss: 0.6741 - regression_loss: 0.5424 - classification_loss: 0.1318
 1539/10000 [===>..........................] - ETA: 1:11:59 - loss: 0.6740 - regression_loss: 0.5423 - classification_loss: 0.1317
 1540/10000 [===>..........................] - ETA: 1:11:58 - loss: 0.6738 - regression_loss: 0.5421 - classification_loss: 0.1317
 1541/10000 [===>..........................] - ETA: 1:11:57 - loss: 0.6735 - regression_loss: 0.5418 - classification_loss: 0.1317
 1542/10000 [===>..........................] - ETA: 1:11:57 - loss: 0.6736 - regression_loss: 0.5419 - classification_loss: 0.1317
 1543/10000 [===>..........................] - ETA: 1:11:56 - loss: 0.6734 - regression_loss: 0.5417 - classification_loss: 0.1317
 1544/10000 [===>..........................] - ETA: 1:11:55 - loss: 0.6733 - regression_loss: 0.5417 - classification_loss: 0.1316
 1545/10000 [===>..........................] - ETA: 1:11:54 - loss: 0.6733 - regression_loss: 0.5418 - classification_loss: 0.1316
 1546/10000 [===>..........................] - ETA: 1:11:53 - loss: 0.6732 - regression_loss: 0.5416 - classification_loss: 0.1315
 1547/10000 [===>..........................] - ETA: 1:11:53 - loss: 0.6729 - regression_loss: 0.5414 - classification_loss: 0.1315
 1548/10000 [===>..........................] - ETA: 1:11:52 - loss: 0.6728 - regression_loss: 0.5413 - classification_loss: 0.1316
 1549/10000 [===>..........................] - ETA: 1:11:51 - loss: 0.6730 - regression_loss: 0.5413 - classification_loss: 0.1317
 1550/10000 [===>..........................] - ETA: 1:11:50 - loss: 0.6738 - regression_loss: 0.5419 - classification_loss: 0.1319
 1551/10000 [===>..........................] - ETA: 1:11:49 - loss: 0.6738 - regression_loss: 0.5418 - classification_loss: 0.1320
 1552/10000 [===>..........................] - ETA: 1:11:48 - loss: 0.6739 - regression_loss: 0.5419 - classification_loss: 0.1320
 1553/10000 [===>..........................] - ETA: 1:11:48 - loss: 0.6737 - regression_loss: 0.5418 - classification_loss: 0.1319
 1554/10000 [===>..........................] - ETA: 1:11:47 - loss: 0.6736 - regression_loss: 0.5417 - classification_loss: 0.1319
 1555/10000 [===>..........................] - ETA: 1:11:46 - loss: 0.6733 - regression_loss: 0.5415 - classification_loss: 0.1318
 1556/10000 [===>..........................] - ETA: 1:11:45 - loss: 0.6734 - regression_loss: 0.5416 - classification_loss: 0.1319
 1557/10000 [===>..........................] - ETA: 1:11:45 - loss: 0.6731 - regression_loss: 0.5413 - classification_loss: 0.1318
 1558/10000 [===>..........................] - ETA: 1:11:44 - loss: 0.6731 - regression_loss: 0.5414 - classification_loss: 0.1318
 1559/10000 [===>..........................] - ETA: 1:11:43 - loss: 0.6732 - regression_loss: 0.5414 - classification_loss: 0.1317
 1560/10000 [===>..........................] - ETA: 1:11:42 - loss: 0.6737 - regression_loss: 0.5419 - classification_loss: 0.1318
 1561/10000 [===>..........................] - ETA: 1:11:41 - loss: 0.6738 - regression_loss: 0.5420 - classification_loss: 0.1318
 1562/10000 [===>..........................] - ETA: 1:11:40 - loss: 0.6741 - regression_loss: 0.5423 - classification_loss: 0.1318
 1563/10000 [===>..........................] - ETA: 1:11:40 - loss: 0.6738 - regression_loss: 0.5421 - classification_loss: 0.1317
 1564/10000 [===>..........................] - ETA: 1:11:39 - loss: 0.6737 - regression_loss: 0.5420 - classification_loss: 0.1317
 1565/10000 [===>..........................] - ETA: 1:11:38 - loss: 0.6734 - regression_loss: 0.5417 - classification_loss: 0.1317
 1566/10000 [===>..........................] - ETA: 1:11:37 - loss: 0.6738 - regression_loss: 0.5420 - classification_loss: 0.1318
 1567/10000 [===>..........................] - ETA: 1:11:37 - loss: 0.6735 - regression_loss: 0.5418 - classification_loss: 0.1318
 1568/10000 [===>..........................] - ETA: 1:11:36 - loss: 0.6735 - regression_loss: 0.5418 - classification_loss: 0.1317
 1569/10000 [===>..........................] - ETA: 1:11:35 - loss: 0.6733 - regression_loss: 0.5416 - classification_loss: 0.1317
 1570/10000 [===>..........................] - ETA: 1:11:34 - loss: 0.6734 - regression_loss: 0.5417 - classification_loss: 0.1317
 1571/10000 [===>..........................] - ETA: 1:11:33 - loss: 0.6735 - regression_loss: 0.5418 - classification_loss: 0.1317
 1572/10000 [===>..........................] - ETA: 1:11:33 - loss: 0.6734 - regression_loss: 0.5417 - classification_loss: 0.1317
 1573/10000 [===>..........................] - ETA: 1:11:32 - loss: 0.6731 - regression_loss: 0.5415 - classification_loss: 0.1316
 1574/10000 [===>..........................] - ETA: 1:11:31 - loss: 0.6729 - regression_loss: 0.5413 - classification_loss: 0.1315
 1575/10000 [===>..........................] - ETA: 1:11:30 - loss: 0.6728 - regression_loss: 0.5413 - classification_loss: 0.1315
 1576/10000 [===>..........................] - ETA: 1:11:29 - loss: 0.6727 - regression_loss: 0.5412 - classification_loss: 0.1315
 1577/10000 [===>..........................] - ETA: 1:11:29 - loss: 0.6728 - regression_loss: 0.5413 - classification_loss: 0.1315
 1578/10000 [===>..........................] - ETA: 1:11:28 - loss: 0.6725 - regression_loss: 0.5411 - classification_loss: 0.1314
 1579/10000 [===>..........................] - ETA: 1:11:27 - loss: 0.6722 - regression_loss: 0.5408 - classification_loss: 0.1314
 1580/10000 [===>..........................] - ETA: 1:11:29 - loss: 0.6724 - regression_loss: 0.5409 - classification_loss: 0.1314
 1581/10000 [===>..........................] - ETA: 1:11:30 - loss: 0.6722 - regression_loss: 0.5408 - classification_loss: 0.1314
 1582/10000 [===>..........................] - ETA: 1:11:29 - loss: 0.6721 - regression_loss: 0.5406 - classification_loss: 0.1314
 1583/10000 [===>..........................] - ETA: 1:11:28 - loss: 0.6717 - regression_loss: 0.5403 - classification_loss: 0.1314
 1584/10000 [===>..........................] - ETA: 1:11:27 - loss: 0.6714 - regression_loss: 0.5401 - classification_loss: 0.1313
 1585/10000 [===>..........................] - ETA: 1:11:26 - loss: 0.6715 - regression_loss: 0.5402 - classification_loss: 0.1313
 1586/10000 [===>..........................] - ETA: 1:11:25 - loss: 0.6717 - regression_loss: 0.5405 - classification_loss: 0.1312
 1587/10000 [===>..........................] - ETA: 1:11:24 - loss: 0.6717 - regression_loss: 0.5405 - classification_loss: 0.1312
 1588/10000 [===>..........................] - ETA: 1:11:24 - loss: 0.6715 - regression_loss: 0.5403 - classification_loss: 0.1312
 1589/10000 [===>..........................] - ETA: 1:11:23 - loss: 0.6717 - regression_loss: 0.5405 - classification_loss: 0.1312
 1590/10000 [===>..........................] - ETA: 1:11:22 - loss: 0.6722 - regression_loss: 0.5409 - classification_loss: 0.1312
 1591/10000 [===>..........................] - ETA: 1:11:21 - loss: 0.6723 - regression_loss: 0.5410 - classification_loss: 0.1312
 1592/10000 [===>..........................] - ETA: 1:11:23 - loss: 0.6724 - regression_loss: 0.5412 - classification_loss: 0.1312
 1593/10000 [===>..........................] - ETA: 1:11:23 - loss: 0.6725 - regression_loss: 0.5413 - classification_loss: 0.1312
 1594/10000 [===>..........................] - ETA: 1:11:22 - loss: 0.6728 - regression_loss: 0.5415 - classification_loss: 0.1313
 1595/10000 [===>..........................] - ETA: 1:11:21 - loss: 0.6728 - regression_loss: 0.5415 - classification_loss: 0.1313
 1596/10000 [===>..........................] - ETA: 1:11:20 - loss: 0.6727 - regression_loss: 0.5414 - classification_loss: 0.1313
 1597/10000 [===>..........................] - ETA: 1:11:19 - loss: 0.6729 - regression_loss: 0.5416 - classification_loss: 0.1313
 1598/10000 [===>..........................] - ETA: 1:11:19 - loss: 0.6728 - regression_loss: 0.5416 - classification_loss: 0.1313
 1599/10000 [===>..........................] - ETA: 1:11:18 - loss: 0.6727 - regression_loss: 0.5414 - classification_loss: 0.1312
 1600/10000 [===>..........................] - ETA: 1:11:17 - loss: 0.6725 - regression_loss: 0.5413 - classification_loss: 0.1312
 1601/10000 [===>..........................] - ETA: 1:11:16 - loss: 0.6729 - regression_loss: 0.5416 - classification_loss: 0.1313
 1602/10000 [===>..........................] - ETA: 1:11:15 - loss: 0.6726 - regression_loss: 0.5414 - classification_loss: 0.1312
 1603/10000 [===>..........................] - ETA: 1:11:15 - loss: 0.6725 - regression_loss: 0.5413 - classification_loss: 0.1312
 1604/10000 [===>..........................] - ETA: 1:11:14 - loss: 0.6725 - regression_loss: 0.5413 - classification_loss: 0.1312
 1605/10000 [===>..........................] - ETA: 1:11:13 - loss: 0.6723 - regression_loss: 0.5412 - classification_loss: 0.1311
 1606/10000 [===>..........................] - ETA: 1:11:12 - loss: 0.6721 - regression_loss: 0.5410 - classification_loss: 0.1311
 1607/10000 [===>..........................] - ETA: 1:11:12 - loss: 0.6725 - regression_loss: 0.5414 - classification_loss: 0.1311
 1608/10000 [===>..........................] - ETA: 1:11:11 - loss: 0.6724 - regression_loss: 0.5413 - classification_loss: 0.1311
 1609/10000 [===>..........................] - ETA: 1:11:10 - loss: 0.6723 - regression_loss: 0.5412 - classification_loss: 0.1311
 1610/10000 [===>..........................] - ETA: 1:11:09 - loss: 0.6731 - regression_loss: 0.5418 - classification_loss: 0.1313
 1611/10000 [===>..........................] - ETA: 1:11:08 - loss: 0.6729 - regression_loss: 0.5416 - classification_loss: 0.1313
 1612/10000 [===>..........................] - ETA: 1:11:07 - loss: 0.6728 - regression_loss: 0.5415 - classification_loss: 0.1313
 1613/10000 [===>..........................] - ETA: 1:11:07 - loss: 0.6731 - regression_loss: 0.5418 - classification_loss: 0.1313
 1614/10000 [===>..........................] - ETA: 1:11:06 - loss: 0.6730 - regression_loss: 0.5418 - classification_loss: 0.1312
 1615/10000 [===>..........................] - ETA: 1:11:05 - loss: 0.6730 - regression_loss: 0.5417 - classification_loss: 0.1312
 1616/10000 [===>..........................] - ETA: 1:11:04 - loss: 0.6727 - regression_loss: 0.5415 - classification_loss: 0.1312
 1617/10000 [===>..........................] - ETA: 1:11:04 - loss: 0.6724 - regression_loss: 0.5413 - classification_loss: 0.1312
 1618/10000 [===>..........................] - ETA: 1:11:03 - loss: 0.6722 - regression_loss: 0.5411 - classification_loss: 0.1311
 1619/10000 [===>..........................] - ETA: 1:11:03 - loss: 0.6720 - regression_loss: 0.5410 - classification_loss: 0.1311
 1620/10000 [===>..........................] - ETA: 1:11:02 - loss: 0.6722 - regression_loss: 0.5411 - classification_loss: 0.1311
 1621/10000 [===>..........................] - ETA: 1:11:01 - loss: 0.6720 - regression_loss: 0.5409 - classification_loss: 0.1311
 1622/10000 [===>..........................] - ETA: 1:11:00 - loss: 0.6721 - regression_loss: 0.5410 - classification_loss: 0.1311
 1623/10000 [===>..........................] - ETA: 1:10:59 - loss: 0.6721 - regression_loss: 0.5411 - classification_loss: 0.1311
 1624/10000 [===>..........................] - ETA: 1:10:59 - loss: 0.6721 - regression_loss: 0.5410 - classification_loss: 0.1311
 1625/10000 [===>..........................] - ETA: 1:10:58 - loss: 0.6723 - regression_loss: 0.5412 - classification_loss: 0.1311
 1626/10000 [===>..........................] - ETA: 1:10:57 - loss: 0.6722 - regression_loss: 0.5411 - classification_loss: 0.1311
 1627/10000 [===>..........................] - ETA: 1:10:56 - loss: 0.6722 - regression_loss: 0.5411 - classification_loss: 0.1311
 1628/10000 [===>..........................] - ETA: 1:10:55 - loss: 0.6727 - regression_loss: 0.5415 - classification_loss: 0.1311
 1629/10000 [===>..........................] - ETA: 1:10:55 - loss: 0.6730 - regression_loss: 0.5418 - classification_loss: 0.1312
 1630/10000 [===>..........................] - ETA: 1:10:54 - loss: 0.6727 - regression_loss: 0.5416 - classification_loss: 0.1311
 1631/10000 [===>..........................] - ETA: 1:10:54 - loss: 0.6727 - regression_loss: 0.5416 - classification_loss: 0.1311
 1632/10000 [===>..........................] - ETA: 1:10:53 - loss: 0.6724 - regression_loss: 0.5414 - classification_loss: 0.1310
 1633/10000 [===>..........................] - ETA: 1:10:52 - loss: 0.6722 - regression_loss: 0.5412 - classification_loss: 0.1310
 1634/10000 [===>..........................] - ETA: 1:10:52 - loss: 0.6721 - regression_loss: 0.5411 - classification_loss: 0.1310
 1635/10000 [===>..........................] - ETA: 1:10:51 - loss: 0.6722 - regression_loss: 0.5412 - classification_loss: 0.1310
 1636/10000 [===>..........................] - ETA: 1:10:51 - loss: 0.6725 - regression_loss: 0.5415 - classification_loss: 0.1310
 1637/10000 [===>..........................] - ETA: 1:10:50 - loss: 0.6724 - regression_loss: 0.5414 - classification_loss: 0.1310
 1638/10000 [===>..........................] - ETA: 1:10:49 - loss: 0.6724 - regression_loss: 0.5413 - classification_loss: 0.1310
 1639/10000 [===>..........................] - ETA: 1:10:48 - loss: 0.6726 - regression_loss: 0.5416 - classification_loss: 0.1310
 1640/10000 [===>..........................] - ETA: 1:10:47 - loss: 0.6726 - regression_loss: 0.5415 - classification_loss: 0.1310
 1641/10000 [===>..........................] - ETA: 1:10:47 - loss: 0.6723 - regression_loss: 0.5413 - classification_loss: 0.1310
 1642/10000 [===>..........................] - ETA: 1:10:46 - loss: 0.6725 - regression_loss: 0.5415 - classification_loss: 0.1310
 1643/10000 [===>..........................] - ETA: 1:10:45 - loss: 0.6723 - regression_loss: 0.5413 - classification_loss: 0.1310
 1644/10000 [===>..........................] - ETA: 1:10:44 - loss: 0.6720 - regression_loss: 0.5411 - classification_loss: 0.1310
 1645/10000 [===>..........................] - ETA: 1:10:44 - loss: 0.6720 - regression_loss: 0.5409 - classification_loss: 0.1311
 1646/10000 [===>..........................] - ETA: 1:10:43 - loss: 0.6722 - regression_loss: 0.5411 - classification_loss: 0.1310
 1647/10000 [===>..........................] - ETA: 1:10:42 - loss: 0.6719 - regression_loss: 0.5410 - classification_loss: 0.1310
 1648/10000 [===>..........................] - ETA: 1:10:41 - loss: 0.6716 - regression_loss: 0.5407 - classification_loss: 0.1309
 1649/10000 [===>..........................] - ETA: 1:10:41 - loss: 0.6714 - regression_loss: 0.5405 - classification_loss: 0.1309
 1650/10000 [===>..........................] - ETA: 1:10:40 - loss: 0.6714 - regression_loss: 0.5405 - classification_loss: 0.1309
 1651/10000 [===>..........................] - ETA: 1:10:39 - loss: 0.6719 - regression_loss: 0.5410 - classification_loss: 0.1309
 1652/10000 [===>..........................] - ETA: 1:10:38 - loss: 0.6718 - regression_loss: 0.5409 - classification_loss: 0.1309
 1653/10000 [===>..........................] - ETA: 1:10:37 - loss: 0.6716 - regression_loss: 0.5408 - classification_loss: 0.1308
 1654/10000 [===>..........................] - ETA: 1:10:37 - loss: 0.6718 - regression_loss: 0.5410 - classification_loss: 0.1308
 1655/10000 [===>..........................] - ETA: 1:10:36 - loss: 0.6716 - regression_loss: 0.5408 - classification_loss: 0.1308
 1656/10000 [===>..........................] - ETA: 1:10:35 - loss: 0.6715 - regression_loss: 0.5407 - classification_loss: 0.1308
 1657/10000 [===>..........................] - ETA: 1:10:34 - loss: 0.6713 - regression_loss: 0.5406 - classification_loss: 0.1307
 1658/10000 [===>..........................] - ETA: 1:10:34 - loss: 0.6711 - regression_loss: 0.5404 - classification_loss: 0.1307
 1659/10000 [===>..........................] - ETA: 1:10:33 - loss: 0.6710 - regression_loss: 0.5403 - classification_loss: 0.1306
 1660/10000 [===>..........................] - ETA: 1:10:32 - loss: 0.6712 - regression_loss: 0.5405 - classification_loss: 0.1306
 1661/10000 [===>..........................] - ETA: 1:10:31 - loss: 0.6712 - regression_loss: 0.5405 - classification_loss: 0.1306
 1662/10000 [===>..........................] - ETA: 1:10:31 - loss: 0.6713 - regression_loss: 0.5407 - classification_loss: 0.1306
 1663/10000 [===>..........................] - ETA: 1:10:30 - loss: 0.6710 - regression_loss: 0.5405 - classification_loss: 0.1306
 1664/10000 [===>..........................] - ETA: 1:10:29 - loss: 0.6708 - regression_loss: 0.5402 - classification_loss: 0.1305
 1665/10000 [===>..........................] - ETA: 1:10:28 - loss: 0.6705 - regression_loss: 0.5400 - classification_loss: 0.1305
 1666/10000 [===>..........................] - ETA: 1:10:28 - loss: 0.6704 - regression_loss: 0.5399 - classification_loss: 0.1305
 1667/10000 [====>.........................] - ETA: 1:10:27 - loss: 0.6702 - regression_loss: 0.5397 - classification_loss: 0.1305
 1668/10000 [====>.........................] - ETA: 1:10:27 - loss: 0.6701 - regression_loss: 0.5396 - classification_loss: 0.1305
 1669/10000 [====>.........................] - ETA: 1:10:26 - loss: 0.6698 - regression_loss: 0.5394 - classification_loss: 0.1304
 1670/10000 [====>.........................] - ETA: 1:10:25 - loss: 0.6695 - regression_loss: 0.5392 - classification_loss: 0.1303
 1671/10000 [====>.........................] - ETA: 1:10:24 - loss: 0.6693 - regression_loss: 0.5390 - classification_loss: 0.1303
 1672/10000 [====>.........................] - ETA: 1:10:23 - loss: 0.6692 - regression_loss: 0.5389 - classification_loss: 0.1303
 1673/10000 [====>.........................] - ETA: 1:10:22 - loss: 0.6691 - regression_loss: 0.5388 - classification_loss: 0.1303
 1674/10000 [====>.........................] - ETA: 1:10:22 - loss: 0.6690 - regression_loss: 0.5387 - classification_loss: 0.1303
 1675/10000 [====>.........................] - ETA: 1:10:21 - loss: 0.6689 - regression_loss: 0.5386 - classification_loss: 0.1303
 1676/10000 [====>.........................] - ETA: 1:10:20 - loss: 0.6687 - regression_loss: 0.5384 - classification_loss: 0.1303
 1677/10000 [====>.........................] - ETA: 1:10:20 - loss: 0.6685 - regression_loss: 0.5383 - classification_loss: 0.1302
 1678/10000 [====>.........................] - ETA: 1:10:19 - loss: 0.6691 - regression_loss: 0.5387 - classification_loss: 0.1304
 1679/10000 [====>.........................] - ETA: 1:10:19 - loss: 0.6692 - regression_loss: 0.5388 - classification_loss: 0.1304
 1680/10000 [====>.........................] - ETA: 1:10:19 - loss: 0.6691 - regression_loss: 0.5387 - classification_loss: 0.1303
 1681/10000 [====>.........................] - ETA: 1:10:18 - loss: 0.6690 - regression_loss: 0.5387 - classification_loss: 0.1303
 1682/10000 [====>.........................] - ETA: 1:10:18 - loss: 0.6688 - regression_loss: 0.5385 - classification_loss: 0.1302
 1683/10000 [====>.........................] - ETA: 1:10:17 - loss: 0.6688 - regression_loss: 0.5386 - classification_loss: 0.1302
 1684/10000 [====>.........................] - ETA: 1:10:16 - loss: 0.6685 - regression_loss: 0.5383 - classification_loss: 0.1302
 1685/10000 [====>.........................] - ETA: 1:10:15 - loss: 0.6684 - regression_loss: 0.5383 - classification_loss: 0.1301
 1686/10000 [====>.........................] - ETA: 1:10:14 - loss: 0.6684 - regression_loss: 0.5383 - classification_loss: 0.1301
 1687/10000 [====>.........................] - ETA: 1:10:14 - loss: 0.6683 - regression_loss: 0.5382 - classification_loss: 0.1301
 1688/10000 [====>.........................] - ETA: 1:10:13 - loss: 0.6683 - regression_loss: 0.5381 - classification_loss: 0.1301
 1689/10000 [====>.........................] - ETA: 1:10:12 - loss: 0.6680 - regression_loss: 0.5379 - classification_loss: 0.1301
 1690/10000 [====>.........................] - ETA: 1:10:12 - loss: 0.6679 - regression_loss: 0.5378 - classification_loss: 0.1301
 1691/10000 [====>.........................] - ETA: 1:10:12 - loss: 0.6681 - regression_loss: 0.5380 - classification_loss: 0.1301
 1692/10000 [====>.........................] - ETA: 1:10:11 - loss: 0.6680 - regression_loss: 0.5379 - classification_loss: 0.1301
 1693/10000 [====>.........................] - ETA: 1:10:10 - loss: 0.6682 - regression_loss: 0.5380 - classification_loss: 0.1301
 1694/10000 [====>.........................] - ETA: 1:10:09 - loss: 0.6683 - regression_loss: 0.5382 - classification_loss: 0.1302
 1695/10000 [====>.........................] - ETA: 1:10:11 - loss: 0.6681 - regression_loss: 0.5380 - classification_loss: 0.1301
 1696/10000 [====>.........................] - ETA: 1:10:10 - loss: 0.6679 - regression_loss: 0.5379 - classification_loss: 0.1300
 1697/10000 [====>.........................] - ETA: 1:10:10 - loss: 0.6680 - regression_loss: 0.5379 - classification_loss: 0.1300
 1698/10000 [====>.........................] - ETA: 1:10:09 - loss: 0.6678 - regression_loss: 0.5377 - classification_loss: 0.1301
 1699/10000 [====>.........................] - ETA: 1:10:08 - loss: 0.6676 - regression_loss: 0.5375 - classification_loss: 0.1301
 1700/10000 [====>.........................] - ETA: 1:10:08 - loss: 0.6677 - regression_loss: 0.5376 - classification_loss: 0.1301
 1701/10000 [====>.........................] - ETA: 1:10:07 - loss: 0.6677 - regression_loss: 0.5376 - classification_loss: 0.1301
 1702/10000 [====>.........................] - ETA: 1:10:06 - loss: 0.6675 - regression_loss: 0.5375 - classification_loss: 0.1300
 1703/10000 [====>.........................] - ETA: 1:10:06 - loss: 0.6674 - regression_loss: 0.5374 - classification_loss: 0.1300
 1704/10000 [====>.........................] - ETA: 1:10:05 - loss: 0.6671 - regression_loss: 0.5372 - classification_loss: 0.1299
 1705/10000 [====>.........................] - ETA: 1:10:04 - loss: 0.6672 - regression_loss: 0.5372 - classification_loss: 0.1299
 1706/10000 [====>.........................] - ETA: 1:10:03 - loss: 0.6671 - regression_loss: 0.5372 - classification_loss: 0.1299
 1707/10000 [====>.........................] - ETA: 1:10:02 - loss: 0.6672 - regression_loss: 0.5373 - classification_loss: 0.1299
 1708/10000 [====>.........................] - ETA: 1:10:02 - loss: 0.6670 - regression_loss: 0.5371 - classification_loss: 0.1299
 1709/10000 [====>.........................] - ETA: 1:10:01 - loss: 0.6668 - regression_loss: 0.5370 - classification_loss: 0.1298
 1710/10000 [====>.........................] - ETA: 1:10:00 - loss: 0.6668 - regression_loss: 0.5369 - classification_loss: 0.1299
 1711/10000 [====>.........................] - ETA: 1:10:00 - loss: 0.6666 - regression_loss: 0.5368 - classification_loss: 0.1298
 1712/10000 [====>.........................] - ETA: 1:09:59 - loss: 0.6664 - regression_loss: 0.5367 - classification_loss: 0.1298
 1713/10000 [====>.........................] - ETA: 1:09:58 - loss: 0.6664 - regression_loss: 0.5366 - classification_loss: 0.1298
 1714/10000 [====>.........................] - ETA: 1:09:57 - loss: 0.6665 - regression_loss: 0.5366 - classification_loss: 0.1299
 1715/10000 [====>.........................] - ETA: 1:09:56 - loss: 0.6666 - regression_loss: 0.5367 - classification_loss: 0.1299
 1716/10000 [====>.........................] - ETA: 1:09:56 - loss: 0.6666 - regression_loss: 0.5367 - classification_loss: 0.1299
 1717/10000 [====>.........................] - ETA: 1:09:55 - loss: 0.6666 - regression_loss: 0.5367 - classification_loss: 0.1299
 1718/10000 [====>.........................] - ETA: 1:09:54 - loss: 0.6666 - regression_loss: 0.5367 - classification_loss: 0.1299
 1719/10000 [====>.........................] - ETA: 1:09:53 - loss: 0.6665 - regression_loss: 0.5366 - classification_loss: 0.1299
 1720/10000 [====>.........................] - ETA: 1:09:53 - loss: 0.6664 - regression_loss: 0.5366 - classification_loss: 0.1299
 1721/10000 [====>.........................] - ETA: 1:09:52 - loss: 0.6665 - regression_loss: 0.5366 - classification_loss: 0.1299
 1722/10000 [====>.........................] - ETA: 1:09:51 - loss: 0.6666 - regression_loss: 0.5367 - classification_loss: 0.1299
 1723/10000 [====>.........................] - ETA: 1:09:50 - loss: 0.6665 - regression_loss: 0.5365 - classification_loss: 0.1300
 1724/10000 [====>.........................] - ETA: 1:09:49 - loss: 0.6668 - regression_loss: 0.5367 - classification_loss: 0.1300
 1725/10000 [====>.........................] - ETA: 1:09:49 - loss: 0.6666 - regression_loss: 0.5366 - classification_loss: 0.1300
 1726/10000 [====>.........................] - ETA: 1:09:48 - loss: 0.6667 - regression_loss: 0.5367 - classification_loss: 0.1300
 1727/10000 [====>.........................] - ETA: 1:09:47 - loss: 0.6666 - regression_loss: 0.5367 - classification_loss: 0.1300
 1728/10000 [====>.........................] - ETA: 1:09:47 - loss: 0.6665 - regression_loss: 0.5366 - classification_loss: 0.1299
 1729/10000 [====>.........................] - ETA: 1:09:46 - loss: 0.6663 - regression_loss: 0.5364 - classification_loss: 0.1299
 1730/10000 [====>.........................] - ETA: 1:09:45 - loss: 0.6662 - regression_loss: 0.5363 - classification_loss: 0.1299
 1731/10000 [====>.........................] - ETA: 1:09:44 - loss: 0.6663 - regression_loss: 0.5364 - classification_loss: 0.1299
 1732/10000 [====>.........................] - ETA: 1:09:44 - loss: 0.6664 - regression_loss: 0.5365 - classification_loss: 0.1299
 1733/10000 [====>.........................] - ETA: 1:09:43 - loss: 0.6666 - regression_loss: 0.5366 - classification_loss: 0.1300
 1734/10000 [====>.........................] - ETA: 1:09:42 - loss: 0.6665 - regression_loss: 0.5366 - classification_loss: 0.1299
 1735/10000 [====>.........................] - ETA: 1:09:43 - loss: 0.6665 - regression_loss: 0.5366 - classification_loss: 0.1299
 1736/10000 [====>.........................] - ETA: 1:09:43 - loss: 0.6662 - regression_loss: 0.5364 - classification_loss: 0.1299
 1737/10000 [====>.........................] - ETA: 1:09:42 - loss: 0.6660 - regression_loss: 0.5361 - classification_loss: 0.1298
 1738/10000 [====>.........................] - ETA: 1:09:41 - loss: 0.6657 - regression_loss: 0.5359 - classification_loss: 0.1298
 1739/10000 [====>.........................] - ETA: 1:09:41 - loss: 0.6658 - regression_loss: 0.5360 - classification_loss: 0.1298
 1740/10000 [====>.........................] - ETA: 1:09:40 - loss: 0.6657 - regression_loss: 0.5359 - classification_loss: 0.1297
 1741/10000 [====>.........................] - ETA: 1:09:39 - loss: 0.6654 - regression_loss: 0.5357 - classification_loss: 0.1297
 1742/10000 [====>.........................] - ETA: 1:09:38 - loss: 0.6651 - regression_loss: 0.5355 - classification_loss: 0.1297
 1743/10000 [====>.........................] - ETA: 1:09:37 - loss: 0.6650 - regression_loss: 0.5354 - classification_loss: 0.1296
 1744/10000 [====>.........................] - ETA: 1:09:37 - loss: 0.6648 - regression_loss: 0.5352 - classification_loss: 0.1296
 1745/10000 [====>.........................] - ETA: 1:09:36 - loss: 0.6649 - regression_loss: 0.5352 - classification_loss: 0.1296
 1746/10000 [====>.........................] - ETA: 1:09:35 - loss: 0.6646 - regression_loss: 0.5350 - classification_loss: 0.1295
 1747/10000 [====>.........................] - ETA: 1:09:35 - loss: 0.6643 - regression_loss: 0.5348 - classification_loss: 0.1295
 1748/10000 [====>.........................] - ETA: 1:09:34 - loss: 0.6642 - regression_loss: 0.5347 - classification_loss: 0.1295
 1749/10000 [====>.........................] - ETA: 1:09:33 - loss: 0.6640 - regression_loss: 0.5346 - classification_loss: 0.1294
 1750/10000 [====>.........................] - ETA: 1:09:32 - loss: 0.6641 - regression_loss: 0.5346 - classification_loss: 0.1294
 1751/10000 [====>.........................] - ETA: 1:09:32 - loss: 0.6641 - regression_loss: 0.5346 - classification_loss: 0.1295
 1752/10000 [====>.........................] - ETA: 1:09:31 - loss: 0.6643 - regression_loss: 0.5347 - classification_loss: 0.1295
 1753/10000 [====>.........................] - ETA: 1:09:30 - loss: 0.6643 - regression_loss: 0.5348 - classification_loss: 0.1295
 1754/10000 [====>.........................] - ETA: 1:09:29 - loss: 0.6644 - regression_loss: 0.5350 - classification_loss: 0.1294
 1755/10000 [====>.........................] - ETA: 1:09:28 - loss: 0.6642 - regression_loss: 0.5348 - classification_loss: 0.1294
 1756/10000 [====>.........................] - ETA: 1:09:28 - loss: 0.6641 - regression_loss: 0.5347 - classification_loss: 0.1294
 1757/10000 [====>.........................] - ETA: 1:09:27 - loss: 0.6639 - regression_loss: 0.5345 - classification_loss: 0.1293
 1758/10000 [====>.........................] - ETA: 1:09:26 - loss: 0.6638 - regression_loss: 0.5344 - classification_loss: 0.1294
 1759/10000 [====>.........................] - ETA: 1:09:25 - loss: 0.6642 - regression_loss: 0.5348 - classification_loss: 0.1294
 1760/10000 [====>.........................] - ETA: 1:09:25 - loss: 0.6640 - regression_loss: 0.5346 - classification_loss: 0.1294
 1761/10000 [====>.........................] - ETA: 1:09:24 - loss: 0.6641 - regression_loss: 0.5347 - classification_loss: 0.1294
 1762/10000 [====>.........................] - ETA: 1:09:23 - loss: 0.6641 - regression_loss: 0.5347 - classification_loss: 0.1294
 1763/10000 [====>.........................] - ETA: 1:09:22 - loss: 0.6645 - regression_loss: 0.5351 - classification_loss: 0.1294
 1764/10000 [====>.........................] - ETA: 1:09:22 - loss: 0.6643 - regression_loss: 0.5349 - classification_loss: 0.1294
 1765/10000 [====>.........................] - ETA: 1:09:21 - loss: 0.6645 - regression_loss: 0.5351 - classification_loss: 0.1294
 1766/10000 [====>.........................] - ETA: 1:09:20 - loss: 0.6646 - regression_loss: 0.5352 - classification_loss: 0.1294
 1767/10000 [====>.........................] - ETA: 1:09:20 - loss: 0.6646 - regression_loss: 0.5351 - classification_loss: 0.1296
 1768/10000 [====>.........................] - ETA: 1:09:19 - loss: 0.6647 - regression_loss: 0.5351 - classification_loss: 0.1296
 1769/10000 [====>.........................] - ETA: 1:09:18 - loss: 0.6646 - regression_loss: 0.5350 - classification_loss: 0.1296
 1770/10000 [====>.........................] - ETA: 1:09:17 - loss: 0.6643 - regression_loss: 0.5348 - classification_loss: 0.1295
 1771/10000 [====>.........................] - ETA: 1:09:17 - loss: 0.6641 - regression_loss: 0.5346 - classification_loss: 0.1295
 1772/10000 [====>.........................] - ETA: 1:09:16 - loss: 0.6640 - regression_loss: 0.5345 - classification_loss: 0.1295
 1773/10000 [====>.........................] - ETA: 1:09:15 - loss: 0.6639 - regression_loss: 0.5344 - classification_loss: 0.1294
 1774/10000 [====>.........................] - ETA: 1:09:14 - loss: 0.6638 - regression_loss: 0.5343 - classification_loss: 0.1295
 1775/10000 [====>.........................] - ETA: 1:09:14 - loss: 0.6638 - regression_loss: 0.5342 - classification_loss: 0.1296
 1776/10000 [====>.........................] - ETA: 1:09:13 - loss: 0.6636 - regression_loss: 0.5341 - classification_loss: 0.1295
 1777/10000 [====>.........................] - ETA: 1:09:12 - loss: 0.6638 - regression_loss: 0.5343 - classification_loss: 0.1296
 1778/10000 [====>.........................] - ETA: 1:09:12 - loss: 0.6636 - regression_loss: 0.5341 - classification_loss: 0.1295
 1779/10000 [====>.........................] - ETA: 1:09:11 - loss: 0.6636 - regression_loss: 0.5340 - classification_loss: 0.1296
 1780/10000 [====>.........................] - ETA: 1:09:10 - loss: 0.6634 - regression_loss: 0.5338 - classification_loss: 0.1296
 1781/10000 [====>.........................] - ETA: 1:09:10 - loss: 0.6632 - regression_loss: 0.5336 - classification_loss: 0.1296
 1782/10000 [====>.........................] - ETA: 1:09:09 - loss: 0.6640 - regression_loss: 0.5342 - classification_loss: 0.1298
 1783/10000 [====>.........................] - ETA: 1:09:08 - loss: 0.6640 - regression_loss: 0.5342 - classification_loss: 0.1297
 1784/10000 [====>.........................] - ETA: 1:09:07 - loss: 0.6637 - regression_loss: 0.5340 - classification_loss: 0.1297
 1785/10000 [====>.........................] - ETA: 1:09:07 - loss: 0.6639 - regression_loss: 0.5341 - classification_loss: 0.1297
 1786/10000 [====>.........................] - ETA: 1:09:06 - loss: 0.6636 - regression_loss: 0.5339 - classification_loss: 0.1297
 1787/10000 [====>.........................] - ETA: 1:09:05 - loss: 0.6639 - regression_loss: 0.5341 - classification_loss: 0.1297
 1788/10000 [====>.........................] - ETA: 1:09:06 - loss: 0.6642 - regression_loss: 0.5344 - classification_loss: 0.1297
 1789/10000 [====>.........................] - ETA: 1:09:05 - loss: 0.6639 - regression_loss: 0.5343 - classification_loss: 0.1297
 1790/10000 [====>.........................] - ETA: 1:09:04 - loss: 0.6637 - regression_loss: 0.5340 - classification_loss: 0.1296
 1791/10000 [====>.........................] - ETA: 1:09:03 - loss: 0.6638 - regression_loss: 0.5339 - classification_loss: 0.1299
 1792/10000 [====>.........................] - ETA: 1:09:03 - loss: 0.6639 - regression_loss: 0.5340 - classification_loss: 0.1299
 1793/10000 [====>.........................] - ETA: 1:09:02 - loss: 0.6640 - regression_loss: 0.5341 - classification_loss: 0.1299
 1794/10000 [====>.........................] - ETA: 1:09:01 - loss: 0.6641 - regression_loss: 0.5342 - classification_loss: 0.1299
 1795/10000 [====>.........................] - ETA: 1:09:01 - loss: 0.6643 - regression_loss: 0.5343 - classification_loss: 0.1300
 1796/10000 [====>.........................] - ETA: 1:09:00 - loss: 0.6646 - regression_loss: 0.5346 - classification_loss: 0.1301
 1797/10000 [====>.........................] - ETA: 1:08:59 - loss: 0.6648 - regression_loss: 0.5348 - classification_loss: 0.1301
 1798/10000 [====>.........................] - ETA: 1:08:58 - loss: 0.6646 - regression_loss: 0.5346 - classification_loss: 0.1301
 1799/10000 [====>.........................] - ETA: 1:08:58 - loss: 0.6645 - regression_loss: 0.5344 - classification_loss: 0.1301
 1800/10000 [====>.........................] - ETA: 1:08:57 - loss: 0.6646 - regression_loss: 0.5345 - classification_loss: 0.1301
 1801/10000 [====>.........................] - ETA: 1:08:56 - loss: 0.6646 - regression_loss: 0.5345 - classification_loss: 0.1301
 1802/10000 [====>.........................] - ETA: 1:08:56 - loss: 0.6643 - regression_loss: 0.5342 - classification_loss: 0.1301
 1803/10000 [====>.........................] - ETA: 1:08:55 - loss: 0.6648 - regression_loss: 0.5347 - classification_loss: 0.1301
 1804/10000 [====>.........................] - ETA: 1:08:55 - loss: 0.6648 - regression_loss: 0.5347 - classification_loss: 0.1301
 1805/10000 [====>.........................] - ETA: 1:08:54 - loss: 0.6648 - regression_loss: 0.5347 - classification_loss: 0.1301
 1806/10000 [====>.........................] - ETA: 1:08:54 - loss: 0.6652 - regression_loss: 0.5351 - classification_loss: 0.1302
 1807/10000 [====>.........................] - ETA: 1:08:54 - loss: 0.6651 - regression_loss: 0.5350 - classification_loss: 0.1301
 1808/10000 [====>.........................] - ETA: 1:08:53 - loss: 0.6653 - regression_loss: 0.5350 - classification_loss: 0.1302
 1809/10000 [====>.........................] - ETA: 1:08:52 - loss: 0.6652 - regression_loss: 0.5350 - classification_loss: 0.1302
 1810/10000 [====>.........................] - ETA: 1:08:52 - loss: 0.6654 - regression_loss: 0.5352 - classification_loss: 0.1302
 1811/10000 [====>.........................] - ETA: 1:08:51 - loss: 0.6655 - regression_loss: 0.5353 - classification_loss: 0.1302
 1812/10000 [====>.........................] - ETA: 1:08:50 - loss: 0.6656 - regression_loss: 0.5353 - classification_loss: 0.1302
 1813/10000 [====>.........................] - ETA: 1:08:50 - loss: 0.6656 - regression_loss: 0.5353 - classification_loss: 0.1303
 1814/10000 [====>.........................] - ETA: 1:08:49 - loss: 0.6654 - regression_loss: 0.5352 - classification_loss: 0.1302
 1815/10000 [====>.........................] - ETA: 1:08:48 - loss: 0.6651 - regression_loss: 0.5350 - classification_loss: 0.1302
 1816/10000 [====>.........................] - ETA: 1:08:47 - loss: 0.6651 - regression_loss: 0.5350 - classification_loss: 0.1301
 1817/10000 [====>.........................] - ETA: 1:08:46 - loss: 0.6654 - regression_loss: 0.5352 - classification_loss: 0.1302
 1818/10000 [====>.........................] - ETA: 1:08:46 - loss: 0.6660 - regression_loss: 0.5357 - classification_loss: 0.1303
 1819/10000 [====>.........................] - ETA: 1:08:45 - loss: 0.6662 - regression_loss: 0.5360 - classification_loss: 0.1303
 1820/10000 [====>.........................] - ETA: 1:08:44 - loss: 0.6662 - regression_loss: 0.5359 - classification_loss: 0.1303
 1821/10000 [====>.........................] - ETA: 1:08:44 - loss: 0.6662 - regression_loss: 0.5359 - classification_loss: 0.1303
 1822/10000 [====>.........................] - ETA: 1:08:43 - loss: 0.6662 - regression_loss: 0.5358 - classification_loss: 0.1304
 1823/10000 [====>.........................] - ETA: 1:08:42 - loss: 0.6662 - regression_loss: 0.5357 - classification_loss: 0.1305
 1824/10000 [====>.........................] - ETA: 1:08:41 - loss: 0.6664 - regression_loss: 0.5359 - classification_loss: 0.1305
 1825/10000 [====>.........................] - ETA: 1:08:43 - loss: 0.6664 - regression_loss: 0.5359 - classification_loss: 0.1305
 1826/10000 [====>.........................] - ETA: 1:08:42 - loss: 0.6668 - regression_loss: 0.5362 - classification_loss: 0.1306
 1827/10000 [====>.........................] - ETA: 1:08:41 - loss: 0.6669 - regression_loss: 0.5362 - classification_loss: 0.1306
 1828/10000 [====>.........................] - ETA: 1:08:40 - loss: 0.6670 - regression_loss: 0.5363 - classification_loss: 0.1306
 1829/10000 [====>.........................] - ETA: 1:08:40 - loss: 0.6672 - regression_loss: 0.5366 - classification_loss: 0.1307
 1830/10000 [====>.........................] - ETA: 1:08:39 - loss: 0.6673 - regression_loss: 0.5366 - classification_loss: 0.1306
 1831/10000 [====>.........................] - ETA: 1:08:38 - loss: 0.6675 - regression_loss: 0.5368 - classification_loss: 0.1307
 1832/10000 [====>.........................] - ETA: 1:08:37 - loss: 0.6674 - regression_loss: 0.5367 - classification_loss: 0.1306
 1833/10000 [====>.........................] - ETA: 1:08:37 - loss: 0.6676 - regression_loss: 0.5370 - classification_loss: 0.1307
 1834/10000 [====>.........................] - ETA: 1:08:36 - loss: 0.6674 - regression_loss: 0.5368 - classification_loss: 0.1306
 1835/10000 [====>.........................] - ETA: 1:08:35 - loss: 0.6670 - regression_loss: 0.5365 - classification_loss: 0.1306
 1836/10000 [====>.........................] - ETA: 1:08:35 - loss: 0.6674 - regression_loss: 0.5368 - classification_loss: 0.1306
 1837/10000 [====>.........................] - ETA: 1:08:34 - loss: 0.6674 - regression_loss: 0.5368 - classification_loss: 0.1306
 1838/10000 [====>.........................] - ETA: 1:08:33 - loss: 0.6673 - regression_loss: 0.5368 - classification_loss: 0.1306
 1839/10000 [====>.........................] - ETA: 1:08:33 - loss: 0.6674 - regression_loss: 0.5369 - classification_loss: 0.1306
 1840/10000 [====>.........................] - ETA: 1:08:32 - loss: 0.6672 - regression_loss: 0.5367 - classification_loss: 0.1305
 1841/10000 [====>.........................] - ETA: 1:08:32 - loss: 0.6674 - regression_loss: 0.5368 - classification_loss: 0.1306
 1842/10000 [====>.........................] - ETA: 1:08:31 - loss: 0.6675 - regression_loss: 0.5369 - classification_loss: 0.1306
 1843/10000 [====>.........................] - ETA: 1:08:30 - loss: 0.6676 - regression_loss: 0.5370 - classification_loss: 0.1307
 1844/10000 [====>.........................] - ETA: 1:08:30 - loss: 0.6675 - regression_loss: 0.5368 - classification_loss: 0.1307
 1845/10000 [====>.........................] - ETA: 1:08:29 - loss: 0.6672 - regression_loss: 0.5366 - classification_loss: 0.1306
 1846/10000 [====>.........................] - ETA: 1:08:28 - loss: 0.6671 - regression_loss: 0.5365 - classification_loss: 0.1306
 1847/10000 [====>.........................] - ETA: 1:08:27 - loss: 0.6669 - regression_loss: 0.5363 - classification_loss: 0.1306
 1848/10000 [====>.........................] - ETA: 1:08:27 - loss: 0.6669 - regression_loss: 0.5364 - classification_loss: 0.1305
 1849/10000 [====>.........................] - ETA: 1:08:26 - loss: 0.6667 - regression_loss: 0.5362 - classification_loss: 0.1305
 1850/10000 [====>.........................] - ETA: 1:08:28 - loss: 0.6670 - regression_loss: 0.5364 - classification_loss: 0.1306
 1851/10000 [====>.........................] - ETA: 1:08:28 - loss: 0.6669 - regression_loss: 0.5363 - classification_loss: 0.1306
 1852/10000 [====>.........................] - ETA: 1:08:27 - loss: 0.6668 - regression_loss: 0.5362 - classification_loss: 0.1305
 1853/10000 [====>.........................] - ETA: 1:08:27 - loss: 0.6668 - regression_loss: 0.5363 - classification_loss: 0.1306
 1854/10000 [====>.........................] - ETA: 1:08:26 - loss: 0.6667 - regression_loss: 0.5361 - classification_loss: 0.1306
 1855/10000 [====>.........................] - ETA: 1:08:25 - loss: 0.6665 - regression_loss: 0.5359 - classification_loss: 0.1306
 1856/10000 [====>.........................] - ETA: 1:08:25 - loss: 0.6665 - regression_loss: 0.5359 - classification_loss: 0.1306
 1857/10000 [====>.........................] - ETA: 1:08:24 - loss: 0.6668 - regression_loss: 0.5362 - classification_loss: 0.1306
 1858/10000 [====>.........................] - ETA: 1:08:23 - loss: 0.6667 - regression_loss: 0.5361 - classification_loss: 0.1306
 1859/10000 [====>.........................] - ETA: 1:08:22 - loss: 0.6664 - regression_loss: 0.5359 - classification_loss: 0.1305
 1860/10000 [====>.........................] - ETA: 1:08:22 - loss: 0.6664 - regression_loss: 0.5359 - classification_loss: 0.1305
 1861/10000 [====>.........................] - ETA: 1:08:21 - loss: 0.6662 - regression_loss: 0.5357 - classification_loss: 0.1305
 1862/10000 [====>.........................] - ETA: 1:08:21 - loss: 0.6665 - regression_loss: 0.5359 - classification_loss: 0.1306
 1863/10000 [====>.........................] - ETA: 1:08:21 - loss: 0.6665 - regression_loss: 0.5359 - classification_loss: 0.1306
 1864/10000 [====>.........................] - ETA: 1:08:20 - loss: 0.6666 - regression_loss: 0.5360 - classification_loss: 0.1306
 1865/10000 [====>.........................] - ETA: 1:08:19 - loss: 0.6664 - regression_loss: 0.5358 - classification_loss: 0.1305
 1866/10000 [====>.........................] - ETA: 1:08:18 - loss: 0.6666 - regression_loss: 0.5360 - classification_loss: 0.1306
 1867/10000 [====>.........................] - ETA: 1:08:18 - loss: 0.6666 - regression_loss: 0.5361 - classification_loss: 0.1305
 1868/10000 [====>.........................] - ETA: 1:08:17 - loss: 0.6665 - regression_loss: 0.5360 - classification_loss: 0.1305
 1869/10000 [====>.........................] - ETA: 1:08:16 - loss: 0.6663 - regression_loss: 0.5358 - classification_loss: 0.1305
 1870/10000 [====>.........................] - ETA: 1:08:16 - loss: 0.6665 - regression_loss: 0.5359 - classification_loss: 0.1305
 1871/10000 [====>.........................] - ETA: 1:08:15 - loss: 0.6668 - regression_loss: 0.5362 - classification_loss: 0.1306
 1872/10000 [====>.........................] - ETA: 1:08:14 - loss: 0.6668 - regression_loss: 0.5362 - classification_loss: 0.1306
 1873/10000 [====>.........................] - ETA: 1:08:14 - loss: 0.6673 - regression_loss: 0.5366 - classification_loss: 0.1307
 1874/10000 [====>.........................] - ETA: 1:08:13 - loss: 0.6671 - regression_loss: 0.5365 - classification_loss: 0.1307
 1875/10000 [====>.........................] - ETA: 1:08:12 - loss: 0.6673 - regression_loss: 0.5366 - classification_loss: 0.1307
 1876/10000 [====>.........................] - ETA: 1:08:12 - loss: 0.6671 - regression_loss: 0.5364 - classification_loss: 0.1306
 1877/10000 [====>.........................] - ETA: 1:08:11 - loss: 0.6671 - regression_loss: 0.5364 - classification_loss: 0.1307
 1878/10000 [====>.........................] - ETA: 1:08:10 - loss: 0.6670 - regression_loss: 0.5363 - classification_loss: 0.1307
 1879/10000 [====>.........................] - ETA: 1:08:10 - loss: 0.6674 - regression_loss: 0.5366 - classification_loss: 0.1308
 1880/10000 [====>.........................] - ETA: 1:08:09 - loss: 0.6675 - regression_loss: 0.5367 - classification_loss: 0.1308
 1881/10000 [====>.........................] - ETA: 1:08:08 - loss: 0.6674 - regression_loss: 0.5366 - classification_loss: 0.1308
 1882/10000 [====>.........................] - ETA: 1:08:08 - loss: 0.6672 - regression_loss: 0.5365 - classification_loss: 0.1307
 1883/10000 [====>.........................] - ETA: 1:08:07 - loss: 0.6671 - regression_loss: 0.5364 - classification_loss: 0.1307
 1884/10000 [====>.........................] - ETA: 1:08:07 - loss: 0.6669 - regression_loss: 0.5362 - classification_loss: 0.1307
 1885/10000 [====>.........................] - ETA: 1:08:06 - loss: 0.6667 - regression_loss: 0.5361 - classification_loss: 0.1306
 1886/10000 [====>.........................] - ETA: 1:08:05 - loss: 0.6669 - regression_loss: 0.5363 - classification_loss: 0.1306
 1887/10000 [====>.........................] - ETA: 1:08:04 - loss: 0.6670 - regression_loss: 0.5364 - classification_loss: 0.1306
 1888/10000 [====>.........................] - ETA: 1:08:04 - loss: 0.6668 - regression_loss: 0.5363 - classification_loss: 0.1306
 1889/10000 [====>.........................] - ETA: 1:08:03 - loss: 0.6668 - regression_loss: 0.5361 - classification_loss: 0.1306
 1890/10000 [====>.........................] - ETA: 1:08:02 - loss: 0.6669 - regression_loss: 0.5363 - classification_loss: 0.1306
 1891/10000 [====>.........................] - ETA: 1:08:02 - loss: 0.6670 - regression_loss: 0.5364 - classification_loss: 0.1306
 1892/10000 [====>.........................] - ETA: 1:08:02 - loss: 0.6671 - regression_loss: 0.5366 - classification_loss: 0.1305
 1893/10000 [====>.........................] - ETA: 1:08:01 - loss: 0.6669 - regression_loss: 0.5364 - classification_loss: 0.1305
 1894/10000 [====>.........................] - ETA: 1:08:01 - loss: 0.6668 - regression_loss: 0.5364 - classification_loss: 0.1305
 1895/10000 [====>.........................] - ETA: 1:08:00 - loss: 0.6672 - regression_loss: 0.5367 - classification_loss: 0.1305
 1896/10000 [====>.........................] - ETA: 1:07:59 - loss: 0.6670 - regression_loss: 0.5365 - classification_loss: 0.1304
 1897/10000 [====>.........................] - ETA: 1:07:59 - loss: 0.6671 - regression_loss: 0.5366 - classification_loss: 0.1304
 1898/10000 [====>.........................] - ETA: 1:07:58 - loss: 0.6675 - regression_loss: 0.5370 - classification_loss: 0.1305
 1899/10000 [====>.........................] - ETA: 1:07:58 - loss: 0.6675 - regression_loss: 0.5369 - classification_loss: 0.1305
 1900/10000 [====>.........................] - ETA: 1:07:57 - loss: 0.6677 - regression_loss: 0.5372 - classification_loss: 0.1305
 1901/10000 [====>.........................] - ETA: 1:07:56 - loss: 0.6678 - regression_loss: 0.5373 - classification_loss: 0.1306
 1902/10000 [====>.........................] - ETA: 1:07:56 - loss: 0.6675 - regression_loss: 0.5370 - classification_loss: 0.1305
 1903/10000 [====>.........................] - ETA: 1:07:55 - loss: 0.6678 - regression_loss: 0.5373 - classification_loss: 0.1305
 1904/10000 [====>.........................] - ETA: 1:07:54 - loss: 0.6676 - regression_loss: 0.5371 - classification_loss: 0.1305
 1905/10000 [====>.........................] - ETA: 1:07:54 - loss: 0.6675 - regression_loss: 0.5370 - classification_loss: 0.1305
 1906/10000 [====>.........................] - ETA: 1:07:53 - loss: 0.6676 - regression_loss: 0.5372 - classification_loss: 0.1305
 1907/10000 [====>.........................] - ETA: 1:07:52 - loss: 0.6675 - regression_loss: 0.5370 - classification_loss: 0.1304
 1908/10000 [====>.........................] - ETA: 1:07:52 - loss: 0.6673 - regression_loss: 0.5369 - classification_loss: 0.1304
 1909/10000 [====>.........................] - ETA: 1:07:51 - loss: 0.6674 - regression_loss: 0.5370 - classification_loss: 0.1304
 1910/10000 [====>.........................] - ETA: 1:07:50 - loss: 0.6677 - regression_loss: 0.5373 - classification_loss: 0.1304
 1911/10000 [====>.........................] - ETA: 1:07:49 - loss: 0.6678 - regression_loss: 0.5373 - classification_loss: 0.1304
 1912/10000 [====>.........................] - ETA: 1:07:49 - loss: 0.6676 - regression_loss: 0.5372 - classification_loss: 0.1304
 1913/10000 [====>.........................] - ETA: 1:07:48 - loss: 0.6675 - regression_loss: 0.5372 - classification_loss: 0.1303
 1914/10000 [====>.........................] - ETA: 1:07:47 - loss: 0.6675 - regression_loss: 0.5371 - classification_loss: 0.1303
 1915/10000 [====>.........................] - ETA: 1:07:47 - loss: 0.6672 - regression_loss: 0.5369 - classification_loss: 0.1303
 1916/10000 [====>.........................] - ETA: 1:07:47 - loss: 0.6670 - regression_loss: 0.5368 - classification_loss: 0.1303
 1917/10000 [====>.........................] - ETA: 1:07:46 - loss: 0.6669 - regression_loss: 0.5367 - classification_loss: 0.1302
 1918/10000 [====>.........................] - ETA: 1:07:45 - loss: 0.6667 - regression_loss: 0.5366 - classification_loss: 0.1302
 1919/10000 [====>.........................] - ETA: 1:07:45 - loss: 0.6665 - regression_loss: 0.5364 - classification_loss: 0.1301
 1920/10000 [====>.........................] - ETA: 1:07:44 - loss: 0.6663 - regression_loss: 0.5362 - classification_loss: 0.1301
 1921/10000 [====>.........................] - ETA: 1:07:43 - loss: 0.6661 - regression_loss: 0.5360 - classification_loss: 0.1301
 1922/10000 [====>.........................] - ETA: 1:07:43 - loss: 0.6662 - regression_loss: 0.5361 - classification_loss: 0.1301
 1923/10000 [====>.........................] - ETA: 1:07:42 - loss: 0.6660 - regression_loss: 0.5360 - classification_loss: 0.1300
 1924/10000 [====>.........................] - ETA: 1:07:41 - loss: 0.6665 - regression_loss: 0.5364 - classification_loss: 0.1301
 1925/10000 [====>.........................] - ETA: 1:07:41 - loss: 0.6666 - regression_loss: 0.5365 - classification_loss: 0.1301
 1926/10000 [====>.........................] - ETA: 1:07:40 - loss: 0.6665 - regression_loss: 0.5364 - classification_loss: 0.1301
 1927/10000 [====>.........................] - ETA: 1:07:39 - loss: 0.6664 - regression_loss: 0.5363 - classification_loss: 0.1300
 1928/10000 [====>.........................] - ETA: 1:07:38 - loss: 0.6661 - regression_loss: 0.5361 - classification_loss: 0.1300
 1929/10000 [====>.........................] - ETA: 1:07:38 - loss: 0.6664 - regression_loss: 0.5364 - classification_loss: 0.1300
 1930/10000 [====>.........................] - ETA: 1:07:37 - loss: 0.6667 - regression_loss: 0.5365 - classification_loss: 0.1301
 1931/10000 [====>.........................] - ETA: 1:07:36 - loss: 0.6666 - regression_loss: 0.5365 - classification_loss: 0.1302
 1932/10000 [====>.........................] - ETA: 1:07:36 - loss: 0.6665 - regression_loss: 0.5363 - classification_loss: 0.1301
 1933/10000 [====>.........................] - ETA: 1:07:35 - loss: 0.6668 - regression_loss: 0.5366 - classification_loss: 0.1302
 1934/10000 [====>.........................] - ETA: 1:07:34 - loss: 0.6666 - regression_loss: 0.5364 - classification_loss: 0.1302
 1935/10000 [====>.........................] - ETA: 1:07:34 - loss: 0.6667 - regression_loss: 0.5365 - classification_loss: 0.1302
 1936/10000 [====>.........................] - ETA: 1:07:33 - loss: 0.6665 - regression_loss: 0.5363 - classification_loss: 0.1302
 1937/10000 [====>.........................] - ETA: 1:07:32 - loss: 0.6663 - regression_loss: 0.5362 - classification_loss: 0.1301
 1938/10000 [====>.........................] - ETA: 1:07:31 - loss: 0.6662 - regression_loss: 0.5361 - classification_loss: 0.1301
 1939/10000 [====>.........................] - ETA: 1:07:31 - loss: 0.6659 - regression_loss: 0.5359 - classification_loss: 0.1301
 1940/10000 [====>.........................] - ETA: 1:07:30 - loss: 0.6660 - regression_loss: 0.5359 - classification_loss: 0.1301
 1941/10000 [====>.........................] - ETA: 1:07:29 - loss: 0.6661 - regression_loss: 0.5360 - classification_loss: 0.1302
 1942/10000 [====>.........................] - ETA: 1:07:29 - loss: 0.6662 - regression_loss: 0.5361 - classification_loss: 0.1302
 1943/10000 [====>.........................] - ETA: 1:07:31 - loss: 0.6663 - regression_loss: 0.5362 - classification_loss: 0.1301
 1944/10000 [====>.........................] - ETA: 1:07:30 - loss: 0.6664 - regression_loss: 0.5362 - classification_loss: 0.1302
 1945/10000 [====>.........................] - ETA: 1:07:29 - loss: 0.6666 - regression_loss: 0.5364 - classification_loss: 0.1302
 1946/10000 [====>.........................] - ETA: 1:07:29 - loss: 0.6668 - regression_loss: 0.5365 - classification_loss: 0.1303
 1947/10000 [====>.........................] - ETA: 1:07:28 - loss: 0.6668 - regression_loss: 0.5365 - classification_loss: 0.1303
 1948/10000 [====>.........................] - ETA: 1:07:27 - loss: 0.6668 - regression_loss: 0.5365 - classification_loss: 0.1303
 1949/10000 [====>.........................] - ETA: 1:07:27 - loss: 0.6669 - regression_loss: 0.5367 - classification_loss: 0.1303
 1950/10000 [====>.........................] - ETA: 1:07:26 - loss: 0.6669 - regression_loss: 0.5366 - classification_loss: 0.1303
 1951/10000 [====>.........................] - ETA: 1:07:25 - loss: 0.6670 - regression_loss: 0.5368 - classification_loss: 0.1302
 1952/10000 [====>.........................] - ETA: 1:07:25 - loss: 0.6672 - regression_loss: 0.5370 - classification_loss: 0.1302
 1953/10000 [====>.........................] - ETA: 1:07:24 - loss: 0.6672 - regression_loss: 0.5370 - classification_loss: 0.1302
 1954/10000 [====>.........................] - ETA: 1:07:24 - loss: 0.6672 - regression_loss: 0.5370 - classification_loss: 0.1302
 1955/10000 [====>.........................] - ETA: 1:07:23 - loss: 0.6670 - regression_loss: 0.5368 - classification_loss: 0.1302
 1956/10000 [====>.........................] - ETA: 1:07:22 - loss: 0.6667 - regression_loss: 0.5366 - classification_loss: 0.1301
 1957/10000 [====>.........................] - ETA: 1:07:21 - loss: 0.6668 - regression_loss: 0.5367 - classification_loss: 0.1301
 1958/10000 [====>.........................] - ETA: 1:07:21 - loss: 0.6666 - regression_loss: 0.5365 - classification_loss: 0.1301
 1959/10000 [====>.........................] - ETA: 1:07:20 - loss: 0.6664 - regression_loss: 0.5364 - classification_loss: 0.1301
 1960/10000 [====>.........................] - ETA: 1:07:19 - loss: 0.6661 - regression_loss: 0.5361 - classification_loss: 0.1300
 1961/10000 [====>.........................] - ETA: 1:07:19 - loss: 0.6662 - regression_loss: 0.5361 - classification_loss: 0.1301
 1962/10000 [====>.........................] - ETA: 1:07:18 - loss: 0.6660 - regression_loss: 0.5360 - classification_loss: 0.1300
 1963/10000 [====>.........................] - ETA: 1:07:17 - loss: 0.6660 - regression_loss: 0.5360 - classification_loss: 0.1301
 1964/10000 [====>.........................] - ETA: 1:07:17 - loss: 0.6658 - regression_loss: 0.5358 - classification_loss: 0.1301
 1965/10000 [====>.........................] - ETA: 1:07:16 - loss: 0.6659 - regression_loss: 0.5358 - classification_loss: 0.1301
 1966/10000 [====>.........................] - ETA: 1:07:15 - loss: 0.6662 - regression_loss: 0.5360 - classification_loss: 0.1301
 1967/10000 [====>.........................] - ETA: 1:07:15 - loss: 0.6660 - regression_loss: 0.5359 - classification_loss: 0.1301
 1968/10000 [====>.........................] - ETA: 1:07:15 - loss: 0.6659 - regression_loss: 0.5358 - classification_loss: 0.1301
 1969/10000 [====>.........................] - ETA: 1:07:15 - loss: 0.6658 - regression_loss: 0.5357 - classification_loss: 0.1300
 1970/10000 [====>.........................] - ETA: 1:07:14 - loss: 0.6657 - regression_loss: 0.5357 - classification_loss: 0.1300
 1971/10000 [====>.........................] - ETA: 1:07:13 - loss: 0.6660 - regression_loss: 0.5359 - classification_loss: 0.1301
 1972/10000 [====>.........................] - ETA: 1:07:13 - loss: 0.6658 - regression_loss: 0.5357 - classification_loss: 0.1301
 1973/10000 [====>.........................] - ETA: 1:07:12 - loss: 0.6660 - regression_loss: 0.5359 - classification_loss: 0.1301
 1974/10000 [====>.........................] - ETA: 1:07:11 - loss: 0.6659 - regression_loss: 0.5358 - classification_loss: 0.1300
 1975/10000 [====>.........................] - ETA: 1:07:11 - loss: 0.6658 - regression_loss: 0.5358 - classification_loss: 0.1300
 1976/10000 [====>.........................] - ETA: 1:07:10 - loss: 0.6657 - regression_loss: 0.5357 - classification_loss: 0.1300
 1977/10000 [====>.........................] - ETA: 1:07:09 - loss: 0.6657 - regression_loss: 0.5357 - classification_loss: 0.1300
 1978/10000 [====>.........................] - ETA: 1:07:09 - loss: 0.6660 - regression_loss: 0.5360 - classification_loss: 0.1301
 1979/10000 [====>.........................] - ETA: 1:07:08 - loss: 0.6665 - regression_loss: 0.5363 - classification_loss: 0.1302
 1980/10000 [====>.........................] - ETA: 1:07:07 - loss: 0.6666 - regression_loss: 0.5364 - classification_loss: 0.1302
 1981/10000 [====>.........................] - ETA: 1:07:07 - loss: 0.6665 - regression_loss: 0.5364 - classification_loss: 0.1302
 1982/10000 [====>.........................] - ETA: 1:07:06 - loss: 0.6666 - regression_loss: 0.5364 - classification_loss: 0.1302
 1983/10000 [====>.........................] - ETA: 1:07:05 - loss: 0.6667 - regression_loss: 0.5365 - classification_loss: 0.1302
 1984/10000 [====>.........................] - ETA: 1:07:04 - loss: 0.6668 - regression_loss: 0.5366 - classification_loss: 0.1302
 1985/10000 [====>.........................] - ETA: 1:07:04 - loss: 0.6666 - regression_loss: 0.5365 - classification_loss: 0.1301
 1986/10000 [====>.........................] - ETA: 1:07:03 - loss: 0.6666 - regression_loss: 0.5365 - classification_loss: 0.1301
 1987/10000 [====>.........................] - ETA: 1:07:03 - loss: 0.6665 - regression_loss: 0.5364 - classification_loss: 0.1301
 1988/10000 [====>.........................] - ETA: 1:07:02 - loss: 0.6664 - regression_loss: 0.5364 - classification_loss: 0.1300
 1989/10000 [====>.........................] - ETA: 1:07:01 - loss: 0.6666 - regression_loss: 0.5365 - classification_loss: 0.1301
 1990/10000 [====>.........................] - ETA: 1:07:00 - loss: 0.6666 - regression_loss: 0.5365 - classification_loss: 0.1301
 1991/10000 [====>.........................] - ETA: 1:07:00 - loss: 0.6668 - regression_loss: 0.5367 - classification_loss: 0.1301
 1992/10000 [====>.........................] - ETA: 1:07:00 - loss: 0.6666 - regression_loss: 0.5366 - classification_loss: 0.1301
 1993/10000 [====>.........................] - ETA: 1:06:59 - loss: 0.6674 - regression_loss: 0.5371 - classification_loss: 0.1303
 1994/10000 [====>.........................] - ETA: 1:06:58 - loss: 0.6672 - regression_loss: 0.5370 - classification_loss: 0.1302
 1995/10000 [====>.........................] - ETA: 1:06:59 - loss: 0.6674 - regression_loss: 0.5371 - classification_loss: 0.1302
 1996/10000 [====>.........................] - ETA: 1:06:59 - loss: 0.6675 - regression_loss: 0.5372 - classification_loss: 0.1302
 1997/10000 [====>.........................] - ETA: 1:06:58 - loss: 0.6674 - regression_loss: 0.5372 - classification_loss: 0.1303
 1998/10000 [====>.........................] - ETA: 1:06:57 - loss: 0.6676 - regression_loss: 0.5373 - classification_loss: 0.1303
 1999/10000 [====>.........................] - ETA: 1:06:57 - loss: 0.6677 - regression_loss: 0.5373 - classification_loss: 0.1303
 2000/10000 [=====>........................] - ETA: 1:06:56 - loss: 0.6678 - regression_loss: 0.5374 - classification_loss: 0.1304
 2001/10000 [=====>........................] - ETA: 1:06:55 - loss: 0.6679 - regression_loss: 0.5375 - classification_loss: 0.1304
 2002/10000 [=====>........................] - ETA: 1:06:55 - loss: 0.6681 - regression_loss: 0.5376 - classification_loss: 0.1306
 2003/10000 [=====>........................] - ETA: 1:06:54 - loss: 0.6682 - regression_loss: 0.5376 - classification_loss: 0.1305
 2004/10000 [=====>........................] - ETA: 1:06:53 - loss: 0.6682 - regression_loss: 0.5376 - classification_loss: 0.1306
 2005/10000 [=====>........................] - ETA: 1:06:53 - loss: 0.6684 - regression_loss: 0.5378 - classification_loss: 0.1306
 2006/10000 [=====>........................] - ETA: 1:06:52 - loss: 0.6687 - regression_loss: 0.5380 - classification_loss: 0.1306
 2007/10000 [=====>........................] - ETA: 1:06:51 - loss: 0.6687 - regression_loss: 0.5381 - classification_loss: 0.1306
 2008/10000 [=====>........................] - ETA: 1:06:51 - loss: 0.6685 - regression_loss: 0.5379 - classification_loss: 0.1306
 2009/10000 [=====>........................] - ETA: 1:06:50 - loss: 0.6683 - regression_loss: 0.5378 - classification_loss: 0.1305
 2010/10000 [=====>........................] - ETA: 1:06:49 - loss: 0.6682 - regression_loss: 0.5377 - classification_loss: 0.1306
 2011/10000 [=====>........................] - ETA: 1:06:49 - loss: 0.6681 - regression_loss: 0.5376 - classification_loss: 0.1305
 2012/10000 [=====>........................] - ETA: 1:06:48 - loss: 0.6681 - regression_loss: 0.5376 - classification_loss: 0.1305
 2013/10000 [=====>........................] - ETA: 1:06:48 - loss: 0.6681 - regression_loss: 0.5376 - classification_loss: 0.1305
 2014/10000 [=====>........................] - ETA: 1:06:47 - loss: 0.6680 - regression_loss: 0.5375 - classification_loss: 0.1305
 2015/10000 [=====>........................] - ETA: 1:06:46 - loss: 0.6679 - regression_loss: 0.5374 - classification_loss: 0.1305
 2016/10000 [=====>........................] - ETA: 1:06:46 - loss: 0.6682 - regression_loss: 0.5375 - classification_loss: 0.1307
 2017/10000 [=====>........................] - ETA: 1:06:45 - loss: 0.6681 - regression_loss: 0.5374 - classification_loss: 0.1307
 2018/10000 [=====>........................] - ETA: 1:06:44 - loss: 0.6680 - regression_loss: 0.5373 - classification_loss: 0.1307
 2019/10000 [=====>........................] - ETA: 1:06:43 - loss: 0.6680 - regression_loss: 0.5373 - classification_loss: 0.1307
 2020/10000 [=====>........................] - ETA: 1:06:43 - loss: 0.6680 - regression_loss: 0.5374 - classification_loss: 0.1307
 2021/10000 [=====>........................] - ETA: 1:06:42 - loss: 0.6681 - regression_loss: 0.5374 - classification_loss: 0.1307
 2022/10000 [=====>........................] - ETA: 1:06:42 - loss: 0.6680 - regression_loss: 0.5373 - classification_loss: 0.1307
 2023/10000 [=====>........................] - ETA: 1:06:41 - loss: 0.6683 - regression_loss: 0.5376 - classification_loss: 0.1307
 2024/10000 [=====>........................] - ETA: 1:06:41 - loss: 0.6690 - regression_loss: 0.5381 - classification_loss: 0.1308
 2025/10000 [=====>........................] - ETA: 1:06:40 - loss: 0.6690 - regression_loss: 0.5381 - classification_loss: 0.1308
 2026/10000 [=====>........................] - ETA: 1:06:39 - loss: 0.6687 - regression_loss: 0.5379 - classification_loss: 0.1308
 2027/10000 [=====>........................] - ETA: 1:06:39 - loss: 0.6684 - regression_loss: 0.5377 - classification_loss: 0.1307
 2028/10000 [=====>........................] - ETA: 1:06:38 - loss: 0.6685 - regression_loss: 0.5376 - classification_loss: 0.1309
 2029/10000 [=====>........................] - ETA: 1:06:37 - loss: 0.6683 - regression_loss: 0.5375 - classification_loss: 0.1308
 2030/10000 [=====>........................] - ETA: 1:06:37 - loss: 0.6685 - regression_loss: 0.5376 - classification_loss: 0.1308
 2031/10000 [=====>........................] - ETA: 1:06:36 - loss: 0.6689 - regression_loss: 0.5379 - classification_loss: 0.1310
 2032/10000 [=====>........................] - ETA: 1:06:36 - loss: 0.6688 - regression_loss: 0.5378 - classification_loss: 0.1309
 2033/10000 [=====>........................] - ETA: 1:06:35 - loss: 0.6691 - regression_loss: 0.5381 - classification_loss: 0.1309
 2034/10000 [=====>........................] - ETA: 1:06:34 - loss: 0.6692 - regression_loss: 0.5382 - classification_loss: 0.1310
 2035/10000 [=====>........................] - ETA: 1:06:33 - loss: 0.6694 - regression_loss: 0.5385 - classification_loss: 0.1309
 2036/10000 [=====>........................] - ETA: 1:06:33 - loss: 0.6693 - regression_loss: 0.5384 - classification_loss: 0.1310
 2037/10000 [=====>........................] - ETA: 1:06:32 - loss: 0.6692 - regression_loss: 0.5383 - classification_loss: 0.1309
 2038/10000 [=====>........................] - ETA: 1:06:32 - loss: 0.6693 - regression_loss: 0.5384 - classification_loss: 0.1309
 2039/10000 [=====>........................] - ETA: 1:06:31 - loss: 0.6692 - regression_loss: 0.5383 - classification_loss: 0.1309
 2040/10000 [=====>........................] - ETA: 1:06:30 - loss: 0.6692 - regression_loss: 0.5383 - classification_loss: 0.1309
 2041/10000 [=====>........................] - ETA: 1:06:30 - loss: 0.6692 - regression_loss: 0.5383 - classification_loss: 0.1309
 2042/10000 [=====>........................] - ETA: 1:06:29 - loss: 0.6694 - regression_loss: 0.5384 - classification_loss: 0.1309
 2043/10000 [=====>........................] - ETA: 1:06:30 - loss: 0.6694 - regression_loss: 0.5384 - classification_loss: 0.1310
 2044/10000 [=====>........................] - ETA: 1:06:29 - loss: 0.6692 - regression_loss: 0.5382 - classification_loss: 0.1310
 2045/10000 [=====>........................] - ETA: 1:06:29 - loss: 0.6691 - regression_loss: 0.5381 - classification_loss: 0.1310
 2046/10000 [=====>........................] - ETA: 1:06:28 - loss: 0.6689 - regression_loss: 0.5379 - classification_loss: 0.1309
 2047/10000 [=====>........................] - ETA: 1:06:27 - loss: 0.6688 - regression_loss: 0.5378 - classification_loss: 0.1309
 2048/10000 [=====>........................] - ETA: 1:06:27 - loss: 0.6686 - regression_loss: 0.5377 - classification_loss: 0.1309
 2049/10000 [=====>........................] - ETA: 1:06:26 - loss: 0.6683 - regression_loss: 0.5375 - classification_loss: 0.1308
 2050/10000 [=====>........................] - ETA: 1:06:25 - loss: 0.6689 - regression_loss: 0.5381 - classification_loss: 0.1308
 2051/10000 [=====>........................] - ETA: 1:06:26 - loss: 0.6689 - regression_loss: 0.5381 - classification_loss: 0.1308
 2052/10000 [=====>........................] - ETA: 1:06:26 - loss: 0.6691 - regression_loss: 0.5382 - classification_loss: 0.1309
 2053/10000 [=====>........................] - ETA: 1:06:25 - loss: 0.6692 - regression_loss: 0.5383 - classification_loss: 0.1309
 2054/10000 [=====>........................] - ETA: 1:06:24 - loss: 0.6693 - regression_loss: 0.5383 - classification_loss: 0.1309
 2055/10000 [=====>........................] - ETA: 1:06:24 - loss: 0.6692 - regression_loss: 0.5383 - classification_loss: 0.1309
 2056/10000 [=====>........................] - ETA: 1:06:23 - loss: 0.6690 - regression_loss: 0.5381 - classification_loss: 0.1309
 2057/10000 [=====>........................] - ETA: 1:06:22 - loss: 0.6695 - regression_loss: 0.5384 - classification_loss: 0.1310
 2058/10000 [=====>........................] - ETA: 1:06:22 - loss: 0.6694 - regression_loss: 0.5384 - classification_loss: 0.1311
 2059/10000 [=====>........................] - ETA: 1:06:21 - loss: 0.6694 - regression_loss: 0.5382 - classification_loss: 0.1311
 2060/10000 [=====>........................] - ETA: 1:06:20 - loss: 0.6699 - regression_loss: 0.5387 - classification_loss: 0.1312
 2061/10000 [=====>........................] - ETA: 1:06:20 - loss: 0.6697 - regression_loss: 0.5386 - classification_loss: 0.1311
 2062/10000 [=====>........................] - ETA: 1:06:19 - loss: 0.6696 - regression_loss: 0.5385 - classification_loss: 0.1311
 2063/10000 [=====>........................] - ETA: 1:06:18 - loss: 0.6695 - regression_loss: 0.5384 - classification_loss: 0.1311
 2064/10000 [=====>........................] - ETA: 1:06:18 - loss: 0.6696 - regression_loss: 0.5385 - classification_loss: 0.1311
 2065/10000 [=====>........................] - ETA: 1:06:17 - loss: 0.6695 - regression_loss: 0.5385 - classification_loss: 0.1311
 2066/10000 [=====>........................] - ETA: 1:06:16 - loss: 0.6693 - regression_loss: 0.5383 - classification_loss: 0.1310
 2067/10000 [=====>........................] - ETA: 1:06:16 - loss: 0.6696 - regression_loss: 0.5386 - classification_loss: 0.1310
 2068/10000 [=====>........................] - ETA: 1:06:15 - loss: 0.6696 - regression_loss: 0.5386 - classification_loss: 0.1310
 2069/10000 [=====>........................] - ETA: 1:06:15 - loss: 0.6694 - regression_loss: 0.5384 - classification_loss: 0.1310
 2070/10000 [=====>........................] - ETA: 1:06:14 - loss: 0.6695 - regression_loss: 0.5385 - classification_loss: 0.1310
 2071/10000 [=====>........................] - ETA: 1:06:13 - loss: 0.6694 - regression_loss: 0.5383 - classification_loss: 0.1310
 2072/10000 [=====>........................] - ETA: 1:06:13 - loss: 0.6696 - regression_loss: 0.5385 - classification_loss: 0.1310
 2073/10000 [=====>........................] - ETA: 1:06:12 - loss: 0.6693 - regression_loss: 0.5384 - classification_loss: 0.1310
 2074/10000 [=====>........................] - ETA: 1:06:11 - loss: 0.6695 - regression_loss: 0.5385 - classification_loss: 0.1310
 2075/10000 [=====>........................] - ETA: 1:06:11 - loss: 0.6694 - regression_loss: 0.5384 - classification_loss: 0.1310
 2076/10000 [=====>........................] - ETA: 1:06:10 - loss: 0.6696 - regression_loss: 0.5385 - classification_loss: 0.1311
 2077/10000 [=====>........................] - ETA: 1:06:09 - loss: 0.6696 - regression_loss: 0.5385 - classification_loss: 0.1311
 2078/10000 [=====>........................] - ETA: 1:06:09 - loss: 0.6697 - regression_loss: 0.5386 - classification_loss: 0.1311
 2079/10000 [=====>........................] - ETA: 1:06:08 - loss: 0.6699 - regression_loss: 0.5388 - classification_loss: 0.1311
 2080/10000 [=====>........................] - ETA: 1:06:07 - loss: 0.6701 - regression_loss: 0.5390 - classification_loss: 0.1311
 2081/10000 [=====>........................] - ETA: 1:06:07 - loss: 0.6700 - regression_loss: 0.5389 - classification_loss: 0.1311
 2082/10000 [=====>........................] - ETA: 1:06:06 - loss: 0.6699 - regression_loss: 0.5388 - classification_loss: 0.1311
 2083/10000 [=====>........................] - ETA: 1:06:05 - loss: 0.6701 - regression_loss: 0.5390 - classification_loss: 0.1311
 2084/10000 [=====>........................] - ETA: 1:06:04 - loss: 0.6703 - regression_loss: 0.5392 - classification_loss: 0.1311
 2085/10000 [=====>........................] - ETA: 1:06:04 - loss: 0.6704 - regression_loss: 0.5393 - classification_loss: 0.1312
 2086/10000 [=====>........................] - ETA: 1:06:03 - loss: 0.6702 - regression_loss: 0.5391 - classification_loss: 0.1311
 2087/10000 [=====>........................] - ETA: 1:06:02 - loss: 0.6702 - regression_loss: 0.5391 - classification_loss: 0.1312
 2088/10000 [=====>........................] - ETA: 1:06:02 - loss: 0.6700 - regression_loss: 0.5389 - classification_loss: 0.1311
 2089/10000 [=====>........................] - ETA: 1:06:01 - loss: 0.6699 - regression_loss: 0.5388 - classification_loss: 0.1311
 2090/10000 [=====>........................] - ETA: 1:06:00 - loss: 0.6698 - regression_loss: 0.5387 - classification_loss: 0.1311
 2091/10000 [=====>........................] - ETA: 1:06:00 - loss: 0.6698 - regression_loss: 0.5387 - classification_loss: 0.1311
 2092/10000 [=====>........................] - ETA: 1:05:59 - loss: 0.6698 - regression_loss: 0.5387 - classification_loss: 0.1310
 2093/10000 [=====>........................] - ETA: 1:05:59 - loss: 0.6695 - regression_loss: 0.5385 - classification_loss: 0.1310
 2094/10000 [=====>........................] - ETA: 1:05:58 - loss: 0.6695 - regression_loss: 0.5384 - classification_loss: 0.1311
 2095/10000 [=====>........................] - ETA: 1:05:57 - loss: 0.6694 - regression_loss: 0.5383 - classification_loss: 0.1311
 2096/10000 [=====>........................] - ETA: 1:05:56 - loss: 0.6700 - regression_loss: 0.5389 - classification_loss: 0.1311
 2097/10000 [=====>........................] - ETA: 1:05:56 - loss: 0.6699 - regression_loss: 0.5388 - classification_loss: 0.1311
 2098/10000 [=====>........................] - ETA: 1:05:55 - loss: 0.6703 - regression_loss: 0.5391 - classification_loss: 0.1312
 2099/10000 [=====>........................] - ETA: 1:05:54 - loss: 0.6703 - regression_loss: 0.5391 - classification_loss: 0.1312
 2100/10000 [=====>........................] - ETA: 1:05:54 - loss: 0.6706 - regression_loss: 0.5394 - classification_loss: 0.1312
 2101/10000 [=====>........................] - ETA: 1:05:53 - loss: 0.6706 - regression_loss: 0.5394 - classification_loss: 0.1312
 2102/10000 [=====>........................] - ETA: 1:05:53 - loss: 0.6706 - regression_loss: 0.5394 - classification_loss: 0.1312
 2103/10000 [=====>........................] - ETA: 1:05:52 - loss: 0.6709 - regression_loss: 0.5397 - classification_loss: 0.1313
 2104/10000 [=====>........................] - ETA: 1:05:51 - loss: 0.6707 - regression_loss: 0.5395 - classification_loss: 0.1312
 2105/10000 [=====>........................] - ETA: 1:05:50 - loss: 0.6713 - regression_loss: 0.5400 - classification_loss: 0.1313
 2106/10000 [=====>........................] - ETA: 1:05:49 - loss: 0.6713 - regression_loss: 0.5400 - classification_loss: 0.1313
 2107/10000 [=====>........................] - ETA: 1:05:49 - loss: 0.6716 - regression_loss: 0.5402 - classification_loss: 0.1314
 2108/10000 [=====>........................] - ETA: 1:05:48 - loss: 0.6715 - regression_loss: 0.5402 - classification_loss: 0.1313
 2109/10000 [=====>........................] - ETA: 1:05:47 - loss: 0.6715 - regression_loss: 0.5401 - classification_loss: 0.1313
 2110/10000 [=====>........................] - ETA: 1:05:47 - loss: 0.6713 - regression_loss: 0.5400 - classification_loss: 0.1313
 2111/10000 [=====>........................] - ETA: 1:05:46 - loss: 0.6710 - regression_loss: 0.5398 - classification_loss: 0.1313
 2112/10000 [=====>........................] - ETA: 1:05:45 - loss: 0.6708 - regression_loss: 0.5396 - classification_loss: 0.1312
 2113/10000 [=====>........................] - ETA: 1:05:45 - loss: 0.6710 - regression_loss: 0.5397 - classification_loss: 0.1313
 2114/10000 [=====>........................] - ETA: 1:05:44 - loss: 0.6709 - regression_loss: 0.5396 - classification_loss: 0.1312
 2115/10000 [=====>........................] - ETA: 1:05:43 - loss: 0.6708 - regression_loss: 0.5396 - classification_loss: 0.1312
 2116/10000 [=====>........................] - ETA: 1:05:43 - loss: 0.6707 - regression_loss: 0.5395 - classification_loss: 0.1312
 2117/10000 [=====>........................] - ETA: 1:05:42 - loss: 0.6707 - regression_loss: 0.5396 - classification_loss: 0.1312
 2118/10000 [=====>........................] - ETA: 1:05:42 - loss: 0.6710 - regression_loss: 0.5398 - classification_loss: 0.1312
 2119/10000 [=====>........................] - ETA: 1:05:41 - loss: 0.6708 - regression_loss: 0.5396 - classification_loss: 0.1312
 2120/10000 [=====>........................] - ETA: 1:05:40 - loss: 0.6710 - regression_loss: 0.5397 - classification_loss: 0.1313
 2121/10000 [=====>........................] - ETA: 1:05:40 - loss: 0.6713 - regression_loss: 0.5400 - classification_loss: 0.1313
 2122/10000 [=====>........................] - ETA: 1:05:39 - loss: 0.6712 - regression_loss: 0.5400 - classification_loss: 0.1313
 2123/10000 [=====>........................] - ETA: 1:05:38 - loss: 0.6714 - regression_loss: 0.5401 - classification_loss: 0.1313
 2124/10000 [=====>........................] - ETA: 1:05:38 - loss: 0.6715 - regression_loss: 0.5402 - classification_loss: 0.1313
 2125/10000 [=====>........................] - ETA: 1:05:37 - loss: 0.6716 - regression_loss: 0.5403 - classification_loss: 0.1314
 2126/10000 [=====>........................] - ETA: 1:05:36 - loss: 0.6718 - regression_loss: 0.5404 - classification_loss: 0.1314
 2127/10000 [=====>........................] - ETA: 1:05:36 - loss: 0.6716 - regression_loss: 0.5403 - classification_loss: 0.1313
 2128/10000 [=====>........................] - ETA: 1:05:35 - loss: 0.6718 - regression_loss: 0.5404 - classification_loss: 0.1314
 2129/10000 [=====>........................] - ETA: 1:05:34 - loss: 0.6717 - regression_loss: 0.5404 - classification_loss: 0.1313
 2130/10000 [=====>........................] - ETA: 1:05:34 - loss: 0.6719 - regression_loss: 0.5405 - classification_loss: 0.1314
 2131/10000 [=====>........................] - ETA: 1:05:33 - loss: 0.6717 - regression_loss: 0.5403 - classification_loss: 0.1314
 2132/10000 [=====>........................] - ETA: 1:05:33 - loss: 0.6718 - regression_loss: 0.5404 - classification_loss: 0.1314
 2133/10000 [=====>........................] - ETA: 1:05:32 - loss: 0.6717 - regression_loss: 0.5403 - classification_loss: 0.1314
 2134/10000 [=====>........................] - ETA: 1:05:31 - loss: 0.6715 - regression_loss: 0.5402 - classification_loss: 0.1313
 2135/10000 [=====>........................] - ETA: 1:05:31 - loss: 0.6715 - regression_loss: 0.5401 - classification_loss: 0.1314
 2136/10000 [=====>........................] - ETA: 1:05:30 - loss: 0.6720 - regression_loss: 0.5406 - classification_loss: 0.1315
 2137/10000 [=====>........................] - ETA: 1:05:29 - loss: 0.6723 - regression_loss: 0.5408 - classification_loss: 0.1315
 2138/10000 [=====>........................] - ETA: 1:05:29 - loss: 0.6721 - regression_loss: 0.5407 - classification_loss: 0.1314
 2139/10000 [=====>........................] - ETA: 1:05:28 - loss: 0.6724 - regression_loss: 0.5409 - classification_loss: 0.1315
 2140/10000 [=====>........................] - ETA: 1:05:28 - loss: 0.6723 - regression_loss: 0.5408 - classification_loss: 0.1315
 2141/10000 [=====>........................] - ETA: 1:05:27 - loss: 0.6724 - regression_loss: 0.5409 - classification_loss: 0.1315
 2142/10000 [=====>........................] - ETA: 1:05:26 - loss: 0.6727 - regression_loss: 0.5411 - classification_loss: 0.1316
 2143/10000 [=====>........................] - ETA: 1:05:26 - loss: 0.6725 - regression_loss: 0.5409 - classification_loss: 0.1316
 2144/10000 [=====>........................] - ETA: 1:05:25 - loss: 0.6724 - regression_loss: 0.5408 - classification_loss: 0.1316
 2145/10000 [=====>........................] - ETA: 1:05:24 - loss: 0.6722 - regression_loss: 0.5406 - classification_loss: 0.1315
 2146/10000 [=====>........................] - ETA: 1:05:24 - loss: 0.6721 - regression_loss: 0.5405 - classification_loss: 0.1316
 2147/10000 [=====>........................] - ETA: 1:05:23 - loss: 0.6721 - regression_loss: 0.5405 - classification_loss: 0.1316
 2148/10000 [=====>........................] - ETA: 1:05:22 - loss: 0.6720 - regression_loss: 0.5405 - classification_loss: 0.1316
 2149/10000 [=====>........................] - ETA: 1:05:21 - loss: 0.6718 - regression_loss: 0.5403 - classification_loss: 0.1315
 2150/10000 [=====>........................] - ETA: 1:05:21 - loss: 0.6719 - regression_loss: 0.5404 - classification_loss: 0.1315
 2151/10000 [=====>........................] - ETA: 1:05:20 - loss: 0.6718 - regression_loss: 0.5403 - classification_loss: 0.1315
 2152/10000 [=====>........................] - ETA: 1:05:20 - loss: 0.6716 - regression_loss: 0.5401 - classification_loss: 0.1315
 2153/10000 [=====>........................] - ETA: 1:05:19 - loss: 0.6714 - regression_loss: 0.5400 - classification_loss: 0.1314
 2154/10000 [=====>........................] - ETA: 1:05:19 - loss: 0.6715 - regression_loss: 0.5401 - classification_loss: 0.1314
 2155/10000 [=====>........................] - ETA: 1:05:18 - loss: 0.6714 - regression_loss: 0.5400 - classification_loss: 0.1314
 2156/10000 [=====>........................] - ETA: 1:05:17 - loss: 0.6716 - regression_loss: 0.5402 - classification_loss: 0.1314
 2157/10000 [=====>........................] - ETA: 1:05:17 - loss: 0.6717 - regression_loss: 0.5403 - classification_loss: 0.1314
 2158/10000 [=====>........................] - ETA: 1:05:16 - loss: 0.6717 - regression_loss: 0.5403 - classification_loss: 0.1314
 2159/10000 [=====>........................] - ETA: 1:05:15 - loss: 0.6720 - regression_loss: 0.5406 - classification_loss: 0.1314
 2160/10000 [=====>........................] - ETA: 1:05:15 - loss: 0.6720 - regression_loss: 0.5404 - classification_loss: 0.1315
 2161/10000 [=====>........................] - ETA: 1:05:14 - loss: 0.6720 - regression_loss: 0.5405 - classification_loss: 0.1315
 2162/10000 [=====>........................] - ETA: 1:05:13 - loss: 0.6719 - regression_loss: 0.5404 - classification_loss: 0.1315
 2163/10000 [=====>........................] - ETA: 1:05:13 - loss: 0.6720 - regression_loss: 0.5405 - classification_loss: 0.1315
 2164/10000 [=====>........................] - ETA: 1:05:12 - loss: 0.6720 - regression_loss: 0.5405 - classification_loss: 0.1315
 2165/10000 [=====>........................] - ETA: 1:05:11 - loss: 0.6721 - regression_loss: 0.5406 - classification_loss: 0.1315
 2166/10000 [=====>........................] - ETA: 1:05:10 - loss: 0.6722 - regression_loss: 0.5407 - classification_loss: 0.1315
 2167/10000 [=====>........................] - ETA: 1:05:10 - loss: 0.6722 - regression_loss: 0.5407 - classification_loss: 0.1315
 2168/10000 [=====>........................] - ETA: 1:05:09 - loss: 0.6723 - regression_loss: 0.5408 - classification_loss: 0.1314
 2169/10000 [=====>........................] - ETA: 1:05:08 - loss: 0.6723 - regression_loss: 0.5409 - classification_loss: 0.1314
 2170/10000 [=====>........................] - ETA: 1:05:08 - loss: 0.6726 - regression_loss: 0.5410 - classification_loss: 0.1316
 2171/10000 [=====>........................] - ETA: 1:05:07 - loss: 0.6728 - regression_loss: 0.5411 - classification_loss: 0.1316
 2172/10000 [=====>........................] - ETA: 1:05:07 - loss: 0.6727 - regression_loss: 0.5411 - classification_loss: 0.1316
 2173/10000 [=====>........................] - ETA: 1:05:06 - loss: 0.6726 - regression_loss: 0.5410 - classification_loss: 0.1316
 2174/10000 [=====>........................] - ETA: 1:05:05 - loss: 0.6727 - regression_loss: 0.5411 - classification_loss: 0.1316
 2175/10000 [=====>........................] - ETA: 1:05:05 - loss: 0.6733 - regression_loss: 0.5414 - classification_loss: 0.1318
 2176/10000 [=====>........................] - ETA: 1:05:04 - loss: 0.6735 - regression_loss: 0.5416 - classification_loss: 0.1319
 2177/10000 [=====>........................] - ETA: 1:05:03 - loss: 0.6736 - regression_loss: 0.5417 - classification_loss: 0.1319
 2178/10000 [=====>........................] - ETA: 1:05:03 - loss: 0.6737 - regression_loss: 0.5418 - classification_loss: 0.1319
 2179/10000 [=====>........................] - ETA: 1:05:02 - loss: 0.6738 - regression_loss: 0.5420 - classification_loss: 0.1319
 2180/10000 [=====>........................] - ETA: 1:05:01 - loss: 0.6738 - regression_loss: 0.5419 - classification_loss: 0.1319
 2181/10000 [=====>........................] - ETA: 1:05:01 - loss: 0.6737 - regression_loss: 0.5418 - classification_loss: 0.1319
 2182/10000 [=====>........................] - ETA: 1:05:00 - loss: 0.6735 - regression_loss: 0.5417 - classification_loss: 0.1318
 2183/10000 [=====>........................] - ETA: 1:04:59 - loss: 0.6734 - regression_loss: 0.5416 - classification_loss: 0.1318
 2184/10000 [=====>........................] - ETA: 1:04:59 - loss: 0.6736 - regression_loss: 0.5418 - classification_loss: 0.1318
 2185/10000 [=====>........................] - ETA: 1:04:58 - loss: 0.6734 - regression_loss: 0.5417 - classification_loss: 0.1318
 2186/10000 [=====>........................] - ETA: 1:04:57 - loss: 0.6733 - regression_loss: 0.5415 - classification_loss: 0.1317
 2187/10000 [=====>........................] - ETA: 1:04:57 - loss: 0.6733 - regression_loss: 0.5416 - classification_loss: 0.1317
 2188/10000 [=====>........................] - ETA: 1:04:56 - loss: 0.6736 - regression_loss: 0.5418 - classification_loss: 0.1317
 2189/10000 [=====>........................] - ETA: 1:04:56 - loss: 0.6734 - regression_loss: 0.5417 - classification_loss: 0.1317
 2190/10000 [=====>........................] - ETA: 1:04:55 - loss: 0.6734 - regression_loss: 0.5417 - classification_loss: 0.1317
 2191/10000 [=====>........................] - ETA: 1:04:54 - loss: 0.6735 - regression_loss: 0.5417 - classification_loss: 0.1317
 2192/10000 [=====>........................] - ETA: 1:04:54 - loss: 0.6735 - regression_loss: 0.5418 - classification_loss: 0.1317
 2193/10000 [=====>........................] - ETA: 1:04:53 - loss: 0.6736 - regression_loss: 0.5419 - classification_loss: 0.1317
 2194/10000 [=====>........................] - ETA: 1:04:52 - loss: 0.6735 - regression_loss: 0.5417 - classification_loss: 0.1318
 2195/10000 [=====>........................] - ETA: 1:04:52 - loss: 0.6736 - regression_loss: 0.5418 - classification_loss: 0.1318
 2196/10000 [=====>........................] - ETA: 1:04:51 - loss: 0.6735 - regression_loss: 0.5417 - classification_loss: 0.1318
 2197/10000 [=====>........................] - ETA: 1:04:50 - loss: 0.6735 - regression_loss: 0.5417 - classification_loss: 0.1318
 2198/10000 [=====>........................] - ETA: 1:04:50 - loss: 0.6738 - regression_loss: 0.5419 - classification_loss: 0.1319
 2199/10000 [=====>........................] - ETA: 1:04:49 - loss: 0.6740 - regression_loss: 0.5421 - classification_loss: 0.1319
 2200/10000 [=====>........................] - ETA: 1:04:48 - loss: 0.6739 - regression_loss: 0.5421 - classification_loss: 0.1318
 2201/10000 [=====>........................] - ETA: 1:04:48 - loss: 0.6738 - regression_loss: 0.5419 - classification_loss: 0.1318
 2202/10000 [=====>........................] - ETA: 1:04:47 - loss: 0.6738 - regression_loss: 0.5420 - classification_loss: 0.1318
 2203/10000 [=====>........................] - ETA: 1:04:46 - loss: 0.6736 - regression_loss: 0.5418 - classification_loss: 0.1318
 2204/10000 [=====>........................] - ETA: 1:04:46 - loss: 0.6734 - regression_loss: 0.5416 - classification_loss: 0.1318
 2205/10000 [=====>........................] - ETA: 1:04:45 - loss: 0.6731 - regression_loss: 0.5414 - classification_loss: 0.1317
 2206/10000 [=====>........................] - ETA: 1:04:44 - loss: 0.6733 - regression_loss: 0.5416 - classification_loss: 0.1317
 2207/10000 [=====>........................] - ETA: 1:04:44 - loss: 0.6731 - regression_loss: 0.5414 - classification_loss: 0.1317
 2208/10000 [=====>........................] - ETA: 1:04:43 - loss: 0.6732 - regression_loss: 0.5415 - classification_loss: 0.1317
 2209/10000 [=====>........................] - ETA: 1:04:42 - loss: 0.6731 - regression_loss: 0.5414 - classification_loss: 0.1317
 2210/10000 [=====>........................] - ETA: 1:04:42 - loss: 0.6731 - regression_loss: 0.5414 - classification_loss: 0.1317
 2211/10000 [=====>........................] - ETA: 1:04:41 - loss: 0.6731 - regression_loss: 0.5414 - classification_loss: 0.1317
 2212/10000 [=====>........................] - ETA: 1:04:40 - loss: 0.6728 - regression_loss: 0.5412 - classification_loss: 0.1317
 2213/10000 [=====>........................] - ETA: 1:04:40 - loss: 0.6727 - regression_loss: 0.5411 - classification_loss: 0.1316
 2214/10000 [=====>........................] - ETA: 1:04:39 - loss: 0.6725 - regression_loss: 0.5409 - classification_loss: 0.1316
 2215/10000 [=====>........................] - ETA: 1:04:39 - loss: 0.6726 - regression_loss: 0.5409 - classification_loss: 0.1317
 2216/10000 [=====>........................] - ETA: 1:04:38 - loss: 0.6727 - regression_loss: 0.5410 - classification_loss: 0.1316
 2217/10000 [=====>........................] - ETA: 1:04:37 - loss: 0.6728 - regression_loss: 0.5411 - classification_loss: 0.1316
 2218/10000 [=====>........................] - ETA: 1:04:37 - loss: 0.6727 - regression_loss: 0.5411 - classification_loss: 0.1316
 2219/10000 [=====>........................] - ETA: 1:04:36 - loss: 0.6727 - regression_loss: 0.5410 - classification_loss: 0.1317
 2220/10000 [=====>........................] - ETA: 1:04:35 - loss: 0.6731 - regression_loss: 0.5414 - classification_loss: 0.1317
 2221/10000 [=====>........................] - ETA: 1:04:35 - loss: 0.6729 - regression_loss: 0.5413 - classification_loss: 0.1316
 2222/10000 [=====>........................] - ETA: 1:04:34 - loss: 0.6730 - regression_loss: 0.5413 - classification_loss: 0.1316
 2223/10000 [=====>........................] - ETA: 1:04:33 - loss: 0.6731 - regression_loss: 0.5414 - classification_loss: 0.1317
 2224/10000 [=====>........................] - ETA: 1:04:33 - loss: 0.6733 - regression_loss: 0.5416 - classification_loss: 0.1317
 2225/10000 [=====>........................] - ETA: 1:04:33 - loss: 0.6735 - regression_loss: 0.5418 - classification_loss: 0.1317
 2226/10000 [=====>........................] - ETA: 1:04:32 - loss: 0.6734 - regression_loss: 0.5417 - classification_loss: 0.1317
 2227/10000 [=====>........................] - ETA: 1:04:31 - loss: 0.6732 - regression_loss: 0.5416 - classification_loss: 0.1317
 2228/10000 [=====>........................] - ETA: 1:04:31 - loss: 0.6732 - regression_loss: 0.5415 - classification_loss: 0.1316
 2229/10000 [=====>........................] - ETA: 1:04:30 - loss: 0.6732 - regression_loss: 0.5415 - classification_loss: 0.1316
 2230/10000 [=====>........................] - ETA: 1:04:30 - loss: 0.6734 - regression_loss: 0.5418 - classification_loss: 0.1317
 2231/10000 [=====>........................] - ETA: 1:04:29 - loss: 0.6733 - regression_loss: 0.5417 - classification_loss: 0.1316
 2232/10000 [=====>........................] - ETA: 1:04:28 - loss: 0.6731 - regression_loss: 0.5415 - classification_loss: 0.1316
 2233/10000 [=====>........................] - ETA: 1:04:28 - loss: 0.6730 - regression_loss: 0.5414 - classification_loss: 0.1316
 2234/10000 [=====>........................] - ETA: 1:04:27 - loss: 0.6730 - regression_loss: 0.5413 - classification_loss: 0.1316
 2235/10000 [=====>........................] - ETA: 1:04:27 - loss: 0.6728 - regression_loss: 0.5412 - classification_loss: 0.1316
 2236/10000 [=====>........................] - ETA: 1:04:26 - loss: 0.6727 - regression_loss: 0.5412 - classification_loss: 0.1316
 2237/10000 [=====>........................] - ETA: 1:04:26 - loss: 0.6730 - regression_loss: 0.5414 - classification_loss: 0.1316
 2238/10000 [=====>........................] - ETA: 1:04:25 - loss: 0.6728 - regression_loss: 0.5412 - classification_loss: 0.1316
 2239/10000 [=====>........................] - ETA: 1:04:24 - loss: 0.6727 - regression_loss: 0.5411 - classification_loss: 0.1315
 2240/10000 [=====>........................] - ETA: 1:04:24 - loss: 0.6725 - regression_loss: 0.5410 - classification_loss: 0.1315
 2241/10000 [=====>........................] - ETA: 1:04:23 - loss: 0.6724 - regression_loss: 0.5409 - classification_loss: 0.1315
 2242/10000 [=====>........................] - ETA: 1:04:23 - loss: 0.6722 - regression_loss: 0.5408 - classification_loss: 0.1314
 2243/10000 [=====>........................] - ETA: 1:04:22 - loss: 0.6721 - regression_loss: 0.5407 - classification_loss: 0.1314
 2244/10000 [=====>........................] - ETA: 1:04:21 - loss: 0.6723 - regression_loss: 0.5409 - classification_loss: 0.1314
 2245/10000 [=====>........................] - ETA: 1:04:21 - loss: 0.6721 - regression_loss: 0.5408 - classification_loss: 0.1314
 2246/10000 [=====>........................] - ETA: 1:04:20 - loss: 0.6720 - regression_loss: 0.5406 - classification_loss: 0.1313
 2247/10000 [=====>........................] - ETA: 1:04:20 - loss: 0.6720 - regression_loss: 0.5407 - classification_loss: 0.1313
 2248/10000 [=====>........................] - ETA: 1:04:19 - loss: 0.6721 - regression_loss: 0.5408 - classification_loss: 0.1313
 2249/10000 [=====>........................] - ETA: 1:04:18 - loss: 0.6721 - regression_loss: 0.5408 - classification_loss: 0.1313
 2250/10000 [=====>........................] - ETA: 1:04:18 - loss: 0.6720 - regression_loss: 0.5407 - classification_loss: 0.1313
 2251/10000 [=====>........................] - ETA: 1:04:17 - loss: 0.6719 - regression_loss: 0.5406 - classification_loss: 0.1313
 2252/10000 [=====>........................] - ETA: 1:04:16 - loss: 0.6720 - regression_loss: 0.5408 - classification_loss: 0.1313
 2253/10000 [=====>........................] - ETA: 1:04:16 - loss: 0.6719 - regression_loss: 0.5407 - classification_loss: 0.1313
 2254/10000 [=====>........................] - ETA: 1:04:15 - loss: 0.6724 - regression_loss: 0.5411 - classification_loss: 0.1313
 2255/10000 [=====>........................] - ETA: 1:04:15 - loss: 0.6723 - regression_loss: 0.5409 - classification_loss: 0.1314
 2256/10000 [=====>........................] - ETA: 1:04:14 - loss: 0.6728 - regression_loss: 0.5413 - classification_loss: 0.1315
 2257/10000 [=====>........................] - ETA: 1:04:13 - loss: 0.6729 - regression_loss: 0.5413 - classification_loss: 0.1316
 2258/10000 [=====>........................] - ETA: 1:04:13 - loss: 0.6727 - regression_loss: 0.5411 - classification_loss: 0.1316
 2259/10000 [=====>........................] - ETA: 1:04:12 - loss: 0.6731 - regression_loss: 0.5413 - classification_loss: 0.1318
 2260/10000 [=====>........................] - ETA: 1:04:11 - loss: 0.6730 - regression_loss: 0.5412 - classification_loss: 0.1318
 2261/10000 [=====>........................] - ETA: 1:04:11 - loss: 0.6729 - regression_loss: 0.5411 - classification_loss: 0.1318
 2262/10000 [=====>........................] - ETA: 1:04:13 - loss: 0.6727 - regression_loss: 0.5409 - classification_loss: 0.1318
 2263/10000 [=====>........................] - ETA: 1:04:12 - loss: 0.6732 - regression_loss: 0.5413 - classification_loss: 0.1318
 2264/10000 [=====>........................] - ETA: 1:04:11 - loss: 0.6730 - regression_loss: 0.5412 - classification_loss: 0.1318
 2265/10000 [=====>........................] - ETA: 1:04:11 - loss: 0.6729 - regression_loss: 0.5411 - classification_loss: 0.1318
 2266/10000 [=====>........................] - ETA: 1:04:10 - loss: 0.6728 - regression_loss: 0.5411 - classification_loss: 0.1317
 2267/10000 [=====>........................] - ETA: 1:04:10 - loss: 0.6728 - regression_loss: 0.5410 - classification_loss: 0.1317
 2268/10000 [=====>........................] - ETA: 1:04:09 - loss: 0.6729 - regression_loss: 0.5412 - classification_loss: 0.1317
 2269/10000 [=====>........................] - ETA: 1:04:08 - loss: 0.6731 - regression_loss: 0.5413 - classification_loss: 0.1318
 2270/10000 [=====>........................] - ETA: 1:04:08 - loss: 0.6732 - regression_loss: 0.5415 - classification_loss: 0.1318
 2271/10000 [=====>........................] - ETA: 1:04:07 - loss: 0.6734 - regression_loss: 0.5416 - classification_loss: 0.1318
 2272/10000 [=====>........................] - ETA: 1:04:06 - loss: 0.6737 - regression_loss: 0.5417 - classification_loss: 0.1320
 2273/10000 [=====>........................] - ETA: 1:04:05 - loss: 0.6735 - regression_loss: 0.5415 - classification_loss: 0.1319
 2274/10000 [=====>........................] - ETA: 1:04:05 - loss: 0.6735 - regression_loss: 0.5415 - classification_loss: 0.1319
 2275/10000 [=====>........................] - ETA: 1:04:04 - loss: 0.6734 - regression_loss: 0.5414 - classification_loss: 0.1319
 2276/10000 [=====>........................] - ETA: 1:04:04 - loss: 0.6732 - regression_loss: 0.5413 - classification_loss: 0.1319
 2277/10000 [=====>........................] - ETA: 1:04:03 - loss: 0.6734 - regression_loss: 0.5415 - classification_loss: 0.1319
 2278/10000 [=====>........................] - ETA: 1:04:05 - loss: 0.6736 - regression_loss: 0.5417 - classification_loss: 0.1320
 2279/10000 [=====>........................] - ETA: 1:04:04 - loss: 0.6738 - regression_loss: 0.5419 - classification_loss: 0.1320
 2280/10000 [=====>........................] - ETA: 1:04:04 - loss: 0.6738 - regression_loss: 0.5419 - classification_loss: 0.1320
 2281/10000 [=====>........................] - ETA: 1:04:03 - loss: 0.6736 - regression_loss: 0.5417 - classification_loss: 0.1319
 2282/10000 [=====>........................] - ETA: 1:04:02 - loss: 0.6734 - regression_loss: 0.5415 - classification_loss: 0.1319
 2283/10000 [=====>........................] - ETA: 1:04:02 - loss: 0.6733 - regression_loss: 0.5414 - classification_loss: 0.1319
 2284/10000 [=====>........................] - ETA: 1:04:01 - loss: 0.6731 - regression_loss: 0.5413 - classification_loss: 0.1319
 2285/10000 [=====>........................] - ETA: 1:04:00 - loss: 0.6731 - regression_loss: 0.5412 - classification_loss: 0.1319
 2286/10000 [=====>........................] - ETA: 1:04:00 - loss: 0.6729 - regression_loss: 0.5410 - classification_loss: 0.1318
 2287/10000 [=====>........................] - ETA: 1:03:59 - loss: 0.6728 - regression_loss: 0.5410 - classification_loss: 0.1318
 2288/10000 [=====>........................] - ETA: 1:03:59 - loss: 0.6726 - regression_loss: 0.5408 - classification_loss: 0.1318
 2289/10000 [=====>........................] - ETA: 1:03:58 - loss: 0.6726 - regression_loss: 0.5408 - classification_loss: 0.1319
 2290/10000 [=====>........................] - ETA: 1:03:57 - loss: 0.6726 - regression_loss: 0.5407 - classification_loss: 0.1318
 2291/10000 [=====>........................] - ETA: 1:03:56 - loss: 0.6728 - regression_loss: 0.5410 - classification_loss: 0.1318
 2292/10000 [=====>........................] - ETA: 1:03:56 - loss: 0.6733 - regression_loss: 0.5414 - classification_loss: 0.1319
 2293/10000 [=====>........................] - ETA: 1:03:56 - loss: 0.6731 - regression_loss: 0.5413 - classification_loss: 0.1319
 2294/10000 [=====>........................] - ETA: 1:03:56 - loss: 0.6732 - regression_loss: 0.5412 - classification_loss: 0.1320
 2295/10000 [=====>........................] - ETA: 1:03:55 - loss: 0.6733 - regression_loss: 0.5413 - classification_loss: 0.1320
 2296/10000 [=====>........................] - ETA: 1:03:54 - loss: 0.6733 - regression_loss: 0.5413 - classification_loss: 0.1320
 2297/10000 [=====>........................] - ETA: 1:03:54 - loss: 0.6732 - regression_loss: 0.5412 - classification_loss: 0.1320
 2298/10000 [=====>........................] - ETA: 1:03:53 - loss: 0.6730 - regression_loss: 0.5410 - classification_loss: 0.1319
 2299/10000 [=====>........................] - ETA: 1:03:52 - loss: 0.6732 - regression_loss: 0.5412 - classification_loss: 0.1319
 2300/10000 [=====>........................] - ETA: 1:03:52 - loss: 0.6733 - regression_loss: 0.5413 - classification_loss: 0.1320
 2301/10000 [=====>........................] - ETA: 1:03:51 - loss: 0.6731 - regression_loss: 0.5411 - classification_loss: 0.1319
 2302/10000 [=====>........................] - ETA: 1:03:51 - loss: 0.6732 - regression_loss: 0.5412 - classification_loss: 0.1320
 2303/10000 [=====>........................] - ETA: 1:03:50 - loss: 0.6733 - regression_loss: 0.5413 - classification_loss: 0.1320
 2304/10000 [=====>........................] - ETA: 1:03:50 - loss: 0.6732 - regression_loss: 0.5411 - classification_loss: 0.1320
 2305/10000 [=====>........................] - ETA: 1:03:49 - loss: 0.6732 - regression_loss: 0.5412 - classification_loss: 0.1320
 2306/10000 [=====>........................] - ETA: 1:03:49 - loss: 0.6737 - regression_loss: 0.5416 - classification_loss: 0.1321
 2307/10000 [=====>........................] - ETA: 1:03:48 - loss: 0.6735 - regression_loss: 0.5415 - classification_loss: 0.1320
 2308/10000 [=====>........................] - ETA: 1:03:47 - loss: 0.6738 - regression_loss: 0.5418 - classification_loss: 0.1321
 2309/10000 [=====>........................] - ETA: 1:03:47 - loss: 0.6737 - regression_loss: 0.5417 - classification_loss: 0.1320
 2310/10000 [=====>........................] - ETA: 1:03:46 - loss: 0.6736 - regression_loss: 0.5416 - classification_loss: 0.1320
 2311/10000 [=====>........................] - ETA: 1:03:45 - loss: 0.6736 - regression_loss: 0.5416 - classification_loss: 0.1320
 2312/10000 [=====>........................] - ETA: 1:03:46 - loss: 0.6737 - regression_loss: 0.5418 - classification_loss: 0.1320
 2313/10000 [=====>........................] - ETA: 1:03:45 - loss: 0.6737 - regression_loss: 0.5417 - classification_loss: 0.1320
 2314/10000 [=====>........................] - ETA: 1:03:45 - loss: 0.6737 - regression_loss: 0.5417 - classification_loss: 0.1320
 2315/10000 [=====>........................] - ETA: 1:03:44 - loss: 0.6742 - regression_loss: 0.5421 - classification_loss: 0.1321
 2316/10000 [=====>........................] - ETA: 1:03:43 - loss: 0.6744 - regression_loss: 0.5423 - classification_loss: 0.1322
 2317/10000 [=====>........................] - ETA: 1:03:43 - loss: 0.6743 - regression_loss: 0.5422 - classification_loss: 0.1321
 2318/10000 [=====>........................] - ETA: 1:03:42 - loss: 0.6743 - regression_loss: 0.5422 - classification_loss: 0.1321
 2319/10000 [=====>........................] - ETA: 1:03:41 - loss: 0.6746 - regression_loss: 0.5423 - classification_loss: 0.1322
 2320/10000 [=====>........................] - ETA: 1:03:41 - loss: 0.6747 - regression_loss: 0.5425 - classification_loss: 0.1323
 2321/10000 [=====>........................] - ETA: 1:03:40 - loss: 0.6746 - regression_loss: 0.5424 - classification_loss: 0.1323
 2322/10000 [=====>........................] - ETA: 1:03:40 - loss: 0.6744 - regression_loss: 0.5422 - classification_loss: 0.1322
 2323/10000 [=====>........................] - ETA: 1:03:39 - loss: 0.6747 - regression_loss: 0.5424 - classification_loss: 0.1323
 2324/10000 [=====>........................] - ETA: 1:03:38 - loss: 0.6747 - regression_loss: 0.5423 - classification_loss: 0.1323
 2325/10000 [=====>........................] - ETA: 1:03:38 - loss: 0.6748 - regression_loss: 0.5424 - classification_loss: 0.1323
 2326/10000 [=====>........................] - ETA: 1:03:37 - loss: 0.6746 - regression_loss: 0.5423 - classification_loss: 0.1323
 2327/10000 [=====>........................] - ETA: 1:03:37 - loss: 0.6746 - regression_loss: 0.5423 - classification_loss: 0.1323
 2328/10000 [=====>........................] - ETA: 1:03:36 - loss: 0.6746 - regression_loss: 0.5423 - classification_loss: 0.1323
 2329/10000 [=====>........................] - ETA: 1:03:35 - loss: 0.6744 - regression_loss: 0.5421 - classification_loss: 0.1323
 2330/10000 [=====>........................] - ETA: 1:03:35 - loss: 0.6746 - regression_loss: 0.5423 - classification_loss: 0.1323
 2331/10000 [=====>........................] - ETA: 1:03:34 - loss: 0.6745 - regression_loss: 0.5422 - classification_loss: 0.1323
 2332/10000 [=====>........................] - ETA: 1:03:34 - loss: 0.6745 - regression_loss: 0.5422 - classification_loss: 0.1323
 2333/10000 [=====>........................] - ETA: 1:03:33 - loss: 0.6746 - regression_loss: 0.5423 - classification_loss: 0.1323
 2334/10000 [======>.......................] - ETA: 1:03:32 - loss: 0.6745 - regression_loss: 0.5422 - classification_loss: 0.1323
 2335/10000 [======>.......................] - ETA: 1:03:32 - loss: 0.6746 - regression_loss: 0.5423 - classification_loss: 0.1323
 2336/10000 [======>.......................] - ETA: 1:03:31 - loss: 0.6745 - regression_loss: 0.5421 - classification_loss: 0.1323
 2337/10000 [======>.......................] - ETA: 1:03:30 - loss: 0.6748 - regression_loss: 0.5422 - classification_loss: 0.1326
 2338/10000 [======>.......................] - ETA: 1:03:30 - loss: 0.6746 - regression_loss: 0.5420 - classification_loss: 0.1326
 2339/10000 [======>.......................] - ETA: 1:03:29 - loss: 0.6746 - regression_loss: 0.5421 - classification_loss: 0.1326
 2340/10000 [======>.......................] - ETA: 1:03:29 - loss: 0.6750 - regression_loss: 0.5424 - classification_loss: 0.1326
 2341/10000 [======>.......................] - ETA: 1:03:28 - loss: 0.6749 - regression_loss: 0.5424 - classification_loss: 0.1325
 2342/10000 [======>.......................] - ETA: 1:03:27 - loss: 0.6750 - regression_loss: 0.5424 - classification_loss: 0.1326
 2343/10000 [======>.......................] - ETA: 1:03:27 - loss: 0.6748 - regression_loss: 0.5422 - classification_loss: 0.1326
 2344/10000 [======>.......................] - ETA: 1:03:26 - loss: 0.6751 - regression_loss: 0.5424 - classification_loss: 0.1327
 2345/10000 [======>.......................] - ETA: 1:03:25 - loss: 0.6749 - regression_loss: 0.5422 - classification_loss: 0.1326
 2346/10000 [======>.......................] - ETA: 1:03:25 - loss: 0.6748 - regression_loss: 0.5421 - classification_loss: 0.1326
 2347/10000 [======>.......................] - ETA: 1:03:24 - loss: 0.6747 - regression_loss: 0.5421 - classification_loss: 0.1326
 2348/10000 [======>.......................] - ETA: 1:03:24 - loss: 0.6745 - regression_loss: 0.5419 - classification_loss: 0.1326
 2349/10000 [======>.......................] - ETA: 1:03:23 - loss: 0.6745 - regression_loss: 0.5419 - classification_loss: 0.1326
 2350/10000 [======>.......................] - ETA: 1:03:22 - loss: 0.6745 - regression_loss: 0.5419 - classification_loss: 0.1326
 2351/10000 [======>.......................] - ETA: 1:03:22 - loss: 0.6747 - regression_loss: 0.5421 - classification_loss: 0.1326
 2352/10000 [======>.......................] - ETA: 1:03:21 - loss: 0.6751 - regression_loss: 0.5424 - classification_loss: 0.1327
 2353/10000 [======>.......................] - ETA: 1:03:21 - loss: 0.6753 - regression_loss: 0.5426 - classification_loss: 0.1327
 2354/10000 [======>.......................] - ETA: 1:03:20 - loss: 0.6755 - regression_loss: 0.5427 - classification_loss: 0.1328
 2355/10000 [======>.......................] - ETA: 1:03:20 - loss: 0.6757 - regression_loss: 0.5429 - classification_loss: 0.1328
 2356/10000 [======>.......................] - ETA: 1:03:19 - loss: 0.6757 - regression_loss: 0.5429 - classification_loss: 0.1328
 2357/10000 [======>.......................] - ETA: 1:03:18 - loss: 0.6756 - regression_loss: 0.5429 - classification_loss: 0.1328
 2358/10000 [======>.......................] - ETA: 1:03:18 - loss: 0.6755 - regression_loss: 0.5428 - classification_loss: 0.1327
 2359/10000 [======>.......................] - ETA: 1:03:17 - loss: 0.6755 - regression_loss: 0.5428 - classification_loss: 0.1328
 2360/10000 [======>.......................] - ETA: 1:03:16 - loss: 0.6754 - regression_loss: 0.5427 - classification_loss: 0.1327
 2361/10000 [======>.......................] - ETA: 1:03:16 - loss: 0.6757 - regression_loss: 0.5429 - classification_loss: 0.1328
 2362/10000 [======>.......................] - ETA: 1:03:15 - loss: 0.6756 - regression_loss: 0.5428 - classification_loss: 0.1328
 2363/10000 [======>.......................] - ETA: 1:03:15 - loss: 0.6755 - regression_loss: 0.5427 - classification_loss: 0.1327
 2364/10000 [======>.......................] - ETA: 1:03:14 - loss: 0.6753 - regression_loss: 0.5426 - classification_loss: 0.1327
 2365/10000 [======>.......................] - ETA: 1:03:13 - loss: 0.6753 - regression_loss: 0.5426 - classification_loss: 0.1327
 2366/10000 [======>.......................] - ETA: 1:03:13 - loss: 0.6752 - regression_loss: 0.5425 - classification_loss: 0.1327
 2367/10000 [======>.......................] - ETA: 1:03:12 - loss: 0.6757 - regression_loss: 0.5429 - classification_loss: 0.1327
 2368/10000 [======>.......................] - ETA: 1:03:12 - loss: 0.6757 - regression_loss: 0.5430 - classification_loss: 0.1327
 2369/10000 [======>.......................] - ETA: 1:03:11 - loss: 0.6757 - regression_loss: 0.5430 - classification_loss: 0.1327
 2370/10000 [======>.......................] - ETA: 1:03:10 - loss: 0.6757 - regression_loss: 0.5430 - classification_loss: 0.1327
 2371/10000 [======>.......................] - ETA: 1:03:10 - loss: 0.6755 - regression_loss: 0.5429 - classification_loss: 0.1327
 2372/10000 [======>.......................] - ETA: 1:03:09 - loss: 0.6755 - regression_loss: 0.5428 - classification_loss: 0.1327
 2373/10000 [======>.......................] - ETA: 1:03:08 - loss: 0.6757 - regression_loss: 0.5430 - classification_loss: 0.1327
 2374/10000 [======>.......................] - ETA: 1:03:08 - loss: 0.6755 - regression_loss: 0.5428 - classification_loss: 0.1327
 2375/10000 [======>.......................] - ETA: 1:03:07 - loss: 0.6755 - regression_loss: 0.5428 - classification_loss: 0.1327
 2376/10000 [======>.......................] - ETA: 1:03:06 - loss: 0.6754 - regression_loss: 0.5427 - classification_loss: 0.1327
 2377/10000 [======>.......................] - ETA: 1:03:06 - loss: 0.6753 - regression_loss: 0.5427 - classification_loss: 0.1327
 2378/10000 [======>.......................] - ETA: 1:03:05 - loss: 0.6756 - regression_loss: 0.5430 - classification_loss: 0.1327
 2379/10000 [======>.......................] - ETA: 1:03:04 - loss: 0.6758 - regression_loss: 0.5431 - classification_loss: 0.1327
 2380/10000 [======>.......................] - ETA: 1:03:04 - loss: 0.6755 - regression_loss: 0.5429 - classification_loss: 0.1327
 2381/10000 [======>.......................] - ETA: 1:03:03 - loss: 0.6754 - regression_loss: 0.5427 - classification_loss: 0.1326
 2382/10000 [======>.......................] - ETA: 1:03:02 - loss: 0.6753 - regression_loss: 0.5426 - classification_loss: 0.1326
 2383/10000 [======>.......................] - ETA: 1:03:02 - loss: 0.6751 - regression_loss: 0.5425 - classification_loss: 0.1326
 2384/10000 [======>.......................] - ETA: 1:03:01 - loss: 0.6752 - regression_loss: 0.5425 - classification_loss: 0.1326
 2385/10000 [======>.......................] - ETA: 1:03:01 - loss: 0.6751 - regression_loss: 0.5425 - classification_loss: 0.1326
 2386/10000 [======>.......................] - ETA: 1:03:00 - loss: 0.6752 - regression_loss: 0.5425 - classification_loss: 0.1327
 2387/10000 [======>.......................] - ETA: 1:02:59 - loss: 0.6752 - regression_loss: 0.5425 - classification_loss: 0.1327
 2388/10000 [======>.......................] - ETA: 1:02:59 - loss: 0.6755 - regression_loss: 0.5428 - classification_loss: 0.1327
 2389/10000 [======>.......................] - ETA: 1:02:58 - loss: 0.6754 - regression_loss: 0.5427 - classification_loss: 0.1327
 2390/10000 [======>.......................] - ETA: 1:02:57 - loss: 0.6753 - regression_loss: 0.5427 - classification_loss: 0.1326
 2391/10000 [======>.......................] - ETA: 1:02:57 - loss: 0.6752 - regression_loss: 0.5426 - classification_loss: 0.1326
 2392/10000 [======>.......................] - ETA: 1:02:56 - loss: 0.6751 - regression_loss: 0.5425 - classification_loss: 0.1326
 2393/10000 [======>.......................] - ETA: 1:02:55 - loss: 0.6749 - regression_loss: 0.5424 - classification_loss: 0.1326
 2394/10000 [======>.......................] - ETA: 1:02:55 - loss: 0.6750 - regression_loss: 0.5424 - classification_loss: 0.1325
 2395/10000 [======>.......................] - ETA: 1:02:54 - loss: 0.6750 - regression_loss: 0.5424 - classification_loss: 0.1325
 2396/10000 [======>.......................] - ETA: 1:02:54 - loss: 0.6751 - regression_loss: 0.5426 - classification_loss: 0.1325
 2397/10000 [======>.......................] - ETA: 1:02:53 - loss: 0.6749 - regression_loss: 0.5424 - classification_loss: 0.1325
 2398/10000 [======>.......................] - ETA: 1:02:53 - loss: 0.6749 - regression_loss: 0.5424 - classification_loss: 0.1325
 2399/10000 [======>.......................] - ETA: 1:02:52 - loss: 0.6748 - regression_loss: 0.5423 - classification_loss: 0.1325
 2400/10000 [======>.......................] - ETA: 1:02:51 - loss: 0.6746 - regression_loss: 0.5421 - classification_loss: 0.1325
 2401/10000 [======>.......................] - ETA: 1:02:51 - loss: 0.6746 - regression_loss: 0.5421 - classification_loss: 0.1325
 2402/10000 [======>.......................] - ETA: 1:02:50 - loss: 0.6747 - regression_loss: 0.5422 - classification_loss: 0.1325
 2403/10000 [======>.......................] - ETA: 1:02:50 - loss: 0.6750 - regression_loss: 0.5424 - classification_loss: 0.1325
 2404/10000 [======>.......................] - ETA: 1:02:49 - loss: 0.6752 - regression_loss: 0.5427 - classification_loss: 0.1325
 2405/10000 [======>.......................] - ETA: 1:02:48 - loss: 0.6750 - regression_loss: 0.5425 - classification_loss: 0.1325
 2406/10000 [======>.......................] - ETA: 1:02:48 - loss: 0.6750 - regression_loss: 0.5425 - classification_loss: 0.1325
 2407/10000 [======>.......................] - ETA: 1:02:47 - loss: 0.6749 - regression_loss: 0.5424 - classification_loss: 0.1325
 2408/10000 [======>.......................] - ETA: 1:02:46 - loss: 0.6749 - regression_loss: 0.5424 - classification_loss: 0.1325
 2409/10000 [======>.......................] - ETA: 1:02:45 - loss: 0.6748 - regression_loss: 0.5423 - classification_loss: 0.1324
 2410/10000 [======>.......................] - ETA: 1:02:45 - loss: 0.6750 - regression_loss: 0.5425 - classification_loss: 0.1325
 2411/10000 [======>.......................] - ETA: 1:02:44 - loss: 0.6748 - regression_loss: 0.5424 - classification_loss: 0.1325
 2412/10000 [======>.......................] - ETA: 1:02:44 - loss: 0.6747 - regression_loss: 0.5422 - classification_loss: 0.1324
 2413/10000 [======>.......................] - ETA: 1:02:45 - loss: 0.6746 - regression_loss: 0.5422 - classification_loss: 0.1324
 2414/10000 [======>.......................] - ETA: 1:02:44 - loss: 0.6744 - regression_loss: 0.5420 - classification_loss: 0.1324
 2415/10000 [======>.......................] - ETA: 1:02:43 - loss: 0.6746 - regression_loss: 0.5421 - classification_loss: 0.1325
 2416/10000 [======>.......................] - ETA: 1:02:43 - loss: 0.6745 - regression_loss: 0.5421 - classification_loss: 0.1325
 2417/10000 [======>.......................] - ETA: 1:02:42 - loss: 0.6746 - regression_loss: 0.5421 - classification_loss: 0.1325
 2418/10000 [======>.......................] - ETA: 1:02:42 - loss: 0.6745 - regression_loss: 0.5420 - classification_loss: 0.1325
 2419/10000 [======>.......................] - ETA: 1:02:41 - loss: 0.6745 - regression_loss: 0.5420 - classification_loss: 0.1325
 2420/10000 [======>.......................] - ETA: 1:02:41 - loss: 0.6746 - regression_loss: 0.5421 - classification_loss: 0.1325
 2421/10000 [======>.......................] - ETA: 1:02:40 - loss: 0.6746 - regression_loss: 0.5421 - classification_loss: 0.1325
 2422/10000 [======>.......................] - ETA: 1:02:39 - loss: 0.6746 - regression_loss: 0.5421 - classification_loss: 0.1324
 2423/10000 [======>.......................] - ETA: 1:02:39 - loss: 0.6744 - regression_loss: 0.5420 - classification_loss: 0.1324
 2424/10000 [======>.......................] - ETA: 1:02:38 - loss: 0.6746 - regression_loss: 0.5421 - classification_loss: 0.1325
 2425/10000 [======>.......................] - ETA: 1:02:38 - loss: 0.6744 - regression_loss: 0.5419 - classification_loss: 0.1324
 2426/10000 [======>.......................] - ETA: 1:02:37 - loss: 0.6745 - regression_loss: 0.5421 - classification_loss: 0.1324
 2427/10000 [======>.......................] - ETA: 1:02:36 - loss: 0.6745 - regression_loss: 0.5421 - classification_loss: 0.1324
 2428/10000 [======>.......................] - ETA: 1:02:36 - loss: 0.6745 - regression_loss: 0.5421 - classification_loss: 0.1324
 2429/10000 [======>.......................] - ETA: 1:02:35 - loss: 0.6746 - regression_loss: 0.5422 - classification_loss: 0.1324
 2430/10000 [======>.......................] - ETA: 1:02:35 - loss: 0.6748 - regression_loss: 0.5423 - classification_loss: 0.1325
 2431/10000 [======>.......................] - ETA: 1:02:34 - loss: 0.6748 - regression_loss: 0.5422 - classification_loss: 0.1326
 2432/10000 [======>.......................] - ETA: 1:02:34 - loss: 0.6748 - regression_loss: 0.5422 - classification_loss: 0.1326
 2433/10000 [======>.......................] - ETA: 1:02:33 - loss: 0.6747 - regression_loss: 0.5421 - classification_loss: 0.1326
 2434/10000 [======>.......................] - ETA: 1:02:32 - loss: 0.6747 - regression_loss: 0.5421 - classification_loss: 0.1326
 2435/10000 [======>.......................] - ETA: 1:02:32 - loss: 0.6748 - regression_loss: 0.5422 - classification_loss: 0.1326
 2436/10000 [======>.......................] - ETA: 1:02:31 - loss: 0.6750 - regression_loss: 0.5424 - classification_loss: 0.1326
 2437/10000 [======>.......................] - ETA: 1:02:31 - loss: 0.6754 - regression_loss: 0.5427 - classification_loss: 0.1326
 2438/10000 [======>.......................] - ETA: 1:02:30 - loss: 0.6754 - regression_loss: 0.5428 - classification_loss: 0.1326
 2439/10000 [======>.......................] - ETA: 1:02:29 - loss: 0.6752 - regression_loss: 0.5427 - classification_loss: 0.1326
 2440/10000 [======>.......................] - ETA: 1:02:29 - loss: 0.6754 - regression_loss: 0.5428 - classification_loss: 0.1326
 2441/10000 [======>.......................] - ETA: 1:02:28 - loss: 0.6759 - regression_loss: 0.5432 - classification_loss: 0.1327
 2442/10000 [======>.......................] - ETA: 1:02:28 - loss: 0.6760 - regression_loss: 0.5433 - classification_loss: 0.1327
 2443/10000 [======>.......................] - ETA: 1:02:27 - loss: 0.6758 - regression_loss: 0.5432 - classification_loss: 0.1327
 2444/10000 [======>.......................] - ETA: 1:02:26 - loss: 0.6757 - regression_loss: 0.5431 - classification_loss: 0.1327
 2445/10000 [======>.......................] - ETA: 1:02:26 - loss: 0.6760 - regression_loss: 0.5433 - classification_loss: 0.1327
 2446/10000 [======>.......................] - ETA: 1:02:25 - loss: 0.6760 - regression_loss: 0.5433 - classification_loss: 0.1327
 2447/10000 [======>.......................] - ETA: 1:02:25 - loss: 0.6761 - regression_loss: 0.5434 - classification_loss: 0.1327
 2448/10000 [======>.......................] - ETA: 1:02:24 - loss: 0.6762 - regression_loss: 0.5435 - classification_loss: 0.1327
 2449/10000 [======>.......................] - ETA: 1:02:24 - loss: 0.6762 - regression_loss: 0.5435 - classification_loss: 0.1327
 2450/10000 [======>.......................] - ETA: 1:02:23 - loss: 0.6760 - regression_loss: 0.5434 - classification_loss: 0.1327
 2451/10000 [======>.......................] - ETA: 1:02:22 - loss: 0.6760 - regression_loss: 0.5434 - classification_loss: 0.1327
 2452/10000 [======>.......................] - ETA: 1:02:22 - loss: 0.6761 - regression_loss: 0.5435 - classification_loss: 0.1327
 2453/10000 [======>.......................] - ETA: 1:02:21 - loss: 0.6764 - regression_loss: 0.5436 - classification_loss: 0.1327
 2454/10000 [======>.......................] - ETA: 1:02:21 - loss: 0.6763 - regression_loss: 0.5436 - classification_loss: 0.1327
 2455/10000 [======>.......................] - ETA: 1:02:20 - loss: 0.6761 - regression_loss: 0.5435 - classification_loss: 0.1327
 2456/10000 [======>.......................] - ETA: 1:02:20 - loss: 0.6764 - regression_loss: 0.5436 - classification_loss: 0.1327
 2457/10000 [======>.......................] - ETA: 1:02:19 - loss: 0.6765 - regression_loss: 0.5438 - classification_loss: 0.1327
 2458/10000 [======>.......................] - ETA: 1:02:18 - loss: 0.6766 - regression_loss: 0.5439 - classification_loss: 0.1327
 2459/10000 [======>.......................] - ETA: 1:02:18 - loss: 0.6767 - regression_loss: 0.5439 - classification_loss: 0.1327
 2460/10000 [======>.......................] - ETA: 1:02:17 - loss: 0.6767 - regression_loss: 0.5440 - classification_loss: 0.1327
 2461/10000 [======>.......................] - ETA: 1:02:17 - loss: 0.6766 - regression_loss: 0.5440 - classification_loss: 0.1327
 2462/10000 [======>.......................] - ETA: 1:02:16 - loss: 0.6766 - regression_loss: 0.5440 - classification_loss: 0.1327
 2463/10000 [======>.......................] - ETA: 1:02:16 - loss: 0.6765 - regression_loss: 0.5439 - classification_loss: 0.1326
 2464/10000 [======>.......................] - ETA: 1:02:15 - loss: 0.6766 - regression_loss: 0.5440 - classification_loss: 0.1326
 2465/10000 [======>.......................] - ETA: 1:02:14 - loss: 0.6768 - regression_loss: 0.5442 - classification_loss: 0.1326
 2466/10000 [======>.......................] - ETA: 1:02:14 - loss: 0.6767 - regression_loss: 0.5440 - classification_loss: 0.1326
 2467/10000 [======>.......................] - ETA: 1:02:13 - loss: 0.6770 - regression_loss: 0.5443 - classification_loss: 0.1327
 2468/10000 [======>.......................] - ETA: 1:02:13 - loss: 0.6769 - regression_loss: 0.5442 - classification_loss: 0.1327
 2469/10000 [======>.......................] - ETA: 1:02:12 - loss: 0.6770 - regression_loss: 0.5444 - classification_loss: 0.1327
 2470/10000 [======>.......................] - ETA: 1:02:12 - loss: 0.6775 - regression_loss: 0.5447 - classification_loss: 0.1328
 2471/10000 [======>.......................] - ETA: 1:02:11 - loss: 0.6774 - regression_loss: 0.5446 - classification_loss: 0.1327
 2472/10000 [======>.......................] - ETA: 1:02:11 - loss: 0.6773 - regression_loss: 0.5446 - classification_loss: 0.1327
 2473/10000 [======>.......................] - ETA: 1:02:10 - loss: 0.6772 - regression_loss: 0.5445 - classification_loss: 0.1327
 2474/10000 [======>.......................] - ETA: 1:02:09 - loss: 0.6771 - regression_loss: 0.5444 - classification_loss: 0.1327
 2475/10000 [======>.......................] - ETA: 1:02:08 - loss: 0.6771 - regression_loss: 0.5445 - classification_loss: 0.1327
 2476/10000 [======>.......................] - ETA: 1:02:08 - loss: 0.6771 - regression_loss: 0.5444 - classification_loss: 0.1327
 2477/10000 [======>.......................] - ETA: 1:02:07 - loss: 0.6771 - regression_loss: 0.5444 - classification_loss: 0.1327
 2478/10000 [======>.......................] - ETA: 1:02:07 - loss: 0.6772 - regression_loss: 0.5445 - classification_loss: 0.1327
 2479/10000 [======>.......................] - ETA: 1:02:06 - loss: 0.6771 - regression_loss: 0.5445 - classification_loss: 0.1326
 2480/10000 [======>.......................] - ETA: 1:02:05 - loss: 0.6774 - regression_loss: 0.5447 - classification_loss: 0.1327
 2481/10000 [======>.......................] - ETA: 1:02:05 - loss: 0.6775 - regression_loss: 0.5448 - classification_loss: 0.1327
 2482/10000 [======>.......................] - ETA: 1:02:04 - loss: 0.6776 - regression_loss: 0.5449 - classification_loss: 0.1327
 2483/10000 [======>.......................] - ETA: 1:02:03 - loss: 0.6775 - regression_loss: 0.5448 - classification_loss: 0.1327
 2484/10000 [======>.......................] - ETA: 1:02:03 - loss: 0.6773 - regression_loss: 0.5446 - classification_loss: 0.1327
 2485/10000 [======>.......................] - ETA: 1:02:02 - loss: 0.6772 - regression_loss: 0.5445 - classification_loss: 0.1327
 2486/10000 [======>.......................] - ETA: 1:02:01 - loss: 0.6773 - regression_loss: 0.5446 - classification_loss: 0.1327
 2487/10000 [======>.......................] - ETA: 1:02:01 - loss: 0.6774 - regression_loss: 0.5447 - classification_loss: 0.1327
 2488/10000 [======>.......................] - ETA: 1:02:00 - loss: 0.6773 - regression_loss: 0.5446 - classification_loss: 0.1327
 2489/10000 [======>.......................] - ETA: 1:01:59 - loss: 0.6772 - regression_loss: 0.5445 - classification_loss: 0.1327
 2490/10000 [======>.......................] - ETA: 1:01:59 - loss: 0.6771 - regression_loss: 0.5444 - classification_loss: 0.1327
 2491/10000 [======>.......................] - ETA: 1:01:58 - loss: 0.6770 - regression_loss: 0.5444 - classification_loss: 0.1326
 2492/10000 [======>.......................] - ETA: 1:01:58 - loss: 0.6772 - regression_loss: 0.5445 - classification_loss: 0.1327
 2493/10000 [======>.......................] - ETA: 1:01:57 - loss: 0.6775 - regression_loss: 0.5447 - classification_loss: 0.1328
 2494/10000 [======>.......................] - ETA: 1:01:56 - loss: 0.6773 - regression_loss: 0.5446 - classification_loss: 0.1328
 2495/10000 [======>.......................] - ETA: 1:01:56 - loss: 0.6773 - regression_loss: 0.5445 - classification_loss: 0.1328
 2496/10000 [======>.......................] - ETA: 1:01:55 - loss: 0.6772 - regression_loss: 0.5445 - classification_loss: 0.1328
 2497/10000 [======>.......................] - ETA: 1:01:55 - loss: 0.6773 - regression_loss: 0.5445 - classification_loss: 0.1327
 2498/10000 [======>.......................] - ETA: 1:01:54 - loss: 0.6772 - regression_loss: 0.5445 - classification_loss: 0.1327
 2499/10000 [======>.......................] - ETA: 1:01:54 - loss: 0.6770 - regression_loss: 0.5444 - classification_loss: 0.1327
 2500/10000 [======>.......................] - ETA: 1:01:53 - loss: 0.6772 - regression_loss: 0.5444 - classification_loss: 0.1327
 2501/10000 [======>.......................] - ETA: 1:01:53 - loss: 0.6773 - regression_loss: 0.5446 - classification_loss: 0.1328
 2502/10000 [======>.......................] - ETA: 1:01:52 - loss: 0.6773 - regression_loss: 0.5445 - classification_loss: 0.1328
 2503/10000 [======>.......................] - ETA: 1:01:51 - loss: 0.6773 - regression_loss: 0.5445 - classification_loss: 0.1328
 2504/10000 [======>.......................] - ETA: 1:01:51 - loss: 0.6774 - regression_loss: 0.5446 - classification_loss: 0.1328
 2505/10000 [======>.......................] - ETA: 1:01:50 - loss: 0.6774 - regression_loss: 0.5447 - classification_loss: 0.1328
 2506/10000 [======>.......................] - ETA: 1:01:50 - loss: 0.6774 - regression_loss: 0.5446 - classification_loss: 0.1327
 2507/10000 [======>.......................] - ETA: 1:01:49 - loss: 0.6773 - regression_loss: 0.5446 - classification_loss: 0.1327
 2508/10000 [======>.......................] - ETA: 1:01:48 - loss: 0.6775 - regression_loss: 0.5447 - classification_loss: 0.1328
 2509/10000 [======>.......................] - ETA: 1:01:48 - loss: 0.6777 - regression_loss: 0.5449 - classification_loss: 0.1328
 2510/10000 [======>.......................] - ETA: 1:01:47 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1328
 2511/10000 [======>.......................] - ETA: 1:01:46 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1328
 2512/10000 [======>.......................] - ETA: 1:01:46 - loss: 0.6778 - regression_loss: 0.5450 - classification_loss: 0.1329
 2513/10000 [======>.......................] - ETA: 1:01:45 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1328
 2514/10000 [======>.......................] - ETA: 1:01:44 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1328
 2515/10000 [======>.......................] - ETA: 1:01:44 - loss: 0.6777 - regression_loss: 0.5450 - classification_loss: 0.1328
 2516/10000 [======>.......................] - ETA: 1:01:43 - loss: 0.6777 - regression_loss: 0.5449 - classification_loss: 0.1328
 2517/10000 [======>.......................] - ETA: 1:01:43 - loss: 0.6776 - regression_loss: 0.5448 - classification_loss: 0.1328
 2518/10000 [======>.......................] - ETA: 1:01:42 - loss: 0.6775 - regression_loss: 0.5447 - classification_loss: 0.1328
 2519/10000 [======>.......................] - ETA: 1:01:41 - loss: 0.6776 - regression_loss: 0.5448 - classification_loss: 0.1328
 2520/10000 [======>.......................] - ETA: 1:01:41 - loss: 0.6776 - regression_loss: 0.5449 - classification_loss: 0.1328
 2521/10000 [======>.......................] - ETA: 1:01:40 - loss: 0.6778 - regression_loss: 0.5450 - classification_loss: 0.1328
 2522/10000 [======>.......................] - ETA: 1:01:40 - loss: 0.6778 - regression_loss: 0.5451 - classification_loss: 0.1328
 2523/10000 [======>.......................] - ETA: 1:01:39 - loss: 0.6779 - regression_loss: 0.5452 - classification_loss: 0.1327
 2524/10000 [======>.......................] - ETA: 1:01:39 - loss: 0.6781 - regression_loss: 0.5454 - classification_loss: 0.1327
 2525/10000 [======>.......................] - ETA: 1:01:38 - loss: 0.6782 - regression_loss: 0.5455 - classification_loss: 0.1328
 2526/10000 [======>.......................] - ETA: 1:01:38 - loss: 0.6784 - regression_loss: 0.5457 - classification_loss: 0.1327
 2527/10000 [======>.......................] - ETA: 1:01:37 - loss: 0.6783 - regression_loss: 0.5456 - classification_loss: 0.1327
 2528/10000 [======>.......................] - ETA: 1:01:36 - loss: 0.6780 - regression_loss: 0.5454 - classification_loss: 0.1327
 2529/10000 [======>.......................] - ETA: 1:01:36 - loss: 0.6782 - regression_loss: 0.5455 - classification_loss: 0.1327
 2530/10000 [======>.......................] - ETA: 1:01:35 - loss: 0.6785 - regression_loss: 0.5458 - classification_loss: 0.1327
 2531/10000 [======>.......................] - ETA: 1:01:35 - loss: 0.6785 - regression_loss: 0.5458 - classification_loss: 0.1327
 2532/10000 [======>.......................] - ETA: 1:01:34 - loss: 0.6786 - regression_loss: 0.5459 - classification_loss: 0.1327
 2533/10000 [======>.......................] - ETA: 1:01:33 - loss: 0.6784 - regression_loss: 0.5457 - classification_loss: 0.1327
 2534/10000 [======>.......................] - ETA: 1:01:33 - loss: 0.6785 - regression_loss: 0.5458 - classification_loss: 0.1327
 2535/10000 [======>.......................] - ETA: 1:01:32 - loss: 0.6785 - regression_loss: 0.5458 - classification_loss: 0.1327
 2536/10000 [======>.......................] - ETA: 1:01:32 - loss: 0.6783 - regression_loss: 0.5456 - classification_loss: 0.1327
 2537/10000 [======>.......................] - ETA: 1:01:31 - loss: 0.6781 - regression_loss: 0.5455 - classification_loss: 0.1326
 2538/10000 [======>.......................] - ETA: 1:01:31 - loss: 0.6781 - regression_loss: 0.5455 - classification_loss: 0.1326
 2539/10000 [======>.......................] - ETA: 1:01:30 - loss: 0.6783 - regression_loss: 0.5457 - classification_loss: 0.1327
 2540/10000 [======>.......................] - ETA: 1:01:29 - loss: 0.6784 - regression_loss: 0.5458 - classification_loss: 0.1327
 2541/10000 [======>.......................] - ETA: 1:01:29 - loss: 0.6783 - regression_loss: 0.5457 - classification_loss: 0.1326
 2542/10000 [======>.......................] - ETA: 1:01:28 - loss: 0.6781 - regression_loss: 0.5455 - classification_loss: 0.1326
 2543/10000 [======>.......................] - ETA: 1:01:28 - loss: 0.6781 - regression_loss: 0.5456 - classification_loss: 0.1326
 2544/10000 [======>.......................] - ETA: 1:01:27 - loss: 0.6780 - regression_loss: 0.5454 - classification_loss: 0.1326
 2545/10000 [======>.......................] - ETA: 1:01:27 - loss: 0.6780 - regression_loss: 0.5454 - classification_loss: 0.1326
 2546/10000 [======>.......................] - ETA: 1:01:26 - loss: 0.6783 - regression_loss: 0.5457 - classification_loss: 0.1326
 2547/10000 [======>.......................] - ETA: 1:01:25 - loss: 0.6781 - regression_loss: 0.5455 - classification_loss: 0.1326
 2548/10000 [======>.......................] - ETA: 1:01:25 - loss: 0.6781 - regression_loss: 0.5455 - classification_loss: 0.1326
 2549/10000 [======>.......................] - ETA: 1:01:24 - loss: 0.6779 - regression_loss: 0.5454 - classification_loss: 0.1325
 2550/10000 [======>.......................] - ETA: 1:01:24 - loss: 0.6785 - regression_loss: 0.5457 - classification_loss: 0.1328
 2551/10000 [======>.......................] - ETA: 1:01:23 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1328
 2552/10000 [======>.......................] - ETA: 1:01:22 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 2553/10000 [======>.......................] - ETA: 1:01:22 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 2554/10000 [======>.......................] - ETA: 1:01:21 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 2555/10000 [======>.......................] - ETA: 1:01:21 - loss: 0.6782 - regression_loss: 0.5454 - classification_loss: 0.1328
 2556/10000 [======>.......................] - ETA: 1:01:20 - loss: 0.6781 - regression_loss: 0.5453 - classification_loss: 0.1328
 2557/10000 [======>.......................] - ETA: 1:01:20 - loss: 0.6782 - regression_loss: 0.5454 - classification_loss: 0.1328
 2558/10000 [======>.......................] - ETA: 1:01:19 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 2559/10000 [======>.......................] - ETA: 1:01:20 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 2560/10000 [======>.......................] - ETA: 1:01:20 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1329
 2561/10000 [======>.......................] - ETA: 1:01:19 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1328
 2562/10000 [======>.......................] - ETA: 1:01:19 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1328
 2563/10000 [======>.......................] - ETA: 1:01:18 - loss: 0.6782 - regression_loss: 0.5454 - classification_loss: 0.1328
 2564/10000 [======>.......................] - ETA: 1:01:18 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 2565/10000 [======>.......................] - ETA: 1:01:17 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 2566/10000 [======>.......................] - ETA: 1:01:16 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 2567/10000 [======>.......................] - ETA: 1:01:16 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 2568/10000 [======>.......................] - ETA: 1:01:15 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 2569/10000 [======>.......................] - ETA: 1:01:15 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 2570/10000 [======>.......................] - ETA: 1:01:14 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 2571/10000 [======>.......................] - ETA: 1:01:13 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1329
 2572/10000 [======>.......................] - ETA: 1:01:13 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1329
 2573/10000 [======>.......................] - ETA: 1:01:12 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1329
 2574/10000 [======>.......................] - ETA: 1:01:11 - loss: 0.6782 - regression_loss: 0.5454 - classification_loss: 0.1328
 2575/10000 [======>.......................] - ETA: 1:01:11 - loss: 0.6782 - regression_loss: 0.5454 - classification_loss: 0.1328
 2576/10000 [======>.......................] - ETA: 1:01:10 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1328
 2577/10000 [======>.......................] - ETA: 1:01:09 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1328
 2578/10000 [======>.......................] - ETA: 1:01:09 - loss: 0.6786 - regression_loss: 0.5458 - classification_loss: 0.1328
 2579/10000 [======>.......................] - ETA: 1:01:08 - loss: 0.6785 - regression_loss: 0.5457 - classification_loss: 0.1328
 2580/10000 [======>.......................] - ETA: 1:01:08 - loss: 0.6784 - regression_loss: 0.5457 - classification_loss: 0.1328
 2581/10000 [======>.......................] - ETA: 1:01:07 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1328
 2582/10000 [======>.......................] - ETA: 1:01:07 - loss: 0.6783 - regression_loss: 0.5456 - classification_loss: 0.1328
 2583/10000 [======>.......................] - ETA: 1:01:06 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1328
 2584/10000 [======>.......................] - ETA: 1:01:05 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1328
 2585/10000 [======>.......................] - ETA: 1:01:05 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1328
 2586/10000 [======>.......................] - ETA: 1:01:04 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1328
 2587/10000 [======>.......................] - ETA: 1:01:04 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1328
 2588/10000 [======>.......................] - ETA: 1:01:03 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1328
 2589/10000 [======>.......................] - ETA: 1:01:03 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1328
 2590/10000 [======>.......................] - ETA: 1:01:02 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1328
 2591/10000 [======>.......................] - ETA: 1:01:01 - loss: 0.6781 - regression_loss: 0.5454 - classification_loss: 0.1327
 2592/10000 [======>.......................] - ETA: 1:01:01 - loss: 0.6783 - regression_loss: 0.5456 - classification_loss: 0.1327
 2593/10000 [======>.......................] - ETA: 1:01:00 - loss: 0.6785 - regression_loss: 0.5457 - classification_loss: 0.1327
 2594/10000 [======>.......................] - ETA: 1:01:00 - loss: 0.6786 - regression_loss: 0.5458 - classification_loss: 0.1328
 2595/10000 [======>.......................] - ETA: 1:00:59 - loss: 0.6786 - regression_loss: 0.5458 - classification_loss: 0.1328
 2596/10000 [======>.......................] - ETA: 1:00:58 - loss: 0.6784 - regression_loss: 0.5457 - classification_loss: 0.1327
 2597/10000 [======>.......................] - ETA: 1:00:58 - loss: 0.6784 - regression_loss: 0.5457 - classification_loss: 0.1328
 2598/10000 [======>.......................] - ETA: 1:00:57 - loss: 0.6783 - regression_loss: 0.5456 - classification_loss: 0.1328
 2599/10000 [======>.......................] - ETA: 1:00:57 - loss: 0.6787 - regression_loss: 0.5459 - classification_loss: 0.1328
 2600/10000 [======>.......................] - ETA: 1:00:56 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 2601/10000 [======>.......................] - ETA: 1:00:55 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 2602/10000 [======>.......................] - ETA: 1:00:55 - loss: 0.6786 - regression_loss: 0.5458 - classification_loss: 0.1328
 2603/10000 [======>.......................] - ETA: 1:00:54 - loss: 0.6785 - regression_loss: 0.5457 - classification_loss: 0.1328
 2604/10000 [======>.......................] - ETA: 1:00:53 - loss: 0.6785 - regression_loss: 0.5457 - classification_loss: 0.1328
 2605/10000 [======>.......................] - ETA: 1:00:53 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1328
 2606/10000 [======>.......................] - ETA: 1:00:52 - loss: 0.6786 - regression_loss: 0.5458 - classification_loss: 0.1328
 2607/10000 [======>.......................] - ETA: 1:00:53 - loss: 0.6790 - regression_loss: 0.5462 - classification_loss: 0.1328
 2608/10000 [======>.......................] - ETA: 1:00:53 - loss: 0.6792 - regression_loss: 0.5464 - classification_loss: 0.1328
 2609/10000 [======>.......................] - ETA: 1:00:52 - loss: 0.6791 - regression_loss: 0.5463 - classification_loss: 0.1328
 2610/10000 [======>.......................] - ETA: 1:00:51 - loss: 0.6790 - regression_loss: 0.5462 - classification_loss: 0.1328
 2611/10000 [======>.......................] - ETA: 1:00:51 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 2612/10000 [======>.......................] - ETA: 1:00:50 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 2613/10000 [======>.......................] - ETA: 1:00:50 - loss: 0.6790 - regression_loss: 0.5462 - classification_loss: 0.1329
 2614/10000 [======>.......................] - ETA: 1:00:49 - loss: 0.6789 - regression_loss: 0.5461 - classification_loss: 0.1328
 2615/10000 [======>.......................] - ETA: 1:00:48 - loss: 0.6789 - regression_loss: 0.5461 - classification_loss: 0.1328
 2616/10000 [======>.......................] - ETA: 1:00:48 - loss: 0.6787 - regression_loss: 0.5459 - classification_loss: 0.1328
 2617/10000 [======>.......................] - ETA: 1:00:47 - loss: 0.6789 - regression_loss: 0.5461 - classification_loss: 0.1328
 2618/10000 [======>.......................] - ETA: 1:00:46 - loss: 0.6792 - regression_loss: 0.5463 - classification_loss: 0.1329
 2619/10000 [======>.......................] - ETA: 1:00:46 - loss: 0.6790 - regression_loss: 0.5462 - classification_loss: 0.1328
 2620/10000 [======>.......................] - ETA: 1:00:45 - loss: 0.6789 - regression_loss: 0.5462 - classification_loss: 0.1328
 2621/10000 [======>.......................] - ETA: 1:00:45 - loss: 0.6790 - regression_loss: 0.5462 - classification_loss: 0.1328
 2622/10000 [======>.......................] - ETA: 1:00:46 - loss: 0.6791 - regression_loss: 0.5463 - classification_loss: 0.1328
 2623/10000 [======>.......................] - ETA: 1:00:45 - loss: 0.6791 - regression_loss: 0.5463 - classification_loss: 0.1328
 2624/10000 [======>.......................] - ETA: 1:00:45 - loss: 0.6789 - regression_loss: 0.5462 - classification_loss: 0.1328
 2625/10000 [======>.......................] - ETA: 1:00:44 - loss: 0.6788 - regression_loss: 0.5460 - classification_loss: 0.1327
 2626/10000 [======>.......................] - ETA: 1:00:43 - loss: 0.6788 - regression_loss: 0.5461 - classification_loss: 0.1327
 2627/10000 [======>.......................] - ETA: 1:00:43 - loss: 0.6787 - regression_loss: 0.5460 - classification_loss: 0.1327
 2628/10000 [======>.......................] - ETA: 1:00:42 - loss: 0.6787 - regression_loss: 0.5460 - classification_loss: 0.1327
 2629/10000 [======>.......................] - ETA: 1:00:42 - loss: 0.6788 - regression_loss: 0.5461 - classification_loss: 0.1327
 2630/10000 [======>.......................] - ETA: 1:00:41 - loss: 0.6787 - regression_loss: 0.5460 - classification_loss: 0.1327
 2631/10000 [======>.......................] - ETA: 1:00:41 - loss: 0.6789 - regression_loss: 0.5462 - classification_loss: 0.1327
 2632/10000 [======>.......................] - ETA: 1:00:40 - loss: 0.6789 - regression_loss: 0.5461 - classification_loss: 0.1328
 2633/10000 [======>.......................] - ETA: 1:00:39 - loss: 0.6789 - regression_loss: 0.5461 - classification_loss: 0.1328
 2634/10000 [======>.......................] - ETA: 1:00:39 - loss: 0.6787 - regression_loss: 0.5460 - classification_loss: 0.1327
 2635/10000 [======>.......................] - ETA: 1:00:38 - loss: 0.6787 - regression_loss: 0.5460 - classification_loss: 0.1327
 2636/10000 [======>.......................] - ETA: 1:00:38 - loss: 0.6786 - regression_loss: 0.5459 - classification_loss: 0.1327
 2637/10000 [======>.......................] - ETA: 1:00:37 - loss: 0.6787 - regression_loss: 0.5460 - classification_loss: 0.1327
 2638/10000 [======>.......................] - ETA: 1:00:36 - loss: 0.6785 - regression_loss: 0.5458 - classification_loss: 0.1327
 2639/10000 [======>.......................] - ETA: 1:00:36 - loss: 0.6784 - regression_loss: 0.5458 - classification_loss: 0.1327
 2640/10000 [======>.......................] - ETA: 1:00:35 - loss: 0.6786 - regression_loss: 0.5460 - classification_loss: 0.1327
 2641/10000 [======>.......................] - ETA: 1:00:35 - loss: 0.6786 - regression_loss: 0.5459 - classification_loss: 0.1327
 2642/10000 [======>.......................] - ETA: 1:00:34 - loss: 0.6785 - regression_loss: 0.5459 - classification_loss: 0.1327
 2643/10000 [======>.......................] - ETA: 1:00:33 - loss: 0.6786 - regression_loss: 0.5459 - classification_loss: 0.1327
 2644/10000 [======>.......................] - ETA: 1:00:33 - loss: 0.6788 - regression_loss: 0.5460 - classification_loss: 0.1327
 2645/10000 [======>.......................] - ETA: 1:00:32 - loss: 0.6786 - regression_loss: 0.5459 - classification_loss: 0.1327
 2646/10000 [======>.......................] - ETA: 1:00:32 - loss: 0.6785 - regression_loss: 0.5458 - classification_loss: 0.1326
 2647/10000 [======>.......................] - ETA: 1:00:31 - loss: 0.6783 - regression_loss: 0.5457 - classification_loss: 0.1326
 2648/10000 [======>.......................] - ETA: 1:00:31 - loss: 0.6782 - regression_loss: 0.5456 - classification_loss: 0.1326
 2649/10000 [======>.......................] - ETA: 1:00:30 - loss: 0.6782 - regression_loss: 0.5456 - classification_loss: 0.1326
 2650/10000 [======>.......................] - ETA: 1:00:29 - loss: 0.6781 - regression_loss: 0.5455 - classification_loss: 0.1326
 2651/10000 [======>.......................] - ETA: 1:00:29 - loss: 0.6784 - regression_loss: 0.5458 - classification_loss: 0.1326
 2652/10000 [======>.......................] - ETA: 1:00:28 - loss: 0.6783 - regression_loss: 0.5457 - classification_loss: 0.1326
 2653/10000 [======>.......................] - ETA: 1:00:28 - loss: 0.6783 - regression_loss: 0.5457 - classification_loss: 0.1326
 2654/10000 [======>.......................] - ETA: 1:00:27 - loss: 0.6783 - regression_loss: 0.5457 - classification_loss: 0.1326
 2655/10000 [======>.......................] - ETA: 1:00:27 - loss: 0.6783 - regression_loss: 0.5457 - classification_loss: 0.1326
 2656/10000 [======>.......................] - ETA: 1:00:26 - loss: 0.6782 - regression_loss: 0.5457 - classification_loss: 0.1326
 2657/10000 [======>.......................] - ETA: 1:00:25 - loss: 0.6781 - regression_loss: 0.5455 - classification_loss: 0.1326
 2658/10000 [======>.......................] - ETA: 1:00:25 - loss: 0.6781 - regression_loss: 0.5455 - classification_loss: 0.1326
 2659/10000 [======>.......................] - ETA: 1:00:24 - loss: 0.6780 - regression_loss: 0.5455 - classification_loss: 0.1325
 2660/10000 [======>.......................] - ETA: 1:00:24 - loss: 0.6779 - regression_loss: 0.5454 - classification_loss: 0.1325
 2661/10000 [======>.......................] - ETA: 1:00:23 - loss: 0.6779 - regression_loss: 0.5454 - classification_loss: 0.1325
 2662/10000 [======>.......................] - ETA: 1:00:22 - loss: 0.6778 - regression_loss: 0.5453 - classification_loss: 0.1325
 2663/10000 [======>.......................] - ETA: 1:00:22 - loss: 0.6778 - regression_loss: 0.5453 - classification_loss: 0.1325
 2664/10000 [======>.......................] - ETA: 1:00:21 - loss: 0.6781 - regression_loss: 0.5455 - classification_loss: 0.1326
 2665/10000 [======>.......................] - ETA: 1:00:21 - loss: 0.6781 - regression_loss: 0.5454 - classification_loss: 0.1326
 2666/10000 [======>.......................] - ETA: 1:00:20 - loss: 0.6779 - regression_loss: 0.5453 - classification_loss: 0.1326
 2667/10000 [=======>......................] - ETA: 1:00:20 - loss: 0.6780 - regression_loss: 0.5454 - classification_loss: 0.1326
 2668/10000 [=======>......................] - ETA: 1:00:19 - loss: 0.6779 - regression_loss: 0.5453 - classification_loss: 0.1326
 2669/10000 [=======>......................] - ETA: 1:00:18 - loss: 0.6782 - regression_loss: 0.5456 - classification_loss: 0.1326
 2670/10000 [=======>......................] - ETA: 1:00:18 - loss: 0.6784 - regression_loss: 0.5458 - classification_loss: 0.1326
 2671/10000 [=======>......................] - ETA: 1:00:17 - loss: 0.6786 - regression_loss: 0.5459 - classification_loss: 0.1327
 2672/10000 [=======>......................] - ETA: 1:00:17 - loss: 0.6784 - regression_loss: 0.5458 - classification_loss: 0.1326
 2673/10000 [=======>......................] - ETA: 1:00:16 - loss: 0.6783 - regression_loss: 0.5457 - classification_loss: 0.1326
 2674/10000 [=======>......................] - ETA: 1:00:16 - loss: 0.6784 - regression_loss: 0.5457 - classification_loss: 0.1326
 2675/10000 [=======>......................] - ETA: 1:00:15 - loss: 0.6786 - regression_loss: 0.5459 - classification_loss: 0.1327
 2676/10000 [=======>......................] - ETA: 1:00:14 - loss: 0.6785 - regression_loss: 0.5458 - classification_loss: 0.1327
 2677/10000 [=======>......................] - ETA: 1:00:14 - loss: 0.6783 - regression_loss: 0.5456 - classification_loss: 0.1326
 2678/10000 [=======>......................] - ETA: 1:00:13 - loss: 0.6781 - regression_loss: 0.5455 - classification_loss: 0.1326
 2679/10000 [=======>......................] - ETA: 1:00:13 - loss: 0.6784 - regression_loss: 0.5457 - classification_loss: 0.1326
 2680/10000 [=======>......................] - ETA: 1:00:12 - loss: 0.6785 - regression_loss: 0.5459 - classification_loss: 0.1327
 2681/10000 [=======>......................] - ETA: 1:00:11 - loss: 0.6786 - regression_loss: 0.5459 - classification_loss: 0.1327
 2682/10000 [=======>......................] - ETA: 1:00:11 - loss: 0.6786 - regression_loss: 0.5459 - classification_loss: 0.1327
 2683/10000 [=======>......................] - ETA: 1:00:10 - loss: 0.6785 - regression_loss: 0.5458 - classification_loss: 0.1327
 2684/10000 [=======>......................] - ETA: 1:00:10 - loss: 0.6784 - regression_loss: 0.5457 - classification_loss: 0.1327
 2685/10000 [=======>......................] - ETA: 1:00:09 - loss: 0.6784 - regression_loss: 0.5457 - classification_loss: 0.1327
 2686/10000 [=======>......................] - ETA: 1:00:09 - loss: 0.6784 - regression_loss: 0.5457 - classification_loss: 0.1327
 2687/10000 [=======>......................] - ETA: 1:00:08 - loss: 0.6787 - regression_loss: 0.5459 - classification_loss: 0.1328
 2688/10000 [=======>......................] - ETA: 1:00:07 - loss: 0.6788 - regression_loss: 0.5460 - classification_loss: 0.1328
 2689/10000 [=======>......................] - ETA: 1:00:07 - loss: 0.6790 - regression_loss: 0.5462 - classification_loss: 0.1328
 2690/10000 [=======>......................] - ETA: 1:00:06 - loss: 0.6789 - regression_loss: 0.5461 - classification_loss: 0.1328
 2691/10000 [=======>......................] - ETA: 1:00:06 - loss: 0.6787 - regression_loss: 0.5459 - classification_loss: 0.1328
 2692/10000 [=======>......................] - ETA: 1:00:05 - loss: 0.6787 - regression_loss: 0.5460 - classification_loss: 0.1328
 2693/10000 [=======>......................] - ETA: 1:00:05 - loss: 0.6787 - regression_loss: 0.5460 - classification_loss: 0.1327
 2694/10000 [=======>......................] - ETA: 1:00:04 - loss: 0.6787 - regression_loss: 0.5459 - classification_loss: 0.1328
 2695/10000 [=======>......................] - ETA: 1:00:03 - loss: 0.6786 - regression_loss: 0.5458 - classification_loss: 0.1328
 2696/10000 [=======>......................] - ETA: 1:00:03 - loss: 0.6786 - regression_loss: 0.5458 - classification_loss: 0.1328
 2697/10000 [=======>......................] - ETA: 1:00:02 - loss: 0.6785 - regression_loss: 0.5457 - classification_loss: 0.1328
 2698/10000 [=======>......................] - ETA: 1:00:02 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1328
 2699/10000 [=======>......................] - ETA: 1:00:01 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1328
 2700/10000 [=======>......................] - ETA: 1:00:01 - loss: 0.6785 - regression_loss: 0.5457 - classification_loss: 0.1329
 2701/10000 [=======>......................] - ETA: 1:00:00 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 2702/10000 [=======>......................] - ETA: 1:00:00 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1328
 2703/10000 [=======>......................] - ETA: 59:59 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329  
 2704/10000 [=======>......................] - ETA: 59:58 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 2705/10000 [=======>......................] - ETA: 59:58 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 2706/10000 [=======>......................] - ETA: 59:57 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 2707/10000 [=======>......................] - ETA: 59:56 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 2708/10000 [=======>......................] - ETA: 59:56 - loss: 0.6786 - regression_loss: 0.5458 - classification_loss: 0.1329
 2709/10000 [=======>......................] - ETA: 59:55 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 2710/10000 [=======>......................] - ETA: 59:55 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 2711/10000 [=======>......................] - ETA: 59:54 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1330
 2712/10000 [=======>......................] - ETA: 59:54 - loss: 0.6792 - regression_loss: 0.5462 - classification_loss: 0.1330
 2713/10000 [=======>......................] - ETA: 59:53 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 2714/10000 [=======>......................] - ETA: 59:53 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1330
 2715/10000 [=======>......................] - ETA: 59:52 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1330
 2716/10000 [=======>......................] - ETA: 59:51 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1329
 2717/10000 [=======>......................] - ETA: 59:51 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 2718/10000 [=======>......................] - ETA: 59:50 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 2719/10000 [=======>......................] - ETA: 59:50 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 2720/10000 [=======>......................] - ETA: 59:49 - loss: 0.6786 - regression_loss: 0.5458 - classification_loss: 0.1328
 2721/10000 [=======>......................] - ETA: 59:48 - loss: 0.6786 - regression_loss: 0.5458 - classification_loss: 0.1328
 2722/10000 [=======>......................] - ETA: 59:48 - loss: 0.6785 - regression_loss: 0.5457 - classification_loss: 0.1328
 2723/10000 [=======>......................] - ETA: 59:47 - loss: 0.6786 - regression_loss: 0.5458 - classification_loss: 0.1328
 2724/10000 [=======>......................] - ETA: 59:47 - loss: 0.6786 - regression_loss: 0.5458 - classification_loss: 0.1328
 2725/10000 [=======>......................] - ETA: 59:46 - loss: 0.6784 - regression_loss: 0.5457 - classification_loss: 0.1328
 2726/10000 [=======>......................] - ETA: 59:46 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1328
 2727/10000 [=======>......................] - ETA: 59:45 - loss: 0.6785 - regression_loss: 0.5457 - classification_loss: 0.1328
 2728/10000 [=======>......................] - ETA: 59:45 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1328
 2729/10000 [=======>......................] - ETA: 59:44 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1328
 2730/10000 [=======>......................] - ETA: 59:43 - loss: 0.6783 - regression_loss: 0.5456 - classification_loss: 0.1328
 2731/10000 [=======>......................] - ETA: 59:43 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1328
 2732/10000 [=======>......................] - ETA: 59:42 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1328
 2733/10000 [=======>......................] - ETA: 59:41 - loss: 0.6785 - regression_loss: 0.5457 - classification_loss: 0.1328
 2734/10000 [=======>......................] - ETA: 59:41 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1328
 2735/10000 [=======>......................] - ETA: 59:41 - loss: 0.6783 - regression_loss: 0.5456 - classification_loss: 0.1328
 2736/10000 [=======>......................] - ETA: 59:40 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1328
 2737/10000 [=======>......................] - ETA: 59:39 - loss: 0.6782 - regression_loss: 0.5455 - classification_loss: 0.1327
 2738/10000 [=======>......................] - ETA: 59:39 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1329
 2739/10000 [=======>......................] - ETA: 59:38 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1329
 2740/10000 [=======>......................] - ETA: 59:38 - loss: 0.6781 - regression_loss: 0.5453 - classification_loss: 0.1328
 2741/10000 [=======>......................] - ETA: 59:37 - loss: 0.6779 - regression_loss: 0.5451 - classification_loss: 0.1328
 2742/10000 [=======>......................] - ETA: 59:36 - loss: 0.6777 - regression_loss: 0.5450 - classification_loss: 0.1327
 2743/10000 [=======>......................] - ETA: 59:36 - loss: 0.6777 - regression_loss: 0.5449 - classification_loss: 0.1327
 2744/10000 [=======>......................] - ETA: 59:35 - loss: 0.6776 - regression_loss: 0.5449 - classification_loss: 0.1327
 2745/10000 [=======>......................] - ETA: 59:35 - loss: 0.6774 - regression_loss: 0.5447 - classification_loss: 0.1327
 2746/10000 [=======>......................] - ETA: 59:34 - loss: 0.6774 - regression_loss: 0.5447 - classification_loss: 0.1327
 2747/10000 [=======>......................] - ETA: 59:33 - loss: 0.6773 - regression_loss: 0.5447 - classification_loss: 0.1326
 2748/10000 [=======>......................] - ETA: 59:33 - loss: 0.6774 - regression_loss: 0.5447 - classification_loss: 0.1327
 2749/10000 [=======>......................] - ETA: 59:32 - loss: 0.6772 - regression_loss: 0.5446 - classification_loss: 0.1326
 2750/10000 [=======>......................] - ETA: 59:32 - loss: 0.6772 - regression_loss: 0.5446 - classification_loss: 0.1326
 2751/10000 [=======>......................] - ETA: 59:31 - loss: 0.6773 - regression_loss: 0.5447 - classification_loss: 0.1326
 2752/10000 [=======>......................] - ETA: 59:31 - loss: 0.6775 - regression_loss: 0.5448 - classification_loss: 0.1327
 2753/10000 [=======>......................] - ETA: 59:30 - loss: 0.6774 - regression_loss: 0.5447 - classification_loss: 0.1326
 2754/10000 [=======>......................] - ETA: 59:29 - loss: 0.6774 - regression_loss: 0.5448 - classification_loss: 0.1327
 2755/10000 [=======>......................] - ETA: 59:29 - loss: 0.6776 - regression_loss: 0.5449 - classification_loss: 0.1327
 2756/10000 [=======>......................] - ETA: 59:28 - loss: 0.6775 - regression_loss: 0.5449 - classification_loss: 0.1327
 2757/10000 [=======>......................] - ETA: 59:28 - loss: 0.6775 - regression_loss: 0.5449 - classification_loss: 0.1326
 2758/10000 [=======>......................] - ETA: 59:27 - loss: 0.6779 - regression_loss: 0.5452 - classification_loss: 0.1327
 2759/10000 [=======>......................] - ETA: 59:26 - loss: 0.6782 - regression_loss: 0.5455 - classification_loss: 0.1327
 2760/10000 [=======>......................] - ETA: 59:27 - loss: 0.6781 - regression_loss: 0.5454 - classification_loss: 0.1327
 2761/10000 [=======>......................] - ETA: 59:26 - loss: 0.6782 - regression_loss: 0.5455 - classification_loss: 0.1327
 2762/10000 [=======>......................] - ETA: 59:25 - loss: 0.6781 - regression_loss: 0.5455 - classification_loss: 0.1327
 2763/10000 [=======>......................] - ETA: 59:25 - loss: 0.6781 - regression_loss: 0.5454 - classification_loss: 0.1326
 2764/10000 [=======>......................] - ETA: 59:24 - loss: 0.6782 - regression_loss: 0.5455 - classification_loss: 0.1326
 2765/10000 [=======>......................] - ETA: 59:24 - loss: 0.6783 - regression_loss: 0.5457 - classification_loss: 0.1327
 2766/10000 [=======>......................] - ETA: 59:23 - loss: 0.6785 - regression_loss: 0.5458 - classification_loss: 0.1327
 2767/10000 [=======>......................] - ETA: 59:23 - loss: 0.6788 - regression_loss: 0.5460 - classification_loss: 0.1327
 2768/10000 [=======>......................] - ETA: 59:22 - loss: 0.6786 - regression_loss: 0.5459 - classification_loss: 0.1327
 2769/10000 [=======>......................] - ETA: 59:21 - loss: 0.6785 - regression_loss: 0.5458 - classification_loss: 0.1327
 2770/10000 [=======>......................] - ETA: 59:21 - loss: 0.6786 - regression_loss: 0.5459 - classification_loss: 0.1327
 2771/10000 [=======>......................] - ETA: 59:20 - loss: 0.6785 - regression_loss: 0.5458 - classification_loss: 0.1327
 2772/10000 [=======>......................] - ETA: 59:19 - loss: 0.6785 - regression_loss: 0.5458 - classification_loss: 0.1326
 2773/10000 [=======>......................] - ETA: 59:19 - loss: 0.6785 - regression_loss: 0.5459 - classification_loss: 0.1326
 2774/10000 [=======>......................] - ETA: 59:18 - loss: 0.6784 - regression_loss: 0.5458 - classification_loss: 0.1326
 2775/10000 [=======>......................] - ETA: 59:18 - loss: 0.6783 - regression_loss: 0.5457 - classification_loss: 0.1326
 2776/10000 [=======>......................] - ETA: 59:17 - loss: 0.6784 - regression_loss: 0.5457 - classification_loss: 0.1327
 2777/10000 [=======>......................] - ETA: 59:17 - loss: 0.6787 - regression_loss: 0.5459 - classification_loss: 0.1327
 2778/10000 [=======>......................] - ETA: 59:16 - loss: 0.6788 - regression_loss: 0.5461 - classification_loss: 0.1327
 2779/10000 [=======>......................] - ETA: 59:16 - loss: 0.6787 - regression_loss: 0.5460 - classification_loss: 0.1327
 2780/10000 [=======>......................] - ETA: 59:15 - loss: 0.6788 - regression_loss: 0.5461 - classification_loss: 0.1328
 2781/10000 [=======>......................] - ETA: 59:14 - loss: 0.6790 - regression_loss: 0.5462 - classification_loss: 0.1328
 2782/10000 [=======>......................] - ETA: 59:14 - loss: 0.6789 - regression_loss: 0.5462 - classification_loss: 0.1328
 2783/10000 [=======>......................] - ETA: 59:13 - loss: 0.6791 - regression_loss: 0.5464 - classification_loss: 0.1328
 2784/10000 [=======>......................] - ETA: 59:13 - loss: 0.6792 - regression_loss: 0.5464 - classification_loss: 0.1328
 2785/10000 [=======>......................] - ETA: 59:12 - loss: 0.6791 - regression_loss: 0.5464 - classification_loss: 0.1328
 2786/10000 [=======>......................] - ETA: 59:12 - loss: 0.6790 - regression_loss: 0.5463 - classification_loss: 0.1327
 2787/10000 [=======>......................] - ETA: 59:11 - loss: 0.6788 - regression_loss: 0.5462 - classification_loss: 0.1327
 2788/10000 [=======>......................] - ETA: 59:11 - loss: 0.6788 - regression_loss: 0.5461 - classification_loss: 0.1327
 2789/10000 [=======>......................] - ETA: 59:10 - loss: 0.6788 - regression_loss: 0.5462 - classification_loss: 0.1327
 2790/10000 [=======>......................] - ETA: 59:09 - loss: 0.6788 - regression_loss: 0.5461 - classification_loss: 0.1326
 2791/10000 [=======>......................] - ETA: 59:09 - loss: 0.6787 - regression_loss: 0.5461 - classification_loss: 0.1326
 2792/10000 [=======>......................] - ETA: 59:08 - loss: 0.6787 - regression_loss: 0.5461 - classification_loss: 0.1326
 2793/10000 [=======>......................] - ETA: 59:08 - loss: 0.6786 - regression_loss: 0.5460 - classification_loss: 0.1326
 2794/10000 [=======>......................] - ETA: 59:07 - loss: 0.6788 - regression_loss: 0.5462 - classification_loss: 0.1326
 2795/10000 [=======>......................] - ETA: 59:07 - loss: 0.6787 - regression_loss: 0.5461 - classification_loss: 0.1326
 2796/10000 [=======>......................] - ETA: 59:06 - loss: 0.6785 - regression_loss: 0.5459 - classification_loss: 0.1326
 2797/10000 [=======>......................] - ETA: 59:05 - loss: 0.6786 - regression_loss: 0.5459 - classification_loss: 0.1327
 2798/10000 [=======>......................] - ETA: 59:05 - loss: 0.6786 - regression_loss: 0.5459 - classification_loss: 0.1326
 2799/10000 [=======>......................] - ETA: 59:04 - loss: 0.6785 - regression_loss: 0.5459 - classification_loss: 0.1326
 2800/10000 [=======>......................] - ETA: 59:04 - loss: 0.6784 - regression_loss: 0.5458 - classification_loss: 0.1326
 2801/10000 [=======>......................] - ETA: 59:03 - loss: 0.6782 - regression_loss: 0.5457 - classification_loss: 0.1326
 2802/10000 [=======>......................] - ETA: 59:02 - loss: 0.6784 - regression_loss: 0.5458 - classification_loss: 0.1326
 2803/10000 [=======>......................] - ETA: 59:02 - loss: 0.6786 - regression_loss: 0.5460 - classification_loss: 0.1326
 2804/10000 [=======>......................] - ETA: 59:01 - loss: 0.6784 - regression_loss: 0.5459 - classification_loss: 0.1326
 2805/10000 [=======>......................] - ETA: 59:01 - loss: 0.6783 - regression_loss: 0.5458 - classification_loss: 0.1326
 2806/10000 [=======>......................] - ETA: 59:00 - loss: 0.6782 - regression_loss: 0.5456 - classification_loss: 0.1325
 2807/10000 [=======>......................] - ETA: 58:59 - loss: 0.6780 - regression_loss: 0.5455 - classification_loss: 0.1325
 2808/10000 [=======>......................] - ETA: 58:59 - loss: 0.6778 - regression_loss: 0.5453 - classification_loss: 0.1325
 2809/10000 [=======>......................] - ETA: 58:58 - loss: 0.6778 - regression_loss: 0.5454 - classification_loss: 0.1325
 2810/10000 [=======>......................] - ETA: 58:57 - loss: 0.6777 - regression_loss: 0.5453 - classification_loss: 0.1325
 2811/10000 [=======>......................] - ETA: 58:57 - loss: 0.6776 - regression_loss: 0.5451 - classification_loss: 0.1325
 2812/10000 [=======>......................] - ETA: 58:56 - loss: 0.6774 - regression_loss: 0.5450 - classification_loss: 0.1324
 2813/10000 [=======>......................] - ETA: 58:56 - loss: 0.6778 - regression_loss: 0.5453 - classification_loss: 0.1325
 2814/10000 [=======>......................] - ETA: 58:55 - loss: 0.6777 - regression_loss: 0.5452 - classification_loss: 0.1325
 2815/10000 [=======>......................] - ETA: 58:55 - loss: 0.6777 - regression_loss: 0.5452 - classification_loss: 0.1325
 2816/10000 [=======>......................] - ETA: 58:54 - loss: 0.6778 - regression_loss: 0.5453 - classification_loss: 0.1325
 2817/10000 [=======>......................] - ETA: 58:54 - loss: 0.6778 - regression_loss: 0.5454 - classification_loss: 0.1324
 2818/10000 [=======>......................] - ETA: 58:53 - loss: 0.6777 - regression_loss: 0.5453 - classification_loss: 0.1325
 2819/10000 [=======>......................] - ETA: 58:53 - loss: 0.6778 - regression_loss: 0.5453 - classification_loss: 0.1325
 2820/10000 [=======>......................] - ETA: 58:52 - loss: 0.6777 - regression_loss: 0.5452 - classification_loss: 0.1324
 2821/10000 [=======>......................] - ETA: 58:51 - loss: 0.6778 - regression_loss: 0.5453 - classification_loss: 0.1325
 2822/10000 [=======>......................] - ETA: 58:51 - loss: 0.6779 - regression_loss: 0.5454 - classification_loss: 0.1325
 2823/10000 [=======>......................] - ETA: 58:50 - loss: 0.6778 - regression_loss: 0.5453 - classification_loss: 0.1325
 2824/10000 [=======>......................] - ETA: 58:50 - loss: 0.6779 - regression_loss: 0.5454 - classification_loss: 0.1325
 2825/10000 [=======>......................] - ETA: 58:49 - loss: 0.6779 - regression_loss: 0.5454 - classification_loss: 0.1325
 2826/10000 [=======>......................] - ETA: 58:48 - loss: 0.6778 - regression_loss: 0.5453 - classification_loss: 0.1325
 2827/10000 [=======>......................] - ETA: 58:48 - loss: 0.6778 - regression_loss: 0.5453 - classification_loss: 0.1325
 2828/10000 [=======>......................] - ETA: 58:47 - loss: 0.6780 - regression_loss: 0.5455 - classification_loss: 0.1325
 2829/10000 [=======>......................] - ETA: 58:46 - loss: 0.6779 - regression_loss: 0.5454 - classification_loss: 0.1325
 2830/10000 [=======>......................] - ETA: 58:46 - loss: 0.6777 - regression_loss: 0.5452 - classification_loss: 0.1325
 2831/10000 [=======>......................] - ETA: 58:45 - loss: 0.6778 - regression_loss: 0.5453 - classification_loss: 0.1324
 2832/10000 [=======>......................] - ETA: 58:45 - loss: 0.6778 - regression_loss: 0.5454 - classification_loss: 0.1324
 2833/10000 [=======>......................] - ETA: 58:44 - loss: 0.6778 - regression_loss: 0.5454 - classification_loss: 0.1324
 2834/10000 [=======>......................] - ETA: 58:44 - loss: 0.6777 - regression_loss: 0.5453 - classification_loss: 0.1324
 2835/10000 [=======>......................] - ETA: 58:43 - loss: 0.6776 - regression_loss: 0.5452 - classification_loss: 0.1324
 2836/10000 [=======>......................] - ETA: 58:42 - loss: 0.6776 - regression_loss: 0.5452 - classification_loss: 0.1324
 2837/10000 [=======>......................] - ETA: 58:42 - loss: 0.6778 - regression_loss: 0.5453 - classification_loss: 0.1324
 2838/10000 [=======>......................] - ETA: 58:41 - loss: 0.6780 - regression_loss: 0.5456 - classification_loss: 0.1325
 2839/10000 [=======>......................] - ETA: 58:41 - loss: 0.6781 - regression_loss: 0.5456 - classification_loss: 0.1325
 2840/10000 [=======>......................] - ETA: 58:40 - loss: 0.6781 - regression_loss: 0.5456 - classification_loss: 0.1325
 2841/10000 [=======>......................] - ETA: 58:40 - loss: 0.6780 - regression_loss: 0.5455 - classification_loss: 0.1325
 2842/10000 [=======>......................] - ETA: 58:39 - loss: 0.6780 - regression_loss: 0.5455 - classification_loss: 0.1325
 2843/10000 [=======>......................] - ETA: 58:38 - loss: 0.6780 - regression_loss: 0.5455 - classification_loss: 0.1325
 2844/10000 [=======>......................] - ETA: 58:38 - loss: 0.6779 - regression_loss: 0.5454 - classification_loss: 0.1325
 2845/10000 [=======>......................] - ETA: 58:37 - loss: 0.6777 - regression_loss: 0.5453 - classification_loss: 0.1325
 2846/10000 [=======>......................] - ETA: 58:37 - loss: 0.6780 - regression_loss: 0.5455 - classification_loss: 0.1325
 2847/10000 [=======>......................] - ETA: 58:36 - loss: 0.6783 - regression_loss: 0.5457 - classification_loss: 0.1326
 2848/10000 [=======>......................] - ETA: 58:35 - loss: 0.6782 - regression_loss: 0.5456 - classification_loss: 0.1326
 2849/10000 [=======>......................] - ETA: 58:35 - loss: 0.6782 - regression_loss: 0.5456 - classification_loss: 0.1326
 2850/10000 [=======>......................] - ETA: 58:34 - loss: 0.6782 - regression_loss: 0.5456 - classification_loss: 0.1326
 2851/10000 [=======>......................] - ETA: 58:34 - loss: 0.6781 - regression_loss: 0.5455 - classification_loss: 0.1326
 2852/10000 [=======>......................] - ETA: 58:33 - loss: 0.6783 - regression_loss: 0.5456 - classification_loss: 0.1327
 2853/10000 [=======>......................] - ETA: 58:33 - loss: 0.6782 - regression_loss: 0.5455 - classification_loss: 0.1327
 2854/10000 [=======>......................] - ETA: 58:32 - loss: 0.6781 - regression_loss: 0.5455 - classification_loss: 0.1326
 2855/10000 [=======>......................] - ETA: 58:31 - loss: 0.6782 - regression_loss: 0.5455 - classification_loss: 0.1327
 2856/10000 [=======>......................] - ETA: 58:31 - loss: 0.6785 - regression_loss: 0.5457 - classification_loss: 0.1327
 2857/10000 [=======>......................] - ETA: 58:30 - loss: 0.6785 - regression_loss: 0.5458 - classification_loss: 0.1327
 2858/10000 [=======>......................] - ETA: 58:30 - loss: 0.6784 - regression_loss: 0.5457 - classification_loss: 0.1327
 2859/10000 [=======>......................] - ETA: 58:29 - loss: 0.6783 - regression_loss: 0.5456 - classification_loss: 0.1327
 2860/10000 [=======>......................] - ETA: 58:28 - loss: 0.6786 - regression_loss: 0.5458 - classification_loss: 0.1327
 2861/10000 [=======>......................] - ETA: 58:28 - loss: 0.6785 - regression_loss: 0.5457 - classification_loss: 0.1327
 2862/10000 [=======>......................] - ETA: 58:27 - loss: 0.6786 - regression_loss: 0.5459 - classification_loss: 0.1327
 2863/10000 [=======>......................] - ETA: 58:27 - loss: 0.6784 - regression_loss: 0.5457 - classification_loss: 0.1327
 2864/10000 [=======>......................] - ETA: 58:27 - loss: 0.6784 - regression_loss: 0.5457 - classification_loss: 0.1327
 2865/10000 [=======>......................] - ETA: 58:27 - loss: 0.6783 - regression_loss: 0.5456 - classification_loss: 0.1327
 2866/10000 [=======>......................] - ETA: 58:27 - loss: 0.6781 - regression_loss: 0.5454 - classification_loss: 0.1327
 2867/10000 [=======>......................] - ETA: 58:26 - loss: 0.6782 - regression_loss: 0.5455 - classification_loss: 0.1326
 2868/10000 [=======>......................] - ETA: 58:26 - loss: 0.6781 - regression_loss: 0.5454 - classification_loss: 0.1326
 2869/10000 [=======>......................] - ETA: 58:25 - loss: 0.6781 - regression_loss: 0.5455 - classification_loss: 0.1326
 2870/10000 [=======>......................] - ETA: 58:24 - loss: 0.6781 - regression_loss: 0.5455 - classification_loss: 0.1327
 2871/10000 [=======>......................] - ETA: 58:24 - loss: 0.6780 - regression_loss: 0.5453 - classification_loss: 0.1326
 2872/10000 [=======>......................] - ETA: 58:23 - loss: 0.6784 - regression_loss: 0.5457 - classification_loss: 0.1327
 2873/10000 [=======>......................] - ETA: 58:23 - loss: 0.6786 - regression_loss: 0.5458 - classification_loss: 0.1328
 2874/10000 [=======>......................] - ETA: 58:22 - loss: 0.6785 - regression_loss: 0.5457 - classification_loss: 0.1328
 2875/10000 [=======>......................] - ETA: 58:22 - loss: 0.6783 - regression_loss: 0.5456 - classification_loss: 0.1327
 2876/10000 [=======>......................] - ETA: 58:21 - loss: 0.6784 - regression_loss: 0.5457 - classification_loss: 0.1327
 2877/10000 [=======>......................] - ETA: 58:20 - loss: 0.6788 - regression_loss: 0.5460 - classification_loss: 0.1328
 2878/10000 [=======>......................] - ETA: 58:20 - loss: 0.6789 - regression_loss: 0.5461 - classification_loss: 0.1328
 2879/10000 [=======>......................] - ETA: 58:19 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 2880/10000 [=======>......................] - ETA: 58:19 - loss: 0.6793 - regression_loss: 0.5464 - classification_loss: 0.1329
 2881/10000 [=======>......................] - ETA: 58:18 - loss: 0.6794 - regression_loss: 0.5465 - classification_loss: 0.1329
 2882/10000 [=======>......................] - ETA: 58:18 - loss: 0.6793 - regression_loss: 0.5464 - classification_loss: 0.1329
 2883/10000 [=======>......................] - ETA: 58:17 - loss: 0.6793 - regression_loss: 0.5464 - classification_loss: 0.1329
 2884/10000 [=======>......................] - ETA: 58:17 - loss: 0.6796 - regression_loss: 0.5466 - classification_loss: 0.1330
 2885/10000 [=======>......................] - ETA: 58:16 - loss: 0.6796 - regression_loss: 0.5467 - classification_loss: 0.1330
 2886/10000 [=======>......................] - ETA: 58:15 - loss: 0.6795 - regression_loss: 0.5466 - classification_loss: 0.1330
 2887/10000 [=======>......................] - ETA: 58:15 - loss: 0.6800 - regression_loss: 0.5469 - classification_loss: 0.1331
 2888/10000 [=======>......................] - ETA: 58:14 - loss: 0.6799 - regression_loss: 0.5468 - classification_loss: 0.1331
 2889/10000 [=======>......................] - ETA: 58:14 - loss: 0.6798 - regression_loss: 0.5467 - classification_loss: 0.1331
 2890/10000 [=======>......................] - ETA: 58:13 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 2891/10000 [=======>......................] - ETA: 58:13 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 2892/10000 [=======>......................] - ETA: 58:12 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 2893/10000 [=======>......................] - ETA: 58:12 - loss: 0.6796 - regression_loss: 0.5466 - classification_loss: 0.1331
 2894/10000 [=======>......................] - ETA: 58:11 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 2895/10000 [=======>......................] - ETA: 58:10 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1330
 2896/10000 [=======>......................] - ETA: 58:10 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 2897/10000 [=======>......................] - ETA: 58:09 - loss: 0.6795 - regression_loss: 0.5464 - classification_loss: 0.1331
 2898/10000 [=======>......................] - ETA: 58:09 - loss: 0.6794 - regression_loss: 0.5463 - classification_loss: 0.1330
 2899/10000 [=======>......................] - ETA: 58:08 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1330
 2900/10000 [=======>......................] - ETA: 58:08 - loss: 0.6792 - regression_loss: 0.5462 - classification_loss: 0.1330
 2901/10000 [=======>......................] - ETA: 58:07 - loss: 0.6792 - regression_loss: 0.5462 - classification_loss: 0.1330
 2902/10000 [=======>......................] - ETA: 58:07 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 2903/10000 [=======>......................] - ETA: 58:07 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 2904/10000 [=======>......................] - ETA: 58:06 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 2905/10000 [=======>......................] - ETA: 58:06 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 2906/10000 [=======>......................] - ETA: 58:05 - loss: 0.6790 - regression_loss: 0.5462 - classification_loss: 0.1329
 2907/10000 [=======>......................] - ETA: 58:05 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 2908/10000 [=======>......................] - ETA: 58:04 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 2909/10000 [=======>......................] - ETA: 58:03 - loss: 0.6787 - regression_loss: 0.5459 - classification_loss: 0.1328
 2910/10000 [=======>......................] - ETA: 58:03 - loss: 0.6787 - regression_loss: 0.5459 - classification_loss: 0.1328
 2911/10000 [=======>......................] - ETA: 58:03 - loss: 0.6787 - regression_loss: 0.5459 - classification_loss: 0.1328
 2912/10000 [=======>......................] - ETA: 58:02 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1328
 2913/10000 [=======>......................] - ETA: 58:01 - loss: 0.6789 - regression_loss: 0.5461 - classification_loss: 0.1328
 2914/10000 [=======>......................] - ETA: 58:01 - loss: 0.6788 - regression_loss: 0.5461 - classification_loss: 0.1328
 2915/10000 [=======>......................] - ETA: 58:00 - loss: 0.6787 - regression_loss: 0.5459 - classification_loss: 0.1327
 2916/10000 [=======>......................] - ETA: 58:00 - loss: 0.6786 - regression_loss: 0.5459 - classification_loss: 0.1327
 2917/10000 [=======>......................] - ETA: 57:59 - loss: 0.6786 - regression_loss: 0.5459 - classification_loss: 0.1327
 2918/10000 [=======>......................] - ETA: 57:58 - loss: 0.6787 - regression_loss: 0.5460 - classification_loss: 0.1328
 2919/10000 [=======>......................] - ETA: 57:58 - loss: 0.6786 - regression_loss: 0.5459 - classification_loss: 0.1327
 2920/10000 [=======>......................] - ETA: 57:57 - loss: 0.6785 - regression_loss: 0.5458 - classification_loss: 0.1327
 2921/10000 [=======>......................] - ETA: 57:57 - loss: 0.6785 - regression_loss: 0.5458 - classification_loss: 0.1327
 2922/10000 [=======>......................] - ETA: 57:56 - loss: 0.6784 - regression_loss: 0.5457 - classification_loss: 0.1327
 2923/10000 [=======>......................] - ETA: 57:55 - loss: 0.6784 - regression_loss: 0.5457 - classification_loss: 0.1327
 2924/10000 [=======>......................] - ETA: 57:56 - loss: 0.6784 - regression_loss: 0.5457 - classification_loss: 0.1327
 2925/10000 [=======>......................] - ETA: 57:55 - loss: 0.6784 - regression_loss: 0.5457 - classification_loss: 0.1327
 2926/10000 [=======>......................] - ETA: 57:54 - loss: 0.6783 - regression_loss: 0.5456 - classification_loss: 0.1327
 2927/10000 [=======>......................] - ETA: 57:54 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1327
 2928/10000 [=======>......................] - ETA: 57:53 - loss: 0.6782 - regression_loss: 0.5455 - classification_loss: 0.1327
 2929/10000 [=======>......................] - ETA: 57:53 - loss: 0.6782 - regression_loss: 0.5455 - classification_loss: 0.1328
 2930/10000 [=======>......................] - ETA: 57:52 - loss: 0.6782 - regression_loss: 0.5454 - classification_loss: 0.1327
 2931/10000 [=======>......................] - ETA: 57:51 - loss: 0.6781 - regression_loss: 0.5454 - classification_loss: 0.1327
 2932/10000 [=======>......................] - ETA: 57:51 - loss: 0.6780 - regression_loss: 0.5453 - classification_loss: 0.1327
 2933/10000 [=======>......................] - ETA: 57:50 - loss: 0.6779 - regression_loss: 0.5452 - classification_loss: 0.1327
 2934/10000 [=======>......................] - ETA: 57:50 - loss: 0.6778 - regression_loss: 0.5451 - classification_loss: 0.1326
 2935/10000 [=======>......................] - ETA: 57:49 - loss: 0.6777 - regression_loss: 0.5451 - classification_loss: 0.1326
 2936/10000 [=======>......................] - ETA: 57:49 - loss: 0.6777 - regression_loss: 0.5451 - classification_loss: 0.1326
 2937/10000 [=======>......................] - ETA: 57:48 - loss: 0.6779 - regression_loss: 0.5452 - classification_loss: 0.1327
 2938/10000 [=======>......................] - ETA: 57:48 - loss: 0.6780 - regression_loss: 0.5454 - classification_loss: 0.1326
 2939/10000 [=======>......................] - ETA: 57:47 - loss: 0.6779 - regression_loss: 0.5453 - classification_loss: 0.1326
 2940/10000 [=======>......................] - ETA: 57:47 - loss: 0.6779 - regression_loss: 0.5453 - classification_loss: 0.1326
 2941/10000 [=======>......................] - ETA: 57:46 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1327
 2942/10000 [=======>......................] - ETA: 57:45 - loss: 0.6782 - regression_loss: 0.5455 - classification_loss: 0.1327
 2943/10000 [=======>......................] - ETA: 57:45 - loss: 0.6782 - regression_loss: 0.5454 - classification_loss: 0.1327
 2944/10000 [=======>......................] - ETA: 57:44 - loss: 0.6781 - regression_loss: 0.5454 - classification_loss: 0.1327
 2945/10000 [=======>......................] - ETA: 57:44 - loss: 0.6781 - regression_loss: 0.5454 - classification_loss: 0.1327
 2946/10000 [=======>......................] - ETA: 57:43 - loss: 0.6780 - regression_loss: 0.5453 - classification_loss: 0.1327
 2947/10000 [=======>......................] - ETA: 57:43 - loss: 0.6784 - regression_loss: 0.5457 - classification_loss: 0.1328
 2948/10000 [=======>......................] - ETA: 57:42 - loss: 0.6785 - regression_loss: 0.5457 - classification_loss: 0.1328
 2949/10000 [=======>......................] - ETA: 57:42 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1328
 2950/10000 [=======>......................] - ETA: 57:41 - loss: 0.6785 - regression_loss: 0.5457 - classification_loss: 0.1328
 2951/10000 [=======>......................] - ETA: 57:40 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1328
 2952/10000 [=======>......................] - ETA: 57:40 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 2953/10000 [=======>......................] - ETA: 57:39 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1329
 2954/10000 [=======>......................] - ETA: 57:39 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1329
 2955/10000 [=======>......................] - ETA: 57:38 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1330
 2956/10000 [=======>......................] - ETA: 57:38 - loss: 0.6795 - regression_loss: 0.5464 - classification_loss: 0.1330
 2957/10000 [=======>......................] - ETA: 57:37 - loss: 0.6793 - regression_loss: 0.5464 - classification_loss: 0.1330
 2958/10000 [=======>......................] - ETA: 57:36 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1330
 2959/10000 [=======>......................] - ETA: 57:36 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1330
 2960/10000 [=======>......................] - ETA: 57:35 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1330
 2961/10000 [=======>......................] - ETA: 57:35 - loss: 0.6796 - regression_loss: 0.5466 - classification_loss: 0.1330
 2962/10000 [=======>......................] - ETA: 57:34 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1330
 2963/10000 [=======>......................] - ETA: 57:34 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1330
 2964/10000 [=======>......................] - ETA: 57:33 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1330
 2965/10000 [=======>......................] - ETA: 57:33 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1330
 2966/10000 [=======>......................] - ETA: 57:32 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1330
 2967/10000 [=======>......................] - ETA: 57:31 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1330
 2968/10000 [=======>......................] - ETA: 57:31 - loss: 0.6792 - regression_loss: 0.5463 - classification_loss: 0.1330
 2969/10000 [=======>......................] - ETA: 57:30 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1329
 2970/10000 [=======>......................] - ETA: 57:30 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1329
 2971/10000 [=======>......................] - ETA: 57:29 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 2972/10000 [=======>......................] - ETA: 57:29 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 2973/10000 [=======>......................] - ETA: 57:28 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 2974/10000 [=======>......................] - ETA: 57:27 - loss: 0.6792 - regression_loss: 0.5462 - classification_loss: 0.1330
 2975/10000 [=======>......................] - ETA: 57:27 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1330
 2976/10000 [=======>......................] - ETA: 57:26 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1330
 2977/10000 [=======>......................] - ETA: 57:26 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 2978/10000 [=======>......................] - ETA: 57:25 - loss: 0.6793 - regression_loss: 0.5462 - classification_loss: 0.1331
 2979/10000 [=======>......................] - ETA: 57:25 - loss: 0.6795 - regression_loss: 0.5464 - classification_loss: 0.1331
 2980/10000 [=======>......................] - ETA: 57:24 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1331
 2981/10000 [=======>......................] - ETA: 57:24 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1330
 2982/10000 [=======>......................] - ETA: 57:23 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 2983/10000 [=======>......................] - ETA: 57:22 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1330
 2984/10000 [=======>......................] - ETA: 57:22 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1330
 2985/10000 [=======>......................] - ETA: 57:21 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 2986/10000 [=======>......................] - ETA: 57:21 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 2987/10000 [=======>......................] - ETA: 57:20 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 2988/10000 [=======>......................] - ETA: 57:19 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 2989/10000 [=======>......................] - ETA: 57:19 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 2990/10000 [=======>......................] - ETA: 57:18 - loss: 0.6795 - regression_loss: 0.5464 - classification_loss: 0.1331
 2991/10000 [=======>......................] - ETA: 57:18 - loss: 0.6794 - regression_loss: 0.5463 - classification_loss: 0.1331
 2992/10000 [=======>......................] - ETA: 57:17 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 2993/10000 [=======>......................] - ETA: 57:17 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 2994/10000 [=======>......................] - ETA: 57:16 - loss: 0.6795 - regression_loss: 0.5464 - classification_loss: 0.1331
 2995/10000 [=======>......................] - ETA: 57:15 - loss: 0.6794 - regression_loss: 0.5463 - classification_loss: 0.1331
 2996/10000 [=======>......................] - ETA: 57:15 - loss: 0.6792 - regression_loss: 0.5462 - classification_loss: 0.1330
 2997/10000 [=======>......................] - ETA: 57:14 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 2998/10000 [=======>......................] - ETA: 57:14 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1330
 2999/10000 [=======>......................] - ETA: 57:13 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1330
 3000/10000 [========>.....................] - ETA: 57:12 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1330
 3001/10000 [========>.....................] - ETA: 57:12 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1330
 3002/10000 [========>.....................] - ETA: 57:11 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1330
 3003/10000 [========>.....................] - ETA: 57:11 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1330
 3004/10000 [========>.....................] - ETA: 57:10 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1330
 3005/10000 [========>.....................] - ETA: 57:10 - loss: 0.6800 - regression_loss: 0.5469 - classification_loss: 0.1331
 3006/10000 [========>.....................] - ETA: 57:09 - loss: 0.6800 - regression_loss: 0.5469 - classification_loss: 0.1331
 3007/10000 [========>.....................] - ETA: 57:09 - loss: 0.6799 - regression_loss: 0.5468 - classification_loss: 0.1331
 3008/10000 [========>.....................] - ETA: 57:08 - loss: 0.6798 - regression_loss: 0.5467 - classification_loss: 0.1331
 3009/10000 [========>.....................] - ETA: 57:07 - loss: 0.6799 - regression_loss: 0.5468 - classification_loss: 0.1331
 3010/10000 [========>.....................] - ETA: 57:07 - loss: 0.6801 - regression_loss: 0.5470 - classification_loss: 0.1331
 3011/10000 [========>.....................] - ETA: 57:06 - loss: 0.6800 - regression_loss: 0.5469 - classification_loss: 0.1331
 3012/10000 [========>.....................] - ETA: 57:06 - loss: 0.6802 - regression_loss: 0.5470 - classification_loss: 0.1331
 3013/10000 [========>.....................] - ETA: 57:05 - loss: 0.6803 - regression_loss: 0.5471 - classification_loss: 0.1331
 3014/10000 [========>.....................] - ETA: 57:05 - loss: 0.6807 - regression_loss: 0.5474 - classification_loss: 0.1332
 3015/10000 [========>.....................] - ETA: 57:04 - loss: 0.6809 - regression_loss: 0.5476 - classification_loss: 0.1333
 3016/10000 [========>.....................] - ETA: 57:03 - loss: 0.6811 - regression_loss: 0.5478 - classification_loss: 0.1333
 3017/10000 [========>.....................] - ETA: 57:03 - loss: 0.6810 - regression_loss: 0.5477 - classification_loss: 0.1333
 3018/10000 [========>.....................] - ETA: 57:02 - loss: 0.6811 - regression_loss: 0.5479 - classification_loss: 0.1333
 3019/10000 [========>.....................] - ETA: 57:02 - loss: 0.6811 - regression_loss: 0.5478 - classification_loss: 0.1332
 3020/10000 [========>.....................] - ETA: 57:01 - loss: 0.6811 - regression_loss: 0.5478 - classification_loss: 0.1332
 3021/10000 [========>.....................] - ETA: 57:01 - loss: 0.6810 - regression_loss: 0.5477 - classification_loss: 0.1333
 3022/10000 [========>.....................] - ETA: 57:00 - loss: 0.6809 - regression_loss: 0.5477 - classification_loss: 0.1332
 3023/10000 [========>.....................] - ETA: 57:00 - loss: 0.6809 - regression_loss: 0.5477 - classification_loss: 0.1333
 3024/10000 [========>.....................] - ETA: 56:59 - loss: 0.6809 - regression_loss: 0.5477 - classification_loss: 0.1333
 3025/10000 [========>.....................] - ETA: 56:58 - loss: 0.6810 - regression_loss: 0.5477 - classification_loss: 0.1333
 3026/10000 [========>.....................] - ETA: 56:58 - loss: 0.6808 - regression_loss: 0.5476 - classification_loss: 0.1332
 3027/10000 [========>.....................] - ETA: 56:57 - loss: 0.6809 - regression_loss: 0.5477 - classification_loss: 0.1333
 3028/10000 [========>.....................] - ETA: 56:57 - loss: 0.6808 - regression_loss: 0.5476 - classification_loss: 0.1332
 3029/10000 [========>.....................] - ETA: 56:56 - loss: 0.6807 - regression_loss: 0.5475 - classification_loss: 0.1332
 3030/10000 [========>.....................] - ETA: 56:56 - loss: 0.6806 - regression_loss: 0.5474 - classification_loss: 0.1332
 3031/10000 [========>.....................] - ETA: 56:56 - loss: 0.6806 - regression_loss: 0.5473 - classification_loss: 0.1333
 3032/10000 [========>.....................] - ETA: 56:55 - loss: 0.6806 - regression_loss: 0.5472 - classification_loss: 0.1333
 3033/10000 [========>.....................] - ETA: 56:54 - loss: 0.6805 - regression_loss: 0.5471 - classification_loss: 0.1334
 3034/10000 [========>.....................] - ETA: 56:54 - loss: 0.6806 - regression_loss: 0.5472 - classification_loss: 0.1334
 3035/10000 [========>.....................] - ETA: 56:53 - loss: 0.6809 - regression_loss: 0.5475 - classification_loss: 0.1334
 3036/10000 [========>.....................] - ETA: 56:53 - loss: 0.6809 - regression_loss: 0.5475 - classification_loss: 0.1334
 3037/10000 [========>.....................] - ETA: 56:52 - loss: 0.6809 - regression_loss: 0.5475 - classification_loss: 0.1334
 3038/10000 [========>.....................] - ETA: 56:51 - loss: 0.6808 - regression_loss: 0.5475 - classification_loss: 0.1334
 3039/10000 [========>.....................] - ETA: 56:51 - loss: 0.6807 - regression_loss: 0.5474 - classification_loss: 0.1333
 3040/10000 [========>.....................] - ETA: 56:50 - loss: 0.6809 - regression_loss: 0.5476 - classification_loss: 0.1334
 3041/10000 [========>.....................] - ETA: 56:50 - loss: 0.6808 - regression_loss: 0.5475 - classification_loss: 0.1333
 3042/10000 [========>.....................] - ETA: 56:49 - loss: 0.6810 - regression_loss: 0.5476 - classification_loss: 0.1334
 3043/10000 [========>.....................] - ETA: 56:49 - loss: 0.6809 - regression_loss: 0.5475 - classification_loss: 0.1334
 3044/10000 [========>.....................] - ETA: 56:48 - loss: 0.6808 - regression_loss: 0.5474 - classification_loss: 0.1334
 3045/10000 [========>.....................] - ETA: 56:47 - loss: 0.6807 - regression_loss: 0.5474 - classification_loss: 0.1334
 3046/10000 [========>.....................] - ETA: 56:47 - loss: 0.6807 - regression_loss: 0.5473 - classification_loss: 0.1334
 3047/10000 [========>.....................] - ETA: 56:46 - loss: 0.6807 - regression_loss: 0.5473 - classification_loss: 0.1334
 3048/10000 [========>.....................] - ETA: 56:46 - loss: 0.6808 - regression_loss: 0.5474 - classification_loss: 0.1334
 3049/10000 [========>.....................] - ETA: 56:45 - loss: 0.6807 - regression_loss: 0.5474 - classification_loss: 0.1333
 3050/10000 [========>.....................] - ETA: 56:45 - loss: 0.6807 - regression_loss: 0.5473 - classification_loss: 0.1334
 3051/10000 [========>.....................] - ETA: 56:44 - loss: 0.6807 - regression_loss: 0.5473 - classification_loss: 0.1334
 3052/10000 [========>.....................] - ETA: 56:43 - loss: 0.6810 - regression_loss: 0.5476 - classification_loss: 0.1335
 3053/10000 [========>.....................] - ETA: 56:43 - loss: 0.6810 - regression_loss: 0.5476 - classification_loss: 0.1334
 3054/10000 [========>.....................] - ETA: 56:42 - loss: 0.6810 - regression_loss: 0.5476 - classification_loss: 0.1334
 3055/10000 [========>.....................] - ETA: 56:43 - loss: 0.6809 - regression_loss: 0.5475 - classification_loss: 0.1334
 3056/10000 [========>.....................] - ETA: 56:42 - loss: 0.6810 - regression_loss: 0.5475 - classification_loss: 0.1334
 3057/10000 [========>.....................] - ETA: 56:42 - loss: 0.6808 - regression_loss: 0.5474 - classification_loss: 0.1334
 3058/10000 [========>.....................] - ETA: 56:41 - loss: 0.6807 - regression_loss: 0.5473 - classification_loss: 0.1334
 3059/10000 [========>.....................] - ETA: 56:40 - loss: 0.6807 - regression_loss: 0.5474 - classification_loss: 0.1334
 3060/10000 [========>.....................] - ETA: 56:40 - loss: 0.6807 - regression_loss: 0.5473 - classification_loss: 0.1334
 3061/10000 [========>.....................] - ETA: 56:39 - loss: 0.6806 - regression_loss: 0.5472 - classification_loss: 0.1333
 3062/10000 [========>.....................] - ETA: 56:39 - loss: 0.6806 - regression_loss: 0.5472 - classification_loss: 0.1334
 3063/10000 [========>.....................] - ETA: 56:38 - loss: 0.6808 - regression_loss: 0.5474 - classification_loss: 0.1334
 3064/10000 [========>.....................] - ETA: 56:37 - loss: 0.6807 - regression_loss: 0.5473 - classification_loss: 0.1334
 3065/10000 [========>.....................] - ETA: 56:37 - loss: 0.6808 - regression_loss: 0.5474 - classification_loss: 0.1334
 3066/10000 [========>.....................] - ETA: 56:36 - loss: 0.6807 - regression_loss: 0.5474 - classification_loss: 0.1334
 3067/10000 [========>.....................] - ETA: 56:36 - loss: 0.6806 - regression_loss: 0.5473 - classification_loss: 0.1333
 3068/10000 [========>.....................] - ETA: 56:35 - loss: 0.6805 - regression_loss: 0.5472 - classification_loss: 0.1333
 3069/10000 [========>.....................] - ETA: 56:35 - loss: 0.6806 - regression_loss: 0.5472 - classification_loss: 0.1333
 3070/10000 [========>.....................] - ETA: 56:34 - loss: 0.6809 - regression_loss: 0.5475 - classification_loss: 0.1334
 3071/10000 [========>.....................] - ETA: 56:34 - loss: 0.6809 - regression_loss: 0.5475 - classification_loss: 0.1334
 3072/10000 [========>.....................] - ETA: 56:33 - loss: 0.6808 - regression_loss: 0.5474 - classification_loss: 0.1334
 3073/10000 [========>.....................] - ETA: 56:32 - loss: 0.6807 - regression_loss: 0.5473 - classification_loss: 0.1334
 3074/10000 [========>.....................] - ETA: 56:32 - loss: 0.6810 - regression_loss: 0.5475 - classification_loss: 0.1334
 3075/10000 [========>.....................] - ETA: 56:31 - loss: 0.6811 - regression_loss: 0.5476 - classification_loss: 0.1335
 3076/10000 [========>.....................] - ETA: 56:31 - loss: 0.6811 - regression_loss: 0.5476 - classification_loss: 0.1335
 3077/10000 [========>.....................] - ETA: 56:30 - loss: 0.6811 - regression_loss: 0.5475 - classification_loss: 0.1335
 3078/10000 [========>.....................] - ETA: 56:31 - loss: 0.6811 - regression_loss: 0.5476 - classification_loss: 0.1335
 3079/10000 [========>.....................] - ETA: 56:30 - loss: 0.6809 - regression_loss: 0.5474 - classification_loss: 0.1335
 3080/10000 [========>.....................] - ETA: 56:30 - loss: 0.6813 - regression_loss: 0.5478 - classification_loss: 0.1335
 3081/10000 [========>.....................] - ETA: 56:29 - loss: 0.6817 - regression_loss: 0.5481 - classification_loss: 0.1336
 3082/10000 [========>.....................] - ETA: 56:28 - loss: 0.6817 - regression_loss: 0.5481 - classification_loss: 0.1336
 3083/10000 [========>.....................] - ETA: 56:28 - loss: 0.6816 - regression_loss: 0.5480 - classification_loss: 0.1336
 3084/10000 [========>.....................] - ETA: 56:27 - loss: 0.6816 - regression_loss: 0.5480 - classification_loss: 0.1336
 3085/10000 [========>.....................] - ETA: 56:27 - loss: 0.6818 - regression_loss: 0.5482 - classification_loss: 0.1337
 3086/10000 [========>.....................] - ETA: 56:26 - loss: 0.6819 - regression_loss: 0.5482 - classification_loss: 0.1336
 3087/10000 [========>.....................] - ETA: 56:26 - loss: 0.6819 - regression_loss: 0.5483 - classification_loss: 0.1336
 3088/10000 [========>.....................] - ETA: 56:25 - loss: 0.6818 - regression_loss: 0.5482 - classification_loss: 0.1336
 3089/10000 [========>.....................] - ETA: 56:24 - loss: 0.6820 - regression_loss: 0.5482 - classification_loss: 0.1337
 3090/10000 [========>.....................] - ETA: 56:24 - loss: 0.6819 - regression_loss: 0.5482 - classification_loss: 0.1337
 3091/10000 [========>.....................] - ETA: 56:23 - loss: 0.6820 - regression_loss: 0.5483 - classification_loss: 0.1337
 3092/10000 [========>.....................] - ETA: 56:23 - loss: 0.6820 - regression_loss: 0.5484 - classification_loss: 0.1337
 3093/10000 [========>.....................] - ETA: 56:22 - loss: 0.6819 - regression_loss: 0.5482 - classification_loss: 0.1337
 3094/10000 [========>.....................] - ETA: 56:21 - loss: 0.6820 - regression_loss: 0.5483 - classification_loss: 0.1337
 3095/10000 [========>.....................] - ETA: 56:21 - loss: 0.6820 - regression_loss: 0.5484 - classification_loss: 0.1337
 3096/10000 [========>.....................] - ETA: 56:20 - loss: 0.6819 - regression_loss: 0.5483 - classification_loss: 0.1336
 3097/10000 [========>.....................] - ETA: 56:20 - loss: 0.6819 - regression_loss: 0.5483 - classification_loss: 0.1336
 3098/10000 [========>.....................] - ETA: 56:19 - loss: 0.6819 - regression_loss: 0.5482 - classification_loss: 0.1337
 3099/10000 [========>.....................] - ETA: 56:19 - loss: 0.6818 - regression_loss: 0.5481 - classification_loss: 0.1337
 3100/10000 [========>.....................] - ETA: 56:18 - loss: 0.6817 - regression_loss: 0.5480 - classification_loss: 0.1337
 3101/10000 [========>.....................] - ETA: 56:18 - loss: 0.6816 - regression_loss: 0.5480 - classification_loss: 0.1337
 3102/10000 [========>.....................] - ETA: 56:17 - loss: 0.6817 - regression_loss: 0.5480 - classification_loss: 0.1336
 3103/10000 [========>.....................] - ETA: 56:16 - loss: 0.6819 - regression_loss: 0.5481 - classification_loss: 0.1337
 3104/10000 [========>.....................] - ETA: 56:16 - loss: 0.6818 - regression_loss: 0.5481 - classification_loss: 0.1337
 3105/10000 [========>.....................] - ETA: 56:15 - loss: 0.6818 - regression_loss: 0.5481 - classification_loss: 0.1337
 3106/10000 [========>.....................] - ETA: 56:15 - loss: 0.6818 - regression_loss: 0.5481 - classification_loss: 0.1337
 3107/10000 [========>.....................] - ETA: 56:14 - loss: 0.6817 - regression_loss: 0.5479 - classification_loss: 0.1337
 3108/10000 [========>.....................] - ETA: 56:13 - loss: 0.6817 - regression_loss: 0.5479 - classification_loss: 0.1337
 3109/10000 [========>.....................] - ETA: 56:13 - loss: 0.6817 - regression_loss: 0.5480 - classification_loss: 0.1337
 3110/10000 [========>.....................] - ETA: 56:12 - loss: 0.6817 - regression_loss: 0.5480 - classification_loss: 0.1337
 3111/10000 [========>.....................] - ETA: 56:12 - loss: 0.6816 - regression_loss: 0.5479 - classification_loss: 0.1337
 3112/10000 [========>.....................] - ETA: 56:11 - loss: 0.6817 - regression_loss: 0.5480 - classification_loss: 0.1337
 3113/10000 [========>.....................] - ETA: 56:11 - loss: 0.6820 - regression_loss: 0.5483 - classification_loss: 0.1337
 3114/10000 [========>.....................] - ETA: 56:10 - loss: 0.6824 - regression_loss: 0.5487 - classification_loss: 0.1338
 3115/10000 [========>.....................] - ETA: 56:10 - loss: 0.6824 - regression_loss: 0.5486 - classification_loss: 0.1337
 3116/10000 [========>.....................] - ETA: 56:09 - loss: 0.6824 - regression_loss: 0.5486 - classification_loss: 0.1338
 3117/10000 [========>.....................] - ETA: 56:09 - loss: 0.6823 - regression_loss: 0.5486 - classification_loss: 0.1337
 3118/10000 [========>.....................] - ETA: 56:08 - loss: 0.6824 - regression_loss: 0.5486 - classification_loss: 0.1338
 3119/10000 [========>.....................] - ETA: 56:07 - loss: 0.6823 - regression_loss: 0.5485 - classification_loss: 0.1338
 3120/10000 [========>.....................] - ETA: 56:07 - loss: 0.6823 - regression_loss: 0.5484 - classification_loss: 0.1339
 3121/10000 [========>.....................] - ETA: 56:06 - loss: 0.6823 - regression_loss: 0.5484 - classification_loss: 0.1338
 3122/10000 [========>.....................] - ETA: 56:06 - loss: 0.6826 - regression_loss: 0.5487 - classification_loss: 0.1339
 3123/10000 [========>.....................] - ETA: 56:05 - loss: 0.6827 - regression_loss: 0.5487 - classification_loss: 0.1340
 3124/10000 [========>.....................] - ETA: 56:05 - loss: 0.6829 - regression_loss: 0.5488 - classification_loss: 0.1340
 3125/10000 [========>.....................] - ETA: 56:04 - loss: 0.6829 - regression_loss: 0.5488 - classification_loss: 0.1341
 3126/10000 [========>.....................] - ETA: 56:03 - loss: 0.6829 - regression_loss: 0.5488 - classification_loss: 0.1341
 3127/10000 [========>.....................] - ETA: 56:03 - loss: 0.6830 - regression_loss: 0.5489 - classification_loss: 0.1341
 3128/10000 [========>.....................] - ETA: 56:02 - loss: 0.6829 - regression_loss: 0.5488 - classification_loss: 0.1341
 3129/10000 [========>.....................] - ETA: 56:02 - loss: 0.6828 - regression_loss: 0.5487 - classification_loss: 0.1341
 3130/10000 [========>.....................] - ETA: 56:01 - loss: 0.6828 - regression_loss: 0.5488 - classification_loss: 0.1340
 3131/10000 [========>.....................] - ETA: 56:01 - loss: 0.6830 - regression_loss: 0.5489 - classification_loss: 0.1341
 3132/10000 [========>.....................] - ETA: 56:00 - loss: 0.6828 - regression_loss: 0.5488 - classification_loss: 0.1340
 3133/10000 [========>.....................] - ETA: 56:00 - loss: 0.6831 - regression_loss: 0.5490 - classification_loss: 0.1341
 3134/10000 [========>.....................] - ETA: 55:59 - loss: 0.6831 - regression_loss: 0.5490 - classification_loss: 0.1341
 3135/10000 [========>.....................] - ETA: 55:59 - loss: 0.6830 - regression_loss: 0.5490 - classification_loss: 0.1341
 3136/10000 [========>.....................] - ETA: 55:58 - loss: 0.6831 - regression_loss: 0.5490 - classification_loss: 0.1341
 3137/10000 [========>.....................] - ETA: 55:57 - loss: 0.6830 - regression_loss: 0.5489 - classification_loss: 0.1341
 3138/10000 [========>.....................] - ETA: 55:57 - loss: 0.6831 - regression_loss: 0.5490 - classification_loss: 0.1341
 3139/10000 [========>.....................] - ETA: 55:56 - loss: 0.6831 - regression_loss: 0.5489 - classification_loss: 0.1341
 3140/10000 [========>.....................] - ETA: 55:56 - loss: 0.6830 - regression_loss: 0.5489 - classification_loss: 0.1341
 3141/10000 [========>.....................] - ETA: 55:55 - loss: 0.6832 - regression_loss: 0.5490 - classification_loss: 0.1341
 3142/10000 [========>.....................] - ETA: 55:55 - loss: 0.6832 - regression_loss: 0.5491 - classification_loss: 0.1341
 3143/10000 [========>.....................] - ETA: 55:54 - loss: 0.6832 - regression_loss: 0.5491 - classification_loss: 0.1341
 3144/10000 [========>.....................] - ETA: 55:53 - loss: 0.6834 - regression_loss: 0.5493 - classification_loss: 0.1341
 3145/10000 [========>.....................] - ETA: 55:53 - loss: 0.6833 - regression_loss: 0.5492 - classification_loss: 0.1341
 3146/10000 [========>.....................] - ETA: 55:52 - loss: 0.6832 - regression_loss: 0.5491 - classification_loss: 0.1341
 3147/10000 [========>.....................] - ETA: 55:52 - loss: 0.6833 - regression_loss: 0.5492 - classification_loss: 0.1341
 3148/10000 [========>.....................] - ETA: 55:51 - loss: 0.6836 - regression_loss: 0.5494 - classification_loss: 0.1342
 3149/10000 [========>.....................] - ETA: 55:51 - loss: 0.6835 - regression_loss: 0.5494 - classification_loss: 0.1342
 3150/10000 [========>.....................] - ETA: 55:50 - loss: 0.6835 - regression_loss: 0.5494 - classification_loss: 0.1341
 3151/10000 [========>.....................] - ETA: 55:50 - loss: 0.6834 - regression_loss: 0.5493 - classification_loss: 0.1341
 3152/10000 [========>.....................] - ETA: 55:49 - loss: 0.6834 - regression_loss: 0.5493 - classification_loss: 0.1341
 3153/10000 [========>.....................] - ETA: 55:49 - loss: 0.6833 - regression_loss: 0.5492 - classification_loss: 0.1341
 3154/10000 [========>.....................] - ETA: 55:48 - loss: 0.6835 - regression_loss: 0.5494 - classification_loss: 0.1341
 3155/10000 [========>.....................] - ETA: 55:47 - loss: 0.6834 - regression_loss: 0.5492 - classification_loss: 0.1341
 3156/10000 [========>.....................] - ETA: 55:47 - loss: 0.6833 - regression_loss: 0.5492 - classification_loss: 0.1341
 3157/10000 [========>.....................] - ETA: 55:46 - loss: 0.6833 - regression_loss: 0.5492 - classification_loss: 0.1341
 3158/10000 [========>.....................] - ETA: 55:46 - loss: 0.6835 - regression_loss: 0.5494 - classification_loss: 0.1342
 3159/10000 [========>.....................] - ETA: 55:45 - loss: 0.6835 - regression_loss: 0.5494 - classification_loss: 0.1342
 3160/10000 [========>.....................] - ETA: 55:45 - loss: 0.6836 - regression_loss: 0.5495 - classification_loss: 0.1342
 3161/10000 [========>.....................] - ETA: 55:44 - loss: 0.6838 - regression_loss: 0.5496 - classification_loss: 0.1342
 3162/10000 [========>.....................] - ETA: 55:44 - loss: 0.6838 - regression_loss: 0.5496 - classification_loss: 0.1342
 3163/10000 [========>.....................] - ETA: 55:43 - loss: 0.6839 - regression_loss: 0.5496 - classification_loss: 0.1342
 3164/10000 [========>.....................] - ETA: 55:43 - loss: 0.6839 - regression_loss: 0.5497 - classification_loss: 0.1342
 3165/10000 [========>.....................] - ETA: 55:42 - loss: 0.6838 - regression_loss: 0.5496 - classification_loss: 0.1342
 3166/10000 [========>.....................] - ETA: 55:41 - loss: 0.6837 - regression_loss: 0.5495 - classification_loss: 0.1342
 3167/10000 [========>.....................] - ETA: 55:41 - loss: 0.6836 - regression_loss: 0.5494 - classification_loss: 0.1342
 3168/10000 [========>.....................] - ETA: 55:40 - loss: 0.6836 - regression_loss: 0.5494 - classification_loss: 0.1342
 3169/10000 [========>.....................] - ETA: 55:40 - loss: 0.6835 - regression_loss: 0.5494 - classification_loss: 0.1341
 3170/10000 [========>.....................] - ETA: 55:39 - loss: 0.6835 - regression_loss: 0.5494 - classification_loss: 0.1342
 3171/10000 [========>.....................] - ETA: 55:39 - loss: 0.6835 - regression_loss: 0.5493 - classification_loss: 0.1342
 3172/10000 [========>.....................] - ETA: 55:38 - loss: 0.6835 - regression_loss: 0.5493 - classification_loss: 0.1342
 3173/10000 [========>.....................] - ETA: 55:38 - loss: 0.6837 - regression_loss: 0.5495 - classification_loss: 0.1343
 3174/10000 [========>.....................] - ETA: 55:37 - loss: 0.6839 - regression_loss: 0.5496 - classification_loss: 0.1343
 3175/10000 [========>.....................] - ETA: 55:37 - loss: 0.6838 - regression_loss: 0.5495 - classification_loss: 0.1343
 3176/10000 [========>.....................] - ETA: 55:36 - loss: 0.6837 - regression_loss: 0.5495 - classification_loss: 0.1343
 3177/10000 [========>.....................] - ETA: 55:35 - loss: 0.6836 - regression_loss: 0.5493 - classification_loss: 0.1342
 3178/10000 [========>.....................] - ETA: 55:35 - loss: 0.6836 - regression_loss: 0.5494 - classification_loss: 0.1342
 3179/10000 [========>.....................] - ETA: 55:34 - loss: 0.6837 - regression_loss: 0.5495 - classification_loss: 0.1342
 3180/10000 [========>.....................] - ETA: 55:34 - loss: 0.6837 - regression_loss: 0.5495 - classification_loss: 0.1342
 3181/10000 [========>.....................] - ETA: 55:33 - loss: 0.6840 - regression_loss: 0.5497 - classification_loss: 0.1342
 3182/10000 [========>.....................] - ETA: 55:33 - loss: 0.6841 - regression_loss: 0.5498 - classification_loss: 0.1342
 3183/10000 [========>.....................] - ETA: 55:32 - loss: 0.6840 - regression_loss: 0.5498 - classification_loss: 0.1342
 3184/10000 [========>.....................] - ETA: 55:32 - loss: 0.6839 - regression_loss: 0.5497 - classification_loss: 0.1342
 3185/10000 [========>.....................] - ETA: 55:31 - loss: 0.6840 - regression_loss: 0.5498 - classification_loss: 0.1342
 3186/10000 [========>.....................] - ETA: 55:31 - loss: 0.6838 - regression_loss: 0.5496 - classification_loss: 0.1342
 3187/10000 [========>.....................] - ETA: 55:30 - loss: 0.6838 - regression_loss: 0.5496 - classification_loss: 0.1342
 3188/10000 [========>.....................] - ETA: 55:29 - loss: 0.6837 - regression_loss: 0.5496 - classification_loss: 0.1341
 3189/10000 [========>.....................] - ETA: 55:29 - loss: 0.6839 - regression_loss: 0.5497 - classification_loss: 0.1342
 3190/10000 [========>.....................] - ETA: 55:29 - loss: 0.6838 - regression_loss: 0.5496 - classification_loss: 0.1341
 3191/10000 [========>.....................] - ETA: 55:28 - loss: 0.6837 - regression_loss: 0.5496 - classification_loss: 0.1341
 3192/10000 [========>.....................] - ETA: 55:27 - loss: 0.6837 - regression_loss: 0.5495 - classification_loss: 0.1341
 3193/10000 [========>.....................] - ETA: 55:27 - loss: 0.6836 - regression_loss: 0.5495 - classification_loss: 0.1341
 3194/10000 [========>.....................] - ETA: 55:26 - loss: 0.6837 - regression_loss: 0.5496 - classification_loss: 0.1341
 3195/10000 [========>.....................] - ETA: 55:26 - loss: 0.6836 - regression_loss: 0.5495 - classification_loss: 0.1341
 3196/10000 [========>.....................] - ETA: 55:26 - loss: 0.6836 - regression_loss: 0.5495 - classification_loss: 0.1341
 3197/10000 [========>.....................] - ETA: 55:25 - loss: 0.6836 - regression_loss: 0.5495 - classification_loss: 0.1341
 3198/10000 [========>.....................] - ETA: 55:25 - loss: 0.6836 - regression_loss: 0.5495 - classification_loss: 0.1341
 3199/10000 [========>.....................] - ETA: 55:24 - loss: 0.6835 - regression_loss: 0.5494 - classification_loss: 0.1341
 3200/10000 [========>.....................] - ETA: 55:24 - loss: 0.6835 - regression_loss: 0.5494 - classification_loss: 0.1341
 3201/10000 [========>.....................] - ETA: 55:23 - loss: 0.6834 - regression_loss: 0.5493 - classification_loss: 0.1341
 3202/10000 [========>.....................] - ETA: 55:23 - loss: 0.6834 - regression_loss: 0.5493 - classification_loss: 0.1341
 3203/10000 [========>.....................] - ETA: 55:22 - loss: 0.6837 - regression_loss: 0.5496 - classification_loss: 0.1341
 3204/10000 [========>.....................] - ETA: 55:22 - loss: 0.6837 - regression_loss: 0.5495 - classification_loss: 0.1341
 3205/10000 [========>.....................] - ETA: 55:21 - loss: 0.6835 - regression_loss: 0.5494 - classification_loss: 0.1341
 3206/10000 [========>.....................] - ETA: 55:21 - loss: 0.6834 - regression_loss: 0.5493 - classification_loss: 0.1341
 3207/10000 [========>.....................] - ETA: 55:20 - loss: 0.6833 - regression_loss: 0.5492 - classification_loss: 0.1341
 3208/10000 [========>.....................] - ETA: 55:19 - loss: 0.6832 - regression_loss: 0.5492 - classification_loss: 0.1341
 3209/10000 [========>.....................] - ETA: 55:20 - loss: 0.6831 - regression_loss: 0.5490 - classification_loss: 0.1341
 3210/10000 [========>.....................] - ETA: 55:19 - loss: 0.6831 - regression_loss: 0.5490 - classification_loss: 0.1341
 3211/10000 [========>.....................] - ETA: 55:18 - loss: 0.6831 - regression_loss: 0.5490 - classification_loss: 0.1341
 3212/10000 [========>.....................] - ETA: 55:18 - loss: 0.6830 - regression_loss: 0.5489 - classification_loss: 0.1340
 3213/10000 [========>.....................] - ETA: 55:17 - loss: 0.6829 - regression_loss: 0.5488 - classification_loss: 0.1340
 3214/10000 [========>.....................] - ETA: 55:17 - loss: 0.6827 - regression_loss: 0.5487 - classification_loss: 0.1340
 3215/10000 [========>.....................] - ETA: 55:16 - loss: 0.6826 - regression_loss: 0.5486 - classification_loss: 0.1340
 3216/10000 [========>.....................] - ETA: 55:16 - loss: 0.6827 - regression_loss: 0.5487 - classification_loss: 0.1340
 3217/10000 [========>.....................] - ETA: 55:15 - loss: 0.6826 - regression_loss: 0.5486 - classification_loss: 0.1340
 3218/10000 [========>.....................] - ETA: 55:15 - loss: 0.6826 - regression_loss: 0.5486 - classification_loss: 0.1340
 3219/10000 [========>.....................] - ETA: 55:14 - loss: 0.6827 - regression_loss: 0.5487 - classification_loss: 0.1340
 3220/10000 [========>.....................] - ETA: 55:13 - loss: 0.6828 - regression_loss: 0.5487 - classification_loss: 0.1341
 3221/10000 [========>.....................] - ETA: 55:13 - loss: 0.6828 - regression_loss: 0.5488 - classification_loss: 0.1340
 3222/10000 [========>.....................] - ETA: 55:12 - loss: 0.6830 - regression_loss: 0.5490 - classification_loss: 0.1341
 3223/10000 [========>.....................] - ETA: 55:12 - loss: 0.6830 - regression_loss: 0.5489 - classification_loss: 0.1341
 3224/10000 [========>.....................] - ETA: 55:11 - loss: 0.6828 - regression_loss: 0.5488 - classification_loss: 0.1340
 3225/10000 [========>.....................] - ETA: 55:11 - loss: 0.6828 - regression_loss: 0.5488 - classification_loss: 0.1340
 3226/10000 [========>.....................] - ETA: 55:10 - loss: 0.6829 - regression_loss: 0.5489 - classification_loss: 0.1340
 3227/10000 [========>.....................] - ETA: 55:10 - loss: 0.6830 - regression_loss: 0.5489 - classification_loss: 0.1341
 3228/10000 [========>.....................] - ETA: 55:09 - loss: 0.6829 - regression_loss: 0.5489 - classification_loss: 0.1340
 3229/10000 [========>.....................] - ETA: 55:08 - loss: 0.6829 - regression_loss: 0.5489 - classification_loss: 0.1340
 3230/10000 [========>.....................] - ETA: 55:08 - loss: 0.6828 - regression_loss: 0.5488 - classification_loss: 0.1340
 3231/10000 [========>.....................] - ETA: 55:07 - loss: 0.6827 - regression_loss: 0.5487 - classification_loss: 0.1340
 3232/10000 [========>.....................] - ETA: 55:07 - loss: 0.6827 - regression_loss: 0.5488 - classification_loss: 0.1340
 3233/10000 [========>.....................] - ETA: 55:06 - loss: 0.6827 - regression_loss: 0.5487 - classification_loss: 0.1340
 3234/10000 [========>.....................] - ETA: 55:06 - loss: 0.6825 - regression_loss: 0.5485 - classification_loss: 0.1340
 3235/10000 [========>.....................] - ETA: 55:05 - loss: 0.6824 - regression_loss: 0.5484 - classification_loss: 0.1340
 3236/10000 [========>.....................] - ETA: 55:05 - loss: 0.6824 - regression_loss: 0.5485 - classification_loss: 0.1340
 3237/10000 [========>.....................] - ETA: 55:04 - loss: 0.6826 - regression_loss: 0.5486 - classification_loss: 0.1340
 3238/10000 [========>.....................] - ETA: 55:04 - loss: 0.6825 - regression_loss: 0.5486 - classification_loss: 0.1339
 3239/10000 [========>.....................] - ETA: 55:03 - loss: 0.6825 - regression_loss: 0.5486 - classification_loss: 0.1339
 3240/10000 [========>.....................] - ETA: 55:02 - loss: 0.6825 - regression_loss: 0.5485 - classification_loss: 0.1340
 3241/10000 [========>.....................] - ETA: 55:02 - loss: 0.6824 - regression_loss: 0.5485 - classification_loss: 0.1339
 3242/10000 [========>.....................] - ETA: 55:01 - loss: 0.6823 - regression_loss: 0.5484 - classification_loss: 0.1339
 3243/10000 [========>.....................] - ETA: 55:01 - loss: 0.6821 - regression_loss: 0.5483 - classification_loss: 0.1339
 3244/10000 [========>.....................] - ETA: 55:00 - loss: 0.6821 - regression_loss: 0.5483 - classification_loss: 0.1339
 3245/10000 [========>.....................] - ETA: 55:00 - loss: 0.6821 - regression_loss: 0.5482 - classification_loss: 0.1339
 3246/10000 [========>.....................] - ETA: 54:59 - loss: 0.6819 - regression_loss: 0.5481 - classification_loss: 0.1338
 3247/10000 [========>.....................] - ETA: 54:59 - loss: 0.6822 - regression_loss: 0.5483 - classification_loss: 0.1339
 3248/10000 [========>.....................] - ETA: 54:58 - loss: 0.6822 - regression_loss: 0.5483 - classification_loss: 0.1339
 3249/10000 [========>.....................] - ETA: 54:57 - loss: 0.6822 - regression_loss: 0.5483 - classification_loss: 0.1340
 3250/10000 [========>.....................] - ETA: 54:57 - loss: 0.6821 - regression_loss: 0.5481 - classification_loss: 0.1339
 3251/10000 [========>.....................] - ETA: 54:56 - loss: 0.6821 - regression_loss: 0.5482 - classification_loss: 0.1339
 3252/10000 [========>.....................] - ETA: 54:56 - loss: 0.6821 - regression_loss: 0.5482 - classification_loss: 0.1339
 3253/10000 [========>.....................] - ETA: 54:56 - loss: 0.6822 - regression_loss: 0.5483 - classification_loss: 0.1339
 3254/10000 [========>.....................] - ETA: 54:56 - loss: 0.6823 - regression_loss: 0.5483 - classification_loss: 0.1339
 3255/10000 [========>.....................] - ETA: 54:55 - loss: 0.6823 - regression_loss: 0.5483 - classification_loss: 0.1340
 3256/10000 [========>.....................] - ETA: 54:55 - loss: 0.6823 - regression_loss: 0.5483 - classification_loss: 0.1340
 3257/10000 [========>.....................] - ETA: 54:54 - loss: 0.6822 - regression_loss: 0.5482 - classification_loss: 0.1340
 3258/10000 [========>.....................] - ETA: 54:54 - loss: 0.6822 - regression_loss: 0.5482 - classification_loss: 0.1339
 3259/10000 [========>.....................] - ETA: 54:53 - loss: 0.6821 - regression_loss: 0.5481 - classification_loss: 0.1340
 3260/10000 [========>.....................] - ETA: 54:53 - loss: 0.6820 - regression_loss: 0.5481 - classification_loss: 0.1340
 3261/10000 [========>.....................] - ETA: 54:52 - loss: 0.6821 - regression_loss: 0.5481 - classification_loss: 0.1340
 3262/10000 [========>.....................] - ETA: 54:51 - loss: 0.6820 - regression_loss: 0.5480 - classification_loss: 0.1340
 3263/10000 [========>.....................] - ETA: 54:51 - loss: 0.6819 - regression_loss: 0.5480 - classification_loss: 0.1339
 3264/10000 [========>.....................] - ETA: 54:50 - loss: 0.6819 - regression_loss: 0.5480 - classification_loss: 0.1339
 3265/10000 [========>.....................] - ETA: 54:50 - loss: 0.6820 - regression_loss: 0.5480 - classification_loss: 0.1340
 3266/10000 [========>.....................] - ETA: 54:49 - loss: 0.6818 - regression_loss: 0.5479 - classification_loss: 0.1340
 3267/10000 [========>.....................] - ETA: 54:50 - loss: 0.6818 - regression_loss: 0.5478 - classification_loss: 0.1340
 3268/10000 [========>.....................] - ETA: 54:49 - loss: 0.6817 - regression_loss: 0.5477 - classification_loss: 0.1340
 3269/10000 [========>.....................] - ETA: 54:49 - loss: 0.6817 - regression_loss: 0.5477 - classification_loss: 0.1340
 3270/10000 [========>.....................] - ETA: 54:48 - loss: 0.6816 - regression_loss: 0.5477 - classification_loss: 0.1340
 3271/10000 [========>.....................] - ETA: 54:48 - loss: 0.6817 - regression_loss: 0.5477 - classification_loss: 0.1340
 3272/10000 [========>.....................] - ETA: 54:47 - loss: 0.6816 - regression_loss: 0.5476 - classification_loss: 0.1339
 3273/10000 [========>.....................] - ETA: 54:47 - loss: 0.6818 - regression_loss: 0.5478 - classification_loss: 0.1340
 3274/10000 [========>.....................] - ETA: 54:46 - loss: 0.6816 - regression_loss: 0.5477 - classification_loss: 0.1339
 3275/10000 [========>.....................] - ETA: 54:46 - loss: 0.6815 - regression_loss: 0.5476 - classification_loss: 0.1339
 3276/10000 [========>.....................] - ETA: 54:45 - loss: 0.6815 - regression_loss: 0.5476 - classification_loss: 0.1339
 3277/10000 [========>.....................] - ETA: 54:44 - loss: 0.6815 - regression_loss: 0.5476 - classification_loss: 0.1339
 3278/10000 [========>.....................] - ETA: 54:44 - loss: 0.6814 - regression_loss: 0.5475 - classification_loss: 0.1339
 3279/10000 [========>.....................] - ETA: 54:43 - loss: 0.6813 - regression_loss: 0.5474 - classification_loss: 0.1339
 3280/10000 [========>.....................] - ETA: 54:43 - loss: 0.6811 - regression_loss: 0.5473 - classification_loss: 0.1339
 3281/10000 [========>.....................] - ETA: 54:42 - loss: 0.6812 - regression_loss: 0.5473 - classification_loss: 0.1339
 3282/10000 [========>.....................] - ETA: 54:42 - loss: 0.6813 - regression_loss: 0.5474 - classification_loss: 0.1339
 3283/10000 [========>.....................] - ETA: 54:41 - loss: 0.6813 - regression_loss: 0.5474 - classification_loss: 0.1339
 3284/10000 [========>.....................] - ETA: 54:41 - loss: 0.6812 - regression_loss: 0.5473 - classification_loss: 0.1339
 3285/10000 [========>.....................] - ETA: 54:40 - loss: 0.6812 - regression_loss: 0.5473 - classification_loss: 0.1339
 3286/10000 [========>.....................] - ETA: 54:40 - loss: 0.6811 - regression_loss: 0.5473 - classification_loss: 0.1338
 3287/10000 [========>.....................] - ETA: 54:40 - loss: 0.6811 - regression_loss: 0.5472 - classification_loss: 0.1338
 3288/10000 [========>.....................] - ETA: 54:39 - loss: 0.6809 - regression_loss: 0.5471 - classification_loss: 0.1338
 3289/10000 [========>.....................] - ETA: 54:38 - loss: 0.6811 - regression_loss: 0.5472 - classification_loss: 0.1339
 3290/10000 [========>.....................] - ETA: 54:38 - loss: 0.6811 - regression_loss: 0.5472 - classification_loss: 0.1339
 3291/10000 [========>.....................] - ETA: 54:37 - loss: 0.6810 - regression_loss: 0.5472 - classification_loss: 0.1338
 3292/10000 [========>.....................] - ETA: 54:37 - loss: 0.6809 - regression_loss: 0.5471 - classification_loss: 0.1338
 3293/10000 [========>.....................] - ETA: 54:36 - loss: 0.6811 - regression_loss: 0.5472 - classification_loss: 0.1339
 3294/10000 [========>.....................] - ETA: 54:36 - loss: 0.6811 - regression_loss: 0.5473 - classification_loss: 0.1339
 3295/10000 [========>.....................] - ETA: 54:35 - loss: 0.6811 - regression_loss: 0.5472 - classification_loss: 0.1339
 3296/10000 [========>.....................] - ETA: 54:35 - loss: 0.6810 - regression_loss: 0.5471 - classification_loss: 0.1339
 3297/10000 [========>.....................] - ETA: 54:34 - loss: 0.6811 - regression_loss: 0.5472 - classification_loss: 0.1339
 3298/10000 [========>.....................] - ETA: 54:33 - loss: 0.6810 - regression_loss: 0.5471 - classification_loss: 0.1339
 3299/10000 [========>.....................] - ETA: 54:33 - loss: 0.6812 - regression_loss: 0.5473 - classification_loss: 0.1339
 3300/10000 [========>.....................] - ETA: 54:32 - loss: 0.6811 - regression_loss: 0.5472 - classification_loss: 0.1339
 3301/10000 [========>.....................] - ETA: 54:32 - loss: 0.6812 - regression_loss: 0.5473 - classification_loss: 0.1339
 3302/10000 [========>.....................] - ETA: 54:31 - loss: 0.6810 - regression_loss: 0.5472 - classification_loss: 0.1339
 3303/10000 [========>.....................] - ETA: 54:31 - loss: 0.6813 - regression_loss: 0.5474 - classification_loss: 0.1339
 3304/10000 [========>.....................] - ETA: 54:30 - loss: 0.6813 - regression_loss: 0.5474 - classification_loss: 0.1339
 3305/10000 [========>.....................] - ETA: 54:30 - loss: 0.6811 - regression_loss: 0.5473 - classification_loss: 0.1339
 3306/10000 [========>.....................] - ETA: 54:29 - loss: 0.6810 - regression_loss: 0.5471 - classification_loss: 0.1339
 3307/10000 [========>.....................] - ETA: 54:29 - loss: 0.6810 - regression_loss: 0.5472 - classification_loss: 0.1339
 3308/10000 [========>.....................] - ETA: 54:28 - loss: 0.6810 - regression_loss: 0.5471 - classification_loss: 0.1339
 3309/10000 [========>.....................] - ETA: 54:28 - loss: 0.6811 - regression_loss: 0.5472 - classification_loss: 0.1339
 3310/10000 [========>.....................] - ETA: 54:27 - loss: 0.6810 - regression_loss: 0.5471 - classification_loss: 0.1339
 3311/10000 [========>.....................] - ETA: 54:27 - loss: 0.6809 - regression_loss: 0.5471 - classification_loss: 0.1338
 3312/10000 [========>.....................] - ETA: 54:26 - loss: 0.6808 - regression_loss: 0.5470 - classification_loss: 0.1338
 3313/10000 [========>.....................] - ETA: 54:26 - loss: 0.6808 - regression_loss: 0.5470 - classification_loss: 0.1338
 3314/10000 [========>.....................] - ETA: 54:25 - loss: 0.6808 - regression_loss: 0.5470 - classification_loss: 0.1338
 3315/10000 [========>.....................] - ETA: 54:25 - loss: 0.6808 - regression_loss: 0.5469 - classification_loss: 0.1338
 3316/10000 [========>.....................] - ETA: 54:24 - loss: 0.6808 - regression_loss: 0.5470 - classification_loss: 0.1338
 3317/10000 [========>.....................] - ETA: 54:23 - loss: 0.6808 - regression_loss: 0.5470 - classification_loss: 0.1338
 3318/10000 [========>.....................] - ETA: 54:23 - loss: 0.6807 - regression_loss: 0.5470 - classification_loss: 0.1338
 3319/10000 [========>.....................] - ETA: 54:22 - loss: 0.6806 - regression_loss: 0.5469 - classification_loss: 0.1338
 3320/10000 [========>.....................] - ETA: 54:22 - loss: 0.6808 - regression_loss: 0.5470 - classification_loss: 0.1339
 3321/10000 [========>.....................] - ETA: 54:21 - loss: 0.6809 - regression_loss: 0.5470 - classification_loss: 0.1339
 3322/10000 [========>.....................] - ETA: 54:21 - loss: 0.6807 - regression_loss: 0.5469 - classification_loss: 0.1338
 3323/10000 [========>.....................] - ETA: 54:20 - loss: 0.6806 - regression_loss: 0.5468 - classification_loss: 0.1338
 3324/10000 [========>.....................] - ETA: 54:19 - loss: 0.6807 - regression_loss: 0.5469 - classification_loss: 0.1338
 3325/10000 [========>.....................] - ETA: 54:19 - loss: 0.6807 - regression_loss: 0.5469 - classification_loss: 0.1338
 3326/10000 [========>.....................] - ETA: 54:18 - loss: 0.6805 - regression_loss: 0.5468 - classification_loss: 0.1338
 3327/10000 [========>.....................] - ETA: 54:18 - loss: 0.6805 - regression_loss: 0.5468 - classification_loss: 0.1338
 3328/10000 [========>.....................] - ETA: 54:17 - loss: 0.6806 - regression_loss: 0.5468 - classification_loss: 0.1338
 3329/10000 [========>.....................] - ETA: 54:17 - loss: 0.6806 - regression_loss: 0.5468 - classification_loss: 0.1338
 3330/10000 [========>.....................] - ETA: 54:16 - loss: 0.6809 - regression_loss: 0.5470 - classification_loss: 0.1338
 3331/10000 [========>.....................] - ETA: 54:15 - loss: 0.6812 - regression_loss: 0.5474 - classification_loss: 0.1339
 3332/10000 [========>.....................] - ETA: 54:15 - loss: 0.6813 - regression_loss: 0.5475 - classification_loss: 0.1339
 3333/10000 [========>.....................] - ETA: 54:14 - loss: 0.6814 - regression_loss: 0.5475 - classification_loss: 0.1339
 3334/10000 [=========>....................] - ETA: 54:14 - loss: 0.6812 - regression_loss: 0.5474 - classification_loss: 0.1338
 3335/10000 [=========>....................] - ETA: 54:13 - loss: 0.6813 - regression_loss: 0.5475 - classification_loss: 0.1338
 3336/10000 [=========>....................] - ETA: 54:13 - loss: 0.6812 - regression_loss: 0.5474 - classification_loss: 0.1338
 3337/10000 [=========>....................] - ETA: 54:12 - loss: 0.6812 - regression_loss: 0.5474 - classification_loss: 0.1338
 3338/10000 [=========>....................] - ETA: 54:12 - loss: 0.6811 - regression_loss: 0.5473 - classification_loss: 0.1338
 3339/10000 [=========>....................] - ETA: 54:11 - loss: 0.6812 - regression_loss: 0.5474 - classification_loss: 0.1338
 3340/10000 [=========>....................] - ETA: 54:10 - loss: 0.6811 - regression_loss: 0.5473 - classification_loss: 0.1338
 3341/10000 [=========>....................] - ETA: 54:10 - loss: 0.6811 - regression_loss: 0.5473 - classification_loss: 0.1338
 3342/10000 [=========>....................] - ETA: 54:09 - loss: 0.6812 - regression_loss: 0.5474 - classification_loss: 0.1338
 3343/10000 [=========>....................] - ETA: 54:09 - loss: 0.6813 - regression_loss: 0.5475 - classification_loss: 0.1338
 3344/10000 [=========>....................] - ETA: 54:08 - loss: 0.6813 - regression_loss: 0.5475 - classification_loss: 0.1338
 3345/10000 [=========>....................] - ETA: 54:08 - loss: 0.6813 - regression_loss: 0.5475 - classification_loss: 0.1338
 3346/10000 [=========>....................] - ETA: 54:07 - loss: 0.6813 - regression_loss: 0.5475 - classification_loss: 0.1338
 3347/10000 [=========>....................] - ETA: 54:07 - loss: 0.6815 - regression_loss: 0.5477 - classification_loss: 0.1338
 3348/10000 [=========>....................] - ETA: 54:06 - loss: 0.6815 - regression_loss: 0.5477 - classification_loss: 0.1338
 3349/10000 [=========>....................] - ETA: 54:05 - loss: 0.6817 - regression_loss: 0.5479 - classification_loss: 0.1338
 3350/10000 [=========>....................] - ETA: 54:05 - loss: 0.6820 - regression_loss: 0.5481 - classification_loss: 0.1339
 3351/10000 [=========>....................] - ETA: 54:05 - loss: 0.6819 - regression_loss: 0.5481 - classification_loss: 0.1339
 3352/10000 [=========>....................] - ETA: 54:05 - loss: 0.6821 - regression_loss: 0.5481 - classification_loss: 0.1339
 3353/10000 [=========>....................] - ETA: 54:04 - loss: 0.6821 - regression_loss: 0.5482 - classification_loss: 0.1339
 3354/10000 [=========>....................] - ETA: 54:04 - loss: 0.6820 - regression_loss: 0.5482 - classification_loss: 0.1339
 3355/10000 [=========>....................] - ETA: 54:03 - loss: 0.6819 - regression_loss: 0.5480 - classification_loss: 0.1338
 3356/10000 [=========>....................] - ETA: 54:03 - loss: 0.6819 - regression_loss: 0.5480 - classification_loss: 0.1339
 3357/10000 [=========>....................] - ETA: 54:02 - loss: 0.6818 - regression_loss: 0.5479 - classification_loss: 0.1338
 3358/10000 [=========>....................] - ETA: 54:02 - loss: 0.6816 - regression_loss: 0.5478 - classification_loss: 0.1338
 3359/10000 [=========>....................] - ETA: 54:01 - loss: 0.6815 - regression_loss: 0.5477 - classification_loss: 0.1338
 3360/10000 [=========>....................] - ETA: 54:01 - loss: 0.6814 - regression_loss: 0.5476 - classification_loss: 0.1338
 3361/10000 [=========>....................] - ETA: 54:00 - loss: 0.6814 - regression_loss: 0.5476 - classification_loss: 0.1338
 3362/10000 [=========>....................] - ETA: 54:00 - loss: 0.6813 - regression_loss: 0.5476 - classification_loss: 0.1337
 3363/10000 [=========>....................] - ETA: 53:59 - loss: 0.6813 - regression_loss: 0.5476 - classification_loss: 0.1338
 3364/10000 [=========>....................] - ETA: 53:59 - loss: 0.6814 - regression_loss: 0.5477 - classification_loss: 0.1338
 3365/10000 [=========>....................] - ETA: 53:58 - loss: 0.6813 - regression_loss: 0.5476 - classification_loss: 0.1337
 3366/10000 [=========>....................] - ETA: 53:58 - loss: 0.6814 - regression_loss: 0.5476 - classification_loss: 0.1338
 3367/10000 [=========>....................] - ETA: 53:57 - loss: 0.6815 - regression_loss: 0.5478 - classification_loss: 0.1337
 3368/10000 [=========>....................] - ETA: 53:57 - loss: 0.6815 - regression_loss: 0.5477 - classification_loss: 0.1338
 3369/10000 [=========>....................] - ETA: 53:56 - loss: 0.6814 - regression_loss: 0.5476 - classification_loss: 0.1338
 3370/10000 [=========>....................] - ETA: 53:55 - loss: 0.6812 - regression_loss: 0.5475 - classification_loss: 0.1337
 3371/10000 [=========>....................] - ETA: 53:55 - loss: 0.6812 - regression_loss: 0.5475 - classification_loss: 0.1337
 3372/10000 [=========>....................] - ETA: 53:54 - loss: 0.6813 - regression_loss: 0.5476 - classification_loss: 0.1338
 3373/10000 [=========>....................] - ETA: 53:54 - loss: 0.6813 - regression_loss: 0.5475 - classification_loss: 0.1338
 3374/10000 [=========>....................] - ETA: 53:53 - loss: 0.6815 - regression_loss: 0.5476 - classification_loss: 0.1338
 3375/10000 [=========>....................] - ETA: 53:53 - loss: 0.6817 - regression_loss: 0.5478 - classification_loss: 0.1338
 3376/10000 [=========>....................] - ETA: 53:52 - loss: 0.6818 - regression_loss: 0.5480 - classification_loss: 0.1338
 3377/10000 [=========>....................] - ETA: 53:52 - loss: 0.6821 - regression_loss: 0.5482 - classification_loss: 0.1339
 3378/10000 [=========>....................] - ETA: 53:51 - loss: 0.6822 - regression_loss: 0.5483 - classification_loss: 0.1339
 3379/10000 [=========>....................] - ETA: 53:50 - loss: 0.6823 - regression_loss: 0.5484 - classification_loss: 0.1339
 3380/10000 [=========>....................] - ETA: 53:50 - loss: 0.6822 - regression_loss: 0.5484 - classification_loss: 0.1339
 3381/10000 [=========>....................] - ETA: 53:49 - loss: 0.6823 - regression_loss: 0.5485 - classification_loss: 0.1338
 3382/10000 [=========>....................] - ETA: 53:49 - loss: 0.6823 - regression_loss: 0.5484 - classification_loss: 0.1338
 3383/10000 [=========>....................] - ETA: 53:48 - loss: 0.6822 - regression_loss: 0.5483 - classification_loss: 0.1338
 3384/10000 [=========>....................] - ETA: 53:48 - loss: 0.6821 - regression_loss: 0.5482 - classification_loss: 0.1338
 3385/10000 [=========>....................] - ETA: 53:47 - loss: 0.6821 - regression_loss: 0.5483 - classification_loss: 0.1338
 3386/10000 [=========>....................] - ETA: 53:47 - loss: 0.6822 - regression_loss: 0.5484 - classification_loss: 0.1338
 3387/10000 [=========>....................] - ETA: 53:46 - loss: 0.6821 - regression_loss: 0.5483 - classification_loss: 0.1338
 3388/10000 [=========>....................] - ETA: 53:46 - loss: 0.6821 - regression_loss: 0.5483 - classification_loss: 0.1338
 3389/10000 [=========>....................] - ETA: 53:45 - loss: 0.6820 - regression_loss: 0.5483 - classification_loss: 0.1338
 3390/10000 [=========>....................] - ETA: 53:45 - loss: 0.6822 - regression_loss: 0.5484 - classification_loss: 0.1338
 3391/10000 [=========>....................] - ETA: 53:44 - loss: 0.6821 - regression_loss: 0.5483 - classification_loss: 0.1338
 3392/10000 [=========>....................] - ETA: 53:44 - loss: 0.6820 - regression_loss: 0.5482 - classification_loss: 0.1338
 3393/10000 [=========>....................] - ETA: 53:43 - loss: 0.6821 - regression_loss: 0.5482 - classification_loss: 0.1338
 3394/10000 [=========>....................] - ETA: 53:43 - loss: 0.6821 - regression_loss: 0.5483 - classification_loss: 0.1338
 3395/10000 [=========>....................] - ETA: 53:42 - loss: 0.6822 - regression_loss: 0.5484 - classification_loss: 0.1338
 3396/10000 [=========>....................] - ETA: 53:42 - loss: 0.6822 - regression_loss: 0.5484 - classification_loss: 0.1338
 3397/10000 [=========>....................] - ETA: 53:41 - loss: 0.6822 - regression_loss: 0.5484 - classification_loss: 0.1338
 3398/10000 [=========>....................] - ETA: 53:41 - loss: 0.6824 - regression_loss: 0.5485 - classification_loss: 0.1339
 3399/10000 [=========>....................] - ETA: 53:40 - loss: 0.6825 - regression_loss: 0.5486 - classification_loss: 0.1339
 3400/10000 [=========>....................] - ETA: 53:39 - loss: 0.6827 - regression_loss: 0.5488 - classification_loss: 0.1339
 3401/10000 [=========>....................] - ETA: 53:39 - loss: 0.6827 - regression_loss: 0.5488 - classification_loss: 0.1339
 3402/10000 [=========>....................] - ETA: 53:38 - loss: 0.6827 - regression_loss: 0.5488 - classification_loss: 0.1339
 3403/10000 [=========>....................] - ETA: 53:38 - loss: 0.6827 - regression_loss: 0.5488 - classification_loss: 0.1339
 3404/10000 [=========>....................] - ETA: 53:37 - loss: 0.6825 - regression_loss: 0.5487 - classification_loss: 0.1339
 3405/10000 [=========>....................] - ETA: 53:37 - loss: 0.6824 - regression_loss: 0.5485 - classification_loss: 0.1338
 3406/10000 [=========>....................] - ETA: 53:36 - loss: 0.6823 - regression_loss: 0.5484 - classification_loss: 0.1338
 3407/10000 [=========>....................] - ETA: 53:36 - loss: 0.6821 - regression_loss: 0.5483 - classification_loss: 0.1338
 3408/10000 [=========>....................] - ETA: 53:35 - loss: 0.6824 - regression_loss: 0.5486 - classification_loss: 0.1338
 3409/10000 [=========>....................] - ETA: 53:35 - loss: 0.6823 - regression_loss: 0.5485 - classification_loss: 0.1338
 3410/10000 [=========>....................] - ETA: 53:34 - loss: 0.6823 - regression_loss: 0.5485 - classification_loss: 0.1338
 3411/10000 [=========>....................] - ETA: 53:34 - loss: 0.6822 - regression_loss: 0.5485 - classification_loss: 0.1337
 3412/10000 [=========>....................] - ETA: 53:33 - loss: 0.6821 - regression_loss: 0.5483 - classification_loss: 0.1337
 3413/10000 [=========>....................] - ETA: 53:33 - loss: 0.6821 - regression_loss: 0.5484 - classification_loss: 0.1337
 3414/10000 [=========>....................] - ETA: 53:32 - loss: 0.6821 - regression_loss: 0.5484 - classification_loss: 0.1337
 3415/10000 [=========>....................] - ETA: 53:31 - loss: 0.6820 - regression_loss: 0.5483 - classification_loss: 0.1338
 3416/10000 [=========>....................] - ETA: 53:31 - loss: 0.6821 - regression_loss: 0.5484 - classification_loss: 0.1338
 3417/10000 [=========>....................] - ETA: 53:30 - loss: 0.6822 - regression_loss: 0.5485 - classification_loss: 0.1338
 3418/10000 [=========>....................] - ETA: 53:30 - loss: 0.6826 - regression_loss: 0.5488 - classification_loss: 0.1338
 3419/10000 [=========>....................] - ETA: 53:29 - loss: 0.6828 - regression_loss: 0.5488 - classification_loss: 0.1340
 3420/10000 [=========>....................] - ETA: 53:29 - loss: 0.6829 - regression_loss: 0.5488 - classification_loss: 0.1340
 3421/10000 [=========>....................] - ETA: 53:28 - loss: 0.6829 - regression_loss: 0.5489 - classification_loss: 0.1340
 3422/10000 [=========>....................] - ETA: 53:28 - loss: 0.6830 - regression_loss: 0.5490 - classification_loss: 0.1340
 3423/10000 [=========>....................] - ETA: 53:27 - loss: 0.6831 - regression_loss: 0.5490 - classification_loss: 0.1341
 3424/10000 [=========>....................] - ETA: 53:26 - loss: 0.6829 - regression_loss: 0.5489 - classification_loss: 0.1340
 3425/10000 [=========>....................] - ETA: 53:26 - loss: 0.6829 - regression_loss: 0.5489 - classification_loss: 0.1340
 3426/10000 [=========>....................] - ETA: 53:25 - loss: 0.6829 - regression_loss: 0.5489 - classification_loss: 0.1340
 3427/10000 [=========>....................] - ETA: 53:25 - loss: 0.6828 - regression_loss: 0.5488 - classification_loss: 0.1340
 3428/10000 [=========>....................] - ETA: 53:24 - loss: 0.6827 - regression_loss: 0.5487 - classification_loss: 0.1340
 3429/10000 [=========>....................] - ETA: 53:24 - loss: 0.6828 - regression_loss: 0.5488 - classification_loss: 0.1340
 3430/10000 [=========>....................] - ETA: 53:23 - loss: 0.6829 - regression_loss: 0.5489 - classification_loss: 0.1340
 3431/10000 [=========>....................] - ETA: 53:23 - loss: 0.6829 - regression_loss: 0.5490 - classification_loss: 0.1340
 3432/10000 [=========>....................] - ETA: 53:22 - loss: 0.6830 - regression_loss: 0.5490 - classification_loss: 0.1340
 3433/10000 [=========>....................] - ETA: 53:21 - loss: 0.6830 - regression_loss: 0.5490 - classification_loss: 0.1340
 3434/10000 [=========>....................] - ETA: 53:21 - loss: 0.6837 - regression_loss: 0.5491 - classification_loss: 0.1345
 3435/10000 [=========>....................] - ETA: 53:20 - loss: 0.6836 - regression_loss: 0.5491 - classification_loss: 0.1346
 3436/10000 [=========>....................] - ETA: 53:20 - loss: 0.6835 - regression_loss: 0.5490 - classification_loss: 0.1346
 3437/10000 [=========>....................] - ETA: 53:19 - loss: 0.6836 - regression_loss: 0.5490 - classification_loss: 0.1346
 3438/10000 [=========>....................] - ETA: 53:19 - loss: 0.6838 - regression_loss: 0.5492 - classification_loss: 0.1346
 3439/10000 [=========>....................] - ETA: 53:19 - loss: 0.6838 - regression_loss: 0.5492 - classification_loss: 0.1346
 3440/10000 [=========>....................] - ETA: 53:19 - loss: 0.6839 - regression_loss: 0.5493 - classification_loss: 0.1346
 3441/10000 [=========>....................] - ETA: 53:18 - loss: 0.6839 - regression_loss: 0.5492 - classification_loss: 0.1346
 3442/10000 [=========>....................] - ETA: 53:18 - loss: 0.6838 - regression_loss: 0.5492 - classification_loss: 0.1347
 3443/10000 [=========>....................] - ETA: 53:17 - loss: 0.6840 - regression_loss: 0.5493 - classification_loss: 0.1347
 3444/10000 [=========>....................] - ETA: 53:17 - loss: 0.6841 - regression_loss: 0.5493 - classification_loss: 0.1348
 3445/10000 [=========>....................] - ETA: 53:16 - loss: 0.6840 - regression_loss: 0.5493 - classification_loss: 0.1348
 3446/10000 [=========>....................] - ETA: 53:16 - loss: 0.6839 - regression_loss: 0.5491 - classification_loss: 0.1347
 3447/10000 [=========>....................] - ETA: 53:15 - loss: 0.6840 - regression_loss: 0.5492 - classification_loss: 0.1348
 3448/10000 [=========>....................] - ETA: 53:14 - loss: 0.6839 - regression_loss: 0.5491 - classification_loss: 0.1348
 3449/10000 [=========>....................] - ETA: 53:14 - loss: 0.6839 - regression_loss: 0.5491 - classification_loss: 0.1347
 3450/10000 [=========>....................] - ETA: 53:13 - loss: 0.6839 - regression_loss: 0.5491 - classification_loss: 0.1349
 3451/10000 [=========>....................] - ETA: 53:13 - loss: 0.6838 - regression_loss: 0.5490 - classification_loss: 0.1348
 3452/10000 [=========>....................] - ETA: 53:12 - loss: 0.6837 - regression_loss: 0.5489 - classification_loss: 0.1348
 3453/10000 [=========>....................] - ETA: 53:12 - loss: 0.6836 - regression_loss: 0.5488 - classification_loss: 0.1348
 3454/10000 [=========>....................] - ETA: 53:11 - loss: 0.6836 - regression_loss: 0.5488 - classification_loss: 0.1348
 3455/10000 [=========>....................] - ETA: 53:11 - loss: 0.6837 - regression_loss: 0.5489 - classification_loss: 0.1348
 3456/10000 [=========>....................] - ETA: 53:10 - loss: 0.6838 - regression_loss: 0.5490 - classification_loss: 0.1348
 3457/10000 [=========>....................] - ETA: 53:10 - loss: 0.6838 - regression_loss: 0.5490 - classification_loss: 0.1348
 3458/10000 [=========>....................] - ETA: 53:09 - loss: 0.6840 - regression_loss: 0.5491 - classification_loss: 0.1348
 3459/10000 [=========>....................] - ETA: 53:09 - loss: 0.6839 - regression_loss: 0.5491 - classification_loss: 0.1349
 3460/10000 [=========>....................] - ETA: 53:08 - loss: 0.6840 - regression_loss: 0.5491 - classification_loss: 0.1348
 3461/10000 [=========>....................] - ETA: 53:07 - loss: 0.6840 - regression_loss: 0.5492 - classification_loss: 0.1349
 3462/10000 [=========>....................] - ETA: 53:07 - loss: 0.6839 - regression_loss: 0.5491 - classification_loss: 0.1348
 3463/10000 [=========>....................] - ETA: 53:06 - loss: 0.6838 - regression_loss: 0.5490 - classification_loss: 0.1348
 3464/10000 [=========>....................] - ETA: 53:06 - loss: 0.6837 - regression_loss: 0.5489 - classification_loss: 0.1348
 3465/10000 [=========>....................] - ETA: 53:05 - loss: 0.6836 - regression_loss: 0.5488 - classification_loss: 0.1348
 3466/10000 [=========>....................] - ETA: 53:05 - loss: 0.6836 - regression_loss: 0.5488 - classification_loss: 0.1348
 3467/10000 [=========>....................] - ETA: 53:04 - loss: 0.6836 - regression_loss: 0.5488 - classification_loss: 0.1348
 3468/10000 [=========>....................] - ETA: 53:04 - loss: 0.6836 - regression_loss: 0.5487 - classification_loss: 0.1348
 3469/10000 [=========>....................] - ETA: 53:03 - loss: 0.6838 - regression_loss: 0.5490 - classification_loss: 0.1348
 3470/10000 [=========>....................] - ETA: 53:03 - loss: 0.6839 - regression_loss: 0.5491 - classification_loss: 0.1348
 3471/10000 [=========>....................] - ETA: 53:02 - loss: 0.6841 - regression_loss: 0.5492 - classification_loss: 0.1349
 3472/10000 [=========>....................] - ETA: 53:02 - loss: 0.6843 - regression_loss: 0.5494 - classification_loss: 0.1349
 3473/10000 [=========>....................] - ETA: 53:01 - loss: 0.6843 - regression_loss: 0.5494 - classification_loss: 0.1349
 3474/10000 [=========>....................] - ETA: 53:00 - loss: 0.6843 - regression_loss: 0.5494 - classification_loss: 0.1349
 3475/10000 [=========>....................] - ETA: 53:00 - loss: 0.6846 - regression_loss: 0.5497 - classification_loss: 0.1349
 3476/10000 [=========>....................] - ETA: 53:00 - loss: 0.6846 - regression_loss: 0.5497 - classification_loss: 0.1349
 3477/10000 [=========>....................] - ETA: 52:59 - loss: 0.6847 - regression_loss: 0.5498 - classification_loss: 0.1349
 3478/10000 [=========>....................] - ETA: 52:58 - loss: 0.6846 - regression_loss: 0.5497 - classification_loss: 0.1349
 3479/10000 [=========>....................] - ETA: 52:58 - loss: 0.6848 - regression_loss: 0.5498 - classification_loss: 0.1349
 3480/10000 [=========>....................] - ETA: 52:57 - loss: 0.6849 - regression_loss: 0.5499 - classification_loss: 0.1349
 3481/10000 [=========>....................] - ETA: 52:57 - loss: 0.6849 - regression_loss: 0.5500 - classification_loss: 0.1349
 3482/10000 [=========>....................] - ETA: 52:56 - loss: 0.6851 - regression_loss: 0.5502 - classification_loss: 0.1350
 3483/10000 [=========>....................] - ETA: 52:56 - loss: 0.6851 - regression_loss: 0.5502 - classification_loss: 0.1350
 3484/10000 [=========>....................] - ETA: 52:55 - loss: 0.6852 - regression_loss: 0.5502 - classification_loss: 0.1350
 3485/10000 [=========>....................] - ETA: 52:54 - loss: 0.6853 - regression_loss: 0.5502 - classification_loss: 0.1351
 3486/10000 [=========>....................] - ETA: 52:54 - loss: 0.6851 - regression_loss: 0.5501 - classification_loss: 0.1350
 3487/10000 [=========>....................] - ETA: 52:53 - loss: 0.6852 - regression_loss: 0.5501 - classification_loss: 0.1350
 3488/10000 [=========>....................] - ETA: 52:53 - loss: 0.6854 - regression_loss: 0.5504 - classification_loss: 0.1351
 3489/10000 [=========>....................] - ETA: 52:52 - loss: 0.6855 - regression_loss: 0.5504 - classification_loss: 0.1351
 3490/10000 [=========>....................] - ETA: 52:52 - loss: 0.6855 - regression_loss: 0.5503 - classification_loss: 0.1351
 3491/10000 [=========>....................] - ETA: 52:51 - loss: 0.6853 - regression_loss: 0.5502 - classification_loss: 0.1351
 3492/10000 [=========>....................] - ETA: 52:50 - loss: 0.6852 - regression_loss: 0.5502 - classification_loss: 0.1351
 3493/10000 [=========>....................] - ETA: 52:50 - loss: 0.6852 - regression_loss: 0.5502 - classification_loss: 0.1351
 3494/10000 [=========>....................] - ETA: 52:50 - loss: 0.6855 - regression_loss: 0.5504 - classification_loss: 0.1351
 3495/10000 [=========>....................] - ETA: 52:49 - loss: 0.6854 - regression_loss: 0.5503 - classification_loss: 0.1351
 3496/10000 [=========>....................] - ETA: 52:49 - loss: 0.6854 - regression_loss: 0.5504 - classification_loss: 0.1351
 3497/10000 [=========>....................] - ETA: 52:48 - loss: 0.6854 - regression_loss: 0.5503 - classification_loss: 0.1351
 3498/10000 [=========>....................] - ETA: 52:47 - loss: 0.6854 - regression_loss: 0.5503 - classification_loss: 0.1351
 3499/10000 [=========>....................] - ETA: 52:47 - loss: 0.6854 - regression_loss: 0.5503 - classification_loss: 0.1351
 3500/10000 [=========>....................] - ETA: 52:46 - loss: 0.6854 - regression_loss: 0.5503 - classification_loss: 0.1351
 3501/10000 [=========>....................] - ETA: 52:46 - loss: 0.6855 - regression_loss: 0.5504 - classification_loss: 0.1351
 3502/10000 [=========>....................] - ETA: 52:45 - loss: 0.6854 - regression_loss: 0.5504 - classification_loss: 0.1351
 3503/10000 [=========>....................] - ETA: 52:45 - loss: 0.6856 - regression_loss: 0.5505 - classification_loss: 0.1351
 3504/10000 [=========>....................] - ETA: 52:44 - loss: 0.6857 - regression_loss: 0.5506 - classification_loss: 0.1351
 3505/10000 [=========>....................] - ETA: 52:44 - loss: 0.6858 - regression_loss: 0.5507 - classification_loss: 0.1351
 3506/10000 [=========>....................] - ETA: 52:43 - loss: 0.6860 - regression_loss: 0.5509 - classification_loss: 0.1351
 3507/10000 [=========>....................] - ETA: 52:43 - loss: 0.6859 - regression_loss: 0.5508 - classification_loss: 0.1351
 3508/10000 [=========>....................] - ETA: 52:42 - loss: 0.6859 - regression_loss: 0.5507 - classification_loss: 0.1351
 3509/10000 [=========>....................] - ETA: 52:42 - loss: 0.6858 - regression_loss: 0.5507 - classification_loss: 0.1351
 3510/10000 [=========>....................] - ETA: 52:42 - loss: 0.6858 - regression_loss: 0.5507 - classification_loss: 0.1351
 3511/10000 [=========>....................] - ETA: 52:41 - loss: 0.6860 - regression_loss: 0.5509 - classification_loss: 0.1351
 3512/10000 [=========>....................] - ETA: 52:41 - loss: 0.6860 - regression_loss: 0.5509 - classification_loss: 0.1351
 3513/10000 [=========>....................] - ETA: 52:40 - loss: 0.6860 - regression_loss: 0.5509 - classification_loss: 0.1351
 3514/10000 [=========>....................] - ETA: 52:40 - loss: 0.6859 - regression_loss: 0.5508 - classification_loss: 0.1351
 3515/10000 [=========>....................] - ETA: 52:39 - loss: 0.6860 - regression_loss: 0.5509 - classification_loss: 0.1350
 3516/10000 [=========>....................] - ETA: 52:39 - loss: 0.6862 - regression_loss: 0.5511 - classification_loss: 0.1351
 3517/10000 [=========>....................] - ETA: 52:38 - loss: 0.6862 - regression_loss: 0.5510 - classification_loss: 0.1351
 3518/10000 [=========>....................] - ETA: 52:37 - loss: 0.6861 - regression_loss: 0.5510 - classification_loss: 0.1351
 3519/10000 [=========>....................] - ETA: 52:37 - loss: 0.6860 - regression_loss: 0.5509 - classification_loss: 0.1351
 3520/10000 [=========>....................] - ETA: 52:36 - loss: 0.6861 - regression_loss: 0.5510 - classification_loss: 0.1351
 3521/10000 [=========>....................] - ETA: 52:36 - loss: 0.6861 - regression_loss: 0.5510 - classification_loss: 0.1351
 3522/10000 [=========>....................] - ETA: 52:35 - loss: 0.6862 - regression_loss: 0.5511 - classification_loss: 0.1351
 3523/10000 [=========>....................] - ETA: 52:35 - loss: 0.6863 - regression_loss: 0.5512 - classification_loss: 0.1351
 3524/10000 [=========>....................] - ETA: 52:34 - loss: 0.6863 - regression_loss: 0.5512 - classification_loss: 0.1351
 3525/10000 [=========>....................] - ETA: 52:34 - loss: 0.6863 - regression_loss: 0.5513 - classification_loss: 0.1351
 3526/10000 [=========>....................] - ETA: 52:33 - loss: 0.6864 - regression_loss: 0.5513 - classification_loss: 0.1350
 3527/10000 [=========>....................] - ETA: 52:33 - loss: 0.6864 - regression_loss: 0.5514 - classification_loss: 0.1351
 3528/10000 [=========>....................] - ETA: 52:32 - loss: 0.6864 - regression_loss: 0.5513 - classification_loss: 0.1351
 3529/10000 [=========>....................] - ETA: 52:32 - loss: 0.6864 - regression_loss: 0.5513 - classification_loss: 0.1351
 3530/10000 [=========>....................] - ETA: 52:31 - loss: 0.6863 - regression_loss: 0.5513 - classification_loss: 0.1351
 3531/10000 [=========>....................] - ETA: 52:31 - loss: 0.6862 - regression_loss: 0.5512 - classification_loss: 0.1351
 3532/10000 [=========>....................] - ETA: 52:30 - loss: 0.6864 - regression_loss: 0.5513 - classification_loss: 0.1351
 3533/10000 [=========>....................] - ETA: 52:30 - loss: 0.6866 - regression_loss: 0.5515 - classification_loss: 0.1351
 3534/10000 [=========>....................] - ETA: 52:29 - loss: 0.6866 - regression_loss: 0.5515 - classification_loss: 0.1351
 3535/10000 [=========>....................] - ETA: 52:29 - loss: 0.6871 - regression_loss: 0.5519 - classification_loss: 0.1352
 3536/10000 [=========>....................] - ETA: 52:28 - loss: 0.6871 - regression_loss: 0.5519 - classification_loss: 0.1352
 3537/10000 [=========>....................] - ETA: 52:27 - loss: 0.6871 - regression_loss: 0.5519 - classification_loss: 0.1352
 3538/10000 [=========>....................] - ETA: 52:27 - loss: 0.6870 - regression_loss: 0.5518 - classification_loss: 0.1351
 3539/10000 [=========>....................] - ETA: 52:26 - loss: 0.6869 - regression_loss: 0.5518 - classification_loss: 0.1351
 3540/10000 [=========>....................] - ETA: 52:26 - loss: 0.6872 - regression_loss: 0.5520 - classification_loss: 0.1352
 3541/10000 [=========>....................] - ETA: 52:25 - loss: 0.6873 - regression_loss: 0.5521 - classification_loss: 0.1352
 3542/10000 [=========>....................] - ETA: 52:25 - loss: 0.6872 - regression_loss: 0.5520 - classification_loss: 0.1352
 3543/10000 [=========>....................] - ETA: 52:24 - loss: 0.6872 - regression_loss: 0.5520 - classification_loss: 0.1352
 3544/10000 [=========>....................] - ETA: 52:24 - loss: 0.6871 - regression_loss: 0.5520 - classification_loss: 0.1352
 3545/10000 [=========>....................] - ETA: 52:23 - loss: 0.6871 - regression_loss: 0.5519 - classification_loss: 0.1353
 3546/10000 [=========>....................] - ETA: 52:23 - loss: 0.6870 - regression_loss: 0.5517 - classification_loss: 0.1352
 3547/10000 [=========>....................] - ETA: 52:22 - loss: 0.6870 - regression_loss: 0.5518 - classification_loss: 0.1352
 3548/10000 [=========>....................] - ETA: 52:22 - loss: 0.6869 - regression_loss: 0.5517 - classification_loss: 0.1352
 3549/10000 [=========>....................] - ETA: 52:21 - loss: 0.6868 - regression_loss: 0.5516 - classification_loss: 0.1352
 3550/10000 [=========>....................] - ETA: 52:21 - loss: 0.6867 - regression_loss: 0.5516 - classification_loss: 0.1351
 3551/10000 [=========>....................] - ETA: 52:20 - loss: 0.6866 - regression_loss: 0.5515 - classification_loss: 0.1351
 3552/10000 [=========>....................] - ETA: 52:20 - loss: 0.6866 - regression_loss: 0.5515 - classification_loss: 0.1351
 3553/10000 [=========>....................] - ETA: 52:19 - loss: 0.6867 - regression_loss: 0.5515 - classification_loss: 0.1351
 3554/10000 [=========>....................] - ETA: 52:19 - loss: 0.6866 - regression_loss: 0.5515 - classification_loss: 0.1351
 3555/10000 [=========>....................] - ETA: 52:18 - loss: 0.6870 - regression_loss: 0.5518 - classification_loss: 0.1352
 3556/10000 [=========>....................] - ETA: 52:18 - loss: 0.6871 - regression_loss: 0.5519 - classification_loss: 0.1352
 3557/10000 [=========>....................] - ETA: 52:17 - loss: 0.6872 - regression_loss: 0.5520 - classification_loss: 0.1352
 3558/10000 [=========>....................] - ETA: 52:16 - loss: 0.6873 - regression_loss: 0.5520 - classification_loss: 0.1352
 3559/10000 [=========>....................] - ETA: 52:16 - loss: 0.6874 - regression_loss: 0.5521 - classification_loss: 0.1353
 3560/10000 [=========>....................] - ETA: 52:15 - loss: 0.6873 - regression_loss: 0.5520 - classification_loss: 0.1353
 3561/10000 [=========>....................] - ETA: 52:15 - loss: 0.6872 - regression_loss: 0.5519 - classification_loss: 0.1353
 3562/10000 [=========>....................] - ETA: 52:14 - loss: 0.6871 - regression_loss: 0.5518 - classification_loss: 0.1353
 3563/10000 [=========>....................] - ETA: 52:14 - loss: 0.6872 - regression_loss: 0.5519 - classification_loss: 0.1353
 3564/10000 [=========>....................] - ETA: 52:13 - loss: 0.6873 - regression_loss: 0.5520 - classification_loss: 0.1353
 3565/10000 [=========>....................] - ETA: 52:13 - loss: 0.6873 - regression_loss: 0.5520 - classification_loss: 0.1353
 3566/10000 [=========>....................] - ETA: 52:12 - loss: 0.6872 - regression_loss: 0.5519 - classification_loss: 0.1353
 3567/10000 [=========>....................] - ETA: 52:11 - loss: 0.6872 - regression_loss: 0.5519 - classification_loss: 0.1353
 3568/10000 [=========>....................] - ETA: 52:11 - loss: 0.6871 - regression_loss: 0.5518 - classification_loss: 0.1353
 3569/10000 [=========>....................] - ETA: 52:10 - loss: 0.6874 - regression_loss: 0.5520 - classification_loss: 0.1354
 3570/10000 [=========>....................] - ETA: 52:10 - loss: 0.6873 - regression_loss: 0.5520 - classification_loss: 0.1353
 3571/10000 [=========>....................] - ETA: 52:09 - loss: 0.6873 - regression_loss: 0.5519 - classification_loss: 0.1353
 3572/10000 [=========>....................] - ETA: 52:09 - loss: 0.6871 - regression_loss: 0.5518 - classification_loss: 0.1353
 3573/10000 [=========>....................] - ETA: 52:08 - loss: 0.6871 - regression_loss: 0.5518 - classification_loss: 0.1353
 3574/10000 [=========>....................] - ETA: 52:08 - loss: 0.6872 - regression_loss: 0.5517 - classification_loss: 0.1354
 3575/10000 [=========>....................] - ETA: 52:07 - loss: 0.6871 - regression_loss: 0.5516 - classification_loss: 0.1354
 3576/10000 [=========>....................] - ETA: 52:07 - loss: 0.6872 - regression_loss: 0.5517 - classification_loss: 0.1354
 3577/10000 [=========>....................] - ETA: 52:06 - loss: 0.6872 - regression_loss: 0.5518 - classification_loss: 0.1355
 3578/10000 [=========>....................] - ETA: 52:06 - loss: 0.6872 - regression_loss: 0.5518 - classification_loss: 0.1355
 3579/10000 [=========>....................] - ETA: 52:05 - loss: 0.6871 - regression_loss: 0.5516 - classification_loss: 0.1355
 3580/10000 [=========>....................] - ETA: 52:05 - loss: 0.6870 - regression_loss: 0.5515 - classification_loss: 0.1355
 3581/10000 [=========>....................] - ETA: 52:04 - loss: 0.6869 - regression_loss: 0.5515 - classification_loss: 0.1355
 3582/10000 [=========>....................] - ETA: 52:04 - loss: 0.6868 - regression_loss: 0.5514 - classification_loss: 0.1354
 3583/10000 [=========>....................] - ETA: 52:03 - loss: 0.6869 - regression_loss: 0.5514 - classification_loss: 0.1355
 3584/10000 [=========>....................] - ETA: 52:03 - loss: 0.6869 - regression_loss: 0.5513 - classification_loss: 0.1355
 3585/10000 [=========>....................] - ETA: 52:02 - loss: 0.6868 - regression_loss: 0.5513 - classification_loss: 0.1355
 3586/10000 [=========>....................] - ETA: 52:02 - loss: 0.6868 - regression_loss: 0.5513 - classification_loss: 0.1355
 3587/10000 [=========>....................] - ETA: 52:01 - loss: 0.6867 - regression_loss: 0.5512 - classification_loss: 0.1355
 3588/10000 [=========>....................] - ETA: 52:01 - loss: 0.6869 - regression_loss: 0.5514 - classification_loss: 0.1355
 3589/10000 [=========>....................] - ETA: 52:00 - loss: 0.6868 - regression_loss: 0.5513 - classification_loss: 0.1355
 3590/10000 [=========>....................] - ETA: 52:00 - loss: 0.6868 - regression_loss: 0.5513 - classification_loss: 0.1355
 3591/10000 [=========>....................] - ETA: 51:59 - loss: 0.6867 - regression_loss: 0.5512 - classification_loss: 0.1355
 3592/10000 [=========>....................] - ETA: 51:58 - loss: 0.6867 - regression_loss: 0.5512 - classification_loss: 0.1355
 3593/10000 [=========>....................] - ETA: 51:58 - loss: 0.6869 - regression_loss: 0.5514 - classification_loss: 0.1355
 3594/10000 [=========>....................] - ETA: 51:57 - loss: 0.6869 - regression_loss: 0.5514 - classification_loss: 0.1355
 3595/10000 [=========>....................] - ETA: 51:57 - loss: 0.6869 - regression_loss: 0.5514 - classification_loss: 0.1355
 3596/10000 [=========>....................] - ETA: 51:56 - loss: 0.6868 - regression_loss: 0.5513 - classification_loss: 0.1355
 3597/10000 [=========>....................] - ETA: 51:56 - loss: 0.6867 - regression_loss: 0.5512 - classification_loss: 0.1355
 3598/10000 [=========>....................] - ETA: 51:55 - loss: 0.6867 - regression_loss: 0.5512 - classification_loss: 0.1355
 3599/10000 [=========>....................] - ETA: 51:55 - loss: 0.6866 - regression_loss: 0.5511 - classification_loss: 0.1355
 3600/10000 [=========>....................] - ETA: 51:54 - loss: 0.6865 - regression_loss: 0.5510 - classification_loss: 0.1355
 3601/10000 [=========>....................] - ETA: 51:54 - loss: 0.6865 - regression_loss: 0.5511 - classification_loss: 0.1354
 3602/10000 [=========>....................] - ETA: 51:53 - loss: 0.6864 - regression_loss: 0.5509 - classification_loss: 0.1354
 3603/10000 [=========>....................] - ETA: 51:52 - loss: 0.6865 - regression_loss: 0.5510 - classification_loss: 0.1355
 3604/10000 [=========>....................] - ETA: 51:52 - loss: 0.6865 - regression_loss: 0.5511 - classification_loss: 0.1355
 3605/10000 [=========>....................] - ETA: 51:51 - loss: 0.6866 - regression_loss: 0.5511 - classification_loss: 0.1355
 3606/10000 [=========>....................] - ETA: 51:51 - loss: 0.6866 - regression_loss: 0.5512 - classification_loss: 0.1355
 3607/10000 [=========>....................] - ETA: 51:50 - loss: 0.6866 - regression_loss: 0.5510 - classification_loss: 0.1355
 3608/10000 [=========>....................] - ETA: 51:50 - loss: 0.6864 - regression_loss: 0.5509 - classification_loss: 0.1355
 3609/10000 [=========>....................] - ETA: 51:49 - loss: 0.6864 - regression_loss: 0.5509 - classification_loss: 0.1355
 3610/10000 [=========>....................] - ETA: 51:49 - loss: 0.6865 - regression_loss: 0.5510 - classification_loss: 0.1355
 3611/10000 [=========>....................] - ETA: 51:48 - loss: 0.6864 - regression_loss: 0.5509 - classification_loss: 0.1355
 3612/10000 [=========>....................] - ETA: 51:48 - loss: 0.6863 - regression_loss: 0.5508 - classification_loss: 0.1355
 3613/10000 [=========>....................] - ETA: 51:47 - loss: 0.6862 - regression_loss: 0.5507 - classification_loss: 0.1355
 3614/10000 [=========>....................] - ETA: 51:46 - loss: 0.6860 - regression_loss: 0.5506 - classification_loss: 0.1354
 3615/10000 [=========>....................] - ETA: 51:46 - loss: 0.6860 - regression_loss: 0.5505 - classification_loss: 0.1354
 3616/10000 [=========>....................] - ETA: 51:45 - loss: 0.6859 - regression_loss: 0.5505 - classification_loss: 0.1354
 3617/10000 [=========>....................] - ETA: 51:45 - loss: 0.6857 - regression_loss: 0.5503 - classification_loss: 0.1354
 3618/10000 [=========>....................] - ETA: 51:44 - loss: 0.6857 - regression_loss: 0.5503 - classification_loss: 0.1354
 3619/10000 [=========>....................] - ETA: 51:44 - loss: 0.6860 - regression_loss: 0.5505 - classification_loss: 0.1354
 3620/10000 [=========>....................] - ETA: 51:43 - loss: 0.6861 - regression_loss: 0.5507 - classification_loss: 0.1354
 3621/10000 [=========>....................] - ETA: 51:43 - loss: 0.6863 - regression_loss: 0.5508 - classification_loss: 0.1355
 3622/10000 [=========>....................] - ETA: 51:42 - loss: 0.6862 - regression_loss: 0.5507 - classification_loss: 0.1355
 3623/10000 [=========>....................] - ETA: 51:42 - loss: 0.6862 - regression_loss: 0.5507 - classification_loss: 0.1354
 3624/10000 [=========>....................] - ETA: 51:41 - loss: 0.6862 - regression_loss: 0.5507 - classification_loss: 0.1354
 3625/10000 [=========>....................] - ETA: 51:41 - loss: 0.6860 - regression_loss: 0.5506 - classification_loss: 0.1354
 3626/10000 [=========>....................] - ETA: 51:41 - loss: 0.6861 - regression_loss: 0.5507 - classification_loss: 0.1354
 3627/10000 [=========>....................] - ETA: 51:40 - loss: 0.6861 - regression_loss: 0.5506 - classification_loss: 0.1354
 3628/10000 [=========>....................] - ETA: 51:40 - loss: 0.6861 - regression_loss: 0.5507 - classification_loss: 0.1354
 3629/10000 [=========>....................] - ETA: 51:39 - loss: 0.6860 - regression_loss: 0.5506 - classification_loss: 0.1354
 3630/10000 [=========>....................] - ETA: 51:38 - loss: 0.6859 - regression_loss: 0.5506 - classification_loss: 0.1354
 3631/10000 [=========>....................] - ETA: 51:38 - loss: 0.6860 - regression_loss: 0.5506 - classification_loss: 0.1354
 3632/10000 [=========>....................] - ETA: 51:37 - loss: 0.6859 - regression_loss: 0.5505 - classification_loss: 0.1354
 3633/10000 [=========>....................] - ETA: 51:37 - loss: 0.6860 - regression_loss: 0.5506 - classification_loss: 0.1354
 3634/10000 [=========>....................] - ETA: 51:36 - loss: 0.6860 - regression_loss: 0.5506 - classification_loss: 0.1354
 3635/10000 [=========>....................] - ETA: 51:36 - loss: 0.6859 - regression_loss: 0.5505 - classification_loss: 0.1354
 3636/10000 [=========>....................] - ETA: 51:35 - loss: 0.6860 - regression_loss: 0.5506 - classification_loss: 0.1354
 3637/10000 [=========>....................] - ETA: 51:35 - loss: 0.6859 - regression_loss: 0.5505 - classification_loss: 0.1354
 3638/10000 [=========>....................] - ETA: 51:34 - loss: 0.6859 - regression_loss: 0.5505 - classification_loss: 0.1353
 3639/10000 [=========>....................] - ETA: 51:33 - loss: 0.6861 - regression_loss: 0.5507 - classification_loss: 0.1354
 3640/10000 [=========>....................] - ETA: 51:33 - loss: 0.6861 - regression_loss: 0.5507 - classification_loss: 0.1354
 3641/10000 [=========>....................] - ETA: 51:33 - loss: 0.6860 - regression_loss: 0.5507 - classification_loss: 0.1353
 3642/10000 [=========>....................] - ETA: 51:32 - loss: 0.6861 - regression_loss: 0.5507 - classification_loss: 0.1353
 3643/10000 [=========>....................] - ETA: 51:32 - loss: 0.6860 - regression_loss: 0.5506 - classification_loss: 0.1353
 3644/10000 [=========>....................] - ETA: 51:31 - loss: 0.6859 - regression_loss: 0.5506 - classification_loss: 0.1353
 3645/10000 [=========>....................] - ETA: 51:31 - loss: 0.6859 - regression_loss: 0.5506 - classification_loss: 0.1353
 3646/10000 [=========>....................] - ETA: 51:30 - loss: 0.6859 - regression_loss: 0.5506 - classification_loss: 0.1353
 3647/10000 [=========>....................] - ETA: 51:29 - loss: 0.6857 - regression_loss: 0.5504 - classification_loss: 0.1353
 3648/10000 [=========>....................] - ETA: 51:29 - loss: 0.6857 - regression_loss: 0.5504 - classification_loss: 0.1353
 3649/10000 [=========>....................] - ETA: 51:28 - loss: 0.6859 - regression_loss: 0.5506 - classification_loss: 0.1353
 3650/10000 [=========>....................] - ETA: 51:28 - loss: 0.6858 - regression_loss: 0.5506 - classification_loss: 0.1353
 3651/10000 [=========>....................] - ETA: 51:27 - loss: 0.6857 - regression_loss: 0.5505 - classification_loss: 0.1352
 3652/10000 [=========>....................] - ETA: 51:27 - loss: 0.6857 - regression_loss: 0.5505 - classification_loss: 0.1352
 3653/10000 [=========>....................] - ETA: 51:26 - loss: 0.6856 - regression_loss: 0.5504 - classification_loss: 0.1352
 3654/10000 [=========>....................] - ETA: 51:26 - loss: 0.6856 - regression_loss: 0.5504 - classification_loss: 0.1352
 3655/10000 [=========>....................] - ETA: 51:25 - loss: 0.6855 - regression_loss: 0.5503 - classification_loss: 0.1352
 3656/10000 [=========>....................] - ETA: 51:25 - loss: 0.6854 - regression_loss: 0.5503 - classification_loss: 0.1352
 3657/10000 [=========>....................] - ETA: 51:24 - loss: 0.6853 - regression_loss: 0.5502 - classification_loss: 0.1351
 3658/10000 [=========>....................] - ETA: 51:24 - loss: 0.6855 - regression_loss: 0.5503 - classification_loss: 0.1352
 3659/10000 [=========>....................] - ETA: 51:24 - loss: 0.6854 - regression_loss: 0.5503 - classification_loss: 0.1351
 3660/10000 [=========>....................] - ETA: 51:23 - loss: 0.6854 - regression_loss: 0.5503 - classification_loss: 0.1351
 3661/10000 [=========>....................] - ETA: 51:23 - loss: 0.6855 - regression_loss: 0.5504 - classification_loss: 0.1351
 3662/10000 [=========>....................] - ETA: 51:22 - loss: 0.6855 - regression_loss: 0.5503 - classification_loss: 0.1351
 3663/10000 [=========>....................] - ETA: 51:22 - loss: 0.6854 - regression_loss: 0.5503 - classification_loss: 0.1351
 3664/10000 [=========>....................] - ETA: 51:21 - loss: 0.6854 - regression_loss: 0.5503 - classification_loss: 0.1351
 3665/10000 [=========>....................] - ETA: 51:21 - loss: 0.6856 - regression_loss: 0.5504 - classification_loss: 0.1351
 3666/10000 [=========>....................] - ETA: 51:20 - loss: 0.6856 - regression_loss: 0.5504 - classification_loss: 0.1351
 3667/10000 [==========>...................] - ETA: 51:19 - loss: 0.6858 - regression_loss: 0.5506 - classification_loss: 0.1352
 3668/10000 [==========>...................] - ETA: 51:19 - loss: 0.6857 - regression_loss: 0.5506 - classification_loss: 0.1352
 3669/10000 [==========>...................] - ETA: 51:18 - loss: 0.6858 - regression_loss: 0.5506 - classification_loss: 0.1352
 3670/10000 [==========>...................] - ETA: 51:18 - loss: 0.6856 - regression_loss: 0.5505 - classification_loss: 0.1352
 3671/10000 [==========>...................] - ETA: 51:17 - loss: 0.6858 - regression_loss: 0.5506 - classification_loss: 0.1352
 3672/10000 [==========>...................] - ETA: 51:17 - loss: 0.6857 - regression_loss: 0.5506 - classification_loss: 0.1352
 3673/10000 [==========>...................] - ETA: 51:16 - loss: 0.6857 - regression_loss: 0.5505 - classification_loss: 0.1351
 3674/10000 [==========>...................] - ETA: 51:16 - loss: 0.6856 - regression_loss: 0.5505 - classification_loss: 0.1351
 3675/10000 [==========>...................] - ETA: 51:15 - loss: 0.6855 - regression_loss: 0.5504 - classification_loss: 0.1351
 3676/10000 [==========>...................] - ETA: 51:15 - loss: 0.6858 - regression_loss: 0.5506 - classification_loss: 0.1351
 3677/10000 [==========>...................] - ETA: 51:14 - loss: 0.6857 - regression_loss: 0.5506 - classification_loss: 0.1351
 3678/10000 [==========>...................] - ETA: 51:14 - loss: 0.6857 - regression_loss: 0.5506 - classification_loss: 0.1351
 3679/10000 [==========>...................] - ETA: 51:13 - loss: 0.6858 - regression_loss: 0.5507 - classification_loss: 0.1351
 3680/10000 [==========>...................] - ETA: 51:13 - loss: 0.6859 - regression_loss: 0.5508 - classification_loss: 0.1351
 3681/10000 [==========>...................] - ETA: 51:12 - loss: 0.6858 - regression_loss: 0.5507 - classification_loss: 0.1351
 3682/10000 [==========>...................] - ETA: 51:12 - loss: 0.6858 - regression_loss: 0.5507 - classification_loss: 0.1351
 3683/10000 [==========>...................] - ETA: 51:11 - loss: 0.6857 - regression_loss: 0.5506 - classification_loss: 0.1351
 3684/10000 [==========>...................] - ETA: 51:11 - loss: 0.6857 - regression_loss: 0.5506 - classification_loss: 0.1351
 3685/10000 [==========>...................] - ETA: 51:10 - loss: 0.6859 - regression_loss: 0.5508 - classification_loss: 0.1351
 3686/10000 [==========>...................] - ETA: 51:09 - loss: 0.6862 - regression_loss: 0.5510 - classification_loss: 0.1351
 3687/10000 [==========>...................] - ETA: 51:09 - loss: 0.6861 - regression_loss: 0.5510 - classification_loss: 0.1351
 3688/10000 [==========>...................] - ETA: 51:08 - loss: 0.6861 - regression_loss: 0.5509 - classification_loss: 0.1351
 3689/10000 [==========>...................] - ETA: 51:08 - loss: 0.6860 - regression_loss: 0.5509 - classification_loss: 0.1351
 3690/10000 [==========>...................] - ETA: 51:07 - loss: 0.6859 - regression_loss: 0.5508 - classification_loss: 0.1351
 3691/10000 [==========>...................] - ETA: 51:07 - loss: 0.6858 - regression_loss: 0.5507 - classification_loss: 0.1351
 3692/10000 [==========>...................] - ETA: 51:06 - loss: 0.6857 - regression_loss: 0.5506 - classification_loss: 0.1351
 3693/10000 [==========>...................] - ETA: 51:05 - loss: 0.6856 - regression_loss: 0.5506 - classification_loss: 0.1350
 3694/10000 [==========>...................] - ETA: 51:05 - loss: 0.6855 - regression_loss: 0.5505 - classification_loss: 0.1350
 3695/10000 [==========>...................] - ETA: 51:05 - loss: 0.6855 - regression_loss: 0.5504 - classification_loss: 0.1351
 3696/10000 [==========>...................] - ETA: 51:04 - loss: 0.6855 - regression_loss: 0.5504 - classification_loss: 0.1351
 3697/10000 [==========>...................] - ETA: 51:03 - loss: 0.6854 - regression_loss: 0.5503 - classification_loss: 0.1351
 3698/10000 [==========>...................] - ETA: 51:03 - loss: 0.6856 - regression_loss: 0.5505 - classification_loss: 0.1351
 3699/10000 [==========>...................] - ETA: 51:02 - loss: 0.6855 - regression_loss: 0.5505 - classification_loss: 0.1351
 3700/10000 [==========>...................] - ETA: 51:02 - loss: 0.6858 - regression_loss: 0.5506 - classification_loss: 0.1352
 3701/10000 [==========>...................] - ETA: 51:01 - loss: 0.6859 - regression_loss: 0.5506 - classification_loss: 0.1352
 3702/10000 [==========>...................] - ETA: 51:01 - loss: 0.6858 - regression_loss: 0.5506 - classification_loss: 0.1352
 3703/10000 [==========>...................] - ETA: 51:00 - loss: 0.6856 - regression_loss: 0.5504 - classification_loss: 0.1352
 3704/10000 [==========>...................] - ETA: 51:00 - loss: 0.6855 - regression_loss: 0.5504 - classification_loss: 0.1352
 3705/10000 [==========>...................] - ETA: 50:59 - loss: 0.6854 - regression_loss: 0.5502 - classification_loss: 0.1352
 3706/10000 [==========>...................] - ETA: 50:59 - loss: 0.6853 - regression_loss: 0.5501 - classification_loss: 0.1351
 3707/10000 [==========>...................] - ETA: 50:58 - loss: 0.6852 - regression_loss: 0.5501 - classification_loss: 0.1351
 3708/10000 [==========>...................] - ETA: 50:58 - loss: 0.6851 - regression_loss: 0.5500 - classification_loss: 0.1351
 3709/10000 [==========>...................] - ETA: 50:57 - loss: 0.6852 - regression_loss: 0.5500 - classification_loss: 0.1352
 3710/10000 [==========>...................] - ETA: 50:57 - loss: 0.6852 - regression_loss: 0.5500 - classification_loss: 0.1351
 3711/10000 [==========>...................] - ETA: 50:56 - loss: 0.6853 - regression_loss: 0.5501 - classification_loss: 0.1351
 3712/10000 [==========>...................] - ETA: 50:56 - loss: 0.6853 - regression_loss: 0.5501 - classification_loss: 0.1352
 3713/10000 [==========>...................] - ETA: 50:55 - loss: 0.6853 - regression_loss: 0.5501 - classification_loss: 0.1352
 3714/10000 [==========>...................] - ETA: 50:54 - loss: 0.6851 - regression_loss: 0.5500 - classification_loss: 0.1351
 3715/10000 [==========>...................] - ETA: 50:54 - loss: 0.6854 - regression_loss: 0.5502 - classification_loss: 0.1351
 3716/10000 [==========>...................] - ETA: 50:53 - loss: 0.6853 - regression_loss: 0.5502 - classification_loss: 0.1351
 3717/10000 [==========>...................] - ETA: 50:53 - loss: 0.6854 - regression_loss: 0.5503 - classification_loss: 0.1351
 3718/10000 [==========>...................] - ETA: 50:52 - loss: 0.6854 - regression_loss: 0.5502 - classification_loss: 0.1352
 3719/10000 [==========>...................] - ETA: 50:52 - loss: 0.6857 - regression_loss: 0.5504 - classification_loss: 0.1352
 3720/10000 [==========>...................] - ETA: 50:51 - loss: 0.6856 - regression_loss: 0.5504 - classification_loss: 0.1352
 3721/10000 [==========>...................] - ETA: 50:51 - loss: 0.6856 - regression_loss: 0.5504 - classification_loss: 0.1352
 3722/10000 [==========>...................] - ETA: 50:50 - loss: 0.6856 - regression_loss: 0.5504 - classification_loss: 0.1352
 3723/10000 [==========>...................] - ETA: 50:50 - loss: 0.6858 - regression_loss: 0.5504 - classification_loss: 0.1353
 3724/10000 [==========>...................] - ETA: 50:49 - loss: 0.6857 - regression_loss: 0.5504 - classification_loss: 0.1353
 3725/10000 [==========>...................] - ETA: 50:49 - loss: 0.6860 - regression_loss: 0.5507 - classification_loss: 0.1353
 3726/10000 [==========>...................] - ETA: 50:48 - loss: 0.6859 - regression_loss: 0.5506 - classification_loss: 0.1353
 3727/10000 [==========>...................] - ETA: 50:48 - loss: 0.6858 - regression_loss: 0.5505 - classification_loss: 0.1353
 3728/10000 [==========>...................] - ETA: 50:47 - loss: 0.6858 - regression_loss: 0.5505 - classification_loss: 0.1353
 3729/10000 [==========>...................] - ETA: 50:47 - loss: 0.6859 - regression_loss: 0.5506 - classification_loss: 0.1353
 3730/10000 [==========>...................] - ETA: 50:46 - loss: 0.6859 - regression_loss: 0.5506 - classification_loss: 0.1353
 3731/10000 [==========>...................] - ETA: 50:46 - loss: 0.6859 - regression_loss: 0.5506 - classification_loss: 0.1353
 3732/10000 [==========>...................] - ETA: 50:45 - loss: 0.6862 - regression_loss: 0.5509 - classification_loss: 0.1353
 3733/10000 [==========>...................] - ETA: 50:45 - loss: 0.6862 - regression_loss: 0.5509 - classification_loss: 0.1354
 3734/10000 [==========>...................] - ETA: 50:44 - loss: 0.6861 - regression_loss: 0.5508 - classification_loss: 0.1353
 3735/10000 [==========>...................] - ETA: 50:44 - loss: 0.6861 - regression_loss: 0.5507 - classification_loss: 0.1353
 3736/10000 [==========>...................] - ETA: 50:43 - loss: 0.6860 - regression_loss: 0.5507 - classification_loss: 0.1353
 3737/10000 [==========>...................] - ETA: 50:42 - loss: 0.6861 - regression_loss: 0.5508 - classification_loss: 0.1353
 3738/10000 [==========>...................] - ETA: 50:42 - loss: 0.6861 - regression_loss: 0.5508 - classification_loss: 0.1353
 3739/10000 [==========>...................] - ETA: 50:41 - loss: 0.6864 - regression_loss: 0.5511 - classification_loss: 0.1353
 3740/10000 [==========>...................] - ETA: 50:41 - loss: 0.6864 - regression_loss: 0.5511 - classification_loss: 0.1353
 3741/10000 [==========>...................] - ETA: 50:40 - loss: 0.6865 - regression_loss: 0.5511 - classification_loss: 0.1353
 3742/10000 [==========>...................] - ETA: 50:40 - loss: 0.6864 - regression_loss: 0.5511 - classification_loss: 0.1353
 3743/10000 [==========>...................] - ETA: 50:39 - loss: 0.6867 - regression_loss: 0.5513 - classification_loss: 0.1354
 3744/10000 [==========>...................] - ETA: 50:39 - loss: 0.6868 - regression_loss: 0.5514 - classification_loss: 0.1354
 3745/10000 [==========>...................] - ETA: 50:38 - loss: 0.6868 - regression_loss: 0.5514 - classification_loss: 0.1354
 3746/10000 [==========>...................] - ETA: 50:38 - loss: 0.6869 - regression_loss: 0.5515 - classification_loss: 0.1354
 3747/10000 [==========>...................] - ETA: 50:37 - loss: 0.6869 - regression_loss: 0.5515 - classification_loss: 0.1354
 3748/10000 [==========>...................] - ETA: 50:37 - loss: 0.6873 - regression_loss: 0.5519 - classification_loss: 0.1354
 3749/10000 [==========>...................] - ETA: 50:36 - loss: 0.6871 - regression_loss: 0.5517 - classification_loss: 0.1354
 3750/10000 [==========>...................] - ETA: 50:36 - loss: 0.6872 - regression_loss: 0.5518 - classification_loss: 0.1354
 3751/10000 [==========>...................] - ETA: 50:35 - loss: 0.6871 - regression_loss: 0.5516 - classification_loss: 0.1354
 3752/10000 [==========>...................] - ETA: 50:34 - loss: 0.6870 - regression_loss: 0.5516 - classification_loss: 0.1354
 3753/10000 [==========>...................] - ETA: 50:34 - loss: 0.6869 - regression_loss: 0.5515 - classification_loss: 0.1354
 3754/10000 [==========>...................] - ETA: 50:33 - loss: 0.6869 - regression_loss: 0.5515 - classification_loss: 0.1354
 3755/10000 [==========>...................] - ETA: 50:33 - loss: 0.6869 - regression_loss: 0.5515 - classification_loss: 0.1354
 3756/10000 [==========>...................] - ETA: 50:32 - loss: 0.6868 - regression_loss: 0.5514 - classification_loss: 0.1354
 3757/10000 [==========>...................] - ETA: 50:32 - loss: 0.6868 - regression_loss: 0.5514 - classification_loss: 0.1354
 3758/10000 [==========>...................] - ETA: 50:31 - loss: 0.6866 - regression_loss: 0.5513 - classification_loss: 0.1353
 3759/10000 [==========>...................] - ETA: 50:31 - loss: 0.6869 - regression_loss: 0.5515 - classification_loss: 0.1354
 3760/10000 [==========>...................] - ETA: 50:30 - loss: 0.6870 - regression_loss: 0.5516 - classification_loss: 0.1354
 3761/10000 [==========>...................] - ETA: 50:30 - loss: 0.6869 - regression_loss: 0.5516 - classification_loss: 0.1354
 3762/10000 [==========>...................] - ETA: 50:29 - loss: 0.6869 - regression_loss: 0.5516 - classification_loss: 0.1354
 3763/10000 [==========>...................] - ETA: 50:29 - loss: 0.6869 - regression_loss: 0.5515 - classification_loss: 0.1353
 3764/10000 [==========>...................] - ETA: 50:28 - loss: 0.6870 - regression_loss: 0.5516 - classification_loss: 0.1353
 3765/10000 [==========>...................] - ETA: 50:28 - loss: 0.6869 - regression_loss: 0.5516 - classification_loss: 0.1353
 3766/10000 [==========>...................] - ETA: 50:27 - loss: 0.6870 - regression_loss: 0.5516 - classification_loss: 0.1354
 3767/10000 [==========>...................] - ETA: 50:27 - loss: 0.6871 - regression_loss: 0.5517 - classification_loss: 0.1354
 3768/10000 [==========>...................] - ETA: 50:26 - loss: 0.6870 - regression_loss: 0.5516 - classification_loss: 0.1354
 3769/10000 [==========>...................] - ETA: 50:26 - loss: 0.6869 - regression_loss: 0.5515 - classification_loss: 0.1354
 3770/10000 [==========>...................] - ETA: 50:25 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 3771/10000 [==========>...................] - ETA: 50:25 - loss: 0.6873 - regression_loss: 0.5519 - classification_loss: 0.1354
 3772/10000 [==========>...................] - ETA: 50:24 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 3773/10000 [==========>...................] - ETA: 50:23 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 3774/10000 [==========>...................] - ETA: 50:23 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 3775/10000 [==========>...................] - ETA: 50:23 - loss: 0.6872 - regression_loss: 0.5518 - classification_loss: 0.1355
 3776/10000 [==========>...................] - ETA: 50:22 - loss: 0.6871 - regression_loss: 0.5517 - classification_loss: 0.1354
 3777/10000 [==========>...................] - ETA: 50:21 - loss: 0.6870 - regression_loss: 0.5516 - classification_loss: 0.1354
 3778/10000 [==========>...................] - ETA: 50:21 - loss: 0.6870 - regression_loss: 0.5516 - classification_loss: 0.1354
 3779/10000 [==========>...................] - ETA: 50:20 - loss: 0.6872 - regression_loss: 0.5517 - classification_loss: 0.1355
 3780/10000 [==========>...................] - ETA: 50:20 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1356
 3781/10000 [==========>...................] - ETA: 50:19 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 3782/10000 [==========>...................] - ETA: 50:19 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 3783/10000 [==========>...................] - ETA: 50:18 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1356
 3784/10000 [==========>...................] - ETA: 50:18 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1356
 3785/10000 [==========>...................] - ETA: 50:17 - loss: 0.6875 - regression_loss: 0.5519 - classification_loss: 0.1356
 3786/10000 [==========>...................] - ETA: 50:17 - loss: 0.6874 - regression_loss: 0.5518 - classification_loss: 0.1356
 3787/10000 [==========>...................] - ETA: 50:16 - loss: 0.6873 - regression_loss: 0.5517 - classification_loss: 0.1356
 3788/10000 [==========>...................] - ETA: 50:16 - loss: 0.6871 - regression_loss: 0.5516 - classification_loss: 0.1355
 3789/10000 [==========>...................] - ETA: 50:15 - loss: 0.6872 - regression_loss: 0.5517 - classification_loss: 0.1355
 3790/10000 [==========>...................] - ETA: 50:15 - loss: 0.6872 - regression_loss: 0.5516 - classification_loss: 0.1355
 3791/10000 [==========>...................] - ETA: 50:14 - loss: 0.6873 - regression_loss: 0.5517 - classification_loss: 0.1355
 3792/10000 [==========>...................] - ETA: 50:14 - loss: 0.6873 - regression_loss: 0.5517 - classification_loss: 0.1355
 3793/10000 [==========>...................] - ETA: 50:13 - loss: 0.6873 - regression_loss: 0.5517 - classification_loss: 0.1356
 3794/10000 [==========>...................] - ETA: 50:13 - loss: 0.6871 - regression_loss: 0.5516 - classification_loss: 0.1355
 3795/10000 [==========>...................] - ETA: 50:12 - loss: 0.6871 - regression_loss: 0.5516 - classification_loss: 0.1355
 3796/10000 [==========>...................] - ETA: 50:12 - loss: 0.6872 - regression_loss: 0.5516 - classification_loss: 0.1355
 3797/10000 [==========>...................] - ETA: 50:11 - loss: 0.6873 - regression_loss: 0.5518 - classification_loss: 0.1355
 3798/10000 [==========>...................] - ETA: 50:11 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 3799/10000 [==========>...................] - ETA: 50:10 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 3800/10000 [==========>...................] - ETA: 50:10 - loss: 0.6877 - regression_loss: 0.5521 - classification_loss: 0.1355
 3801/10000 [==========>...................] - ETA: 50:09 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1355
 3802/10000 [==========>...................] - ETA: 50:09 - loss: 0.6876 - regression_loss: 0.5520 - classification_loss: 0.1355
 3803/10000 [==========>...................] - ETA: 50:08 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 3804/10000 [==========>...................] - ETA: 50:08 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 3805/10000 [==========>...................] - ETA: 50:07 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1355
 3806/10000 [==========>...................] - ETA: 50:07 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 3807/10000 [==========>...................] - ETA: 50:06 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 3808/10000 [==========>...................] - ETA: 50:06 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1354
 3809/10000 [==========>...................] - ETA: 50:05 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 3810/10000 [==========>...................] - ETA: 50:04 - loss: 0.6875 - regression_loss: 0.5519 - classification_loss: 0.1355
 3811/10000 [==========>...................] - ETA: 50:04 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 3812/10000 [==========>...................] - ETA: 50:03 - loss: 0.6873 - regression_loss: 0.5518 - classification_loss: 0.1355
 3813/10000 [==========>...................] - ETA: 50:03 - loss: 0.6872 - regression_loss: 0.5518 - classification_loss: 0.1354
 3814/10000 [==========>...................] - ETA: 50:03 - loss: 0.6873 - regression_loss: 0.5518 - classification_loss: 0.1355
 3815/10000 [==========>...................] - ETA: 50:02 - loss: 0.6872 - regression_loss: 0.5517 - classification_loss: 0.1354
 3816/10000 [==========>...................] - ETA: 50:02 - loss: 0.6872 - regression_loss: 0.5518 - classification_loss: 0.1355
 3817/10000 [==========>...................] - ETA: 50:01 - loss: 0.6872 - regression_loss: 0.5518 - classification_loss: 0.1355
 3818/10000 [==========>...................] - ETA: 50:00 - loss: 0.6872 - regression_loss: 0.5517 - classification_loss: 0.1355
 3819/10000 [==========>...................] - ETA: 50:00 - loss: 0.6871 - regression_loss: 0.5517 - classification_loss: 0.1354
 3820/10000 [==========>...................] - ETA: 49:59 - loss: 0.6872 - regression_loss: 0.5517 - classification_loss: 0.1355
 3821/10000 [==========>...................] - ETA: 49:59 - loss: 0.6871 - regression_loss: 0.5516 - classification_loss: 0.1354
 3822/10000 [==========>...................] - ETA: 49:58 - loss: 0.6870 - regression_loss: 0.5516 - classification_loss: 0.1355
 3823/10000 [==========>...................] - ETA: 49:58 - loss: 0.6869 - regression_loss: 0.5515 - classification_loss: 0.1354
 3824/10000 [==========>...................] - ETA: 49:57 - loss: 0.6870 - regression_loss: 0.5516 - classification_loss: 0.1355
 3825/10000 [==========>...................] - ETA: 49:57 - loss: 0.6871 - regression_loss: 0.5516 - classification_loss: 0.1355
 3826/10000 [==========>...................] - ETA: 49:56 - loss: 0.6872 - regression_loss: 0.5517 - classification_loss: 0.1355
 3827/10000 [==========>...................] - ETA: 49:56 - loss: 0.6872 - regression_loss: 0.5517 - classification_loss: 0.1355
 3828/10000 [==========>...................] - ETA: 49:55 - loss: 0.6871 - regression_loss: 0.5517 - classification_loss: 0.1355
 3829/10000 [==========>...................] - ETA: 49:55 - loss: 0.6872 - regression_loss: 0.5517 - classification_loss: 0.1355
 3830/10000 [==========>...................] - ETA: 49:54 - loss: 0.6871 - regression_loss: 0.5516 - classification_loss: 0.1355
 3831/10000 [==========>...................] - ETA: 49:54 - loss: 0.6871 - regression_loss: 0.5516 - classification_loss: 0.1355
 3832/10000 [==========>...................] - ETA: 49:53 - loss: 0.6871 - regression_loss: 0.5516 - classification_loss: 0.1355
 3833/10000 [==========>...................] - ETA: 49:53 - loss: 0.6870 - regression_loss: 0.5515 - classification_loss: 0.1355
 3834/10000 [==========>...................] - ETA: 49:52 - loss: 0.6869 - regression_loss: 0.5514 - classification_loss: 0.1355
 3835/10000 [==========>...................] - ETA: 49:52 - loss: 0.6870 - regression_loss: 0.5515 - classification_loss: 0.1355
 3836/10000 [==========>...................] - ETA: 49:51 - loss: 0.6871 - regression_loss: 0.5516 - classification_loss: 0.1355
 3837/10000 [==========>...................] - ETA: 49:51 - loss: 0.6874 - regression_loss: 0.5518 - classification_loss: 0.1356
 3838/10000 [==========>...................] - ETA: 49:50 - loss: 0.6874 - regression_loss: 0.5518 - classification_loss: 0.1356
 3839/10000 [==========>...................] - ETA: 49:50 - loss: 0.6874 - regression_loss: 0.5518 - classification_loss: 0.1356
 3840/10000 [==========>...................] - ETA: 49:49 - loss: 0.6873 - regression_loss: 0.5517 - classification_loss: 0.1356
 3841/10000 [==========>...................] - ETA: 49:48 - loss: 0.6873 - regression_loss: 0.5517 - classification_loss: 0.1356
 3842/10000 [==========>...................] - ETA: 49:48 - loss: 0.6872 - regression_loss: 0.5516 - classification_loss: 0.1356
 3843/10000 [==========>...................] - ETA: 49:47 - loss: 0.6874 - regression_loss: 0.5518 - classification_loss: 0.1356
 3844/10000 [==========>...................] - ETA: 49:47 - loss: 0.6873 - regression_loss: 0.5517 - classification_loss: 0.1356
 3845/10000 [==========>...................] - ETA: 49:46 - loss: 0.6874 - regression_loss: 0.5518 - classification_loss: 0.1356
 3846/10000 [==========>...................] - ETA: 49:46 - loss: 0.6874 - regression_loss: 0.5518 - classification_loss: 0.1356
 3847/10000 [==========>...................] - ETA: 49:45 - loss: 0.6874 - regression_loss: 0.5518 - classification_loss: 0.1356
 3848/10000 [==========>...................] - ETA: 49:45 - loss: 0.6877 - regression_loss: 0.5520 - classification_loss: 0.1357
 3849/10000 [==========>...................] - ETA: 49:44 - loss: 0.6877 - regression_loss: 0.5520 - classification_loss: 0.1357
 3850/10000 [==========>...................] - ETA: 49:44 - loss: 0.6875 - regression_loss: 0.5519 - classification_loss: 0.1357
 3851/10000 [==========>...................] - ETA: 49:43 - loss: 0.6877 - regression_loss: 0.5520 - classification_loss: 0.1357
 3852/10000 [==========>...................] - ETA: 49:42 - loss: 0.6876 - regression_loss: 0.5519 - classification_loss: 0.1357
 3853/10000 [==========>...................] - ETA: 49:42 - loss: 0.6877 - regression_loss: 0.5520 - classification_loss: 0.1357
 3854/10000 [==========>...................] - ETA: 49:42 - loss: 0.6876 - regression_loss: 0.5519 - classification_loss: 0.1357
 3855/10000 [==========>...................] - ETA: 49:41 - loss: 0.6877 - regression_loss: 0.5520 - classification_loss: 0.1357
 3856/10000 [==========>...................] - ETA: 49:41 - loss: 0.6877 - regression_loss: 0.5520 - classification_loss: 0.1357
 3857/10000 [==========>...................] - ETA: 49:40 - loss: 0.6876 - regression_loss: 0.5520 - classification_loss: 0.1356
 3858/10000 [==========>...................] - ETA: 49:39 - loss: 0.6876 - regression_loss: 0.5519 - classification_loss: 0.1356
 3859/10000 [==========>...................] - ETA: 49:39 - loss: 0.6876 - regression_loss: 0.5520 - classification_loss: 0.1356
 3860/10000 [==========>...................] - ETA: 49:38 - loss: 0.6876 - regression_loss: 0.5519 - classification_loss: 0.1356
 3861/10000 [==========>...................] - ETA: 49:38 - loss: 0.6875 - regression_loss: 0.5519 - classification_loss: 0.1356
 3862/10000 [==========>...................] - ETA: 49:37 - loss: 0.6877 - regression_loss: 0.5520 - classification_loss: 0.1356
 3863/10000 [==========>...................] - ETA: 49:37 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1357
 3864/10000 [==========>...................] - ETA: 49:36 - loss: 0.6878 - regression_loss: 0.5521 - classification_loss: 0.1357
 3865/10000 [==========>...................] - ETA: 49:36 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1357
 3866/10000 [==========>...................] - ETA: 49:35 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1357
 3867/10000 [==========>...................] - ETA: 49:35 - loss: 0.6879 - regression_loss: 0.5523 - classification_loss: 0.1357
 3868/10000 [==========>...................] - ETA: 49:34 - loss: 0.6879 - regression_loss: 0.5522 - classification_loss: 0.1356
 3869/10000 [==========>...................] - ETA: 49:34 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1356
 3870/10000 [==========>...................] - ETA: 49:33 - loss: 0.6880 - regression_loss: 0.5523 - classification_loss: 0.1356
 3871/10000 [==========>...................] - ETA: 49:33 - loss: 0.6879 - regression_loss: 0.5523 - classification_loss: 0.1356
 3872/10000 [==========>...................] - ETA: 49:32 - loss: 0.6881 - regression_loss: 0.5524 - classification_loss: 0.1356
 3873/10000 [==========>...................] - ETA: 49:32 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1356
 3874/10000 [==========>...................] - ETA: 49:31 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 3875/10000 [==========>...................] - ETA: 49:31 - loss: 0.6882 - regression_loss: 0.5525 - classification_loss: 0.1357
 3876/10000 [==========>...................] - ETA: 49:30 - loss: 0.6883 - regression_loss: 0.5526 - classification_loss: 0.1357
 3877/10000 [==========>...................] - ETA: 49:30 - loss: 0.6883 - regression_loss: 0.5526 - classification_loss: 0.1357
 3878/10000 [==========>...................] - ETA: 49:29 - loss: 0.6884 - regression_loss: 0.5527 - classification_loss: 0.1357
 3879/10000 [==========>...................] - ETA: 49:28 - loss: 0.6885 - regression_loss: 0.5528 - classification_loss: 0.1357
 3880/10000 [==========>...................] - ETA: 49:28 - loss: 0.6885 - regression_loss: 0.5528 - classification_loss: 0.1357
 3881/10000 [==========>...................] - ETA: 49:27 - loss: 0.6885 - regression_loss: 0.5528 - classification_loss: 0.1357
 3882/10000 [==========>...................] - ETA: 49:27 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 3883/10000 [==========>...................] - ETA: 49:26 - loss: 0.6885 - regression_loss: 0.5528 - classification_loss: 0.1357
 3884/10000 [==========>...................] - ETA: 49:26 - loss: 0.6885 - regression_loss: 0.5528 - classification_loss: 0.1357
 3885/10000 [==========>...................] - ETA: 49:25 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 3886/10000 [==========>...................] - ETA: 49:25 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 3887/10000 [==========>...................] - ETA: 49:24 - loss: 0.6885 - regression_loss: 0.5528 - classification_loss: 0.1357
 3888/10000 [==========>...................] - ETA: 49:24 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 3889/10000 [==========>...................] - ETA: 49:23 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 3890/10000 [==========>...................] - ETA: 49:23 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1357
 3891/10000 [==========>...................] - ETA: 49:22 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1357
 3892/10000 [==========>...................] - ETA: 49:21 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1357
 3893/10000 [==========>...................] - ETA: 49:21 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1357
 3894/10000 [==========>...................] - ETA: 49:21 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1356
 3895/10000 [==========>...................] - ETA: 49:20 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1356
 3896/10000 [==========>...................] - ETA: 49:20 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1356
 3897/10000 [==========>...................] - ETA: 49:19 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1356
 3898/10000 [==========>...................] - ETA: 49:18 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1356
 3899/10000 [==========>...................] - ETA: 49:18 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1356
 3900/10000 [==========>...................] - ETA: 49:17 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1356
 3901/10000 [==========>...................] - ETA: 49:17 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1356
 3902/10000 [==========>...................] - ETA: 49:16 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1356
 3903/10000 [==========>...................] - ETA: 49:16 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1356
 3904/10000 [==========>...................] - ETA: 49:15 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1356
 3905/10000 [==========>...................] - ETA: 49:15 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1356
 3906/10000 [==========>...................] - ETA: 49:14 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 3907/10000 [==========>...................] - ETA: 49:14 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1356
 3908/10000 [==========>...................] - ETA: 49:13 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1356
 3909/10000 [==========>...................] - ETA: 49:12 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 3910/10000 [==========>...................] - ETA: 49:12 - loss: 0.6882 - regression_loss: 0.5527 - classification_loss: 0.1356
 3911/10000 [==========>...................] - ETA: 49:11 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1356
 3912/10000 [==========>...................] - ETA: 49:11 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 3913/10000 [==========>...................] - ETA: 49:10 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1355
 3914/10000 [==========>...................] - ETA: 49:10 - loss: 0.6879 - regression_loss: 0.5524 - classification_loss: 0.1355
 3915/10000 [==========>...................] - ETA: 49:09 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1356
 3916/10000 [==========>...................] - ETA: 49:09 - loss: 0.6881 - regression_loss: 0.5526 - classification_loss: 0.1356
 3917/10000 [==========>...................] - ETA: 49:08 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1356
 3918/10000 [==========>...................] - ETA: 49:08 - loss: 0.6881 - regression_loss: 0.5526 - classification_loss: 0.1355
 3919/10000 [==========>...................] - ETA: 49:07 - loss: 0.6880 - regression_loss: 0.5525 - classification_loss: 0.1355
 3920/10000 [==========>...................] - ETA: 49:07 - loss: 0.6880 - regression_loss: 0.5525 - classification_loss: 0.1355
 3921/10000 [==========>...................] - ETA: 49:06 - loss: 0.6879 - regression_loss: 0.5524 - classification_loss: 0.1355
 3922/10000 [==========>...................] - ETA: 49:06 - loss: 0.6880 - regression_loss: 0.5525 - classification_loss: 0.1355
 3923/10000 [==========>...................] - ETA: 49:05 - loss: 0.6880 - regression_loss: 0.5526 - classification_loss: 0.1355
 3924/10000 [==========>...................] - ETA: 49:04 - loss: 0.6882 - regression_loss: 0.5527 - classification_loss: 0.1355
 3925/10000 [==========>...................] - ETA: 49:04 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 3926/10000 [==========>...................] - ETA: 49:03 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1355
 3927/10000 [==========>...................] - ETA: 49:03 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 3928/10000 [==========>...................] - ETA: 49:02 - loss: 0.6881 - regression_loss: 0.5526 - classification_loss: 0.1356
 3929/10000 [==========>...................] - ETA: 49:02 - loss: 0.6881 - regression_loss: 0.5526 - classification_loss: 0.1355
 3930/10000 [==========>...................] - ETA: 49:01 - loss: 0.6880 - regression_loss: 0.5525 - classification_loss: 0.1355
 3931/10000 [==========>...................] - ETA: 49:01 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1355
 3932/10000 [==========>...................] - ETA: 49:00 - loss: 0.6881 - regression_loss: 0.5526 - classification_loss: 0.1355
 3933/10000 [==========>...................] - ETA: 49:00 - loss: 0.6881 - regression_loss: 0.5526 - classification_loss: 0.1355
 3934/10000 [==========>...................] - ETA: 48:59 - loss: 0.6880 - regression_loss: 0.5525 - classification_loss: 0.1355
 3935/10000 [==========>...................] - ETA: 48:59 - loss: 0.6880 - regression_loss: 0.5525 - classification_loss: 0.1355
 3936/10000 [==========>...................] - ETA: 48:58 - loss: 0.6880 - regression_loss: 0.5525 - classification_loss: 0.1355
 3937/10000 [==========>...................] - ETA: 48:58 - loss: 0.6880 - regression_loss: 0.5525 - classification_loss: 0.1355
 3938/10000 [==========>...................] - ETA: 48:57 - loss: 0.6881 - regression_loss: 0.5526 - classification_loss: 0.1355
 3939/10000 [==========>...................] - ETA: 48:56 - loss: 0.6880 - regression_loss: 0.5526 - classification_loss: 0.1355
 3940/10000 [==========>...................] - ETA: 48:56 - loss: 0.6879 - regression_loss: 0.5525 - classification_loss: 0.1355
 3941/10000 [==========>...................] - ETA: 48:55 - loss: 0.6878 - regression_loss: 0.5524 - classification_loss: 0.1355
 3942/10000 [==========>...................] - ETA: 48:55 - loss: 0.6879 - regression_loss: 0.5524 - classification_loss: 0.1355
 3943/10000 [==========>...................] - ETA: 48:54 - loss: 0.6878 - regression_loss: 0.5523 - classification_loss: 0.1355
 3944/10000 [==========>...................] - ETA: 48:54 - loss: 0.6878 - regression_loss: 0.5523 - classification_loss: 0.1355
 3945/10000 [==========>...................] - ETA: 48:53 - loss: 0.6879 - regression_loss: 0.5524 - classification_loss: 0.1355
 3946/10000 [==========>...................] - ETA: 48:53 - loss: 0.6880 - regression_loss: 0.5525 - classification_loss: 0.1355
 3947/10000 [==========>...................] - ETA: 48:52 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 3948/10000 [==========>...................] - ETA: 48:52 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 3949/10000 [==========>...................] - ETA: 48:51 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 3950/10000 [==========>...................] - ETA: 48:51 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1356
 3951/10000 [==========>...................] - ETA: 48:50 - loss: 0.6886 - regression_loss: 0.5530 - classification_loss: 0.1356
 3952/10000 [==========>...................] - ETA: 48:50 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1356
 3953/10000 [==========>...................] - ETA: 48:49 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1356
 3954/10000 [==========>...................] - ETA: 48:49 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1356
 3955/10000 [==========>...................] - ETA: 48:48 - loss: 0.6886 - regression_loss: 0.5530 - classification_loss: 0.1356
 3956/10000 [==========>...................] - ETA: 48:48 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1356
 3957/10000 [==========>...................] - ETA: 48:47 - loss: 0.6887 - regression_loss: 0.5531 - classification_loss: 0.1356
 3958/10000 [==========>...................] - ETA: 48:47 - loss: 0.6886 - regression_loss: 0.5530 - classification_loss: 0.1356
 3959/10000 [==========>...................] - ETA: 48:46 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1356
 3960/10000 [==========>...................] - ETA: 48:46 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1356
 3961/10000 [==========>...................] - ETA: 48:45 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1356
 3962/10000 [==========>...................] - ETA: 48:45 - loss: 0.6883 - regression_loss: 0.5528 - classification_loss: 0.1356
 3963/10000 [==========>...................] - ETA: 48:44 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1356
 3964/10000 [==========>...................] - ETA: 48:44 - loss: 0.6884 - regression_loss: 0.5529 - classification_loss: 0.1356
 3965/10000 [==========>...................] - ETA: 48:43 - loss: 0.6883 - regression_loss: 0.5528 - classification_loss: 0.1356
 3966/10000 [==========>...................] - ETA: 48:43 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1356
 3967/10000 [==========>...................] - ETA: 48:42 - loss: 0.6884 - regression_loss: 0.5529 - classification_loss: 0.1356
 3968/10000 [==========>...................] - ETA: 48:41 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1356
 3969/10000 [==========>...................] - ETA: 48:41 - loss: 0.6883 - regression_loss: 0.5528 - classification_loss: 0.1355
 3970/10000 [==========>...................] - ETA: 48:40 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1356
 3971/10000 [==========>...................] - ETA: 48:40 - loss: 0.6887 - regression_loss: 0.5531 - classification_loss: 0.1356
 3972/10000 [==========>...................] - ETA: 48:39 - loss: 0.6887 - regression_loss: 0.5531 - classification_loss: 0.1356
 3973/10000 [==========>...................] - ETA: 48:39 - loss: 0.6886 - regression_loss: 0.5531 - classification_loss: 0.1356
 3974/10000 [==========>...................] - ETA: 48:38 - loss: 0.6885 - regression_loss: 0.5530 - classification_loss: 0.1356
 3975/10000 [==========>...................] - ETA: 48:38 - loss: 0.6886 - regression_loss: 0.5530 - classification_loss: 0.1356
 3976/10000 [==========>...................] - ETA: 48:37 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1356
 3977/10000 [==========>...................] - ETA: 48:37 - loss: 0.6886 - regression_loss: 0.5530 - classification_loss: 0.1356
 3978/10000 [==========>...................] - ETA: 48:36 - loss: 0.6886 - regression_loss: 0.5531 - classification_loss: 0.1356
 3979/10000 [==========>...................] - ETA: 48:36 - loss: 0.6886 - regression_loss: 0.5530 - classification_loss: 0.1356
 3980/10000 [==========>...................] - ETA: 48:35 - loss: 0.6886 - regression_loss: 0.5530 - classification_loss: 0.1356
 3981/10000 [==========>...................] - ETA: 48:35 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1356
 3982/10000 [==========>...................] - ETA: 48:34 - loss: 0.6884 - regression_loss: 0.5529 - classification_loss: 0.1355
 3983/10000 [==========>...................] - ETA: 48:34 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1355
 3984/10000 [==========>...................] - ETA: 48:33 - loss: 0.6884 - regression_loss: 0.5529 - classification_loss: 0.1355
 3985/10000 [==========>...................] - ETA: 48:33 - loss: 0.6885 - regression_loss: 0.5530 - classification_loss: 0.1355
 3986/10000 [==========>...................] - ETA: 48:32 - loss: 0.6884 - regression_loss: 0.5529 - classification_loss: 0.1355
 3987/10000 [==========>...................] - ETA: 48:32 - loss: 0.6883 - regression_loss: 0.5528 - classification_loss: 0.1355
 3988/10000 [==========>...................] - ETA: 48:31 - loss: 0.6883 - regression_loss: 0.5528 - classification_loss: 0.1355
 3989/10000 [==========>...................] - ETA: 48:31 - loss: 0.6882 - regression_loss: 0.5528 - classification_loss: 0.1355
 3990/10000 [==========>...................] - ETA: 48:30 - loss: 0.6881 - regression_loss: 0.5526 - classification_loss: 0.1354
 3991/10000 [==========>...................] - ETA: 48:30 - loss: 0.6880 - regression_loss: 0.5526 - classification_loss: 0.1354
 3992/10000 [==========>...................] - ETA: 48:29 - loss: 0.6879 - regression_loss: 0.5525 - classification_loss: 0.1354
 3993/10000 [==========>...................] - ETA: 48:29 - loss: 0.6879 - regression_loss: 0.5525 - classification_loss: 0.1354
 3994/10000 [==========>...................] - ETA: 48:28 - loss: 0.6879 - regression_loss: 0.5525 - classification_loss: 0.1354
 3995/10000 [==========>...................] - ETA: 48:28 - loss: 0.6878 - regression_loss: 0.5525 - classification_loss: 0.1353
 3996/10000 [==========>...................] - ETA: 48:27 - loss: 0.6878 - regression_loss: 0.5524 - classification_loss: 0.1353
 3997/10000 [==========>...................] - ETA: 48:26 - loss: 0.6877 - regression_loss: 0.5524 - classification_loss: 0.1353
 3998/10000 [==========>...................] - ETA: 48:26 - loss: 0.6876 - regression_loss: 0.5523 - classification_loss: 0.1353
 3999/10000 [==========>...................] - ETA: 48:25 - loss: 0.6875 - regression_loss: 0.5522 - classification_loss: 0.1353
 4000/10000 [===========>..................] - ETA: 48:25 - loss: 0.6875 - regression_loss: 0.5522 - classification_loss: 0.1353
 4001/10000 [===========>..................] - ETA: 48:25 - loss: 0.6878 - regression_loss: 0.5524 - classification_loss: 0.1353
 4002/10000 [===========>..................] - ETA: 48:24 - loss: 0.6877 - regression_loss: 0.5524 - classification_loss: 0.1353
 4003/10000 [===========>..................] - ETA: 48:23 - loss: 0.6877 - regression_loss: 0.5524 - classification_loss: 0.1353
 4004/10000 [===========>..................] - ETA: 48:23 - loss: 0.6878 - regression_loss: 0.5524 - classification_loss: 0.1354
 4005/10000 [===========>..................] - ETA: 48:22 - loss: 0.6878 - regression_loss: 0.5524 - classification_loss: 0.1353
 4006/10000 [===========>..................] - ETA: 48:22 - loss: 0.6878 - regression_loss: 0.5524 - classification_loss: 0.1353
 4007/10000 [===========>..................] - ETA: 48:21 - loss: 0.6877 - regression_loss: 0.5524 - classification_loss: 0.1353
 4008/10000 [===========>..................] - ETA: 48:21 - loss: 0.6876 - regression_loss: 0.5523 - classification_loss: 0.1353
 4009/10000 [===========>..................] - ETA: 48:20 - loss: 0.6879 - regression_loss: 0.5525 - classification_loss: 0.1354
 4010/10000 [===========>..................] - ETA: 48:20 - loss: 0.6879 - regression_loss: 0.5525 - classification_loss: 0.1354
 4011/10000 [===========>..................] - ETA: 48:19 - loss: 0.6878 - regression_loss: 0.5524 - classification_loss: 0.1354
 4012/10000 [===========>..................] - ETA: 48:19 - loss: 0.6877 - regression_loss: 0.5523 - classification_loss: 0.1354
 4013/10000 [===========>..................] - ETA: 48:18 - loss: 0.6876 - regression_loss: 0.5523 - classification_loss: 0.1353
 4014/10000 [===========>..................] - ETA: 48:18 - loss: 0.6877 - regression_loss: 0.5523 - classification_loss: 0.1353
 4015/10000 [===========>..................] - ETA: 48:17 - loss: 0.6877 - regression_loss: 0.5523 - classification_loss: 0.1353
 4016/10000 [===========>..................] - ETA: 48:17 - loss: 0.6876 - regression_loss: 0.5523 - classification_loss: 0.1353
 4017/10000 [===========>..................] - ETA: 48:16 - loss: 0.6877 - regression_loss: 0.5523 - classification_loss: 0.1353
 4018/10000 [===========>..................] - ETA: 48:16 - loss: 0.6878 - regression_loss: 0.5525 - classification_loss: 0.1353
 4019/10000 [===========>..................] - ETA: 48:15 - loss: 0.6877 - regression_loss: 0.5524 - classification_loss: 0.1353
 4020/10000 [===========>..................] - ETA: 48:15 - loss: 0.6877 - regression_loss: 0.5524 - classification_loss: 0.1353
 4021/10000 [===========>..................] - ETA: 48:14 - loss: 0.6876 - regression_loss: 0.5523 - classification_loss: 0.1353
 4022/10000 [===========>..................] - ETA: 48:14 - loss: 0.6876 - regression_loss: 0.5523 - classification_loss: 0.1353
 4023/10000 [===========>..................] - ETA: 48:13 - loss: 0.6876 - regression_loss: 0.5523 - classification_loss: 0.1353
 4024/10000 [===========>..................] - ETA: 48:13 - loss: 0.6877 - regression_loss: 0.5524 - classification_loss: 0.1354
 4025/10000 [===========>..................] - ETA: 48:12 - loss: 0.6877 - regression_loss: 0.5523 - classification_loss: 0.1354
 4026/10000 [===========>..................] - ETA: 48:12 - loss: 0.6877 - regression_loss: 0.5523 - classification_loss: 0.1353
 4027/10000 [===========>..................] - ETA: 48:11 - loss: 0.6876 - regression_loss: 0.5523 - classification_loss: 0.1353
 4028/10000 [===========>..................] - ETA: 48:11 - loss: 0.6877 - regression_loss: 0.5524 - classification_loss: 0.1353
 4029/10000 [===========>..................] - ETA: 48:10 - loss: 0.6876 - regression_loss: 0.5523 - classification_loss: 0.1353
 4030/10000 [===========>..................] - ETA: 48:10 - loss: 0.6878 - regression_loss: 0.5525 - classification_loss: 0.1353
 4031/10000 [===========>..................] - ETA: 48:09 - loss: 0.6878 - regression_loss: 0.5524 - classification_loss: 0.1353
 4032/10000 [===========>..................] - ETA: 48:09 - loss: 0.6879 - regression_loss: 0.5525 - classification_loss: 0.1354
 4033/10000 [===========>..................] - ETA: 48:08 - loss: 0.6878 - regression_loss: 0.5525 - classification_loss: 0.1353
 4034/10000 [===========>..................] - ETA: 48:08 - loss: 0.6878 - regression_loss: 0.5524 - classification_loss: 0.1353
 4035/10000 [===========>..................] - ETA: 48:07 - loss: 0.6877 - regression_loss: 0.5524 - classification_loss: 0.1353
 4036/10000 [===========>..................] - ETA: 48:07 - loss: 0.6879 - regression_loss: 0.5525 - classification_loss: 0.1353
 4037/10000 [===========>..................] - ETA: 48:06 - loss: 0.6877 - regression_loss: 0.5524 - classification_loss: 0.1353
 4038/10000 [===========>..................] - ETA: 48:06 - loss: 0.6879 - regression_loss: 0.5526 - classification_loss: 0.1353
 4039/10000 [===========>..................] - ETA: 48:05 - loss: 0.6879 - regression_loss: 0.5526 - classification_loss: 0.1353
 4040/10000 [===========>..................] - ETA: 48:05 - loss: 0.6879 - regression_loss: 0.5526 - classification_loss: 0.1353
 4041/10000 [===========>..................] - ETA: 48:04 - loss: 0.6880 - regression_loss: 0.5527 - classification_loss: 0.1353
 4042/10000 [===========>..................] - ETA: 48:03 - loss: 0.6880 - regression_loss: 0.5527 - classification_loss: 0.1353
 4043/10000 [===========>..................] - ETA: 48:03 - loss: 0.6881 - regression_loss: 0.5528 - classification_loss: 0.1353
 4044/10000 [===========>..................] - ETA: 48:02 - loss: 0.6881 - regression_loss: 0.5528 - classification_loss: 0.1353
 4045/10000 [===========>..................] - ETA: 48:02 - loss: 0.6879 - regression_loss: 0.5527 - classification_loss: 0.1353
 4046/10000 [===========>..................] - ETA: 48:02 - loss: 0.6878 - regression_loss: 0.5526 - classification_loss: 0.1353
 4047/10000 [===========>..................] - ETA: 48:01 - loss: 0.6880 - regression_loss: 0.5527 - classification_loss: 0.1353
 4048/10000 [===========>..................] - ETA: 48:00 - loss: 0.6880 - regression_loss: 0.5527 - classification_loss: 0.1353
 4049/10000 [===========>..................] - ETA: 48:00 - loss: 0.6879 - regression_loss: 0.5526 - classification_loss: 0.1353
 4050/10000 [===========>..................] - ETA: 47:59 - loss: 0.6878 - regression_loss: 0.5526 - classification_loss: 0.1353
 4051/10000 [===========>..................] - ETA: 47:59 - loss: 0.6878 - regression_loss: 0.5526 - classification_loss: 0.1352
 4052/10000 [===========>..................] - ETA: 47:58 - loss: 0.6879 - regression_loss: 0.5526 - classification_loss: 0.1353
 4053/10000 [===========>..................] - ETA: 47:58 - loss: 0.6881 - regression_loss: 0.5527 - classification_loss: 0.1354
 4054/10000 [===========>..................] - ETA: 47:57 - loss: 0.6880 - regression_loss: 0.5526 - classification_loss: 0.1353
 4055/10000 [===========>..................] - ETA: 47:57 - loss: 0.6880 - regression_loss: 0.5526 - classification_loss: 0.1353
 4056/10000 [===========>..................] - ETA: 47:56 - loss: 0.6880 - regression_loss: 0.5526 - classification_loss: 0.1353
 4057/10000 [===========>..................] - ETA: 47:56 - loss: 0.6879 - regression_loss: 0.5525 - classification_loss: 0.1353
 4058/10000 [===========>..................] - ETA: 47:55 - loss: 0.6878 - regression_loss: 0.5525 - classification_loss: 0.1353
 4059/10000 [===========>..................] - ETA: 47:55 - loss: 0.6877 - regression_loss: 0.5524 - classification_loss: 0.1353
 4060/10000 [===========>..................] - ETA: 47:54 - loss: 0.6882 - regression_loss: 0.5527 - classification_loss: 0.1354
 4061/10000 [===========>..................] - ETA: 47:54 - loss: 0.6882 - regression_loss: 0.5527 - classification_loss: 0.1355
 4062/10000 [===========>..................] - ETA: 47:53 - loss: 0.6884 - regression_loss: 0.5529 - classification_loss: 0.1355
 4063/10000 [===========>..................] - ETA: 47:53 - loss: 0.6883 - regression_loss: 0.5529 - classification_loss: 0.1355
 4064/10000 [===========>..................] - ETA: 47:52 - loss: 0.6883 - regression_loss: 0.5528 - classification_loss: 0.1355
 4065/10000 [===========>..................] - ETA: 47:52 - loss: 0.6884 - regression_loss: 0.5529 - classification_loss: 0.1355
 4066/10000 [===========>..................] - ETA: 47:51 - loss: 0.6885 - regression_loss: 0.5530 - classification_loss: 0.1355
 4067/10000 [===========>..................] - ETA: 47:50 - loss: 0.6887 - regression_loss: 0.5532 - classification_loss: 0.1355
 4068/10000 [===========>..................] - ETA: 47:50 - loss: 0.6888 - regression_loss: 0.5532 - classification_loss: 0.1356
 4069/10000 [===========>..................] - ETA: 47:49 - loss: 0.6888 - regression_loss: 0.5532 - classification_loss: 0.1356
 4070/10000 [===========>..................] - ETA: 47:49 - loss: 0.6889 - regression_loss: 0.5533 - classification_loss: 0.1356
 4071/10000 [===========>..................] - ETA: 47:48 - loss: 0.6891 - regression_loss: 0.5535 - classification_loss: 0.1356
 4072/10000 [===========>..................] - ETA: 47:48 - loss: 0.6891 - regression_loss: 0.5535 - classification_loss: 0.1356
 4073/10000 [===========>..................] - ETA: 47:47 - loss: 0.6890 - regression_loss: 0.5535 - classification_loss: 0.1356
 4074/10000 [===========>..................] - ETA: 47:47 - loss: 0.6890 - regression_loss: 0.5534 - classification_loss: 0.1356
 4075/10000 [===========>..................] - ETA: 47:46 - loss: 0.6892 - regression_loss: 0.5536 - classification_loss: 0.1356
 4076/10000 [===========>..................] - ETA: 47:46 - loss: 0.6893 - regression_loss: 0.5537 - classification_loss: 0.1356
 4077/10000 [===========>..................] - ETA: 47:45 - loss: 0.6892 - regression_loss: 0.5536 - classification_loss: 0.1356
 4078/10000 [===========>..................] - ETA: 47:44 - loss: 0.6892 - regression_loss: 0.5536 - classification_loss: 0.1356
 4079/10000 [===========>..................] - ETA: 47:44 - loss: 0.6893 - regression_loss: 0.5537 - classification_loss: 0.1356
 4080/10000 [===========>..................] - ETA: 47:43 - loss: 0.6893 - regression_loss: 0.5537 - classification_loss: 0.1356
 4081/10000 [===========>..................] - ETA: 47:43 - loss: 0.6892 - regression_loss: 0.5536 - classification_loss: 0.1356
 4082/10000 [===========>..................] - ETA: 47:42 - loss: 0.6893 - regression_loss: 0.5537 - classification_loss: 0.1356
 4083/10000 [===========>..................] - ETA: 47:42 - loss: 0.6892 - regression_loss: 0.5536 - classification_loss: 0.1356
 4084/10000 [===========>..................] - ETA: 47:41 - loss: 0.6892 - regression_loss: 0.5536 - classification_loss: 0.1356
 4085/10000 [===========>..................] - ETA: 47:41 - loss: 0.6892 - regression_loss: 0.5535 - classification_loss: 0.1356
 4086/10000 [===========>..................] - ETA: 47:40 - loss: 0.6891 - regression_loss: 0.5534 - classification_loss: 0.1356
 4087/10000 [===========>..................] - ETA: 47:40 - loss: 0.6890 - regression_loss: 0.5534 - classification_loss: 0.1356
 4088/10000 [===========>..................] - ETA: 47:39 - loss: 0.6889 - regression_loss: 0.5532 - classification_loss: 0.1356
 4089/10000 [===========>..................] - ETA: 47:39 - loss: 0.6888 - regression_loss: 0.5532 - classification_loss: 0.1356
 4090/10000 [===========>..................] - ETA: 47:38 - loss: 0.6889 - regression_loss: 0.5533 - classification_loss: 0.1356
 4091/10000 [===========>..................] - ETA: 47:38 - loss: 0.6890 - regression_loss: 0.5534 - classification_loss: 0.1356
 4092/10000 [===========>..................] - ETA: 47:37 - loss: 0.6890 - regression_loss: 0.5534 - classification_loss: 0.1356
 4093/10000 [===========>..................] - ETA: 47:37 - loss: 0.6889 - regression_loss: 0.5533 - classification_loss: 0.1356
 4094/10000 [===========>..................] - ETA: 47:37 - loss: 0.6889 - regression_loss: 0.5534 - classification_loss: 0.1356
 4095/10000 [===========>..................] - ETA: 47:36 - loss: 0.6889 - regression_loss: 0.5533 - classification_loss: 0.1356
 4096/10000 [===========>..................] - ETA: 47:36 - loss: 0.6888 - regression_loss: 0.5533 - classification_loss: 0.1355
 4097/10000 [===========>..................] - ETA: 47:35 - loss: 0.6888 - regression_loss: 0.5532 - classification_loss: 0.1356
 4098/10000 [===========>..................] - ETA: 47:35 - loss: 0.6887 - regression_loss: 0.5532 - classification_loss: 0.1355
 4099/10000 [===========>..................] - ETA: 47:34 - loss: 0.6887 - regression_loss: 0.5532 - classification_loss: 0.1355
 4100/10000 [===========>..................] - ETA: 47:34 - loss: 0.6886 - regression_loss: 0.5531 - classification_loss: 0.1355
 4101/10000 [===========>..................] - ETA: 47:33 - loss: 0.6886 - regression_loss: 0.5531 - classification_loss: 0.1355
 4102/10000 [===========>..................] - ETA: 47:33 - loss: 0.6885 - regression_loss: 0.5531 - classification_loss: 0.1355
 4103/10000 [===========>..................] - ETA: 47:32 - loss: 0.6886 - regression_loss: 0.5531 - classification_loss: 0.1355
 4104/10000 [===========>..................] - ETA: 47:32 - loss: 0.6887 - regression_loss: 0.5532 - classification_loss: 0.1355
 4105/10000 [===========>..................] - ETA: 47:31 - loss: 0.6886 - regression_loss: 0.5531 - classification_loss: 0.1355
 4106/10000 [===========>..................] - ETA: 47:31 - loss: 0.6886 - regression_loss: 0.5531 - classification_loss: 0.1355
 4107/10000 [===========>..................] - ETA: 47:30 - loss: 0.6885 - regression_loss: 0.5530 - classification_loss: 0.1355
 4108/10000 [===========>..................] - ETA: 47:30 - loss: 0.6885 - regression_loss: 0.5531 - classification_loss: 0.1355
 4109/10000 [===========>..................] - ETA: 47:29 - loss: 0.6888 - regression_loss: 0.5533 - classification_loss: 0.1355
 4110/10000 [===========>..................] - ETA: 47:29 - loss: 0.6888 - regression_loss: 0.5532 - classification_loss: 0.1355
 4111/10000 [===========>..................] - ETA: 47:28 - loss: 0.6888 - regression_loss: 0.5533 - classification_loss: 0.1355
 4112/10000 [===========>..................] - ETA: 47:28 - loss: 0.6888 - regression_loss: 0.5532 - classification_loss: 0.1355
 4113/10000 [===========>..................] - ETA: 47:27 - loss: 0.6886 - regression_loss: 0.5531 - classification_loss: 0.1355
 4114/10000 [===========>..................] - ETA: 47:27 - loss: 0.6885 - regression_loss: 0.5531 - classification_loss: 0.1355
 4115/10000 [===========>..................] - ETA: 47:26 - loss: 0.6886 - regression_loss: 0.5531 - classification_loss: 0.1355
 4116/10000 [===========>..................] - ETA: 47:25 - loss: 0.6886 - regression_loss: 0.5531 - classification_loss: 0.1355
 4117/10000 [===========>..................] - ETA: 47:25 - loss: 0.6885 - regression_loss: 0.5530 - classification_loss: 0.1355
 4118/10000 [===========>..................] - ETA: 47:24 - loss: 0.6884 - regression_loss: 0.5530 - classification_loss: 0.1354
 4119/10000 [===========>..................] - ETA: 47:24 - loss: 0.6885 - regression_loss: 0.5530 - classification_loss: 0.1354
 4120/10000 [===========>..................] - ETA: 47:23 - loss: 0.6884 - regression_loss: 0.5530 - classification_loss: 0.1354
 4121/10000 [===========>..................] - ETA: 47:23 - loss: 0.6884 - regression_loss: 0.5530 - classification_loss: 0.1354
 4122/10000 [===========>..................] - ETA: 47:22 - loss: 0.6883 - regression_loss: 0.5529 - classification_loss: 0.1354
 4123/10000 [===========>..................] - ETA: 47:22 - loss: 0.6882 - regression_loss: 0.5528 - classification_loss: 0.1354
 4124/10000 [===========>..................] - ETA: 47:21 - loss: 0.6881 - regression_loss: 0.5527 - classification_loss: 0.1353
 4125/10000 [===========>..................] - ETA: 47:21 - loss: 0.6880 - regression_loss: 0.5527 - classification_loss: 0.1353
 4126/10000 [===========>..................] - ETA: 47:20 - loss: 0.6881 - regression_loss: 0.5527 - classification_loss: 0.1353
 4127/10000 [===========>..................] - ETA: 47:20 - loss: 0.6880 - regression_loss: 0.5527 - classification_loss: 0.1353
 4128/10000 [===========>..................] - ETA: 47:19 - loss: 0.6880 - regression_loss: 0.5527 - classification_loss: 0.1353
 4129/10000 [===========>..................] - ETA: 47:19 - loss: 0.6880 - regression_loss: 0.5527 - classification_loss: 0.1353
 4130/10000 [===========>..................] - ETA: 47:18 - loss: 0.6880 - regression_loss: 0.5527 - classification_loss: 0.1353
 4131/10000 [===========>..................] - ETA: 47:18 - loss: 0.6880 - regression_loss: 0.5528 - classification_loss: 0.1353
 4132/10000 [===========>..................] - ETA: 47:17 - loss: 0.6882 - regression_loss: 0.5529 - classification_loss: 0.1353
 4133/10000 [===========>..................] - ETA: 47:17 - loss: 0.6881 - regression_loss: 0.5528 - classification_loss: 0.1353
 4134/10000 [===========>..................] - ETA: 47:16 - loss: 0.6881 - regression_loss: 0.5527 - classification_loss: 0.1353
 4135/10000 [===========>..................] - ETA: 47:16 - loss: 0.6880 - regression_loss: 0.5527 - classification_loss: 0.1353
 4136/10000 [===========>..................] - ETA: 47:15 - loss: 0.6879 - regression_loss: 0.5526 - classification_loss: 0.1353
 4137/10000 [===========>..................] - ETA: 47:14 - loss: 0.6879 - regression_loss: 0.5526 - classification_loss: 0.1353
 4138/10000 [===========>..................] - ETA: 47:14 - loss: 0.6880 - regression_loss: 0.5527 - classification_loss: 0.1353
 4139/10000 [===========>..................] - ETA: 47:13 - loss: 0.6880 - regression_loss: 0.5527 - classification_loss: 0.1353
 4140/10000 [===========>..................] - ETA: 47:14 - loss: 0.6881 - regression_loss: 0.5528 - classification_loss: 0.1353
 4141/10000 [===========>..................] - ETA: 47:13 - loss: 0.6881 - regression_loss: 0.5528 - classification_loss: 0.1353
 4142/10000 [===========>..................] - ETA: 47:12 - loss: 0.6880 - regression_loss: 0.5527 - classification_loss: 0.1353
 4143/10000 [===========>..................] - ETA: 47:12 - loss: 0.6880 - regression_loss: 0.5527 - classification_loss: 0.1353
 4144/10000 [===========>..................] - ETA: 47:11 - loss: 0.6878 - regression_loss: 0.5526 - classification_loss: 0.1353
 4145/10000 [===========>..................] - ETA: 47:11 - loss: 0.6877 - regression_loss: 0.5525 - classification_loss: 0.1352
 4146/10000 [===========>..................] - ETA: 47:10 - loss: 0.6878 - regression_loss: 0.5526 - classification_loss: 0.1353
 4147/10000 [===========>..................] - ETA: 47:10 - loss: 0.6877 - regression_loss: 0.5525 - classification_loss: 0.1353
 4148/10000 [===========>..................] - ETA: 47:09 - loss: 0.6877 - regression_loss: 0.5525 - classification_loss: 0.1352
 4149/10000 [===========>..................] - ETA: 47:09 - loss: 0.6877 - regression_loss: 0.5525 - classification_loss: 0.1352
 4150/10000 [===========>..................] - ETA: 47:08 - loss: 0.6877 - regression_loss: 0.5525 - classification_loss: 0.1352
 4151/10000 [===========>..................] - ETA: 47:08 - loss: 0.6879 - regression_loss: 0.5526 - classification_loss: 0.1353
 4152/10000 [===========>..................] - ETA: 47:07 - loss: 0.6879 - regression_loss: 0.5526 - classification_loss: 0.1353
 4153/10000 [===========>..................] - ETA: 47:07 - loss: 0.6879 - regression_loss: 0.5525 - classification_loss: 0.1353
 4154/10000 [===========>..................] - ETA: 47:06 - loss: 0.6877 - regression_loss: 0.5524 - classification_loss: 0.1353
 4155/10000 [===========>..................] - ETA: 47:06 - loss: 0.6877 - regression_loss: 0.5524 - classification_loss: 0.1353
 4156/10000 [===========>..................] - ETA: 47:05 - loss: 0.6876 - regression_loss: 0.5523 - classification_loss: 0.1353
 4157/10000 [===========>..................] - ETA: 47:05 - loss: 0.6877 - regression_loss: 0.5524 - classification_loss: 0.1353
 4158/10000 [===========>..................] - ETA: 47:04 - loss: 0.6878 - regression_loss: 0.5525 - classification_loss: 0.1353
 4159/10000 [===========>..................] - ETA: 47:04 - loss: 0.6877 - regression_loss: 0.5524 - classification_loss: 0.1353
 4160/10000 [===========>..................] - ETA: 47:03 - loss: 0.6879 - regression_loss: 0.5526 - classification_loss: 0.1353
 4161/10000 [===========>..................] - ETA: 47:03 - loss: 0.6878 - regression_loss: 0.5525 - classification_loss: 0.1353
 4162/10000 [===========>..................] - ETA: 47:02 - loss: 0.6877 - regression_loss: 0.5524 - classification_loss: 0.1353
 4163/10000 [===========>..................] - ETA: 47:02 - loss: 0.6877 - regression_loss: 0.5524 - classification_loss: 0.1353
 4164/10000 [===========>..................] - ETA: 47:01 - loss: 0.6876 - regression_loss: 0.5523 - classification_loss: 0.1353
 4165/10000 [===========>..................] - ETA: 47:01 - loss: 0.6877 - regression_loss: 0.5524 - classification_loss: 0.1353
 4166/10000 [===========>..................] - ETA: 47:00 - loss: 0.6876 - regression_loss: 0.5523 - classification_loss: 0.1353
 4167/10000 [===========>..................] - ETA: 47:00 - loss: 0.6875 - regression_loss: 0.5522 - classification_loss: 0.1353
 4168/10000 [===========>..................] - ETA: 46:59 - loss: 0.6876 - regression_loss: 0.5523 - classification_loss: 0.1353
 4169/10000 [===========>..................] - ETA: 46:59 - loss: 0.6876 - regression_loss: 0.5523 - classification_loss: 0.1353
 4170/10000 [===========>..................] - ETA: 46:58 - loss: 0.6875 - regression_loss: 0.5522 - classification_loss: 0.1353
 4171/10000 [===========>..................] - ETA: 46:57 - loss: 0.6876 - regression_loss: 0.5523 - classification_loss: 0.1353
 4172/10000 [===========>..................] - ETA: 46:57 - loss: 0.6876 - regression_loss: 0.5522 - classification_loss: 0.1353
 4173/10000 [===========>..................] - ETA: 46:56 - loss: 0.6875 - regression_loss: 0.5522 - classification_loss: 0.1353
 4174/10000 [===========>..................] - ETA: 46:57 - loss: 0.6878 - regression_loss: 0.5524 - classification_loss: 0.1354
 4175/10000 [===========>..................] - ETA: 46:56 - loss: 0.6881 - regression_loss: 0.5526 - classification_loss: 0.1355
 4176/10000 [===========>..................] - ETA: 46:56 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1356
 4177/10000 [===========>..................] - ETA: 46:55 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1356
 4178/10000 [===========>..................] - ETA: 46:54 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1356
 4179/10000 [===========>..................] - ETA: 46:54 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1356
 4180/10000 [===========>..................] - ETA: 46:53 - loss: 0.6883 - regression_loss: 0.5528 - classification_loss: 0.1356
 4181/10000 [===========>..................] - ETA: 46:53 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1356
 4182/10000 [===========>..................] - ETA: 46:52 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1356
 4183/10000 [===========>..................] - ETA: 46:52 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1356
 4184/10000 [===========>..................] - ETA: 46:51 - loss: 0.6886 - regression_loss: 0.5530 - classification_loss: 0.1356
 4185/10000 [===========>..................] - ETA: 46:51 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1355
 4186/10000 [===========>..................] - ETA: 46:50 - loss: 0.6887 - regression_loss: 0.5531 - classification_loss: 0.1356
 4187/10000 [===========>..................] - ETA: 46:50 - loss: 0.6887 - regression_loss: 0.5531 - classification_loss: 0.1356
 4188/10000 [===========>..................] - ETA: 46:49 - loss: 0.6886 - regression_loss: 0.5530 - classification_loss: 0.1355
 4189/10000 [===========>..................] - ETA: 46:49 - loss: 0.6885 - regression_loss: 0.5530 - classification_loss: 0.1355
 4190/10000 [===========>..................] - ETA: 46:48 - loss: 0.6886 - regression_loss: 0.5530 - classification_loss: 0.1355
 4191/10000 [===========>..................] - ETA: 46:48 - loss: 0.6884 - regression_loss: 0.5529 - classification_loss: 0.1355
 4192/10000 [===========>..................] - ETA: 46:47 - loss: 0.6884 - regression_loss: 0.5529 - classification_loss: 0.1355
 4193/10000 [===========>..................] - ETA: 46:47 - loss: 0.6884 - regression_loss: 0.5529 - classification_loss: 0.1355
 4194/10000 [===========>..................] - ETA: 46:46 - loss: 0.6883 - regression_loss: 0.5529 - classification_loss: 0.1355
 4195/10000 [===========>..................] - ETA: 46:45 - loss: 0.6882 - regression_loss: 0.5528 - classification_loss: 0.1354
 4196/10000 [===========>..................] - ETA: 46:45 - loss: 0.6884 - regression_loss: 0.5529 - classification_loss: 0.1355
 4197/10000 [===========>..................] - ETA: 46:44 - loss: 0.6884 - regression_loss: 0.5529 - classification_loss: 0.1355
 4198/10000 [===========>..................] - ETA: 46:44 - loss: 0.6882 - regression_loss: 0.5528 - classification_loss: 0.1354
 4199/10000 [===========>..................] - ETA: 46:43 - loss: 0.6882 - regression_loss: 0.5528 - classification_loss: 0.1354
 4200/10000 [===========>..................] - ETA: 46:43 - loss: 0.6884 - regression_loss: 0.5529 - classification_loss: 0.1354
 4201/10000 [===========>..................] - ETA: 46:42 - loss: 0.6883 - regression_loss: 0.5529 - classification_loss: 0.1354
 4202/10000 [===========>..................] - ETA: 46:42 - loss: 0.6883 - regression_loss: 0.5528 - classification_loss: 0.1354
 4203/10000 [===========>..................] - ETA: 46:41 - loss: 0.6882 - regression_loss: 0.5528 - classification_loss: 0.1354
 4204/10000 [===========>..................] - ETA: 46:41 - loss: 0.6882 - regression_loss: 0.5528 - classification_loss: 0.1354
 4205/10000 [===========>..................] - ETA: 46:40 - loss: 0.6881 - regression_loss: 0.5527 - classification_loss: 0.1354
 4206/10000 [===========>..................] - ETA: 46:40 - loss: 0.6882 - regression_loss: 0.5528 - classification_loss: 0.1354
 4207/10000 [===========>..................] - ETA: 46:39 - loss: 0.6881 - regression_loss: 0.5527 - classification_loss: 0.1354
 4208/10000 [===========>..................] - ETA: 46:39 - loss: 0.6880 - regression_loss: 0.5527 - classification_loss: 0.1354
 4209/10000 [===========>..................] - ETA: 46:38 - loss: 0.6881 - regression_loss: 0.5527 - classification_loss: 0.1354
 4210/10000 [===========>..................] - ETA: 46:38 - loss: 0.6881 - regression_loss: 0.5527 - classification_loss: 0.1354
 4211/10000 [===========>..................] - ETA: 46:37 - loss: 0.6883 - regression_loss: 0.5528 - classification_loss: 0.1354
 4212/10000 [===========>..................] - ETA: 46:37 - loss: 0.6883 - regression_loss: 0.5529 - classification_loss: 0.1354
 4213/10000 [===========>..................] - ETA: 46:36 - loss: 0.6883 - regression_loss: 0.5529 - classification_loss: 0.1354
 4214/10000 [===========>..................] - ETA: 46:36 - loss: 0.6883 - regression_loss: 0.5529 - classification_loss: 0.1354
 4215/10000 [===========>..................] - ETA: 46:35 - loss: 0.6882 - regression_loss: 0.5528 - classification_loss: 0.1354
 4216/10000 [===========>..................] - ETA: 46:35 - loss: 0.6883 - regression_loss: 0.5529 - classification_loss: 0.1354
 4217/10000 [===========>..................] - ETA: 46:34 - loss: 0.6884 - regression_loss: 0.5530 - classification_loss: 0.1354
 4218/10000 [===========>..................] - ETA: 46:34 - loss: 0.6884 - regression_loss: 0.5530 - classification_loss: 0.1354
 4219/10000 [===========>..................] - ETA: 46:33 - loss: 0.6883 - regression_loss: 0.5529 - classification_loss: 0.1354
 4220/10000 [===========>..................] - ETA: 46:32 - loss: 0.6883 - regression_loss: 0.5529 - classification_loss: 0.1354
 4221/10000 [===========>..................] - ETA: 46:32 - loss: 0.6882 - regression_loss: 0.5528 - classification_loss: 0.1354
 4222/10000 [===========>..................] - ETA: 46:31 - loss: 0.6881 - regression_loss: 0.5527 - classification_loss: 0.1354
 4223/10000 [===========>..................] - ETA: 46:31 - loss: 0.6881 - regression_loss: 0.5527 - classification_loss: 0.1354
 4224/10000 [===========>..................] - ETA: 46:30 - loss: 0.6881 - regression_loss: 0.5527 - classification_loss: 0.1354
 4225/10000 [===========>..................] - ETA: 46:30 - loss: 0.6880 - regression_loss: 0.5527 - classification_loss: 0.1354
 4226/10000 [===========>..................] - ETA: 46:29 - loss: 0.6881 - regression_loss: 0.5527 - classification_loss: 0.1354
 4227/10000 [===========>..................] - ETA: 46:29 - loss: 0.6881 - regression_loss: 0.5527 - classification_loss: 0.1354
 4228/10000 [===========>..................] - ETA: 46:28 - loss: 0.6879 - regression_loss: 0.5526 - classification_loss: 0.1353
 4229/10000 [===========>..................] - ETA: 46:28 - loss: 0.6879 - regression_loss: 0.5525 - classification_loss: 0.1353
 4230/10000 [===========>..................] - ETA: 46:27 - loss: 0.6879 - regression_loss: 0.5525 - classification_loss: 0.1354
 4231/10000 [===========>..................] - ETA: 46:27 - loss: 0.6879 - regression_loss: 0.5525 - classification_loss: 0.1354
 4232/10000 [===========>..................] - ETA: 46:26 - loss: 0.6880 - regression_loss: 0.5526 - classification_loss: 0.1354
 4233/10000 [===========>..................] - ETA: 46:26 - loss: 0.6879 - regression_loss: 0.5525 - classification_loss: 0.1354
 4234/10000 [===========>..................] - ETA: 46:25 - loss: 0.6878 - regression_loss: 0.5525 - classification_loss: 0.1354
 4235/10000 [===========>..................] - ETA: 46:25 - loss: 0.6878 - regression_loss: 0.5524 - classification_loss: 0.1354
 4236/10000 [===========>..................] - ETA: 46:24 - loss: 0.6877 - regression_loss: 0.5523 - classification_loss: 0.1354
 4237/10000 [===========>..................] - ETA: 46:24 - loss: 0.6878 - regression_loss: 0.5524 - classification_loss: 0.1355
 4238/10000 [===========>..................] - ETA: 46:23 - loss: 0.6879 - regression_loss: 0.5524 - classification_loss: 0.1355
 4239/10000 [===========>..................] - ETA: 46:23 - loss: 0.6880 - regression_loss: 0.5525 - classification_loss: 0.1355
 4240/10000 [===========>..................] - ETA: 46:22 - loss: 0.6881 - regression_loss: 0.5526 - classification_loss: 0.1355
 4241/10000 [===========>..................] - ETA: 46:21 - loss: 0.6881 - regression_loss: 0.5526 - classification_loss: 0.1355
 4242/10000 [===========>..................] - ETA: 46:21 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1355
 4243/10000 [===========>..................] - ETA: 46:20 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1356
 4244/10000 [===========>..................] - ETA: 46:20 - loss: 0.6881 - regression_loss: 0.5526 - classification_loss: 0.1356
 4245/10000 [===========>..................] - ETA: 46:19 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1356
 4246/10000 [===========>..................] - ETA: 46:19 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 4247/10000 [===========>..................] - ETA: 46:18 - loss: 0.6883 - regression_loss: 0.5528 - classification_loss: 0.1356
 4248/10000 [===========>..................] - ETA: 46:18 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 4249/10000 [===========>..................] - ETA: 46:17 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1356
 4250/10000 [===========>..................] - ETA: 46:17 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1356
 4251/10000 [===========>..................] - ETA: 46:16 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1356
 4252/10000 [===========>..................] - ETA: 46:16 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1356
 4253/10000 [===========>..................] - ETA: 46:15 - loss: 0.6883 - regression_loss: 0.5528 - classification_loss: 0.1355
 4254/10000 [===========>..................] - ETA: 46:15 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1355
 4255/10000 [===========>..................] - ETA: 46:14 - loss: 0.6883 - regression_loss: 0.5528 - classification_loss: 0.1355
 4256/10000 [===========>..................] - ETA: 46:14 - loss: 0.6882 - regression_loss: 0.5527 - classification_loss: 0.1355
 4257/10000 [===========>..................] - ETA: 46:13 - loss: 0.6882 - regression_loss: 0.5527 - classification_loss: 0.1355
 4258/10000 [===========>..................] - ETA: 46:13 - loss: 0.6882 - regression_loss: 0.5527 - classification_loss: 0.1355
 4259/10000 [===========>..................] - ETA: 46:12 - loss: 0.6884 - regression_loss: 0.5529 - classification_loss: 0.1355
 4260/10000 [===========>..................] - ETA: 46:12 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1355
 4261/10000 [===========>..................] - ETA: 46:11 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1355
 4262/10000 [===========>..................] - ETA: 46:11 - loss: 0.6884 - regression_loss: 0.5529 - classification_loss: 0.1355
 4263/10000 [===========>..................] - ETA: 46:10 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1355
 4264/10000 [===========>..................] - ETA: 46:10 - loss: 0.6883 - regression_loss: 0.5528 - classification_loss: 0.1355
 4265/10000 [===========>..................] - ETA: 46:09 - loss: 0.6883 - regression_loss: 0.5528 - classification_loss: 0.1355
 4266/10000 [===========>..................] - ETA: 46:09 - loss: 0.6882 - regression_loss: 0.5527 - classification_loss: 0.1355
 4267/10000 [===========>..................] - ETA: 46:08 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 4268/10000 [===========>..................] - ETA: 46:08 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 4269/10000 [===========>..................] - ETA: 46:07 - loss: 0.6883 - regression_loss: 0.5528 - classification_loss: 0.1356
 4270/10000 [===========>..................] - ETA: 46:07 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1356
 4271/10000 [===========>..................] - ETA: 46:06 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 4272/10000 [===========>..................] - ETA: 46:06 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 4273/10000 [===========>..................] - ETA: 46:05 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 4274/10000 [===========>..................] - ETA: 46:05 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 4275/10000 [===========>..................] - ETA: 46:04 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1356
 4276/10000 [===========>..................] - ETA: 46:04 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4277/10000 [===========>..................] - ETA: 46:03 - loss: 0.6881 - regression_loss: 0.5526 - classification_loss: 0.1356
 4278/10000 [===========>..................] - ETA: 46:03 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4279/10000 [===========>..................] - ETA: 46:02 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1355
 4280/10000 [===========>..................] - ETA: 46:02 - loss: 0.6879 - regression_loss: 0.5524 - classification_loss: 0.1355
 4281/10000 [===========>..................] - ETA: 46:01 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4282/10000 [===========>..................] - ETA: 46:01 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4283/10000 [===========>..................] - ETA: 46:00 - loss: 0.6882 - regression_loss: 0.5525 - classification_loss: 0.1357
 4284/10000 [===========>..................] - ETA: 46:00 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4285/10000 [===========>..................] - ETA: 45:59 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1357
 4286/10000 [===========>..................] - ETA: 45:58 - loss: 0.6884 - regression_loss: 0.5527 - classification_loss: 0.1357
 4287/10000 [===========>..................] - ETA: 45:58 - loss: 0.6883 - regression_loss: 0.5526 - classification_loss: 0.1356
 4288/10000 [===========>..................] - ETA: 45:57 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1356
 4289/10000 [===========>..................] - ETA: 45:57 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1356
 4290/10000 [===========>..................] - ETA: 45:56 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4291/10000 [===========>..................] - ETA: 45:56 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4292/10000 [===========>..................] - ETA: 45:55 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4293/10000 [===========>..................] - ETA: 45:55 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4294/10000 [===========>..................] - ETA: 45:54 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1356
 4295/10000 [===========>..................] - ETA: 45:54 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1356
 4296/10000 [===========>..................] - ETA: 45:53 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4297/10000 [===========>..................] - ETA: 45:53 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1356
 4298/10000 [===========>..................] - ETA: 45:52 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1356
 4299/10000 [===========>..................] - ETA: 45:52 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1356
 4300/10000 [===========>..................] - ETA: 45:51 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1356
 4301/10000 [===========>..................] - ETA: 45:51 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1356
 4302/10000 [===========>..................] - ETA: 45:50 - loss: 0.6879 - regression_loss: 0.5523 - classification_loss: 0.1355
 4303/10000 [===========>..................] - ETA: 45:50 - loss: 0.6878 - regression_loss: 0.5523 - classification_loss: 0.1355
 4304/10000 [===========>..................] - ETA: 45:49 - loss: 0.6879 - regression_loss: 0.5524 - classification_loss: 0.1355
 4305/10000 [===========>..................] - ETA: 45:49 - loss: 0.6879 - regression_loss: 0.5524 - classification_loss: 0.1355
 4306/10000 [===========>..................] - ETA: 45:48 - loss: 0.6878 - regression_loss: 0.5523 - classification_loss: 0.1355
 4307/10000 [===========>..................] - ETA: 45:48 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1355
 4308/10000 [===========>..................] - ETA: 45:47 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1355
 4309/10000 [===========>..................] - ETA: 45:47 - loss: 0.6879 - regression_loss: 0.5524 - classification_loss: 0.1355
 4310/10000 [===========>..................] - ETA: 45:46 - loss: 0.6879 - regression_loss: 0.5524 - classification_loss: 0.1355
 4311/10000 [===========>..................] - ETA: 45:46 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1356
 4312/10000 [===========>..................] - ETA: 45:45 - loss: 0.6879 - regression_loss: 0.5524 - classification_loss: 0.1355
 4313/10000 [===========>..................] - ETA: 45:45 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1355
 4314/10000 [===========>..................] - ETA: 45:44 - loss: 0.6880 - regression_loss: 0.5525 - classification_loss: 0.1355
 4315/10000 [===========>..................] - ETA: 45:43 - loss: 0.6879 - regression_loss: 0.5524 - classification_loss: 0.1355
 4316/10000 [===========>..................] - ETA: 45:43 - loss: 0.6878 - regression_loss: 0.5523 - classification_loss: 0.1355
 4317/10000 [===========>..................] - ETA: 45:42 - loss: 0.6878 - regression_loss: 0.5523 - classification_loss: 0.1355
 4318/10000 [===========>..................] - ETA: 45:42 - loss: 0.6878 - regression_loss: 0.5523 - classification_loss: 0.1355
 4319/10000 [===========>..................] - ETA: 45:41 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1355
 4320/10000 [===========>..................] - ETA: 45:41 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1355
 4321/10000 [===========>..................] - ETA: 45:40 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1354
 4322/10000 [===========>..................] - ETA: 45:40 - loss: 0.6874 - regression_loss: 0.5520 - classification_loss: 0.1354
 4323/10000 [===========>..................] - ETA: 45:39 - loss: 0.6873 - regression_loss: 0.5519 - classification_loss: 0.1354
 4324/10000 [===========>..................] - ETA: 45:39 - loss: 0.6873 - regression_loss: 0.5519 - classification_loss: 0.1354
 4325/10000 [===========>..................] - ETA: 45:38 - loss: 0.6874 - regression_loss: 0.5520 - classification_loss: 0.1354
 4326/10000 [===========>..................] - ETA: 45:38 - loss: 0.6876 - regression_loss: 0.5522 - classification_loss: 0.1354
 4327/10000 [===========>..................] - ETA: 45:37 - loss: 0.6877 - regression_loss: 0.5523 - classification_loss: 0.1354
 4328/10000 [===========>..................] - ETA: 45:37 - loss: 0.6877 - regression_loss: 0.5523 - classification_loss: 0.1354
 4329/10000 [===========>..................] - ETA: 45:36 - loss: 0.6878 - regression_loss: 0.5524 - classification_loss: 0.1354
 4330/10000 [===========>..................] - ETA: 45:36 - loss: 0.6878 - regression_loss: 0.5524 - classification_loss: 0.1354
 4331/10000 [===========>..................] - ETA: 45:35 - loss: 0.6879 - regression_loss: 0.5524 - classification_loss: 0.1355
 4332/10000 [===========>..................] - ETA: 45:35 - loss: 0.6879 - regression_loss: 0.5524 - classification_loss: 0.1355
 4333/10000 [===========>..................] - ETA: 45:35 - loss: 0.6880 - regression_loss: 0.5525 - classification_loss: 0.1355
 4334/10000 [============>.................] - ETA: 45:34 - loss: 0.6880 - regression_loss: 0.5525 - classification_loss: 0.1355
 4335/10000 [============>.................] - ETA: 45:34 - loss: 0.6879 - regression_loss: 0.5524 - classification_loss: 0.1355
 4336/10000 [============>.................] - ETA: 45:33 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1356
 4337/10000 [============>.................] - ETA: 45:32 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1356
 4338/10000 [============>.................] - ETA: 45:32 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4339/10000 [============>.................] - ETA: 45:31 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4340/10000 [============>.................] - ETA: 45:31 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4341/10000 [============>.................] - ETA: 45:30 - loss: 0.6883 - regression_loss: 0.5526 - classification_loss: 0.1356
 4342/10000 [============>.................] - ETA: 45:30 - loss: 0.6884 - regression_loss: 0.5527 - classification_loss: 0.1356
 4343/10000 [============>.................] - ETA: 45:29 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 4344/10000 [============>.................] - ETA: 45:29 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1356
 4345/10000 [============>.................] - ETA: 45:29 - loss: 0.6883 - regression_loss: 0.5526 - classification_loss: 0.1356
 4346/10000 [============>.................] - ETA: 45:28 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1356
 4347/10000 [============>.................] - ETA: 45:28 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 4348/10000 [============>.................] - ETA: 45:27 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 4349/10000 [============>.................] - ETA: 45:27 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 4350/10000 [============>.................] - ETA: 45:26 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1356
 4351/10000 [============>.................] - ETA: 45:26 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 4352/10000 [============>.................] - ETA: 45:25 - loss: 0.6883 - regression_loss: 0.5526 - classification_loss: 0.1356
 4353/10000 [============>.................] - ETA: 45:25 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1356
 4354/10000 [============>.................] - ETA: 45:24 - loss: 0.6881 - regression_loss: 0.5526 - classification_loss: 0.1356
 4355/10000 [============>.................] - ETA: 45:24 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4356/10000 [============>.................] - ETA: 45:23 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1356
 4357/10000 [============>.................] - ETA: 45:23 - loss: 0.6879 - regression_loss: 0.5524 - classification_loss: 0.1356
 4358/10000 [============>.................] - ETA: 45:22 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1356
 4359/10000 [============>.................] - ETA: 45:22 - loss: 0.6879 - regression_loss: 0.5524 - classification_loss: 0.1356
 4360/10000 [============>.................] - ETA: 45:21 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1356
 4361/10000 [============>.................] - ETA: 45:21 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1355
 4362/10000 [============>.................] - ETA: 45:20 - loss: 0.6877 - regression_loss: 0.5521 - classification_loss: 0.1356
 4363/10000 [============>.................] - ETA: 45:19 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1355
 4364/10000 [============>.................] - ETA: 45:19 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 4365/10000 [============>.................] - ETA: 45:19 - loss: 0.6875 - regression_loss: 0.5519 - classification_loss: 0.1355
 4366/10000 [============>.................] - ETA: 45:18 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1355
 4367/10000 [============>.................] - ETA: 45:17 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1355
 4368/10000 [============>.................] - ETA: 45:17 - loss: 0.6876 - regression_loss: 0.5520 - classification_loss: 0.1355
 4369/10000 [============>.................] - ETA: 45:16 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1355
 4370/10000 [============>.................] - ETA: 45:16 - loss: 0.6876 - regression_loss: 0.5520 - classification_loss: 0.1356
 4371/10000 [============>.................] - ETA: 45:15 - loss: 0.6875 - regression_loss: 0.5519 - classification_loss: 0.1356
 4372/10000 [============>.................] - ETA: 45:15 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 4373/10000 [============>.................] - ETA: 45:14 - loss: 0.6875 - regression_loss: 0.5519 - classification_loss: 0.1355
 4374/10000 [============>.................] - ETA: 45:14 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 4375/10000 [============>.................] - ETA: 45:13 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1356
 4376/10000 [============>.................] - ETA: 45:13 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1356
 4377/10000 [============>.................] - ETA: 45:12 - loss: 0.6877 - regression_loss: 0.5521 - classification_loss: 0.1356
 4378/10000 [============>.................] - ETA: 45:12 - loss: 0.6877 - regression_loss: 0.5521 - classification_loss: 0.1355
 4379/10000 [============>.................] - ETA: 45:11 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1355
 4380/10000 [============>.................] - ETA: 45:11 - loss: 0.6876 - regression_loss: 0.5520 - classification_loss: 0.1355
 4381/10000 [============>.................] - ETA: 45:10 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1355
 4382/10000 [============>.................] - ETA: 45:10 - loss: 0.6877 - regression_loss: 0.5521 - classification_loss: 0.1356
 4383/10000 [============>.................] - ETA: 45:09 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1355
 4384/10000 [============>.................] - ETA: 45:09 - loss: 0.6877 - regression_loss: 0.5521 - classification_loss: 0.1356
 4385/10000 [============>.................] - ETA: 45:08 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1356
 4386/10000 [============>.................] - ETA: 45:08 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1356
 4387/10000 [============>.................] - ETA: 45:07 - loss: 0.6877 - regression_loss: 0.5521 - classification_loss: 0.1356
 4388/10000 [============>.................] - ETA: 45:07 - loss: 0.6877 - regression_loss: 0.5521 - classification_loss: 0.1355
 4389/10000 [============>.................] - ETA: 45:06 - loss: 0.6879 - regression_loss: 0.5523 - classification_loss: 0.1356
 4390/10000 [============>.................] - ETA: 45:06 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1356
 4391/10000 [============>.................] - ETA: 45:05 - loss: 0.6879 - regression_loss: 0.5523 - classification_loss: 0.1356
 4392/10000 [============>.................] - ETA: 45:05 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1356
 4393/10000 [============>.................] - ETA: 45:04 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1356
 4394/10000 [============>.................] - ETA: 45:04 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1356
 4395/10000 [============>.................] - ETA: 45:03 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1356
 4396/10000 [============>.................] - ETA: 45:03 - loss: 0.6879 - regression_loss: 0.5522 - classification_loss: 0.1356
 4397/10000 [============>.................] - ETA: 45:02 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1356
 4398/10000 [============>.................] - ETA: 45:02 - loss: 0.6879 - regression_loss: 0.5522 - classification_loss: 0.1356
 4399/10000 [============>.................] - ETA: 45:01 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1356
 4400/10000 [============>.................] - ETA: 45:01 - loss: 0.6877 - regression_loss: 0.5521 - classification_loss: 0.1356
 4401/10000 [============>.................] - ETA: 45:00 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1356
 4402/10000 [============>.................] - ETA: 45:00 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1356
 4403/10000 [============>.................] - ETA: 44:59 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1356
 4404/10000 [============>.................] - ETA: 44:59 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1355
 4405/10000 [============>.................] - ETA: 44:58 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1356
 4406/10000 [============>.................] - ETA: 44:57 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 4407/10000 [============>.................] - ETA: 44:57 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 4408/10000 [============>.................] - ETA: 44:57 - loss: 0.6876 - regression_loss: 0.5520 - classification_loss: 0.1355
 4409/10000 [============>.................] - ETA: 44:56 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 4410/10000 [============>.................] - ETA: 44:56 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1355
 4411/10000 [============>.................] - ETA: 44:55 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1355
 4412/10000 [============>.................] - ETA: 44:55 - loss: 0.6879 - regression_loss: 0.5523 - classification_loss: 0.1356
 4413/10000 [============>.................] - ETA: 44:54 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1356
 4414/10000 [============>.................] - ETA: 44:54 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1355
 4415/10000 [============>.................] - ETA: 44:53 - loss: 0.6878 - regression_loss: 0.5523 - classification_loss: 0.1355
 4416/10000 [============>.................] - ETA: 44:52 - loss: 0.6878 - regression_loss: 0.5523 - classification_loss: 0.1355
 4417/10000 [============>.................] - ETA: 44:52 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1355
 4418/10000 [============>.................] - ETA: 44:51 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1355
 4419/10000 [============>.................] - ETA: 44:51 - loss: 0.6878 - regression_loss: 0.5523 - classification_loss: 0.1355
 4420/10000 [============>.................] - ETA: 44:50 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1355
 4421/10000 [============>.................] - ETA: 44:50 - loss: 0.6880 - regression_loss: 0.5525 - classification_loss: 0.1355
 4422/10000 [============>.................] - ETA: 44:49 - loss: 0.6879 - regression_loss: 0.5524 - classification_loss: 0.1355
 4423/10000 [============>.................] - ETA: 44:49 - loss: 0.6878 - regression_loss: 0.5523 - classification_loss: 0.1355
 4424/10000 [============>.................] - ETA: 44:48 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1355
 4425/10000 [============>.................] - ETA: 44:48 - loss: 0.6876 - regression_loss: 0.5522 - classification_loss: 0.1354
 4426/10000 [============>.................] - ETA: 44:47 - loss: 0.6877 - regression_loss: 0.5523 - classification_loss: 0.1355
 4427/10000 [============>.................] - ETA: 44:47 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1355
 4428/10000 [============>.................] - ETA: 44:46 - loss: 0.6878 - regression_loss: 0.5523 - classification_loss: 0.1355
 4429/10000 [============>.................] - ETA: 44:46 - loss: 0.6878 - regression_loss: 0.5523 - classification_loss: 0.1355
 4430/10000 [============>.................] - ETA: 44:45 - loss: 0.6878 - regression_loss: 0.5523 - classification_loss: 0.1355
 4431/10000 [============>.................] - ETA: 44:45 - loss: 0.6878 - regression_loss: 0.5523 - classification_loss: 0.1355
 4432/10000 [============>.................] - ETA: 44:44 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1355
 4433/10000 [============>.................] - ETA: 44:44 - loss: 0.6876 - regression_loss: 0.5522 - classification_loss: 0.1355
 4434/10000 [============>.................] - ETA: 44:43 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1355
 4435/10000 [============>.................] - ETA: 44:43 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1355
 4436/10000 [============>.................] - ETA: 44:42 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1355
 4437/10000 [============>.................] - ETA: 44:42 - loss: 0.6879 - regression_loss: 0.5523 - classification_loss: 0.1355
 4438/10000 [============>.................] - ETA: 44:41 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4439/10000 [============>.................] - ETA: 44:41 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4440/10000 [============>.................] - ETA: 44:40 - loss: 0.6880 - regression_loss: 0.5525 - classification_loss: 0.1356
 4441/10000 [============>.................] - ETA: 44:40 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1356
 4442/10000 [============>.................] - ETA: 44:39 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1356
 4443/10000 [============>.................] - ETA: 44:39 - loss: 0.6879 - regression_loss: 0.5523 - classification_loss: 0.1356
 4444/10000 [============>.................] - ETA: 44:38 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1355
 4445/10000 [============>.................] - ETA: 44:38 - loss: 0.6877 - regression_loss: 0.5521 - classification_loss: 0.1356
 4446/10000 [============>.................] - ETA: 44:37 - loss: 0.6877 - regression_loss: 0.5521 - classification_loss: 0.1355
 4447/10000 [============>.................] - ETA: 44:37 - loss: 0.6879 - regression_loss: 0.5523 - classification_loss: 0.1356
 4448/10000 [============>.................] - ETA: 44:36 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1356
 4449/10000 [============>.................] - ETA: 44:36 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1356
 4450/10000 [============>.................] - ETA: 44:35 - loss: 0.6877 - regression_loss: 0.5521 - classification_loss: 0.1356
 4451/10000 [============>.................] - ETA: 44:35 - loss: 0.6877 - regression_loss: 0.5521 - classification_loss: 0.1356
 4452/10000 [============>.................] - ETA: 44:34 - loss: 0.6877 - regression_loss: 0.5521 - classification_loss: 0.1355
 4453/10000 [============>.................] - ETA: 44:34 - loss: 0.6876 - regression_loss: 0.5520 - classification_loss: 0.1355
 4454/10000 [============>.................] - ETA: 44:33 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 4455/10000 [============>.................] - ETA: 44:32 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 4456/10000 [============>.................] - ETA: 44:32 - loss: 0.6873 - regression_loss: 0.5518 - classification_loss: 0.1355
 4457/10000 [============>.................] - ETA: 44:32 - loss: 0.6873 - regression_loss: 0.5518 - classification_loss: 0.1355
 4458/10000 [============>.................] - ETA: 44:31 - loss: 0.6873 - regression_loss: 0.5518 - classification_loss: 0.1355
 4459/10000 [============>.................] - ETA: 44:31 - loss: 0.6872 - regression_loss: 0.5517 - classification_loss: 0.1355
 4460/10000 [============>.................] - ETA: 44:30 - loss: 0.6873 - regression_loss: 0.5518 - classification_loss: 0.1355
 4461/10000 [============>.................] - ETA: 44:30 - loss: 0.6873 - regression_loss: 0.5518 - classification_loss: 0.1355
 4462/10000 [============>.................] - ETA: 44:29 - loss: 0.6872 - regression_loss: 0.5517 - classification_loss: 0.1355
 4463/10000 [============>.................] - ETA: 44:29 - loss: 0.6872 - regression_loss: 0.5517 - classification_loss: 0.1355
 4464/10000 [============>.................] - ETA: 44:28 - loss: 0.6871 - regression_loss: 0.5516 - classification_loss: 0.1355
 4465/10000 [============>.................] - ETA: 44:28 - loss: 0.6873 - regression_loss: 0.5518 - classification_loss: 0.1355
 4466/10000 [============>.................] - ETA: 44:27 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 4467/10000 [============>.................] - ETA: 44:27 - loss: 0.6873 - regression_loss: 0.5519 - classification_loss: 0.1355
 4468/10000 [============>.................] - ETA: 44:26 - loss: 0.6873 - regression_loss: 0.5519 - classification_loss: 0.1355
 4469/10000 [============>.................] - ETA: 44:26 - loss: 0.6872 - regression_loss: 0.5518 - classification_loss: 0.1355
 4470/10000 [============>.................] - ETA: 44:25 - loss: 0.6873 - regression_loss: 0.5518 - classification_loss: 0.1355
 4471/10000 [============>.................] - ETA: 44:25 - loss: 0.6873 - regression_loss: 0.5518 - classification_loss: 0.1355
 4472/10000 [============>.................] - ETA: 44:24 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 4473/10000 [============>.................] - ETA: 44:24 - loss: 0.6873 - regression_loss: 0.5518 - classification_loss: 0.1355
 4474/10000 [============>.................] - ETA: 44:23 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 4475/10000 [============>.................] - ETA: 44:23 - loss: 0.6873 - regression_loss: 0.5519 - classification_loss: 0.1354
 4476/10000 [============>.................] - ETA: 44:22 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1354
 4477/10000 [============>.................] - ETA: 44:22 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1354
 4478/10000 [============>.................] - ETA: 44:21 - loss: 0.6873 - regression_loss: 0.5519 - classification_loss: 0.1354
 4479/10000 [============>.................] - ETA: 44:21 - loss: 0.6872 - regression_loss: 0.5518 - classification_loss: 0.1354
 4480/10000 [============>.................] - ETA: 44:20 - loss: 0.6872 - regression_loss: 0.5518 - classification_loss: 0.1354
 4481/10000 [============>.................] - ETA: 44:20 - loss: 0.6872 - regression_loss: 0.5518 - classification_loss: 0.1354
 4482/10000 [============>.................] - ETA: 44:19 - loss: 0.6872 - regression_loss: 0.5518 - classification_loss: 0.1354
 4483/10000 [============>.................] - ETA: 44:19 - loss: 0.6871 - regression_loss: 0.5517 - classification_loss: 0.1354
 4484/10000 [============>.................] - ETA: 44:18 - loss: 0.6870 - regression_loss: 0.5516 - classification_loss: 0.1354
 4485/10000 [============>.................] - ETA: 44:18 - loss: 0.6870 - regression_loss: 0.5516 - classification_loss: 0.1354
 4486/10000 [============>.................] - ETA: 44:17 - loss: 0.6871 - regression_loss: 0.5517 - classification_loss: 0.1354
 4487/10000 [============>.................] - ETA: 44:16 - loss: 0.6871 - regression_loss: 0.5517 - classification_loss: 0.1354
 4488/10000 [============>.................] - ETA: 44:16 - loss: 0.6870 - regression_loss: 0.5516 - classification_loss: 0.1354
 4489/10000 [============>.................] - ETA: 44:15 - loss: 0.6871 - regression_loss: 0.5517 - classification_loss: 0.1354
 4490/10000 [============>.................] - ETA: 44:15 - loss: 0.6872 - regression_loss: 0.5519 - classification_loss: 0.1354
 4491/10000 [============>.................] - ETA: 44:14 - loss: 0.6871 - regression_loss: 0.5518 - classification_loss: 0.1354
 4492/10000 [============>.................] - ETA: 44:14 - loss: 0.6871 - regression_loss: 0.5518 - classification_loss: 0.1353
 4493/10000 [============>.................] - ETA: 44:13 - loss: 0.6871 - regression_loss: 0.5517 - classification_loss: 0.1353
 4494/10000 [============>.................] - ETA: 44:13 - loss: 0.6871 - regression_loss: 0.5517 - classification_loss: 0.1354
 4495/10000 [============>.................] - ETA: 44:12 - loss: 0.6871 - regression_loss: 0.5518 - classification_loss: 0.1354
 4496/10000 [============>.................] - ETA: 44:12 - loss: 0.6873 - regression_loss: 0.5519 - classification_loss: 0.1354
 4497/10000 [============>.................] - ETA: 44:11 - loss: 0.6872 - regression_loss: 0.5519 - classification_loss: 0.1354
 4498/10000 [============>.................] - ETA: 44:11 - loss: 0.6872 - regression_loss: 0.5518 - classification_loss: 0.1353
 4499/10000 [============>.................] - ETA: 44:10 - loss: 0.6873 - regression_loss: 0.5519 - classification_loss: 0.1354
 4500/10000 [============>.................] - ETA: 44:10 - loss: 0.6873 - regression_loss: 0.5520 - classification_loss: 0.1354
 4501/10000 [============>.................] - ETA: 44:09 - loss: 0.6872 - regression_loss: 0.5519 - classification_loss: 0.1353
 4502/10000 [============>.................] - ETA: 44:09 - loss: 0.6873 - regression_loss: 0.5519 - classification_loss: 0.1353
 4503/10000 [============>.................] - ETA: 44:09 - loss: 0.6874 - regression_loss: 0.5521 - classification_loss: 0.1354
 4504/10000 [============>.................] - ETA: 44:08 - loss: 0.6876 - regression_loss: 0.5522 - classification_loss: 0.1354
 4505/10000 [============>.................] - ETA: 44:08 - loss: 0.6875 - regression_loss: 0.5521 - classification_loss: 0.1354
 4506/10000 [============>.................] - ETA: 44:07 - loss: 0.6877 - regression_loss: 0.5523 - classification_loss: 0.1354
 4507/10000 [============>.................] - ETA: 44:07 - loss: 0.6879 - regression_loss: 0.5525 - classification_loss: 0.1355
 4508/10000 [============>.................] - ETA: 44:06 - loss: 0.6881 - regression_loss: 0.5526 - classification_loss: 0.1355
 4509/10000 [============>.................] - ETA: 44:06 - loss: 0.6880 - regression_loss: 0.5526 - classification_loss: 0.1355
 4510/10000 [============>.................] - ETA: 44:05 - loss: 0.6879 - regression_loss: 0.5525 - classification_loss: 0.1354
 4511/10000 [============>.................] - ETA: 44:05 - loss: 0.6878 - regression_loss: 0.5524 - classification_loss: 0.1354
 4512/10000 [============>.................] - ETA: 44:04 - loss: 0.6879 - regression_loss: 0.5525 - classification_loss: 0.1354
 4513/10000 [============>.................] - ETA: 44:04 - loss: 0.6878 - regression_loss: 0.5524 - classification_loss: 0.1354
 4514/10000 [============>.................] - ETA: 44:03 - loss: 0.6878 - regression_loss: 0.5523 - classification_loss: 0.1355
 4515/10000 [============>.................] - ETA: 44:03 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1354
 4516/10000 [============>.................] - ETA: 44:02 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1354
 4517/10000 [============>.................] - ETA: 44:02 - loss: 0.6876 - regression_loss: 0.5522 - classification_loss: 0.1354
 4518/10000 [============>.................] - ETA: 44:01 - loss: 0.6877 - regression_loss: 0.5523 - classification_loss: 0.1354
 4519/10000 [============>.................] - ETA: 44:01 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1354
 4520/10000 [============>.................] - ETA: 44:00 - loss: 0.6876 - regression_loss: 0.5522 - classification_loss: 0.1354
 4521/10000 [============>.................] - ETA: 44:00 - loss: 0.6879 - regression_loss: 0.5524 - classification_loss: 0.1355
 4522/10000 [============>.................] - ETA: 43:59 - loss: 0.6878 - regression_loss: 0.5524 - classification_loss: 0.1355
 4523/10000 [============>.................] - ETA: 43:59 - loss: 0.6877 - regression_loss: 0.5523 - classification_loss: 0.1355
 4524/10000 [============>.................] - ETA: 43:58 - loss: 0.6876 - regression_loss: 0.5522 - classification_loss: 0.1354
 4525/10000 [============>.................] - ETA: 43:57 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1354
 4526/10000 [============>.................] - ETA: 43:57 - loss: 0.6876 - regression_loss: 0.5522 - classification_loss: 0.1354
 4527/10000 [============>.................] - ETA: 43:56 - loss: 0.6876 - regression_loss: 0.5522 - classification_loss: 0.1354
 4528/10000 [============>.................] - ETA: 43:56 - loss: 0.6876 - regression_loss: 0.5522 - classification_loss: 0.1354
 4529/10000 [============>.................] - ETA: 43:55 - loss: 0.6875 - regression_loss: 0.5521 - classification_loss: 0.1354
 4530/10000 [============>.................] - ETA: 43:55 - loss: 0.6875 - regression_loss: 0.5521 - classification_loss: 0.1354
 4531/10000 [============>.................] - ETA: 43:54 - loss: 0.6875 - regression_loss: 0.5521 - classification_loss: 0.1354
 4532/10000 [============>.................] - ETA: 43:54 - loss: 0.6875 - regression_loss: 0.5521 - classification_loss: 0.1354
 4533/10000 [============>.................] - ETA: 43:54 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1355
 4534/10000 [============>.................] - ETA: 43:53 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1355
 4535/10000 [============>.................] - ETA: 43:52 - loss: 0.6875 - regression_loss: 0.5521 - classification_loss: 0.1355
 4536/10000 [============>.................] - ETA: 43:52 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 4537/10000 [============>.................] - ETA: 43:51 - loss: 0.6875 - regression_loss: 0.5519 - classification_loss: 0.1356
 4538/10000 [============>.................] - ETA: 43:51 - loss: 0.6874 - regression_loss: 0.5518 - classification_loss: 0.1355
 4539/10000 [============>.................] - ETA: 43:50 - loss: 0.6874 - regression_loss: 0.5518 - classification_loss: 0.1356
 4540/10000 [============>.................] - ETA: 43:50 - loss: 0.6874 - regression_loss: 0.5518 - classification_loss: 0.1356
 4541/10000 [============>.................] - ETA: 43:49 - loss: 0.6873 - regression_loss: 0.5518 - classification_loss: 0.1356
 4542/10000 [============>.................] - ETA: 43:49 - loss: 0.6873 - regression_loss: 0.5517 - classification_loss: 0.1355
 4543/10000 [============>.................] - ETA: 43:48 - loss: 0.6872 - regression_loss: 0.5517 - classification_loss: 0.1355
 4544/10000 [============>.................] - ETA: 43:48 - loss: 0.6872 - regression_loss: 0.5516 - classification_loss: 0.1355
 4545/10000 [============>.................] - ETA: 43:47 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 4546/10000 [============>.................] - ETA: 43:47 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 4547/10000 [============>.................] - ETA: 43:46 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 4548/10000 [============>.................] - ETA: 43:46 - loss: 0.6873 - regression_loss: 0.5518 - classification_loss: 0.1355
 4549/10000 [============>.................] - ETA: 43:45 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 4550/10000 [============>.................] - ETA: 43:45 - loss: 0.6873 - regression_loss: 0.5518 - classification_loss: 0.1355
 4551/10000 [============>.................] - ETA: 43:44 - loss: 0.6872 - regression_loss: 0.5517 - classification_loss: 0.1355
 4552/10000 [============>.................] - ETA: 43:44 - loss: 0.6872 - regression_loss: 0.5518 - classification_loss: 0.1355
 4553/10000 [============>.................] - ETA: 43:43 - loss: 0.6873 - regression_loss: 0.5518 - classification_loss: 0.1355
 4554/10000 [============>.................] - ETA: 43:43 - loss: 0.6873 - regression_loss: 0.5518 - classification_loss: 0.1355
 4555/10000 [============>.................] - ETA: 43:42 - loss: 0.6873 - regression_loss: 0.5518 - classification_loss: 0.1355
 4556/10000 [============>.................] - ETA: 43:42 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 4557/10000 [============>.................] - ETA: 43:41 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 4558/10000 [============>.................] - ETA: 43:41 - loss: 0.6873 - regression_loss: 0.5518 - classification_loss: 0.1355
 4559/10000 [============>.................] - ETA: 43:40 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 4560/10000 [============>.................] - ETA: 43:40 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 4561/10000 [============>.................] - ETA: 43:39 - loss: 0.6875 - regression_loss: 0.5519 - classification_loss: 0.1355
 4562/10000 [============>.................] - ETA: 43:39 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 4563/10000 [============>.................] - ETA: 43:38 - loss: 0.6874 - regression_loss: 0.5518 - classification_loss: 0.1355
 4564/10000 [============>.................] - ETA: 43:38 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 4565/10000 [============>.................] - ETA: 43:37 - loss: 0.6876 - regression_loss: 0.5520 - classification_loss: 0.1355
 4566/10000 [============>.................] - ETA: 43:37 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 4567/10000 [============>.................] - ETA: 43:36 - loss: 0.6876 - regression_loss: 0.5520 - classification_loss: 0.1355
 4568/10000 [============>.................] - ETA: 43:36 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 4569/10000 [============>.................] - ETA: 43:35 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 4570/10000 [============>.................] - ETA: 43:35 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 4571/10000 [============>.................] - ETA: 43:34 - loss: 0.6875 - regression_loss: 0.5519 - classification_loss: 0.1355
 4572/10000 [============>.................] - ETA: 43:34 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 4573/10000 [============>.................] - ETA: 43:33 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 4574/10000 [============>.................] - ETA: 43:33 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 4575/10000 [============>.................] - ETA: 43:32 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1355
 4576/10000 [============>.................] - ETA: 43:32 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1355
 4577/10000 [============>.................] - ETA: 43:31 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1356
 4578/10000 [============>.................] - ETA: 43:31 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1355
 4579/10000 [============>.................] - ETA: 43:30 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1356
 4580/10000 [============>.................] - ETA: 43:30 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1356
 4581/10000 [============>.................] - ETA: 43:29 - loss: 0.6877 - regression_loss: 0.5521 - classification_loss: 0.1356
 4582/10000 [============>.................] - ETA: 43:29 - loss: 0.6876 - regression_loss: 0.5520 - classification_loss: 0.1356
 4583/10000 [============>.................] - ETA: 43:28 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 4584/10000 [============>.................] - ETA: 43:28 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 4585/10000 [============>.................] - ETA: 43:27 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 4586/10000 [============>.................] - ETA: 43:27 - loss: 0.6873 - regression_loss: 0.5518 - classification_loss: 0.1355
 4587/10000 [============>.................] - ETA: 43:26 - loss: 0.6875 - regression_loss: 0.5519 - classification_loss: 0.1355
 4588/10000 [============>.................] - ETA: 43:26 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 4589/10000 [============>.................] - ETA: 43:25 - loss: 0.6873 - regression_loss: 0.5518 - classification_loss: 0.1355
 4590/10000 [============>.................] - ETA: 43:25 - loss: 0.6874 - regression_loss: 0.5519 - classification_loss: 0.1355
 4591/10000 [============>.................] - ETA: 43:24 - loss: 0.6876 - regression_loss: 0.5520 - classification_loss: 0.1355
 4592/10000 [============>.................] - ETA: 43:24 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1356
 4593/10000 [============>.................] - ETA: 43:23 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1356
 4594/10000 [============>.................] - ETA: 43:23 - loss: 0.6877 - regression_loss: 0.5521 - classification_loss: 0.1356
 4595/10000 [============>.................] - ETA: 43:22 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1356
 4596/10000 [============>.................] - ETA: 43:22 - loss: 0.6879 - regression_loss: 0.5523 - classification_loss: 0.1356
 4597/10000 [============>.................] - ETA: 43:21 - loss: 0.6879 - regression_loss: 0.5523 - classification_loss: 0.1356
 4598/10000 [============>.................] - ETA: 43:21 - loss: 0.6879 - regression_loss: 0.5523 - classification_loss: 0.1356
 4599/10000 [============>.................] - ETA: 43:20 - loss: 0.6879 - regression_loss: 0.5523 - classification_loss: 0.1356
 4600/10000 [============>.................] - ETA: 43:20 - loss: 0.6879 - regression_loss: 0.5523 - classification_loss: 0.1356
 4601/10000 [============>.................] - ETA: 43:19 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1356
 4602/10000 [============>.................] - ETA: 43:18 - loss: 0.6879 - regression_loss: 0.5523 - classification_loss: 0.1356
 4603/10000 [============>.................] - ETA: 43:18 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1356
 4604/10000 [============>.................] - ETA: 43:17 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1356
 4605/10000 [============>.................] - ETA: 43:17 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1356
 4606/10000 [============>.................] - ETA: 43:16 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1356
 4607/10000 [============>.................] - ETA: 43:16 - loss: 0.6877 - regression_loss: 0.5521 - classification_loss: 0.1356
 4608/10000 [============>.................] - ETA: 43:15 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1355
 4609/10000 [============>.................] - ETA: 43:15 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1355
 4610/10000 [============>.................] - ETA: 43:14 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 4611/10000 [============>.................] - ETA: 43:14 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 4612/10000 [============>.................] - ETA: 43:13 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1355
 4613/10000 [============>.................] - ETA: 43:13 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1355
 4614/10000 [============>.................] - ETA: 43:12 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1355
 4615/10000 [============>.................] - ETA: 43:12 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1355
 4616/10000 [============>.................] - ETA: 43:11 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1355
 4617/10000 [============>.................] - ETA: 43:11 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1355
 4618/10000 [============>.................] - ETA: 43:10 - loss: 0.6877 - regression_loss: 0.5521 - classification_loss: 0.1355
 4619/10000 [============>.................] - ETA: 43:10 - loss: 0.6877 - regression_loss: 0.5521 - classification_loss: 0.1355
 4620/10000 [============>.................] - ETA: 43:09 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1355
 4621/10000 [============>.................] - ETA: 43:09 - loss: 0.6878 - regression_loss: 0.5523 - classification_loss: 0.1355
 4622/10000 [============>.................] - ETA: 43:08 - loss: 0.6878 - regression_loss: 0.5523 - classification_loss: 0.1355
 4623/10000 [============>.................] - ETA: 43:08 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1355
 4624/10000 [============>.................] - ETA: 43:07 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1355
 4625/10000 [============>.................] - ETA: 43:07 - loss: 0.6878 - regression_loss: 0.5523 - classification_loss: 0.1356
 4626/10000 [============>.................] - ETA: 43:06 - loss: 0.6879 - regression_loss: 0.5523 - classification_loss: 0.1356
 4627/10000 [============>.................] - ETA: 43:06 - loss: 0.6879 - regression_loss: 0.5523 - classification_loss: 0.1356
 4628/10000 [============>.................] - ETA: 43:05 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1356
 4629/10000 [============>.................] - ETA: 43:05 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1355
 4630/10000 [============>.................] - ETA: 43:04 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1356
 4631/10000 [============>.................] - ETA: 43:04 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1356
 4632/10000 [============>.................] - ETA: 43:03 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1356
 4633/10000 [============>.................] - ETA: 43:03 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1355
 4634/10000 [============>.................] - ETA: 43:02 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1355
 4635/10000 [============>.................] - ETA: 43:02 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1355
 4636/10000 [============>.................] - ETA: 43:01 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1355
 4637/10000 [============>.................] - ETA: 43:01 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1355
 4638/10000 [============>.................] - ETA: 43:00 - loss: 0.6878 - regression_loss: 0.5523 - classification_loss: 0.1355
 4639/10000 [============>.................] - ETA: 43:00 - loss: 0.6879 - regression_loss: 0.5524 - classification_loss: 0.1355
 4640/10000 [============>.................] - ETA: 42:59 - loss: 0.6880 - regression_loss: 0.5525 - classification_loss: 0.1355
 4641/10000 [============>.................] - ETA: 42:59 - loss: 0.6879 - regression_loss: 0.5524 - classification_loss: 0.1355
 4642/10000 [============>.................] - ETA: 42:58 - loss: 0.6879 - regression_loss: 0.5524 - classification_loss: 0.1355
 4643/10000 [============>.................] - ETA: 42:58 - loss: 0.6878 - regression_loss: 0.5523 - classification_loss: 0.1355
 4644/10000 [============>.................] - ETA: 42:57 - loss: 0.6877 - regression_loss: 0.5523 - classification_loss: 0.1355
 4645/10000 [============>.................] - ETA: 42:57 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1355
 4646/10000 [============>.................] - ETA: 42:56 - loss: 0.6876 - regression_loss: 0.5522 - classification_loss: 0.1355
 4647/10000 [============>.................] - ETA: 42:56 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1355
 4648/10000 [============>.................] - ETA: 42:55 - loss: 0.6877 - regression_loss: 0.5522 - classification_loss: 0.1355
 4649/10000 [============>.................] - ETA: 42:55 - loss: 0.6879 - regression_loss: 0.5523 - classification_loss: 0.1356
 4650/10000 [============>.................] - ETA: 42:54 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4651/10000 [============>.................] - ETA: 42:54 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1356
 4652/10000 [============>.................] - ETA: 42:53 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4653/10000 [============>.................] - ETA: 42:53 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1356
 4654/10000 [============>.................] - ETA: 42:52 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4655/10000 [============>.................] - ETA: 42:52 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1356
 4656/10000 [============>.................] - ETA: 42:51 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4657/10000 [============>.................] - ETA: 42:51 - loss: 0.6883 - regression_loss: 0.5526 - classification_loss: 0.1357
 4658/10000 [============>.................] - ETA: 42:50 - loss: 0.6882 - regression_loss: 0.5525 - classification_loss: 0.1357
 4659/10000 [============>.................] - ETA: 42:49 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1357
 4660/10000 [============>.................] - ETA: 42:49 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1357
 4661/10000 [============>.................] - ETA: 42:48 - loss: 0.6881 - regression_loss: 0.5524 - classification_loss: 0.1357
 4662/10000 [============>.................] - ETA: 42:48 - loss: 0.6879 - regression_loss: 0.5523 - classification_loss: 0.1356
 4663/10000 [============>.................] - ETA: 42:47 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1356
 4664/10000 [============>.................] - ETA: 42:47 - loss: 0.6879 - regression_loss: 0.5523 - classification_loss: 0.1356
 4665/10000 [============>.................] - ETA: 42:46 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1356
 4666/10000 [============>.................] - ETA: 42:46 - loss: 0.6877 - regression_loss: 0.5521 - classification_loss: 0.1356
 4667/10000 [=============>................] - ETA: 42:45 - loss: 0.6877 - regression_loss: 0.5521 - classification_loss: 0.1356
 4668/10000 [=============>................] - ETA: 42:45 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1356
 4669/10000 [=============>................] - ETA: 42:44 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1356
 4670/10000 [=============>................] - ETA: 42:44 - loss: 0.6876 - regression_loss: 0.5521 - classification_loss: 0.1356
 4671/10000 [=============>................] - ETA: 42:43 - loss: 0.6877 - regression_loss: 0.5521 - classification_loss: 0.1356
 4672/10000 [=============>................] - ETA: 42:43 - loss: 0.6876 - regression_loss: 0.5520 - classification_loss: 0.1356
 4673/10000 [=============>................] - ETA: 42:42 - loss: 0.6876 - regression_loss: 0.5520 - classification_loss: 0.1356
 4674/10000 [=============>................] - ETA: 42:42 - loss: 0.6875 - regression_loss: 0.5520 - classification_loss: 0.1356
 4675/10000 [=============>................] - ETA: 42:41 - loss: 0.6875 - regression_loss: 0.5519 - classification_loss: 0.1356
 4676/10000 [=============>................] - ETA: 42:41 - loss: 0.6877 - regression_loss: 0.5521 - classification_loss: 0.1356
 4677/10000 [=============>................] - ETA: 42:40 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1356
 4678/10000 [=============>................] - ETA: 42:40 - loss: 0.6877 - regression_loss: 0.5521 - classification_loss: 0.1356
 4679/10000 [=============>................] - ETA: 42:40 - loss: 0.6878 - regression_loss: 0.5522 - classification_loss: 0.1356
 4680/10000 [=============>................] - ETA: 42:39 - loss: 0.6880 - regression_loss: 0.5523 - classification_loss: 0.1357
 4681/10000 [=============>................] - ETA: 42:38 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1357
 4682/10000 [=============>................] - ETA: 42:38 - loss: 0.6882 - regression_loss: 0.5524 - classification_loss: 0.1357
 4683/10000 [=============>................] - ETA: 42:37 - loss: 0.6884 - regression_loss: 0.5526 - classification_loss: 0.1357
 4684/10000 [=============>................] - ETA: 42:37 - loss: 0.6883 - regression_loss: 0.5526 - classification_loss: 0.1357
 4685/10000 [=============>................] - ETA: 42:36 - loss: 0.6882 - regression_loss: 0.5525 - classification_loss: 0.1357
 4686/10000 [=============>................] - ETA: 42:36 - loss: 0.6881 - regression_loss: 0.5524 - classification_loss: 0.1357
 4687/10000 [=============>................] - ETA: 42:35 - loss: 0.6881 - regression_loss: 0.5524 - classification_loss: 0.1357
 4688/10000 [=============>................] - ETA: 42:35 - loss: 0.6881 - regression_loss: 0.5524 - classification_loss: 0.1357
 4689/10000 [=============>................] - ETA: 42:34 - loss: 0.6880 - regression_loss: 0.5523 - classification_loss: 0.1357
 4690/10000 [=============>................] - ETA: 42:34 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1357
 4691/10000 [=============>................] - ETA: 42:33 - loss: 0.6882 - regression_loss: 0.5525 - classification_loss: 0.1357
 4692/10000 [=============>................] - ETA: 42:33 - loss: 0.6884 - regression_loss: 0.5527 - classification_loss: 0.1358
 4693/10000 [=============>................] - ETA: 42:32 - loss: 0.6885 - regression_loss: 0.5528 - classification_loss: 0.1358
 4694/10000 [=============>................] - ETA: 42:32 - loss: 0.6885 - regression_loss: 0.5528 - classification_loss: 0.1358
 4695/10000 [=============>................] - ETA: 42:31 - loss: 0.6885 - regression_loss: 0.5527 - classification_loss: 0.1358
 4696/10000 [=============>................] - ETA: 42:31 - loss: 0.6885 - regression_loss: 0.5527 - classification_loss: 0.1358
 4697/10000 [=============>................] - ETA: 42:30 - loss: 0.6886 - regression_loss: 0.5528 - classification_loss: 0.1358
 4698/10000 [=============>................] - ETA: 42:30 - loss: 0.6885 - regression_loss: 0.5528 - classification_loss: 0.1358
 4699/10000 [=============>................] - ETA: 42:29 - loss: 0.6887 - regression_loss: 0.5529 - classification_loss: 0.1358
 4700/10000 [=============>................] - ETA: 42:29 - loss: 0.6887 - regression_loss: 0.5529 - classification_loss: 0.1358
 4701/10000 [=============>................] - ETA: 42:28 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 4702/10000 [=============>................] - ETA: 42:28 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1358
 4703/10000 [=============>................] - ETA: 42:27 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 4704/10000 [=============>................] - ETA: 42:27 - loss: 0.6885 - regression_loss: 0.5528 - classification_loss: 0.1357
 4705/10000 [=============>................] - ETA: 42:26 - loss: 0.6885 - regression_loss: 0.5528 - classification_loss: 0.1357
 4706/10000 [=============>................] - ETA: 42:26 - loss: 0.6885 - regression_loss: 0.5528 - classification_loss: 0.1357
 4707/10000 [=============>................] - ETA: 42:25 - loss: 0.6885 - regression_loss: 0.5528 - classification_loss: 0.1357
 4708/10000 [=============>................] - ETA: 42:25 - loss: 0.6885 - regression_loss: 0.5528 - classification_loss: 0.1357
 4709/10000 [=============>................] - ETA: 42:25 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 4710/10000 [=============>................] - ETA: 42:24 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1357
 4711/10000 [=============>................] - ETA: 42:24 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 4712/10000 [=============>................] - ETA: 42:23 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1357
 4713/10000 [=============>................] - ETA: 42:23 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 4714/10000 [=============>................] - ETA: 42:22 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 4715/10000 [=============>................] - ETA: 42:22 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 4716/10000 [=============>................] - ETA: 42:21 - loss: 0.6885 - regression_loss: 0.5528 - classification_loss: 0.1357
 4717/10000 [=============>................] - ETA: 42:21 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1357
 4718/10000 [=============>................] - ETA: 42:20 - loss: 0.6885 - regression_loss: 0.5528 - classification_loss: 0.1357
 4719/10000 [=============>................] - ETA: 42:20 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1357
 4720/10000 [=============>................] - ETA: 42:19 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1357
 4721/10000 [=============>................] - ETA: 42:19 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 4722/10000 [=============>................] - ETA: 42:18 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1357
 4723/10000 [=============>................] - ETA: 42:18 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 4724/10000 [=============>................] - ETA: 42:17 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1356
 4725/10000 [=============>................] - ETA: 42:17 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1356
 4726/10000 [=============>................] - ETA: 42:16 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1357
 4727/10000 [=============>................] - ETA: 42:16 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1356
 4728/10000 [=============>................] - ETA: 42:15 - loss: 0.6885 - regression_loss: 0.5528 - classification_loss: 0.1357
 4729/10000 [=============>................] - ETA: 42:15 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1357
 4730/10000 [=============>................] - ETA: 42:14 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 4731/10000 [=============>................] - ETA: 42:14 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 4732/10000 [=============>................] - ETA: 42:13 - loss: 0.6885 - regression_loss: 0.5528 - classification_loss: 0.1357
 4733/10000 [=============>................] - ETA: 42:13 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1357
 4734/10000 [=============>................] - ETA: 42:12 - loss: 0.6884 - regression_loss: 0.5527 - classification_loss: 0.1357
 4735/10000 [=============>................] - ETA: 42:12 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 4736/10000 [=============>................] - ETA: 42:11 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1356
 4737/10000 [=============>................] - ETA: 42:11 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1356
 4738/10000 [=============>................] - ETA: 42:10 - loss: 0.6885 - regression_loss: 0.5528 - classification_loss: 0.1356
 4739/10000 [=============>................] - ETA: 42:10 - loss: 0.6885 - regression_loss: 0.5528 - classification_loss: 0.1356
 4740/10000 [=============>................] - ETA: 42:09 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1356
 4741/10000 [=============>................] - ETA: 42:10 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 4742/10000 [=============>................] - ETA: 42:09 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1356
 4743/10000 [=============>................] - ETA: 42:08 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1356
 4744/10000 [=============>................] - ETA: 42:08 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 4745/10000 [=============>................] - ETA: 42:07 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 4746/10000 [=============>................] - ETA: 42:07 - loss: 0.6886 - regression_loss: 0.5530 - classification_loss: 0.1357
 4747/10000 [=============>................] - ETA: 42:06 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 4748/10000 [=============>................] - ETA: 42:06 - loss: 0.6885 - regression_loss: 0.5529 - classification_loss: 0.1357
 4749/10000 [=============>................] - ETA: 42:05 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1357
 4750/10000 [=============>................] - ETA: 42:05 - loss: 0.6884 - regression_loss: 0.5527 - classification_loss: 0.1356
 4751/10000 [=============>................] - ETA: 42:04 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 4752/10000 [=============>................] - ETA: 42:04 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 4753/10000 [=============>................] - ETA: 42:03 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 4754/10000 [=============>................] - ETA: 42:03 - loss: 0.6883 - regression_loss: 0.5527 - classification_loss: 0.1356
 4755/10000 [=============>................] - ETA: 42:02 - loss: 0.6882 - regression_loss: 0.5527 - classification_loss: 0.1356
 4756/10000 [=============>................] - ETA: 42:02 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1356
 4757/10000 [=============>................] - ETA: 42:01 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1356
 4758/10000 [=============>................] - ETA: 42:01 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1355
 4759/10000 [=============>................] - ETA: 42:00 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1355
 4760/10000 [=============>................] - ETA: 42:00 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1355
 4761/10000 [=============>................] - ETA: 41:59 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1356
 4762/10000 [=============>................] - ETA: 41:59 - loss: 0.6881 - regression_loss: 0.5526 - classification_loss: 0.1356
 4763/10000 [=============>................] - ETA: 41:59 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1356
 4764/10000 [=============>................] - ETA: 41:58 - loss: 0.6881 - regression_loss: 0.5526 - classification_loss: 0.1356
 4765/10000 [=============>................] - ETA: 41:58 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4766/10000 [=============>................] - ETA: 41:57 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1356
 4767/10000 [=============>................] - ETA: 41:57 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4768/10000 [=============>................] - ETA: 41:56 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4769/10000 [=============>................] - ETA: 41:56 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4770/10000 [=============>................] - ETA: 41:55 - loss: 0.6881 - regression_loss: 0.5525 - classification_loss: 0.1356
 4771/10000 [=============>................] - ETA: 41:54 - loss: 0.6880 - regression_loss: 0.5524 - classification_loss: 0.1356
 4772/10000 [=============>................] - ETA: 41:54 - loss: 0.6879 - regression_loss: 0.5524 - classification_loss: 0.1356
 4773/10000 [=============>................] - ETA: 41:53 - loss: 0.6881 - regression_loss: 0.5524 - classification_loss: 0.1356
 4774/10000 [=============>................] - ETA: 41:53 - loss: 0.6883 - regression_loss: 0.5526 - classification_loss: 0.1357
 4775/10000 [=============>................] - ETA: 41:52 - loss: 0.6883 - regression_loss: 0.5526 - classification_loss: 0.1357
 4776/10000 [=============>................] - ETA: 41:52 - loss: 0.6882 - regression_loss: 0.5526 - classification_loss: 0.1356
 4777/10000 [=============>................] - ETA: 41:51 - loss: 0.6884 - regression_loss: 0.5527 - classification_loss: 0.1357
 4778/10000 [=============>................] - ETA: 41:51 - loss: 0.6884 - regression_loss: 0.5528 - classification_loss: 0.1357
 4779/10000 [=============>................] - ETA: 41:50 - loss: 0.6885 - regression_loss: 0.5528 - classification_loss: 0.1357
 4780/10000 [=============>................] - ETA: 41:50 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 4781/10000 [=============>................] - ETA: 41:49 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 4782/10000 [=============>................] - ETA: 41:49 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1357
 4783/10000 [=============>................] - ETA: 41:49 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1357
 4784/10000 [=============>................] - ETA: 41:48 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1357
 4785/10000 [=============>................] - ETA: 41:47 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1357
 4786/10000 [=============>................] - ETA: 41:47 - loss: 0.6888 - regression_loss: 0.5531 - classification_loss: 0.1357
 4787/10000 [=============>................] - ETA: 41:46 - loss: 0.6888 - regression_loss: 0.5530 - classification_loss: 0.1358
 4788/10000 [=============>................] - ETA: 41:46 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1358
 4789/10000 [=============>................] - ETA: 41:45 - loss: 0.6887 - regression_loss: 0.5529 - classification_loss: 0.1358
 4790/10000 [=============>................] - ETA: 41:45 - loss: 0.6887 - regression_loss: 0.5529 - classification_loss: 0.1357
 4791/10000 [=============>................] - ETA: 41:45 - loss: 0.6888 - regression_loss: 0.5531 - classification_loss: 0.1357
 4792/10000 [=============>................] - ETA: 41:44 - loss: 0.6888 - regression_loss: 0.5530 - classification_loss: 0.1357
 4793/10000 [=============>................] - ETA: 41:44 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1357
 4794/10000 [=============>................] - ETA: 41:43 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1357
 4795/10000 [=============>................] - ETA: 41:43 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1357
 4796/10000 [=============>................] - ETA: 41:42 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 4797/10000 [=============>................] - ETA: 41:42 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 4798/10000 [=============>................] - ETA: 41:41 - loss: 0.6888 - regression_loss: 0.5531 - classification_loss: 0.1357
 4799/10000 [=============>................] - ETA: 41:41 - loss: 0.6889 - regression_loss: 0.5532 - classification_loss: 0.1357
 4800/10000 [=============>................] - ETA: 41:40 - loss: 0.6889 - regression_loss: 0.5531 - classification_loss: 0.1358
 4801/10000 [=============>................] - ETA: 41:40 - loss: 0.6889 - regression_loss: 0.5532 - classification_loss: 0.1358
 4802/10000 [=============>................] - ETA: 41:39 - loss: 0.6889 - regression_loss: 0.5532 - classification_loss: 0.1357
 4803/10000 [=============>................] - ETA: 41:39 - loss: 0.6889 - regression_loss: 0.5532 - classification_loss: 0.1357
 4804/10000 [=============>................] - ETA: 41:38 - loss: 0.6888 - regression_loss: 0.5531 - classification_loss: 0.1357
 4805/10000 [=============>................] - ETA: 41:38 - loss: 0.6890 - regression_loss: 0.5532 - classification_loss: 0.1358
 4806/10000 [=============>................] - ETA: 41:37 - loss: 0.6891 - regression_loss: 0.5533 - classification_loss: 0.1358
 4807/10000 [=============>................] - ETA: 41:37 - loss: 0.6892 - regression_loss: 0.5533 - classification_loss: 0.1358
 4808/10000 [=============>................] - ETA: 41:36 - loss: 0.6890 - regression_loss: 0.5532 - classification_loss: 0.1358
 4809/10000 [=============>................] - ETA: 41:36 - loss: 0.6889 - regression_loss: 0.5532 - classification_loss: 0.1358
 4810/10000 [=============>................] - ETA: 41:35 - loss: 0.6888 - regression_loss: 0.5531 - classification_loss: 0.1358
 4811/10000 [=============>................] - ETA: 41:35 - loss: 0.6890 - regression_loss: 0.5532 - classification_loss: 0.1358
 4812/10000 [=============>................] - ETA: 41:34 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 4813/10000 [=============>................] - ETA: 41:34 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 4814/10000 [=============>................] - ETA: 41:33 - loss: 0.6889 - regression_loss: 0.5532 - classification_loss: 0.1357
 4815/10000 [=============>................] - ETA: 41:33 - loss: 0.6889 - regression_loss: 0.5532 - classification_loss: 0.1357
 4816/10000 [=============>................] - ETA: 41:32 - loss: 0.6889 - regression_loss: 0.5532 - classification_loss: 0.1357
 4817/10000 [=============>................] - ETA: 41:32 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 4818/10000 [=============>................] - ETA: 41:31 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 4819/10000 [=============>................] - ETA: 41:31 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 4820/10000 [=============>................] - ETA: 41:30 - loss: 0.6892 - regression_loss: 0.5535 - classification_loss: 0.1357
 4821/10000 [=============>................] - ETA: 41:29 - loss: 0.6893 - regression_loss: 0.5536 - classification_loss: 0.1357
 4822/10000 [=============>................] - ETA: 41:29 - loss: 0.6893 - regression_loss: 0.5536 - classification_loss: 0.1357
 4823/10000 [=============>................] - ETA: 41:28 - loss: 0.6894 - regression_loss: 0.5537 - classification_loss: 0.1357
 4824/10000 [=============>................] - ETA: 41:28 - loss: 0.6894 - regression_loss: 0.5537 - classification_loss: 0.1357
 4825/10000 [=============>................] - ETA: 41:27 - loss: 0.6893 - regression_loss: 0.5536 - classification_loss: 0.1357
 4826/10000 [=============>................] - ETA: 41:27 - loss: 0.6892 - regression_loss: 0.5535 - classification_loss: 0.1357
 4827/10000 [=============>................] - ETA: 41:27 - loss: 0.6892 - regression_loss: 0.5535 - classification_loss: 0.1357
 4828/10000 [=============>................] - ETA: 41:26 - loss: 0.6893 - regression_loss: 0.5536 - classification_loss: 0.1357
 4829/10000 [=============>................] - ETA: 41:26 - loss: 0.6893 - regression_loss: 0.5536 - classification_loss: 0.1357
 4830/10000 [=============>................] - ETA: 41:25 - loss: 0.6893 - regression_loss: 0.5536 - classification_loss: 0.1357
 4831/10000 [=============>................] - ETA: 41:25 - loss: 0.6893 - regression_loss: 0.5536 - classification_loss: 0.1357
 4832/10000 [=============>................] - ETA: 41:24 - loss: 0.6892 - regression_loss: 0.5535 - classification_loss: 0.1357
 4833/10000 [=============>................] - ETA: 41:24 - loss: 0.6894 - regression_loss: 0.5537 - classification_loss: 0.1357
 4834/10000 [=============>................] - ETA: 41:23 - loss: 0.6894 - regression_loss: 0.5537 - classification_loss: 0.1357
 4835/10000 [=============>................] - ETA: 41:23 - loss: 0.6896 - regression_loss: 0.5538 - classification_loss: 0.1358
 4836/10000 [=============>................] - ETA: 41:22 - loss: 0.6897 - regression_loss: 0.5539 - classification_loss: 0.1358
 4837/10000 [=============>................] - ETA: 41:22 - loss: 0.6897 - regression_loss: 0.5538 - classification_loss: 0.1358
 4838/10000 [=============>................] - ETA: 41:21 - loss: 0.6896 - regression_loss: 0.5537 - classification_loss: 0.1358
 4839/10000 [=============>................] - ETA: 41:21 - loss: 0.6896 - regression_loss: 0.5538 - classification_loss: 0.1358
 4840/10000 [=============>................] - ETA: 41:20 - loss: 0.6895 - regression_loss: 0.5537 - classification_loss: 0.1358
 4841/10000 [=============>................] - ETA: 41:20 - loss: 0.6895 - regression_loss: 0.5537 - classification_loss: 0.1358
 4842/10000 [=============>................] - ETA: 41:19 - loss: 0.6894 - regression_loss: 0.5536 - classification_loss: 0.1358
 4843/10000 [=============>................] - ETA: 41:19 - loss: 0.6895 - regression_loss: 0.5537 - classification_loss: 0.1358
 4844/10000 [=============>................] - ETA: 41:18 - loss: 0.6895 - regression_loss: 0.5537 - classification_loss: 0.1358
 4845/10000 [=============>................] - ETA: 41:18 - loss: 0.6895 - regression_loss: 0.5538 - classification_loss: 0.1358
 4846/10000 [=============>................] - ETA: 41:17 - loss: 0.6897 - regression_loss: 0.5539 - classification_loss: 0.1359
 4847/10000 [=============>................] - ETA: 41:17 - loss: 0.6898 - regression_loss: 0.5539 - classification_loss: 0.1359
 4848/10000 [=============>................] - ETA: 41:16 - loss: 0.6900 - regression_loss: 0.5540 - classification_loss: 0.1360
 4849/10000 [=============>................] - ETA: 41:16 - loss: 0.6899 - regression_loss: 0.5540 - classification_loss: 0.1359
 4850/10000 [=============>................] - ETA: 41:15 - loss: 0.6899 - regression_loss: 0.5540 - classification_loss: 0.1359
 4851/10000 [=============>................] - ETA: 41:15 - loss: 0.6900 - regression_loss: 0.5541 - classification_loss: 0.1359
 4852/10000 [=============>................] - ETA: 41:14 - loss: 0.6901 - regression_loss: 0.5541 - classification_loss: 0.1359
 4853/10000 [=============>................] - ETA: 41:14 - loss: 0.6900 - regression_loss: 0.5541 - classification_loss: 0.1359
 4854/10000 [=============>................] - ETA: 41:13 - loss: 0.6901 - regression_loss: 0.5541 - classification_loss: 0.1359
 4855/10000 [=============>................] - ETA: 41:13 - loss: 0.6901 - regression_loss: 0.5542 - classification_loss: 0.1359
 4856/10000 [=============>................] - ETA: 41:12 - loss: 0.6900 - regression_loss: 0.5541 - classification_loss: 0.1359
 4857/10000 [=============>................] - ETA: 41:12 - loss: 0.6899 - regression_loss: 0.5540 - classification_loss: 0.1359
 4858/10000 [=============>................] - ETA: 41:11 - loss: 0.6900 - regression_loss: 0.5541 - classification_loss: 0.1359
 4859/10000 [=============>................] - ETA: 41:11 - loss: 0.6900 - regression_loss: 0.5541 - classification_loss: 0.1359
 4860/10000 [=============>................] - ETA: 41:10 - loss: 0.6901 - regression_loss: 0.5542 - classification_loss: 0.1359
 4861/10000 [=============>................] - ETA: 41:10 - loss: 0.6901 - regression_loss: 0.5542 - classification_loss: 0.1359
 4862/10000 [=============>................] - ETA: 41:09 - loss: 0.6901 - regression_loss: 0.5542 - classification_loss: 0.1359
 4863/10000 [=============>................] - ETA: 41:09 - loss: 0.6901 - regression_loss: 0.5542 - classification_loss: 0.1359
 4864/10000 [=============>................] - ETA: 41:08 - loss: 0.6900 - regression_loss: 0.5541 - classification_loss: 0.1359
 4865/10000 [=============>................] - ETA: 41:08 - loss: 0.6899 - regression_loss: 0.5540 - classification_loss: 0.1359
 4866/10000 [=============>................] - ETA: 41:07 - loss: 0.6899 - regression_loss: 0.5540 - classification_loss: 0.1359
 4867/10000 [=============>................] - ETA: 41:07 - loss: 0.6899 - regression_loss: 0.5540 - classification_loss: 0.1359
 4868/10000 [=============>................] - ETA: 41:06 - loss: 0.6898 - regression_loss: 0.5540 - classification_loss: 0.1359
 4869/10000 [=============>................] - ETA: 41:06 - loss: 0.6898 - regression_loss: 0.5539 - classification_loss: 0.1358
 4870/10000 [=============>................] - ETA: 41:05 - loss: 0.6896 - regression_loss: 0.5538 - classification_loss: 0.1358
 4871/10000 [=============>................] - ETA: 41:05 - loss: 0.6896 - regression_loss: 0.5538 - classification_loss: 0.1358
 4872/10000 [=============>................] - ETA: 41:04 - loss: 0.6896 - regression_loss: 0.5538 - classification_loss: 0.1358
 4873/10000 [=============>................] - ETA: 41:04 - loss: 0.6898 - regression_loss: 0.5539 - classification_loss: 0.1358
 4874/10000 [=============>................] - ETA: 41:03 - loss: 0.6897 - regression_loss: 0.5539 - classification_loss: 0.1358
 4875/10000 [=============>................] - ETA: 41:03 - loss: 0.6896 - regression_loss: 0.5538 - classification_loss: 0.1358
 4876/10000 [=============>................] - ETA: 41:02 - loss: 0.6897 - regression_loss: 0.5539 - classification_loss: 0.1358
 4877/10000 [=============>................] - ETA: 41:02 - loss: 0.6896 - regression_loss: 0.5538 - classification_loss: 0.1358
 4878/10000 [=============>................] - ETA: 41:01 - loss: 0.6896 - regression_loss: 0.5538 - classification_loss: 0.1358
 4879/10000 [=============>................] - ETA: 41:01 - loss: 0.6896 - regression_loss: 0.5538 - classification_loss: 0.1358
 4880/10000 [=============>................] - ETA: 41:00 - loss: 0.6895 - regression_loss: 0.5537 - classification_loss: 0.1358
 4881/10000 [=============>................] - ETA: 41:00 - loss: 0.6894 - regression_loss: 0.5536 - classification_loss: 0.1358
 4882/10000 [=============>................] - ETA: 40:59 - loss: 0.6894 - regression_loss: 0.5536 - classification_loss: 0.1358
 4883/10000 [=============>................] - ETA: 40:59 - loss: 0.6894 - regression_loss: 0.5536 - classification_loss: 0.1358
 4884/10000 [=============>................] - ETA: 40:58 - loss: 0.6894 - regression_loss: 0.5536 - classification_loss: 0.1358
 4885/10000 [=============>................] - ETA: 40:58 - loss: 0.6894 - regression_loss: 0.5535 - classification_loss: 0.1358
 4886/10000 [=============>................] - ETA: 40:57 - loss: 0.6894 - regression_loss: 0.5536 - classification_loss: 0.1358
 4887/10000 [=============>................] - ETA: 40:57 - loss: 0.6893 - regression_loss: 0.5535 - classification_loss: 0.1358
 4888/10000 [=============>................] - ETA: 40:56 - loss: 0.6895 - regression_loss: 0.5536 - classification_loss: 0.1359
 4889/10000 [=============>................] - ETA: 40:56 - loss: 0.6895 - regression_loss: 0.5536 - classification_loss: 0.1359
 4890/10000 [=============>................] - ETA: 40:55 - loss: 0.6895 - regression_loss: 0.5536 - classification_loss: 0.1359
 4891/10000 [=============>................] - ETA: 40:55 - loss: 0.6894 - regression_loss: 0.5535 - classification_loss: 0.1359
 4892/10000 [=============>................] - ETA: 40:54 - loss: 0.6895 - regression_loss: 0.5536 - classification_loss: 0.1359
 4893/10000 [=============>................] - ETA: 40:54 - loss: 0.6896 - regression_loss: 0.5537 - classification_loss: 0.1358
 4894/10000 [=============>................] - ETA: 40:53 - loss: 0.6897 - regression_loss: 0.5538 - classification_loss: 0.1359
 4895/10000 [=============>................] - ETA: 40:53 - loss: 0.6896 - regression_loss: 0.5538 - classification_loss: 0.1358
 4896/10000 [=============>................] - ETA: 40:52 - loss: 0.6895 - regression_loss: 0.5537 - classification_loss: 0.1358
 4897/10000 [=============>................] - ETA: 40:52 - loss: 0.6895 - regression_loss: 0.5536 - classification_loss: 0.1359
 4898/10000 [=============>................] - ETA: 40:51 - loss: 0.6894 - regression_loss: 0.5536 - classification_loss: 0.1358
 4899/10000 [=============>................] - ETA: 40:51 - loss: 0.6893 - regression_loss: 0.5535 - classification_loss: 0.1358
 4900/10000 [=============>................] - ETA: 40:50 - loss: 0.6893 - regression_loss: 0.5535 - classification_loss: 0.1358
 4901/10000 [=============>................] - ETA: 40:50 - loss: 0.6893 - regression_loss: 0.5535 - classification_loss: 0.1358
 4902/10000 [=============>................] - ETA: 40:49 - loss: 0.6892 - regression_loss: 0.5534 - classification_loss: 0.1358
 4903/10000 [=============>................] - ETA: 40:49 - loss: 0.6893 - regression_loss: 0.5535 - classification_loss: 0.1358
 4904/10000 [=============>................] - ETA: 40:48 - loss: 0.6892 - regression_loss: 0.5534 - classification_loss: 0.1358
 4905/10000 [=============>................] - ETA: 40:48 - loss: 0.6891 - regression_loss: 0.5534 - classification_loss: 0.1358
 4906/10000 [=============>................] - ETA: 40:47 - loss: 0.6891 - regression_loss: 0.5533 - classification_loss: 0.1358
 4907/10000 [=============>................] - ETA: 40:47 - loss: 0.6891 - regression_loss: 0.5533 - classification_loss: 0.1358
 4908/10000 [=============>................] - ETA: 40:46 - loss: 0.6890 - regression_loss: 0.5532 - classification_loss: 0.1357
 4909/10000 [=============>................] - ETA: 40:46 - loss: 0.6889 - regression_loss: 0.5532 - classification_loss: 0.1357
 4910/10000 [=============>................] - ETA: 40:45 - loss: 0.6888 - regression_loss: 0.5531 - classification_loss: 0.1357
 4911/10000 [=============>................] - ETA: 40:45 - loss: 0.6888 - regression_loss: 0.5531 - classification_loss: 0.1357
 4912/10000 [=============>................] - ETA: 40:44 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1357
 4913/10000 [=============>................] - ETA: 40:44 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1357
 4914/10000 [=============>................] - ETA: 40:43 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 4915/10000 [=============>................] - ETA: 40:43 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 4916/10000 [=============>................] - ETA: 40:42 - loss: 0.6887 - regression_loss: 0.5529 - classification_loss: 0.1357
 4917/10000 [=============>................] - ETA: 40:42 - loss: 0.6888 - regression_loss: 0.5530 - classification_loss: 0.1357
 4918/10000 [=============>................] - ETA: 40:41 - loss: 0.6888 - regression_loss: 0.5531 - classification_loss: 0.1357
 4919/10000 [=============>................] - ETA: 40:41 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1357
 4920/10000 [=============>................] - ETA: 40:40 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 4921/10000 [=============>................] - ETA: 40:40 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1357
 4922/10000 [=============>................] - ETA: 40:39 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 4923/10000 [=============>................] - ETA: 40:39 - loss: 0.6886 - regression_loss: 0.5530 - classification_loss: 0.1357
 4924/10000 [=============>................] - ETA: 40:38 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1357
 4925/10000 [=============>................] - ETA: 40:38 - loss: 0.6886 - regression_loss: 0.5530 - classification_loss: 0.1357
 4926/10000 [=============>................] - ETA: 40:37 - loss: 0.6887 - regression_loss: 0.5531 - classification_loss: 0.1356
 4927/10000 [=============>................] - ETA: 40:37 - loss: 0.6888 - regression_loss: 0.5531 - classification_loss: 0.1356
 4928/10000 [=============>................] - ETA: 40:36 - loss: 0.6887 - regression_loss: 0.5531 - classification_loss: 0.1356
 4929/10000 [=============>................] - ETA: 40:36 - loss: 0.6887 - regression_loss: 0.5531 - classification_loss: 0.1356
 4930/10000 [=============>................] - ETA: 40:35 - loss: 0.6887 - regression_loss: 0.5531 - classification_loss: 0.1356
 4931/10000 [=============>................] - ETA: 40:35 - loss: 0.6886 - regression_loss: 0.5530 - classification_loss: 0.1356
 4932/10000 [=============>................] - ETA: 40:34 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1356
 4933/10000 [=============>................] - ETA: 40:34 - loss: 0.6889 - regression_loss: 0.5532 - classification_loss: 0.1357
 4934/10000 [=============>................] - ETA: 40:33 - loss: 0.6889 - regression_loss: 0.5533 - classification_loss: 0.1356
 4935/10000 [=============>................] - ETA: 40:33 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1356
 4936/10000 [=============>................] - ETA: 40:32 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 4937/10000 [=============>................] - ETA: 40:32 - loss: 0.6889 - regression_loss: 0.5532 - classification_loss: 0.1357
 4938/10000 [=============>................] - ETA: 40:31 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 4939/10000 [=============>................] - ETA: 40:31 - loss: 0.6891 - regression_loss: 0.5533 - classification_loss: 0.1357
 4940/10000 [=============>................] - ETA: 40:31 - loss: 0.6891 - regression_loss: 0.5534 - classification_loss: 0.1357
 4941/10000 [=============>................] - ETA: 40:30 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 4942/10000 [=============>................] - ETA: 40:30 - loss: 0.6892 - regression_loss: 0.5535 - classification_loss: 0.1357
 4943/10000 [=============>................] - ETA: 40:29 - loss: 0.6891 - regression_loss: 0.5534 - classification_loss: 0.1357
 4944/10000 [=============>................] - ETA: 40:29 - loss: 0.6893 - regression_loss: 0.5535 - classification_loss: 0.1357
 4945/10000 [=============>................] - ETA: 40:28 - loss: 0.6892 - regression_loss: 0.5535 - classification_loss: 0.1357
 4946/10000 [=============>................] - ETA: 40:28 - loss: 0.6892 - regression_loss: 0.5535 - classification_loss: 0.1357
 4947/10000 [=============>................] - ETA: 40:27 - loss: 0.6893 - regression_loss: 0.5536 - classification_loss: 0.1357
 4948/10000 [=============>................] - ETA: 40:27 - loss: 0.6892 - regression_loss: 0.5535 - classification_loss: 0.1357
 4949/10000 [=============>................] - ETA: 40:26 - loss: 0.6892 - regression_loss: 0.5535 - classification_loss: 0.1357
 4950/10000 [=============>................] - ETA: 40:26 - loss: 0.6893 - regression_loss: 0.5536 - classification_loss: 0.1358
 4951/10000 [=============>................] - ETA: 40:25 - loss: 0.6893 - regression_loss: 0.5535 - classification_loss: 0.1358
 4952/10000 [=============>................] - ETA: 40:25 - loss: 0.6892 - regression_loss: 0.5535 - classification_loss: 0.1357
 4953/10000 [=============>................] - ETA: 40:24 - loss: 0.6894 - regression_loss: 0.5536 - classification_loss: 0.1358
 4954/10000 [=============>................] - ETA: 40:24 - loss: 0.6894 - regression_loss: 0.5536 - classification_loss: 0.1357
 4955/10000 [=============>................] - ETA: 40:23 - loss: 0.6894 - regression_loss: 0.5536 - classification_loss: 0.1357
 4956/10000 [=============>................] - ETA: 40:23 - loss: 0.6894 - regression_loss: 0.5536 - classification_loss: 0.1357
 4957/10000 [=============>................] - ETA: 40:22 - loss: 0.6893 - regression_loss: 0.5535 - classification_loss: 0.1357
 4958/10000 [=============>................] - ETA: 40:22 - loss: 0.6892 - regression_loss: 0.5535 - classification_loss: 0.1357
 4959/10000 [=============>................] - ETA: 40:21 - loss: 0.6891 - regression_loss: 0.5534 - classification_loss: 0.1357
 4960/10000 [=============>................] - ETA: 40:21 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 4961/10000 [=============>................] - ETA: 40:20 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 4962/10000 [=============>................] - ETA: 40:20 - loss: 0.6891 - regression_loss: 0.5534 - classification_loss: 0.1357
 4963/10000 [=============>................] - ETA: 40:19 - loss: 0.6891 - regression_loss: 0.5534 - classification_loss: 0.1357
 4964/10000 [=============>................] - ETA: 40:19 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 4965/10000 [=============>................] - ETA: 40:18 - loss: 0.6889 - regression_loss: 0.5533 - classification_loss: 0.1357
 4966/10000 [=============>................] - ETA: 40:18 - loss: 0.6889 - regression_loss: 0.5532 - classification_loss: 0.1357
 4967/10000 [=============>................] - ETA: 40:17 - loss: 0.6888 - regression_loss: 0.5531 - classification_loss: 0.1356
 4968/10000 [=============>................] - ETA: 40:17 - loss: 0.6887 - regression_loss: 0.5531 - classification_loss: 0.1356
 4969/10000 [=============>................] - ETA: 40:16 - loss: 0.6888 - regression_loss: 0.5532 - classification_loss: 0.1356
 4970/10000 [=============>................] - ETA: 40:16 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 4971/10000 [=============>................] - ETA: 40:15 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 4972/10000 [=============>................] - ETA: 40:14 - loss: 0.6890 - regression_loss: 0.5532 - classification_loss: 0.1358
 4973/10000 [=============>................] - ETA: 40:14 - loss: 0.6890 - regression_loss: 0.5532 - classification_loss: 0.1358
 4974/10000 [=============>................] - ETA: 40:13 - loss: 0.6889 - regression_loss: 0.5532 - classification_loss: 0.1358
 4975/10000 [=============>................] - ETA: 40:13 - loss: 0.6890 - regression_loss: 0.5532 - classification_loss: 0.1358
 4976/10000 [=============>................] - ETA: 40:12 - loss: 0.6891 - regression_loss: 0.5533 - classification_loss: 0.1358
 4977/10000 [=============>................] - ETA: 40:12 - loss: 0.6892 - regression_loss: 0.5533 - classification_loss: 0.1358
 4978/10000 [=============>................] - ETA: 40:11 - loss: 0.6891 - regression_loss: 0.5533 - classification_loss: 0.1358
 4979/10000 [=============>................] - ETA: 40:11 - loss: 0.6891 - regression_loss: 0.5533 - classification_loss: 0.1358
 4980/10000 [=============>................] - ETA: 40:11 - loss: 0.6891 - regression_loss: 0.5533 - classification_loss: 0.1358
 4981/10000 [=============>................] - ETA: 40:10 - loss: 0.6891 - regression_loss: 0.5532 - classification_loss: 0.1358
 4982/10000 [=============>................] - ETA: 40:09 - loss: 0.6892 - regression_loss: 0.5533 - classification_loss: 0.1358
 4983/10000 [=============>................] - ETA: 40:09 - loss: 0.6892 - regression_loss: 0.5534 - classification_loss: 0.1358
 4984/10000 [=============>................] - ETA: 40:09 - loss: 0.6892 - regression_loss: 0.5534 - classification_loss: 0.1359
 4985/10000 [=============>................] - ETA: 40:08 - loss: 0.6892 - regression_loss: 0.5533 - classification_loss: 0.1358
 4986/10000 [=============>................] - ETA: 40:07 - loss: 0.6891 - regression_loss: 0.5532 - classification_loss: 0.1358
 4987/10000 [=============>................] - ETA: 40:07 - loss: 0.6890 - regression_loss: 0.5532 - classification_loss: 0.1358
 4988/10000 [=============>................] - ETA: 40:07 - loss: 0.6890 - regression_loss: 0.5532 - classification_loss: 0.1358
 4989/10000 [=============>................] - ETA: 40:06 - loss: 0.6891 - regression_loss: 0.5533 - classification_loss: 0.1359
 4990/10000 [=============>................] - ETA: 40:05 - loss: 0.6891 - regression_loss: 0.5532 - classification_loss: 0.1359
 4991/10000 [=============>................] - ETA: 40:05 - loss: 0.6891 - regression_loss: 0.5532 - classification_loss: 0.1359
 4992/10000 [=============>................] - ETA: 40:04 - loss: 0.6890 - regression_loss: 0.5531 - classification_loss: 0.1358
 4993/10000 [=============>................] - ETA: 40:04 - loss: 0.6890 - regression_loss: 0.5531 - classification_loss: 0.1358
 4994/10000 [=============>................] - ETA: 40:03 - loss: 0.6890 - regression_loss: 0.5531 - classification_loss: 0.1358
 4995/10000 [=============>................] - ETA: 40:03 - loss: 0.6890 - regression_loss: 0.5531 - classification_loss: 0.1358
 4996/10000 [=============>................] - ETA: 40:02 - loss: 0.6889 - regression_loss: 0.5531 - classification_loss: 0.1358
 4997/10000 [=============>................] - ETA: 40:02 - loss: 0.6890 - regression_loss: 0.5532 - classification_loss: 0.1358
 4998/10000 [=============>................] - ETA: 40:01 - loss: 0.6890 - regression_loss: 0.5531 - classification_loss: 0.1358
 4999/10000 [=============>................] - ETA: 40:01 - loss: 0.6890 - regression_loss: 0.5531 - classification_loss: 0.1358
 5000/10000 [==============>...............] - ETA: 40:00 - loss: 0.6889 - regression_loss: 0.5531 - classification_loss: 0.1358
 5001/10000 [==============>...............] - ETA: 40:00 - loss: 0.6888 - regression_loss: 0.5530 - classification_loss: 0.1358
 5002/10000 [==============>...............] - ETA: 39:59 - loss: 0.6887 - regression_loss: 0.5529 - classification_loss: 0.1358
 5003/10000 [==============>...............] - ETA: 39:59 - loss: 0.6887 - regression_loss: 0.5529 - classification_loss: 0.1358
 5004/10000 [==============>...............] - ETA: 39:59 - loss: 0.6889 - regression_loss: 0.5531 - classification_loss: 0.1358
 5005/10000 [==============>...............] - ETA: 39:58 - loss: 0.6888 - regression_loss: 0.5530 - classification_loss: 0.1358
 5006/10000 [==============>...............] - ETA: 39:57 - loss: 0.6889 - regression_loss: 0.5531 - classification_loss: 0.1358
 5007/10000 [==============>...............] - ETA: 39:57 - loss: 0.6888 - regression_loss: 0.5531 - classification_loss: 0.1358
 5008/10000 [==============>...............] - ETA: 39:56 - loss: 0.6890 - regression_loss: 0.5532 - classification_loss: 0.1358
 5009/10000 [==============>...............] - ETA: 39:56 - loss: 0.6890 - regression_loss: 0.5532 - classification_loss: 0.1358
 5010/10000 [==============>...............] - ETA: 39:55 - loss: 0.6890 - regression_loss: 0.5532 - classification_loss: 0.1358
 5011/10000 [==============>...............] - ETA: 39:55 - loss: 0.6890 - regression_loss: 0.5532 - classification_loss: 0.1358
 5012/10000 [==============>...............] - ETA: 39:54 - loss: 0.6889 - regression_loss: 0.5531 - classification_loss: 0.1358
 5013/10000 [==============>...............] - ETA: 39:54 - loss: 0.6888 - regression_loss: 0.5531 - classification_loss: 0.1358
 5014/10000 [==============>...............] - ETA: 39:53 - loss: 0.6889 - regression_loss: 0.5532 - classification_loss: 0.1358
 5015/10000 [==============>...............] - ETA: 39:53 - loss: 0.6890 - regression_loss: 0.5532 - classification_loss: 0.1357
 5016/10000 [==============>...............] - ETA: 39:52 - loss: 0.6889 - regression_loss: 0.5532 - classification_loss: 0.1357
 5017/10000 [==============>...............] - ETA: 39:52 - loss: 0.6888 - regression_loss: 0.5531 - classification_loss: 0.1357
 5018/10000 [==============>...............] - ETA: 39:51 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1357
 5019/10000 [==============>...............] - ETA: 39:51 - loss: 0.6888 - regression_loss: 0.5531 - classification_loss: 0.1357
 5020/10000 [==============>...............] - ETA: 39:50 - loss: 0.6888 - regression_loss: 0.5531 - classification_loss: 0.1357
 5021/10000 [==============>...............] - ETA: 39:50 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1357
 5022/10000 [==============>...............] - ETA: 39:49 - loss: 0.6886 - regression_loss: 0.5529 - classification_loss: 0.1357
 5023/10000 [==============>...............] - ETA: 39:49 - loss: 0.6888 - regression_loss: 0.5531 - classification_loss: 0.1357
 5024/10000 [==============>...............] - ETA: 39:48 - loss: 0.6888 - regression_loss: 0.5531 - classification_loss: 0.1357
 5025/10000 [==============>...............] - ETA: 39:48 - loss: 0.6887 - regression_loss: 0.5530 - classification_loss: 0.1357
 5026/10000 [==============>...............] - ETA: 39:47 - loss: 0.6886 - regression_loss: 0.5530 - classification_loss: 0.1357
 5027/10000 [==============>...............] - ETA: 39:47 - loss: 0.6886 - regression_loss: 0.5530 - classification_loss: 0.1357
 5028/10000 [==============>...............] - ETA: 39:46 - loss: 0.6887 - regression_loss: 0.5531 - classification_loss: 0.1357
 5029/10000 [==============>...............] - ETA: 39:46 - loss: 0.6889 - regression_loss: 0.5532 - classification_loss: 0.1357
 5030/10000 [==============>...............] - ETA: 39:45 - loss: 0.6889 - regression_loss: 0.5532 - classification_loss: 0.1357
 5031/10000 [==============>...............] - ETA: 39:45 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 5032/10000 [==============>...............] - ETA: 39:44 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 5033/10000 [==============>...............] - ETA: 39:44 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 5034/10000 [==============>...............] - ETA: 39:43 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 5035/10000 [==============>...............] - ETA: 39:43 - loss: 0.6891 - regression_loss: 0.5533 - classification_loss: 0.1357
 5036/10000 [==============>...............] - ETA: 39:42 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 5037/10000 [==============>...............] - ETA: 39:42 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 5038/10000 [==============>...............] - ETA: 39:41 - loss: 0.6891 - regression_loss: 0.5534 - classification_loss: 0.1357
 5039/10000 [==============>...............] - ETA: 39:41 - loss: 0.6890 - regression_loss: 0.5534 - classification_loss: 0.1357
 5040/10000 [==============>...............] - ETA: 39:40 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 5041/10000 [==============>...............] - ETA: 39:40 - loss: 0.6889 - regression_loss: 0.5533 - classification_loss: 0.1357
 5042/10000 [==============>...............] - ETA: 39:39 - loss: 0.6891 - regression_loss: 0.5534 - classification_loss: 0.1357
 5043/10000 [==============>...............] - ETA: 39:39 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 5044/10000 [==============>...............] - ETA: 39:38 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 5045/10000 [==============>...............] - ETA: 39:38 - loss: 0.6889 - regression_loss: 0.5533 - classification_loss: 0.1357
 5046/10000 [==============>...............] - ETA: 39:37 - loss: 0.6889 - regression_loss: 0.5532 - classification_loss: 0.1357
 5047/10000 [==============>...............] - ETA: 39:37 - loss: 0.6889 - regression_loss: 0.5532 - classification_loss: 0.1357
 5048/10000 [==============>...............] - ETA: 39:36 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 5049/10000 [==============>...............] - ETA: 39:36 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 5050/10000 [==============>...............] - ETA: 39:35 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 5051/10000 [==============>...............] - ETA: 39:35 - loss: 0.6891 - regression_loss: 0.5534 - classification_loss: 0.1357
 5052/10000 [==============>...............] - ETA: 39:34 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 5053/10000 [==============>...............] - ETA: 39:34 - loss: 0.6889 - regression_loss: 0.5533 - classification_loss: 0.1357
 5054/10000 [==============>...............] - ETA: 39:33 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 5055/10000 [==============>...............] - ETA: 39:33 - loss: 0.6889 - regression_loss: 0.5533 - classification_loss: 0.1357
 5056/10000 [==============>...............] - ETA: 39:32 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 5057/10000 [==============>...............] - ETA: 39:32 - loss: 0.6890 - regression_loss: 0.5533 - classification_loss: 0.1357
 5058/10000 [==============>...............] - ETA: 39:31 - loss: 0.6891 - regression_loss: 0.5534 - classification_loss: 0.1357
 5059/10000 [==============>...............] - ETA: 39:31 - loss: 0.6891 - regression_loss: 0.5535 - classification_loss: 0.1357
 5060/10000 [==============>...............] - ETA: 39:30 - loss: 0.6892 - regression_loss: 0.5535 - classification_loss: 0.1357
 5061/10000 [==============>...............] - ETA: 39:30 - loss: 0.6892 - regression_loss: 0.5535 - classification_loss: 0.1357
 5062/10000 [==============>...............] - ETA: 39:29 - loss: 0.6894 - regression_loss: 0.5537 - classification_loss: 0.1357
 5063/10000 [==============>...............] - ETA: 39:29 - loss: 0.6896 - regression_loss: 0.5539 - classification_loss: 0.1357
 5064/10000 [==============>...............] - ETA: 39:28 - loss: 0.6897 - regression_loss: 0.5539 - classification_loss: 0.1357
 5065/10000 [==============>...............] - ETA: 39:28 - loss: 0.6897 - regression_loss: 0.5540 - classification_loss: 0.1357
 5066/10000 [==============>...............] - ETA: 39:27 - loss: 0.6896 - regression_loss: 0.5539 - classification_loss: 0.1357
 5067/10000 [==============>...............] - ETA: 39:27 - loss: 0.6898 - regression_loss: 0.5540 - classification_loss: 0.1357
 5068/10000 [==============>...............] - ETA: 39:26 - loss: 0.6897 - regression_loss: 0.5540 - classification_loss: 0.1357
 5069/10000 [==============>...............] - ETA: 39:26 - loss: 0.6898 - regression_loss: 0.5541 - classification_loss: 0.1357
 5070/10000 [==============>...............] - ETA: 39:25 - loss: 0.6898 - regression_loss: 0.5540 - classification_loss: 0.1357
 5071/10000 [==============>...............] - ETA: 39:25 - loss: 0.6899 - regression_loss: 0.5541 - classification_loss: 0.1357
 5072/10000 [==============>...............] - ETA: 39:24 - loss: 0.6898 - regression_loss: 0.5541 - classification_loss: 0.1357
 5073/10000 [==============>...............] - ETA: 39:24 - loss: 0.6897 - regression_loss: 0.5540 - classification_loss: 0.1357
 5074/10000 [==============>...............] - ETA: 39:23 - loss: 0.6898 - regression_loss: 0.5541 - classification_loss: 0.1357
 5075/10000 [==============>...............] - ETA: 39:23 - loss: 0.6898 - regression_loss: 0.5541 - classification_loss: 0.1357
 5076/10000 [==============>...............] - ETA: 39:22 - loss: 0.6899 - regression_loss: 0.5542 - classification_loss: 0.1357
 5077/10000 [==============>...............] - ETA: 39:22 - loss: 0.6899 - regression_loss: 0.5542 - classification_loss: 0.1357
 5078/10000 [==============>...............] - ETA: 39:21 - loss: 0.6898 - regression_loss: 0.5541 - classification_loss: 0.1357
 5079/10000 [==============>...............] - ETA: 39:21 - loss: 0.6898 - regression_loss: 0.5541 - classification_loss: 0.1357
 5080/10000 [==============>...............] - ETA: 39:20 - loss: 0.6898 - regression_loss: 0.5541 - classification_loss: 0.1357
 5081/10000 [==============>...............] - ETA: 39:20 - loss: 0.6899 - regression_loss: 0.5542 - classification_loss: 0.1357
 5082/10000 [==============>...............] - ETA: 39:19 - loss: 0.6901 - regression_loss: 0.5544 - classification_loss: 0.1357
 5083/10000 [==============>...............] - ETA: 39:19 - loss: 0.6900 - regression_loss: 0.5543 - classification_loss: 0.1357
 5084/10000 [==============>...............] - ETA: 39:18 - loss: 0.6899 - regression_loss: 0.5543 - classification_loss: 0.1357
 5085/10000 [==============>...............] - ETA: 39:18 - loss: 0.6899 - regression_loss: 0.5542 - classification_loss: 0.1357
 5086/10000 [==============>...............] - ETA: 39:17 - loss: 0.6898 - regression_loss: 0.5541 - classification_loss: 0.1357
 5087/10000 [==============>...............] - ETA: 39:17 - loss: 0.6897 - regression_loss: 0.5540 - classification_loss: 0.1356
 5088/10000 [==============>...............] - ETA: 39:16 - loss: 0.6896 - regression_loss: 0.5539 - classification_loss: 0.1356
 5089/10000 [==============>...............] - ETA: 39:16 - loss: 0.6895 - regression_loss: 0.5539 - classification_loss: 0.1356
 5090/10000 [==============>...............] - ETA: 39:15 - loss: 0.6896 - regression_loss: 0.5540 - classification_loss: 0.1356
 5091/10000 [==============>...............] - ETA: 39:15 - loss: 0.6896 - regression_loss: 0.5539 - classification_loss: 0.1356
 5092/10000 [==============>...............] - ETA: 39:14 - loss: 0.6895 - regression_loss: 0.5539 - classification_loss: 0.1356
 5093/10000 [==============>...............] - ETA: 39:14 - loss: 0.6895 - regression_loss: 0.5538 - classification_loss: 0.1356
 5094/10000 [==============>...............] - ETA: 39:13 - loss: 0.6895 - regression_loss: 0.5539 - classification_loss: 0.1356
 5095/10000 [==============>...............] - ETA: 39:13 - loss: 0.6898 - regression_loss: 0.5541 - classification_loss: 0.1357
 5096/10000 [==============>...............] - ETA: 39:12 - loss: 0.6900 - regression_loss: 0.5543 - classification_loss: 0.1357
 5097/10000 [==============>...............] - ETA: 39:12 - loss: 0.6900 - regression_loss: 0.5543 - classification_loss: 0.1357
 5098/10000 [==============>...............] - ETA: 39:11 - loss: 0.6899 - regression_loss: 0.5542 - classification_loss: 0.1357
 5099/10000 [==============>...............] - ETA: 39:11 - loss: 0.6898 - regression_loss: 0.5541 - classification_loss: 0.1357
 5100/10000 [==============>...............] - ETA: 39:10 - loss: 0.6898 - regression_loss: 0.5541 - classification_loss: 0.1357
 5101/10000 [==============>...............] - ETA: 39:10 - loss: 0.6897 - regression_loss: 0.5540 - classification_loss: 0.1357
 5102/10000 [==============>...............] - ETA: 39:09 - loss: 0.6897 - regression_loss: 0.5540 - classification_loss: 0.1357
 5103/10000 [==============>...............] - ETA: 39:09 - loss: 0.6897 - regression_loss: 0.5541 - classification_loss: 0.1357
 5104/10000 [==============>...............] - ETA: 39:08 - loss: 0.6897 - regression_loss: 0.5541 - classification_loss: 0.1357
 5105/10000 [==============>...............] - ETA: 39:08 - loss: 0.6897 - regression_loss: 0.5540 - classification_loss: 0.1357
 5106/10000 [==============>...............] - ETA: 39:07 - loss: 0.6896 - regression_loss: 0.5540 - classification_loss: 0.1356
 5107/10000 [==============>...............] - ETA: 39:07 - loss: 0.6897 - regression_loss: 0.5540 - classification_loss: 0.1357
 5108/10000 [==============>...............] - ETA: 39:06 - loss: 0.6897 - regression_loss: 0.5540 - classification_loss: 0.1357
 5109/10000 [==============>...............] - ETA: 39:06 - loss: 0.6896 - regression_loss: 0.5539 - classification_loss: 0.1356
 5110/10000 [==============>...............] - ETA: 39:05 - loss: 0.6896 - regression_loss: 0.5539 - classification_loss: 0.1356
 5111/10000 [==============>...............] - ETA: 39:05 - loss: 0.6897 - regression_loss: 0.5540 - classification_loss: 0.1356
 5112/10000 [==============>...............] - ETA: 39:04 - loss: 0.6896 - regression_loss: 0.5540 - classification_loss: 0.1356
 5113/10000 [==============>...............] - ETA: 39:04 - loss: 0.6895 - regression_loss: 0.5539 - classification_loss: 0.1356
 5114/10000 [==============>...............] - ETA: 39:03 - loss: 0.6896 - regression_loss: 0.5540 - classification_loss: 0.1356
 5115/10000 [==============>...............] - ETA: 39:03 - loss: 0.6896 - regression_loss: 0.5540 - classification_loss: 0.1356
 5116/10000 [==============>...............] - ETA: 39:02 - loss: 0.6895 - regression_loss: 0.5539 - classification_loss: 0.1356
 5117/10000 [==============>...............] - ETA: 39:02 - loss: 0.6896 - regression_loss: 0.5540 - classification_loss: 0.1356
 5118/10000 [==============>...............] - ETA: 39:01 - loss: 0.6895 - regression_loss: 0.5539 - classification_loss: 0.1356
 5119/10000 [==============>...............] - ETA: 39:01 - loss: 0.6895 - regression_loss: 0.5539 - classification_loss: 0.1356
 5120/10000 [==============>...............] - ETA: 39:00 - loss: 0.6895 - regression_loss: 0.5539 - classification_loss: 0.1356
 5121/10000 [==============>...............] - ETA: 39:00 - loss: 0.6897 - regression_loss: 0.5540 - classification_loss: 0.1356
 5122/10000 [==============>...............] - ETA: 38:59 - loss: 0.6897 - regression_loss: 0.5541 - classification_loss: 0.1357
 5123/10000 [==============>...............] - ETA: 38:59 - loss: 0.6898 - regression_loss: 0.5542 - classification_loss: 0.1357
 5124/10000 [==============>...............] - ETA: 38:58 - loss: 0.6899 - regression_loss: 0.5542 - classification_loss: 0.1357
 5125/10000 [==============>...............] - ETA: 38:58 - loss: 0.6899 - regression_loss: 0.5542 - classification_loss: 0.1357
 5126/10000 [==============>...............] - ETA: 38:57 - loss: 0.6899 - regression_loss: 0.5542 - classification_loss: 0.1357
 5127/10000 [==============>...............] - ETA: 38:57 - loss: 0.6898 - regression_loss: 0.5542 - classification_loss: 0.1357
 5128/10000 [==============>...............] - ETA: 38:56 - loss: 0.6900 - regression_loss: 0.5543 - classification_loss: 0.1357
 5129/10000 [==============>...............] - ETA: 38:56 - loss: 0.6899 - regression_loss: 0.5542 - classification_loss: 0.1357
 5130/10000 [==============>...............] - ETA: 38:55 - loss: 0.6899 - regression_loss: 0.5542 - classification_loss: 0.1357
 5131/10000 [==============>...............] - ETA: 38:55 - loss: 0.6899 - regression_loss: 0.5542 - classification_loss: 0.1357
 5132/10000 [==============>...............] - ETA: 38:54 - loss: 0.6901 - regression_loss: 0.5543 - classification_loss: 0.1357
 5133/10000 [==============>...............] - ETA: 38:54 - loss: 0.6900 - regression_loss: 0.5543 - classification_loss: 0.1357
 5134/10000 [==============>...............] - ETA: 38:53 - loss: 0.6900 - regression_loss: 0.5543 - classification_loss: 0.1357
 5135/10000 [==============>...............] - ETA: 38:53 - loss: 0.6900 - regression_loss: 0.5543 - classification_loss: 0.1357
 5136/10000 [==============>...............] - ETA: 38:52 - loss: 0.6900 - regression_loss: 0.5544 - classification_loss: 0.1357
 5137/10000 [==============>...............] - ETA: 38:52 - loss: 0.6900 - regression_loss: 0.5543 - classification_loss: 0.1356
 5138/10000 [==============>...............] - ETA: 38:51 - loss: 0.6899 - regression_loss: 0.5543 - classification_loss: 0.1356
 5139/10000 [==============>...............] - ETA: 38:51 - loss: 0.6899 - regression_loss: 0.5542 - classification_loss: 0.1356
 5140/10000 [==============>...............] - ETA: 38:50 - loss: 0.6899 - regression_loss: 0.5543 - classification_loss: 0.1356
 5141/10000 [==============>...............] - ETA: 38:50 - loss: 0.6900 - regression_loss: 0.5543 - classification_loss: 0.1356
 5142/10000 [==============>...............] - ETA: 38:49 - loss: 0.6901 - regression_loss: 0.5544 - classification_loss: 0.1356
 5143/10000 [==============>...............] - ETA: 38:49 - loss: 0.6901 - regression_loss: 0.5545 - classification_loss: 0.1356
 5144/10000 [==============>...............] - ETA: 38:48 - loss: 0.6902 - regression_loss: 0.5545 - classification_loss: 0.1356
 5145/10000 [==============>...............] - ETA: 38:48 - loss: 0.6901 - regression_loss: 0.5544 - classification_loss: 0.1356
 5146/10000 [==============>...............] - ETA: 38:47 - loss: 0.6901 - regression_loss: 0.5545 - classification_loss: 0.1356
 5147/10000 [==============>...............] - ETA: 38:47 - loss: 0.6901 - regression_loss: 0.5545 - classification_loss: 0.1356
 5148/10000 [==============>...............] - ETA: 38:46 - loss: 0.6900 - regression_loss: 0.5544 - classification_loss: 0.1356
 5149/10000 [==============>...............] - ETA: 38:46 - loss: 0.6900 - regression_loss: 0.5544 - classification_loss: 0.1356
 5150/10000 [==============>...............] - ETA: 38:45 - loss: 0.6900 - regression_loss: 0.5544 - classification_loss: 0.1356
 5151/10000 [==============>...............] - ETA: 38:45 - loss: 0.6900 - regression_loss: 0.5544 - classification_loss: 0.1356
 5152/10000 [==============>...............] - ETA: 38:44 - loss: 0.6900 - regression_loss: 0.5544 - classification_loss: 0.1356
 5153/10000 [==============>...............] - ETA: 38:44 - loss: 0.6900 - regression_loss: 0.5545 - classification_loss: 0.1356
 5154/10000 [==============>...............] - ETA: 38:43 - loss: 0.6901 - regression_loss: 0.5545 - classification_loss: 0.1356
 5155/10000 [==============>...............] - ETA: 38:43 - loss: 0.6900 - regression_loss: 0.5544 - classification_loss: 0.1355
 5156/10000 [==============>...............] - ETA: 38:42 - loss: 0.6899 - regression_loss: 0.5544 - classification_loss: 0.1355
 5157/10000 [==============>...............] - ETA: 38:42 - loss: 0.6899 - regression_loss: 0.5543 - classification_loss: 0.1355
 5158/10000 [==============>...............] - ETA: 38:41 - loss: 0.6898 - regression_loss: 0.5543 - classification_loss: 0.1355
 5159/10000 [==============>...............] - ETA: 38:41 - loss: 0.6898 - regression_loss: 0.5542 - classification_loss: 0.1355
 5160/10000 [==============>...............] - ETA: 38:40 - loss: 0.6896 - regression_loss: 0.5542 - classification_loss: 0.1355
 5161/10000 [==============>...............] - ETA: 38:40 - loss: 0.6896 - regression_loss: 0.5541 - classification_loss: 0.1355
 5162/10000 [==============>...............] - ETA: 38:39 - loss: 0.6899 - regression_loss: 0.5543 - classification_loss: 0.1355
 5163/10000 [==============>...............] - ETA: 38:39 - loss: 0.6899 - regression_loss: 0.5544 - classification_loss: 0.1355
 5164/10000 [==============>...............] - ETA: 38:38 - loss: 0.6899 - regression_loss: 0.5544 - classification_loss: 0.1355
 5165/10000 [==============>...............] - ETA: 38:38 - loss: 0.6901 - regression_loss: 0.5545 - classification_loss: 0.1355
 5166/10000 [==============>...............] - ETA: 38:37 - loss: 0.6900 - regression_loss: 0.5545 - classification_loss: 0.1355
 5167/10000 [==============>...............] - ETA: 38:37 - loss: 0.6899 - regression_loss: 0.5544 - classification_loss: 0.1355
 5168/10000 [==============>...............] - ETA: 38:36 - loss: 0.6901 - regression_loss: 0.5546 - classification_loss: 0.1355
 5169/10000 [==============>...............] - ETA: 38:36 - loss: 0.6901 - regression_loss: 0.5545 - classification_loss: 0.1355
 5170/10000 [==============>...............] - ETA: 38:35 - loss: 0.6900 - regression_loss: 0.5545 - classification_loss: 0.1355
 5171/10000 [==============>...............] - ETA: 38:35 - loss: 0.6899 - regression_loss: 0.5545 - classification_loss: 0.1355
 5172/10000 [==============>...............] - ETA: 38:34 - loss: 0.6899 - regression_loss: 0.5544 - classification_loss: 0.1355
 5173/10000 [==============>...............] - ETA: 38:34 - loss: 0.6899 - regression_loss: 0.5544 - classification_loss: 0.1355
 5174/10000 [==============>...............] - ETA: 38:33 - loss: 0.6899 - regression_loss: 0.5544 - classification_loss: 0.1355
 5175/10000 [==============>...............] - ETA: 38:33 - loss: 0.6898 - regression_loss: 0.5543 - classification_loss: 0.1355
 5176/10000 [==============>...............] - ETA: 38:32 - loss: 0.6897 - regression_loss: 0.5543 - classification_loss: 0.1355
 5177/10000 [==============>...............] - ETA: 38:32 - loss: 0.6897 - regression_loss: 0.5542 - classification_loss: 0.1355
 5178/10000 [==============>...............] - ETA: 38:31 - loss: 0.6896 - regression_loss: 0.5542 - classification_loss: 0.1354
 5179/10000 [==============>...............] - ETA: 38:31 - loss: 0.6896 - regression_loss: 0.5541 - classification_loss: 0.1354
 5180/10000 [==============>...............] - ETA: 38:30 - loss: 0.6896 - regression_loss: 0.5542 - classification_loss: 0.1354
 5181/10000 [==============>...............] - ETA: 38:30 - loss: 0.6896 - regression_loss: 0.5542 - classification_loss: 0.1355
 5182/10000 [==============>...............] - ETA: 38:29 - loss: 0.6895 - regression_loss: 0.5541 - classification_loss: 0.1354
 5183/10000 [==============>...............] - ETA: 38:29 - loss: 0.6895 - regression_loss: 0.5541 - classification_loss: 0.1354
 5184/10000 [==============>...............] - ETA: 38:28 - loss: 0.6894 - regression_loss: 0.5540 - classification_loss: 0.1354
 5185/10000 [==============>...............] - ETA: 38:28 - loss: 0.6894 - regression_loss: 0.5540 - classification_loss: 0.1354
 5186/10000 [==============>...............] - ETA: 38:27 - loss: 0.6894 - regression_loss: 0.5540 - classification_loss: 0.1354
 5187/10000 [==============>...............] - ETA: 38:27 - loss: 0.6893 - regression_loss: 0.5539 - classification_loss: 0.1354
 5188/10000 [==============>...............] - ETA: 38:26 - loss: 0.6893 - regression_loss: 0.5539 - classification_loss: 0.1354
 5189/10000 [==============>...............] - ETA: 38:26 - loss: 0.6892 - regression_loss: 0.5539 - classification_loss: 0.1354
 5190/10000 [==============>...............] - ETA: 38:25 - loss: 0.6893 - regression_loss: 0.5539 - classification_loss: 0.1354
 5191/10000 [==============>...............] - ETA: 38:25 - loss: 0.6894 - regression_loss: 0.5540 - classification_loss: 0.1354
 5192/10000 [==============>...............] - ETA: 38:24 - loss: 0.6893 - regression_loss: 0.5539 - classification_loss: 0.1354
 5193/10000 [==============>...............] - ETA: 38:24 - loss: 0.6893 - regression_loss: 0.5539 - classification_loss: 0.1354
 5194/10000 [==============>...............] - ETA: 38:23 - loss: 0.6894 - regression_loss: 0.5540 - classification_loss: 0.1354
 5195/10000 [==============>...............] - ETA: 38:23 - loss: 0.6893 - regression_loss: 0.5539 - classification_loss: 0.1353
 5196/10000 [==============>...............] - ETA: 38:22 - loss: 0.6894 - regression_loss: 0.5540 - classification_loss: 0.1354
 5197/10000 [==============>...............] - ETA: 38:22 - loss: 0.6894 - regression_loss: 0.5540 - classification_loss: 0.1354
 5198/10000 [==============>...............] - ETA: 38:21 - loss: 0.6893 - regression_loss: 0.5540 - classification_loss: 0.1354
 5199/10000 [==============>...............] - ETA: 38:21 - loss: 0.6895 - regression_loss: 0.5541 - classification_loss: 0.1354
 5200/10000 [==============>...............] - ETA: 38:20 - loss: 0.6895 - regression_loss: 0.5541 - classification_loss: 0.1354
 5201/10000 [==============>...............] - ETA: 38:20 - loss: 0.6896 - regression_loss: 0.5542 - classification_loss: 0.1354
 5202/10000 [==============>...............] - ETA: 38:19 - loss: 0.6897 - regression_loss: 0.5543 - classification_loss: 0.1354
 5203/10000 [==============>...............] - ETA: 38:19 - loss: 0.6897 - regression_loss: 0.5542 - classification_loss: 0.1354
 5204/10000 [==============>...............] - ETA: 38:18 - loss: 0.6897 - regression_loss: 0.5542 - classification_loss: 0.1354
 5205/10000 [==============>...............] - ETA: 38:18 - loss: 0.6896 - regression_loss: 0.5542 - classification_loss: 0.1354
 5206/10000 [==============>...............] - ETA: 38:17 - loss: 0.6896 - regression_loss: 0.5542 - classification_loss: 0.1354
 5207/10000 [==============>...............] - ETA: 38:17 - loss: 0.6896 - regression_loss: 0.5542 - classification_loss: 0.1354
 5208/10000 [==============>...............] - ETA: 38:16 - loss: 0.6895 - regression_loss: 0.5541 - classification_loss: 0.1354
 5209/10000 [==============>...............] - ETA: 38:16 - loss: 0.6897 - regression_loss: 0.5542 - classification_loss: 0.1354
 5210/10000 [==============>...............] - ETA: 38:15 - loss: 0.6897 - regression_loss: 0.5542 - classification_loss: 0.1355
 5211/10000 [==============>...............] - ETA: 38:15 - loss: 0.6897 - regression_loss: 0.5543 - classification_loss: 0.1355
 5212/10000 [==============>...............] - ETA: 38:14 - loss: 0.6896 - regression_loss: 0.5542 - classification_loss: 0.1354
 5213/10000 [==============>...............] - ETA: 38:14 - loss: 0.6897 - regression_loss: 0.5543 - classification_loss: 0.1354
 5214/10000 [==============>...............] - ETA: 38:13 - loss: 0.6897 - regression_loss: 0.5543 - classification_loss: 0.1354
 5215/10000 [==============>...............] - ETA: 38:13 - loss: 0.6897 - regression_loss: 0.5543 - classification_loss: 0.1354
 5216/10000 [==============>...............] - ETA: 38:12 - loss: 0.6896 - regression_loss: 0.5542 - classification_loss: 0.1354
 5217/10000 [==============>...............] - ETA: 38:12 - loss: 0.6895 - regression_loss: 0.5541 - classification_loss: 0.1354
 5218/10000 [==============>...............] - ETA: 38:11 - loss: 0.6894 - regression_loss: 0.5540 - classification_loss: 0.1354
 5219/10000 [==============>...............] - ETA: 38:11 - loss: 0.6893 - regression_loss: 0.5540 - classification_loss: 0.1354
 5220/10000 [==============>...............] - ETA: 38:10 - loss: 0.6892 - regression_loss: 0.5539 - classification_loss: 0.1353
 5221/10000 [==============>...............] - ETA: 38:10 - loss: 0.6892 - regression_loss: 0.5539 - classification_loss: 0.1353
 5222/10000 [==============>...............] - ETA: 38:09 - loss: 0.6893 - regression_loss: 0.5540 - classification_loss: 0.1353
 5223/10000 [==============>...............] - ETA: 38:09 - loss: 0.6894 - regression_loss: 0.5541 - classification_loss: 0.1353
 5224/10000 [==============>...............] - ETA: 38:08 - loss: 0.6894 - regression_loss: 0.5541 - classification_loss: 0.1353
 5225/10000 [==============>...............] - ETA: 38:08 - loss: 0.6893 - regression_loss: 0.5540 - classification_loss: 0.1353
 5226/10000 [==============>...............] - ETA: 38:07 - loss: 0.6892 - regression_loss: 0.5539 - classification_loss: 0.1353
 5227/10000 [==============>...............] - ETA: 38:07 - loss: 0.6892 - regression_loss: 0.5539 - classification_loss: 0.1353
 5228/10000 [==============>...............] - ETA: 38:06 - loss: 0.6891 - regression_loss: 0.5538 - classification_loss: 0.1353
 5229/10000 [==============>...............] - ETA: 38:06 - loss: 0.6890 - regression_loss: 0.5537 - classification_loss: 0.1353
 5230/10000 [==============>...............] - ETA: 38:05 - loss: 0.6891 - regression_loss: 0.5538 - classification_loss: 0.1353
 5231/10000 [==============>...............] - ETA: 38:05 - loss: 0.6891 - regression_loss: 0.5539 - classification_loss: 0.1353
 5232/10000 [==============>...............] - ETA: 38:04 - loss: 0.6892 - regression_loss: 0.5539 - classification_loss: 0.1353
 5233/10000 [==============>...............] - ETA: 38:04 - loss: 0.6892 - regression_loss: 0.5539 - classification_loss: 0.1353
 5234/10000 [==============>...............] - ETA: 38:03 - loss: 0.6891 - regression_loss: 0.5539 - classification_loss: 0.1353
 5235/10000 [==============>...............] - ETA: 38:03 - loss: 0.6893 - regression_loss: 0.5540 - classification_loss: 0.1353
 5236/10000 [==============>...............] - ETA: 38:02 - loss: 0.6892 - regression_loss: 0.5540 - classification_loss: 0.1353
 5237/10000 [==============>...............] - ETA: 38:02 - loss: 0.6893 - regression_loss: 0.5540 - classification_loss: 0.1353
 5238/10000 [==============>...............] - ETA: 38:01 - loss: 0.6893 - regression_loss: 0.5540 - classification_loss: 0.1353
 5239/10000 [==============>...............] - ETA: 38:01 - loss: 0.6893 - regression_loss: 0.5540 - classification_loss: 0.1353
 5240/10000 [==============>...............] - ETA: 38:00 - loss: 0.6893 - regression_loss: 0.5540 - classification_loss: 0.1353
 5241/10000 [==============>...............] - ETA: 38:00 - loss: 0.6892 - regression_loss: 0.5539 - classification_loss: 0.1353
 5242/10000 [==============>...............] - ETA: 37:59 - loss: 0.6891 - regression_loss: 0.5538 - classification_loss: 0.1353
 5243/10000 [==============>...............] - ETA: 37:59 - loss: 0.6893 - regression_loss: 0.5540 - classification_loss: 0.1353
 5244/10000 [==============>...............] - ETA: 37:58 - loss: 0.6892 - regression_loss: 0.5539 - classification_loss: 0.1353
 5245/10000 [==============>...............] - ETA: 37:58 - loss: 0.6891 - regression_loss: 0.5539 - classification_loss: 0.1353
 5246/10000 [==============>...............] - ETA: 37:57 - loss: 0.6893 - regression_loss: 0.5540 - classification_loss: 0.1353
 5247/10000 [==============>...............] - ETA: 37:57 - loss: 0.6895 - regression_loss: 0.5542 - classification_loss: 0.1354
 5248/10000 [==============>...............] - ETA: 37:57 - loss: 0.6895 - regression_loss: 0.5542 - classification_loss: 0.1353
 5249/10000 [==============>...............] - ETA: 37:56 - loss: 0.6895 - regression_loss: 0.5542 - classification_loss: 0.1353
 5250/10000 [==============>...............] - ETA: 37:56 - loss: 0.6896 - regression_loss: 0.5542 - classification_loss: 0.1354
 5251/10000 [==============>...............] - ETA: 37:55 - loss: 0.6895 - regression_loss: 0.5542 - classification_loss: 0.1354
 5252/10000 [==============>...............] - ETA: 37:55 - loss: 0.6894 - regression_loss: 0.5541 - classification_loss: 0.1353
 5253/10000 [==============>...............] - ETA: 37:54 - loss: 0.6894 - regression_loss: 0.5541 - classification_loss: 0.1353
 5254/10000 [==============>...............] - ETA: 37:54 - loss: 0.6895 - regression_loss: 0.5542 - classification_loss: 0.1353
 5255/10000 [==============>...............] - ETA: 37:53 - loss: 0.6895 - regression_loss: 0.5542 - classification_loss: 0.1353
 5256/10000 [==============>...............] - ETA: 37:53 - loss: 0.6894 - regression_loss: 0.5541 - classification_loss: 0.1353
 5257/10000 [==============>...............] - ETA: 37:52 - loss: 0.6895 - regression_loss: 0.5542 - classification_loss: 0.1353
 5258/10000 [==============>...............] - ETA: 37:52 - loss: 0.6896 - regression_loss: 0.5542 - classification_loss: 0.1353
 5259/10000 [==============>...............] - ETA: 37:51 - loss: 0.6895 - regression_loss: 0.5542 - classification_loss: 0.1353
 5260/10000 [==============>...............] - ETA: 37:51 - loss: 0.6895 - regression_loss: 0.5542 - classification_loss: 0.1353
 5261/10000 [==============>...............] - ETA: 37:50 - loss: 0.6894 - regression_loss: 0.5541 - classification_loss: 0.1353
 5262/10000 [==============>...............] - ETA: 37:50 - loss: 0.6894 - regression_loss: 0.5541 - classification_loss: 0.1353
 5263/10000 [==============>...............] - ETA: 37:49 - loss: 0.6894 - regression_loss: 0.5541 - classification_loss: 0.1353
 5264/10000 [==============>...............] - ETA: 37:49 - loss: 0.6893 - regression_loss: 0.5540 - classification_loss: 0.1353
 5265/10000 [==============>...............] - ETA: 37:48 - loss: 0.6892 - regression_loss: 0.5540 - classification_loss: 0.1353
 5266/10000 [==============>...............] - ETA: 37:48 - loss: 0.6892 - regression_loss: 0.5539 - classification_loss: 0.1353
 5267/10000 [==============>...............] - ETA: 37:47 - loss: 0.6891 - regression_loss: 0.5539 - classification_loss: 0.1352
 5268/10000 [==============>...............] - ETA: 37:47 - loss: 0.6892 - regression_loss: 0.5539 - classification_loss: 0.1353
 5269/10000 [==============>...............] - ETA: 37:46 - loss: 0.6892 - regression_loss: 0.5540 - classification_loss: 0.1352
 5270/10000 [==============>...............] - ETA: 37:46 - loss: 0.6893 - regression_loss: 0.5540 - classification_loss: 0.1352
 5271/10000 [==============>...............] - ETA: 37:45 - loss: 0.6893 - regression_loss: 0.5541 - classification_loss: 0.1352
 5272/10000 [==============>...............] - ETA: 37:45 - loss: 0.6893 - regression_loss: 0.5540 - classification_loss: 0.1352
 5273/10000 [==============>...............] - ETA: 37:44 - loss: 0.6892 - regression_loss: 0.5540 - classification_loss: 0.1352
 5274/10000 [==============>...............] - ETA: 37:44 - loss: 0.6891 - regression_loss: 0.5539 - classification_loss: 0.1352
 5275/10000 [==============>...............] - ETA: 37:43 - loss: 0.6890 - regression_loss: 0.5538 - classification_loss: 0.1352
 5276/10000 [==============>...............] - ETA: 37:43 - loss: 0.6890 - regression_loss: 0.5538 - classification_loss: 0.1352
 5277/10000 [==============>...............] - ETA: 37:42 - loss: 0.6889 - regression_loss: 0.5537 - classification_loss: 0.1352
 5278/10000 [==============>...............] - ETA: 37:42 - loss: 0.6888 - regression_loss: 0.5536 - classification_loss: 0.1352
 5279/10000 [==============>...............] - ETA: 37:41 - loss: 0.6887 - regression_loss: 0.5536 - classification_loss: 0.1352
 5280/10000 [==============>...............] - ETA: 37:41 - loss: 0.6886 - regression_loss: 0.5535 - classification_loss: 0.1351
 5281/10000 [==============>...............] - ETA: 37:40 - loss: 0.6885 - regression_loss: 0.5534 - classification_loss: 0.1351
 5282/10000 [==============>...............] - ETA: 37:40 - loss: 0.6886 - regression_loss: 0.5535 - classification_loss: 0.1351
 5283/10000 [==============>...............] - ETA: 37:39 - loss: 0.6885 - regression_loss: 0.5534 - classification_loss: 0.1351
 5284/10000 [==============>...............] - ETA: 37:39 - loss: 0.6885 - regression_loss: 0.5534 - classification_loss: 0.1351
 5285/10000 [==============>...............] - ETA: 37:38 - loss: 0.6886 - regression_loss: 0.5534 - classification_loss: 0.1351
 5286/10000 [==============>...............] - ETA: 37:38 - loss: 0.6887 - regression_loss: 0.5535 - classification_loss: 0.1351
 5287/10000 [==============>...............] - ETA: 37:37 - loss: 0.6885 - regression_loss: 0.5534 - classification_loss: 0.1351
 5288/10000 [==============>...............] - ETA: 37:37 - loss: 0.6885 - regression_loss: 0.5534 - classification_loss: 0.1351
 5289/10000 [==============>...............] - ETA: 37:36 - loss: 0.6885 - regression_loss: 0.5533 - classification_loss: 0.1351
 5290/10000 [==============>...............] - ETA: 37:36 - loss: 0.6884 - regression_loss: 0.5533 - classification_loss: 0.1351
 5291/10000 [==============>...............] - ETA: 37:35 - loss: 0.6884 - regression_loss: 0.5533 - classification_loss: 0.1351
 5292/10000 [==============>...............] - ETA: 37:35 - loss: 0.6884 - regression_loss: 0.5533 - classification_loss: 0.1351
 5293/10000 [==============>...............] - ETA: 37:34 - loss: 0.6884 - regression_loss: 0.5533 - classification_loss: 0.1351
 5294/10000 [==============>...............] - ETA: 37:34 - loss: 0.6883 - regression_loss: 0.5532 - classification_loss: 0.1351
 5295/10000 [==============>...............] - ETA: 37:34 - loss: 0.6882 - regression_loss: 0.5532 - classification_loss: 0.1350
 5296/10000 [==============>...............] - ETA: 37:33 - loss: 0.6882 - regression_loss: 0.5532 - classification_loss: 0.1350
 5297/10000 [==============>...............] - ETA: 37:33 - loss: 0.6882 - regression_loss: 0.5532 - classification_loss: 0.1350
 5298/10000 [==============>...............] - ETA: 37:32 - loss: 0.6883 - regression_loss: 0.5533 - classification_loss: 0.1350
 5299/10000 [==============>...............] - ETA: 37:32 - loss: 0.6883 - regression_loss: 0.5533 - classification_loss: 0.1350
 5300/10000 [==============>...............] - ETA: 37:31 - loss: 0.6882 - regression_loss: 0.5532 - classification_loss: 0.1350
 5301/10000 [==============>...............] - ETA: 37:31 - loss: 0.6882 - regression_loss: 0.5532 - classification_loss: 0.1350
 5302/10000 [==============>...............] - ETA: 37:30 - loss: 0.6881 - regression_loss: 0.5531 - classification_loss: 0.1350
 5303/10000 [==============>...............] - ETA: 37:30 - loss: 0.6880 - regression_loss: 0.5530 - classification_loss: 0.1350
 5304/10000 [==============>...............] - ETA: 37:29 - loss: 0.6880 - regression_loss: 0.5530 - classification_loss: 0.1350
 5305/10000 [==============>...............] - ETA: 37:29 - loss: 0.6878 - regression_loss: 0.5529 - classification_loss: 0.1349
 5306/10000 [==============>...............] - ETA: 37:28 - loss: 0.6878 - regression_loss: 0.5529 - classification_loss: 0.1349
 5307/10000 [==============>...............] - ETA: 37:28 - loss: 0.6877 - regression_loss: 0.5528 - classification_loss: 0.1349
 5308/10000 [==============>...............] - ETA: 37:27 - loss: 0.6879 - regression_loss: 0.5530 - classification_loss: 0.1349
 5309/10000 [==============>...............] - ETA: 37:27 - loss: 0.6880 - regression_loss: 0.5531 - classification_loss: 0.1349
 5310/10000 [==============>...............] - ETA: 37:26 - loss: 0.6879 - regression_loss: 0.5530 - classification_loss: 0.1349
 5311/10000 [==============>...............] - ETA: 37:26 - loss: 0.6881 - regression_loss: 0.5532 - classification_loss: 0.1349
 5312/10000 [==============>...............] - ETA: 37:25 - loss: 0.6880 - regression_loss: 0.5531 - classification_loss: 0.1349
 5313/10000 [==============>...............] - ETA: 37:25 - loss: 0.6880 - regression_loss: 0.5531 - classification_loss: 0.1349
 5314/10000 [==============>...............] - ETA: 37:24 - loss: 0.6881 - regression_loss: 0.5532 - classification_loss: 0.1349
 5315/10000 [==============>...............] - ETA: 37:24 - loss: 0.6880 - regression_loss: 0.5531 - classification_loss: 0.1349
 5316/10000 [==============>...............] - ETA: 37:23 - loss: 0.6879 - regression_loss: 0.5530 - classification_loss: 0.1349
 5317/10000 [==============>...............] - ETA: 37:23 - loss: 0.6879 - regression_loss: 0.5530 - classification_loss: 0.1349
 5318/10000 [==============>...............] - ETA: 37:22 - loss: 0.6878 - regression_loss: 0.5529 - classification_loss: 0.1349
 5319/10000 [==============>...............] - ETA: 37:22 - loss: 0.6878 - regression_loss: 0.5529 - classification_loss: 0.1348
 5320/10000 [==============>...............] - ETA: 37:21 - loss: 0.6879 - regression_loss: 0.5530 - classification_loss: 0.1349
 5321/10000 [==============>...............] - ETA: 37:21 - loss: 0.6878 - regression_loss: 0.5530 - classification_loss: 0.1348
 5322/10000 [==============>...............] - ETA: 37:20 - loss: 0.6878 - regression_loss: 0.5529 - classification_loss: 0.1348
 5323/10000 [==============>...............] - ETA: 37:20 - loss: 0.6877 - regression_loss: 0.5529 - classification_loss: 0.1348
 5324/10000 [==============>...............] - ETA: 37:19 - loss: 0.6877 - regression_loss: 0.5529 - classification_loss: 0.1348
 5325/10000 [==============>...............] - ETA: 37:19 - loss: 0.6877 - regression_loss: 0.5529 - classification_loss: 0.1348
 5326/10000 [==============>...............] - ETA: 37:18 - loss: 0.6877 - regression_loss: 0.5528 - classification_loss: 0.1348
 5327/10000 [==============>...............] - ETA: 37:18 - loss: 0.6876 - regression_loss: 0.5528 - classification_loss: 0.1348
 5328/10000 [==============>...............] - ETA: 37:17 - loss: 0.6875 - regression_loss: 0.5527 - classification_loss: 0.1348
 5329/10000 [==============>...............] - ETA: 37:17 - loss: 0.6874 - regression_loss: 0.5526 - classification_loss: 0.1348
 5330/10000 [==============>...............] - ETA: 37:16 - loss: 0.6875 - regression_loss: 0.5527 - classification_loss: 0.1348
 5331/10000 [==============>...............] - ETA: 37:16 - loss: 0.6874 - regression_loss: 0.5527 - classification_loss: 0.1348
 5332/10000 [==============>...............] - ETA: 37:15 - loss: 0.6876 - regression_loss: 0.5528 - classification_loss: 0.1348
 5333/10000 [==============>...............] - ETA: 37:15 - loss: 0.6877 - regression_loss: 0.5529 - classification_loss: 0.1348
 5334/10000 [===============>..............] - ETA: 37:14 - loss: 0.6877 - regression_loss: 0.5529 - classification_loss: 0.1348
 5335/10000 [===============>..............] - ETA: 37:14 - loss: 0.6877 - regression_loss: 0.5529 - classification_loss: 0.1348
 5336/10000 [===============>..............] - ETA: 37:13 - loss: 0.6876 - regression_loss: 0.5528 - classification_loss: 0.1348
 5337/10000 [===============>..............] - ETA: 37:13 - loss: 0.6876 - regression_loss: 0.5528 - classification_loss: 0.1348
 5338/10000 [===============>..............] - ETA: 37:12 - loss: 0.6876 - regression_loss: 0.5528 - classification_loss: 0.1348
 5339/10000 [===============>..............] - ETA: 37:12 - loss: 0.6876 - regression_loss: 0.5528 - classification_loss: 0.1348
 5340/10000 [===============>..............] - ETA: 37:11 - loss: 0.6875 - regression_loss: 0.5527 - classification_loss: 0.1348
 5341/10000 [===============>..............] - ETA: 37:11 - loss: 0.6875 - regression_loss: 0.5527 - classification_loss: 0.1348
 5342/10000 [===============>..............] - ETA: 37:10 - loss: 0.6876 - regression_loss: 0.5528 - classification_loss: 0.1348
 5343/10000 [===============>..............] - ETA: 37:10 - loss: 0.6876 - regression_loss: 0.5528 - classification_loss: 0.1348
 5344/10000 [===============>..............] - ETA: 37:09 - loss: 0.6875 - regression_loss: 0.5527 - classification_loss: 0.1348
 5345/10000 [===============>..............] - ETA: 37:09 - loss: 0.6875 - regression_loss: 0.5527 - classification_loss: 0.1348
 5346/10000 [===============>..............] - ETA: 37:08 - loss: 0.6875 - regression_loss: 0.5527 - classification_loss: 0.1348
 5347/10000 [===============>..............] - ETA: 37:08 - loss: 0.6874 - regression_loss: 0.5526 - classification_loss: 0.1348
 5348/10000 [===============>..............] - ETA: 37:07 - loss: 0.6874 - regression_loss: 0.5526 - classification_loss: 0.1348
 5349/10000 [===============>..............] - ETA: 37:07 - loss: 0.6873 - regression_loss: 0.5525 - classification_loss: 0.1348
 5350/10000 [===============>..............] - ETA: 37:06 - loss: 0.6873 - regression_loss: 0.5525 - classification_loss: 0.1348
 5351/10000 [===============>..............] - ETA: 37:06 - loss: 0.6873 - regression_loss: 0.5525 - classification_loss: 0.1348
 5352/10000 [===============>..............] - ETA: 37:05 - loss: 0.6873 - regression_loss: 0.5525 - classification_loss: 0.1348
 5353/10000 [===============>..............] - ETA: 37:05 - loss: 0.6874 - regression_loss: 0.5526 - classification_loss: 0.1348
 5354/10000 [===============>..............] - ETA: 37:04 - loss: 0.6873 - regression_loss: 0.5525 - classification_loss: 0.1348
 5355/10000 [===============>..............] - ETA: 37:04 - loss: 0.6874 - regression_loss: 0.5526 - classification_loss: 0.1348
 5356/10000 [===============>..............] - ETA: 37:03 - loss: 0.6873 - regression_loss: 0.5526 - classification_loss: 0.1348
 5357/10000 [===============>..............] - ETA: 37:03 - loss: 0.6873 - regression_loss: 0.5525 - classification_loss: 0.1348
 5358/10000 [===============>..............] - ETA: 37:02 - loss: 0.6872 - regression_loss: 0.5525 - classification_loss: 0.1347
 5359/10000 [===============>..............] - ETA: 37:02 - loss: 0.6873 - regression_loss: 0.5526 - classification_loss: 0.1347
 5360/10000 [===============>..............] - ETA: 37:01 - loss: 0.6873 - regression_loss: 0.5525 - classification_loss: 0.1347
 5361/10000 [===============>..............] - ETA: 37:01 - loss: 0.6874 - regression_loss: 0.5526 - classification_loss: 0.1348
 5362/10000 [===============>..............] - ETA: 37:00 - loss: 0.6875 - regression_loss: 0.5527 - classification_loss: 0.1348
 5363/10000 [===============>..............] - ETA: 37:00 - loss: 0.6874 - regression_loss: 0.5526 - classification_loss: 0.1348
 5364/10000 [===============>..............] - ETA: 36:59 - loss: 0.6875 - regression_loss: 0.5527 - classification_loss: 0.1348
 5365/10000 [===============>..............] - ETA: 36:59 - loss: 0.6874 - regression_loss: 0.5526 - classification_loss: 0.1348
 5366/10000 [===============>..............] - ETA: 36:58 - loss: 0.6874 - regression_loss: 0.5526 - classification_loss: 0.1348
 5367/10000 [===============>..............] - ETA: 36:58 - loss: 0.6874 - regression_loss: 0.5525 - classification_loss: 0.1348
 5368/10000 [===============>..............] - ETA: 36:57 - loss: 0.6873 - regression_loss: 0.5524 - classification_loss: 0.1348
 5369/10000 [===============>..............] - ETA: 36:57 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5370/10000 [===============>..............] - ETA: 36:56 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5371/10000 [===============>..............] - ETA: 36:56 - loss: 0.6871 - regression_loss: 0.5523 - classification_loss: 0.1348
 5372/10000 [===============>..............] - ETA: 36:55 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5373/10000 [===============>..............] - ETA: 36:55 - loss: 0.6871 - regression_loss: 0.5523 - classification_loss: 0.1348
 5374/10000 [===============>..............] - ETA: 36:54 - loss: 0.6871 - regression_loss: 0.5523 - classification_loss: 0.1348
 5375/10000 [===============>..............] - ETA: 36:54 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5376/10000 [===============>..............] - ETA: 36:53 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5377/10000 [===============>..............] - ETA: 36:53 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1347
 5378/10000 [===============>..............] - ETA: 36:52 - loss: 0.6869 - regression_loss: 0.5522 - classification_loss: 0.1348
 5379/10000 [===============>..............] - ETA: 36:52 - loss: 0.6868 - regression_loss: 0.5521 - classification_loss: 0.1348
 5380/10000 [===============>..............] - ETA: 36:51 - loss: 0.6868 - regression_loss: 0.5521 - classification_loss: 0.1348
 5381/10000 [===============>..............] - ETA: 36:51 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5382/10000 [===============>..............] - ETA: 36:50 - loss: 0.6868 - regression_loss: 0.5521 - classification_loss: 0.1347
 5383/10000 [===============>..............] - ETA: 36:50 - loss: 0.6868 - regression_loss: 0.5520 - classification_loss: 0.1347
 5384/10000 [===============>..............] - ETA: 36:49 - loss: 0.6868 - regression_loss: 0.5521 - classification_loss: 0.1347
 5385/10000 [===============>..............] - ETA: 36:49 - loss: 0.6868 - regression_loss: 0.5521 - classification_loss: 0.1347
 5386/10000 [===============>..............] - ETA: 36:48 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1347
 5387/10000 [===============>..............] - ETA: 36:48 - loss: 0.6868 - regression_loss: 0.5521 - classification_loss: 0.1347
 5388/10000 [===============>..............] - ETA: 36:47 - loss: 0.6869 - regression_loss: 0.5522 - classification_loss: 0.1347
 5389/10000 [===============>..............] - ETA: 36:47 - loss: 0.6868 - regression_loss: 0.5521 - classification_loss: 0.1347
 5390/10000 [===============>..............] - ETA: 36:46 - loss: 0.6869 - regression_loss: 0.5522 - classification_loss: 0.1347
 5391/10000 [===============>..............] - ETA: 36:46 - loss: 0.6870 - regression_loss: 0.5523 - classification_loss: 0.1347
 5392/10000 [===============>..............] - ETA: 36:45 - loss: 0.6870 - regression_loss: 0.5523 - classification_loss: 0.1347
 5393/10000 [===============>..............] - ETA: 36:45 - loss: 0.6870 - regression_loss: 0.5523 - classification_loss: 0.1347
 5394/10000 [===============>..............] - ETA: 36:44 - loss: 0.6869 - regression_loss: 0.5522 - classification_loss: 0.1347
 5395/10000 [===============>..............] - ETA: 36:44 - loss: 0.6869 - regression_loss: 0.5522 - classification_loss: 0.1347
 5396/10000 [===============>..............] - ETA: 36:43 - loss: 0.6869 - regression_loss: 0.5522 - classification_loss: 0.1347
 5397/10000 [===============>..............] - ETA: 36:43 - loss: 0.6868 - regression_loss: 0.5521 - classification_loss: 0.1347
 5398/10000 [===============>..............] - ETA: 36:42 - loss: 0.6869 - regression_loss: 0.5522 - classification_loss: 0.1347
 5399/10000 [===============>..............] - ETA: 36:42 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5400/10000 [===============>..............] - ETA: 36:41 - loss: 0.6873 - regression_loss: 0.5525 - classification_loss: 0.1348
 5401/10000 [===============>..............] - ETA: 36:41 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5402/10000 [===============>..............] - ETA: 36:40 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5403/10000 [===============>..............] - ETA: 36:40 - loss: 0.6873 - regression_loss: 0.5524 - classification_loss: 0.1349
 5404/10000 [===============>..............] - ETA: 36:39 - loss: 0.6873 - regression_loss: 0.5524 - classification_loss: 0.1349
 5405/10000 [===============>..............] - ETA: 36:39 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1349
 5406/10000 [===============>..............] - ETA: 36:38 - loss: 0.6872 - regression_loss: 0.5523 - classification_loss: 0.1349
 5407/10000 [===============>..............] - ETA: 36:38 - loss: 0.6872 - regression_loss: 0.5523 - classification_loss: 0.1349
 5408/10000 [===============>..............] - ETA: 36:37 - loss: 0.6871 - regression_loss: 0.5523 - classification_loss: 0.1349
 5409/10000 [===============>..............] - ETA: 36:37 - loss: 0.6871 - regression_loss: 0.5522 - classification_loss: 0.1349
 5410/10000 [===============>..............] - ETA: 36:36 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1349
 5411/10000 [===============>..............] - ETA: 36:36 - loss: 0.6871 - regression_loss: 0.5523 - classification_loss: 0.1349
 5412/10000 [===============>..............] - ETA: 36:35 - loss: 0.6871 - regression_loss: 0.5522 - classification_loss: 0.1349
 5413/10000 [===============>..............] - ETA: 36:35 - loss: 0.6871 - regression_loss: 0.5522 - classification_loss: 0.1349
 5414/10000 [===============>..............] - ETA: 36:34 - loss: 0.6870 - regression_loss: 0.5521 - classification_loss: 0.1348
 5415/10000 [===============>..............] - ETA: 36:34 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5416/10000 [===============>..............] - ETA: 36:33 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5417/10000 [===============>..............] - ETA: 36:33 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5418/10000 [===============>..............] - ETA: 36:32 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5419/10000 [===============>..............] - ETA: 36:32 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5420/10000 [===============>..............] - ETA: 36:31 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5421/10000 [===============>..............] - ETA: 36:31 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5422/10000 [===============>..............] - ETA: 36:30 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5423/10000 [===============>..............] - ETA: 36:30 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5424/10000 [===============>..............] - ETA: 36:29 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5425/10000 [===============>..............] - ETA: 36:29 - loss: 0.6869 - regression_loss: 0.5522 - classification_loss: 0.1348
 5426/10000 [===============>..............] - ETA: 36:28 - loss: 0.6868 - regression_loss: 0.5521 - classification_loss: 0.1348
 5427/10000 [===============>..............] - ETA: 36:28 - loss: 0.6868 - regression_loss: 0.5520 - classification_loss: 0.1348
 5428/10000 [===============>..............] - ETA: 36:27 - loss: 0.6869 - regression_loss: 0.5522 - classification_loss: 0.1348
 5429/10000 [===============>..............] - ETA: 36:27 - loss: 0.6868 - regression_loss: 0.5521 - classification_loss: 0.1348
 5430/10000 [===============>..............] - ETA: 36:26 - loss: 0.6868 - regression_loss: 0.5521 - classification_loss: 0.1347
 5431/10000 [===============>..............] - ETA: 36:26 - loss: 0.6868 - regression_loss: 0.5521 - classification_loss: 0.1348
 5432/10000 [===============>..............] - ETA: 36:25 - loss: 0.6868 - regression_loss: 0.5521 - classification_loss: 0.1348
 5433/10000 [===============>..............] - ETA: 36:25 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5434/10000 [===============>..............] - ETA: 36:24 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5435/10000 [===============>..............] - ETA: 36:24 - loss: 0.6869 - regression_loss: 0.5522 - classification_loss: 0.1348
 5436/10000 [===============>..............] - ETA: 36:23 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1347
 5437/10000 [===============>..............] - ETA: 36:23 - loss: 0.6868 - regression_loss: 0.5521 - classification_loss: 0.1347
 5438/10000 [===============>..............] - ETA: 36:22 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5439/10000 [===============>..............] - ETA: 36:22 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5440/10000 [===============>..............] - ETA: 36:21 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5441/10000 [===============>..............] - ETA: 36:21 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5442/10000 [===============>..............] - ETA: 36:20 - loss: 0.6871 - regression_loss: 0.5523 - classification_loss: 0.1348
 5443/10000 [===============>..............] - ETA: 36:20 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5444/10000 [===============>..............] - ETA: 36:19 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1349
 5445/10000 [===============>..............] - ETA: 36:19 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5446/10000 [===============>..............] - ETA: 36:18 - loss: 0.6869 - regression_loss: 0.5520 - classification_loss: 0.1348
 5447/10000 [===============>..............] - ETA: 36:18 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5448/10000 [===============>..............] - ETA: 36:17 - loss: 0.6868 - regression_loss: 0.5520 - classification_loss: 0.1348
 5449/10000 [===============>..............] - ETA: 36:17 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5450/10000 [===============>..............] - ETA: 36:16 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5451/10000 [===============>..............] - ETA: 36:16 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5452/10000 [===============>..............] - ETA: 36:15 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5453/10000 [===============>..............] - ETA: 36:15 - loss: 0.6871 - regression_loss: 0.5522 - classification_loss: 0.1348
 5454/10000 [===============>..............] - ETA: 36:14 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5455/10000 [===============>..............] - ETA: 36:14 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5456/10000 [===============>..............] - ETA: 36:13 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5457/10000 [===============>..............] - ETA: 36:13 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5458/10000 [===============>..............] - ETA: 36:12 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5459/10000 [===============>..............] - ETA: 36:12 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5460/10000 [===============>..............] - ETA: 36:11 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5461/10000 [===============>..............] - ETA: 36:11 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5462/10000 [===============>..............] - ETA: 36:10 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5463/10000 [===============>..............] - ETA: 36:10 - loss: 0.6871 - regression_loss: 0.5523 - classification_loss: 0.1348
 5464/10000 [===============>..............] - ETA: 36:09 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5465/10000 [===============>..............] - ETA: 36:09 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5466/10000 [===============>..............] - ETA: 36:08 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5467/10000 [===============>..............] - ETA: 36:08 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5468/10000 [===============>..............] - ETA: 36:07 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5469/10000 [===============>..............] - ETA: 36:07 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5470/10000 [===============>..............] - ETA: 36:06 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5471/10000 [===============>..............] - ETA: 36:06 - loss: 0.6870 - regression_loss: 0.5521 - classification_loss: 0.1348
 5472/10000 [===============>..............] - ETA: 36:05 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5473/10000 [===============>..............] - ETA: 36:05 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5474/10000 [===============>..............] - ETA: 36:04 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5475/10000 [===============>..............] - ETA: 36:04 - loss: 0.6871 - regression_loss: 0.5523 - classification_loss: 0.1348
 5476/10000 [===============>..............] - ETA: 36:04 - loss: 0.6871 - regression_loss: 0.5523 - classification_loss: 0.1348
 5477/10000 [===============>..............] - ETA: 36:03 - loss: 0.6871 - regression_loss: 0.5523 - classification_loss: 0.1349
 5478/10000 [===============>..............] - ETA: 36:03 - loss: 0.6871 - regression_loss: 0.5522 - classification_loss: 0.1348
 5479/10000 [===============>..............] - ETA: 36:02 - loss: 0.6872 - regression_loss: 0.5523 - classification_loss: 0.1349
 5480/10000 [===============>..............] - ETA: 36:02 - loss: 0.6873 - regression_loss: 0.5524 - classification_loss: 0.1349
 5481/10000 [===============>..............] - ETA: 36:01 - loss: 0.6875 - regression_loss: 0.5525 - classification_loss: 0.1349
 5482/10000 [===============>..............] - ETA: 36:01 - loss: 0.6875 - regression_loss: 0.5525 - classification_loss: 0.1349
 5483/10000 [===============>..............] - ETA: 36:00 - loss: 0.6875 - regression_loss: 0.5526 - classification_loss: 0.1349
 5484/10000 [===============>..............] - ETA: 35:59 - loss: 0.6875 - regression_loss: 0.5526 - classification_loss: 0.1349
 5485/10000 [===============>..............] - ETA: 35:59 - loss: 0.6874 - regression_loss: 0.5525 - classification_loss: 0.1349
 5486/10000 [===============>..............] - ETA: 35:58 - loss: 0.6874 - regression_loss: 0.5525 - classification_loss: 0.1349
 5487/10000 [===============>..............] - ETA: 35:58 - loss: 0.6875 - regression_loss: 0.5526 - classification_loss: 0.1349
 5488/10000 [===============>..............] - ETA: 35:57 - loss: 0.6874 - regression_loss: 0.5525 - classification_loss: 0.1349
 5489/10000 [===============>..............] - ETA: 35:57 - loss: 0.6875 - regression_loss: 0.5526 - classification_loss: 0.1349
 5490/10000 [===============>..............] - ETA: 35:56 - loss: 0.6875 - regression_loss: 0.5526 - classification_loss: 0.1349
 5491/10000 [===============>..............] - ETA: 35:56 - loss: 0.6874 - regression_loss: 0.5526 - classification_loss: 0.1349
 5492/10000 [===============>..............] - ETA: 35:55 - loss: 0.6875 - regression_loss: 0.5526 - classification_loss: 0.1349
 5493/10000 [===============>..............] - ETA: 35:55 - loss: 0.6874 - regression_loss: 0.5525 - classification_loss: 0.1349
 5494/10000 [===============>..............] - ETA: 35:54 - loss: 0.6875 - regression_loss: 0.5526 - classification_loss: 0.1349
 5495/10000 [===============>..............] - ETA: 35:54 - loss: 0.6875 - regression_loss: 0.5527 - classification_loss: 0.1349
 5496/10000 [===============>..............] - ETA: 35:54 - loss: 0.6875 - regression_loss: 0.5526 - classification_loss: 0.1348
 5497/10000 [===============>..............] - ETA: 35:53 - loss: 0.6874 - regression_loss: 0.5526 - classification_loss: 0.1348
 5498/10000 [===============>..............] - ETA: 35:53 - loss: 0.6874 - regression_loss: 0.5526 - classification_loss: 0.1349
 5499/10000 [===============>..............] - ETA: 35:52 - loss: 0.6874 - regression_loss: 0.5526 - classification_loss: 0.1349
 5500/10000 [===============>..............] - ETA: 35:52 - loss: 0.6875 - regression_loss: 0.5526 - classification_loss: 0.1349
 5501/10000 [===============>..............] - ETA: 35:51 - loss: 0.6874 - regression_loss: 0.5525 - classification_loss: 0.1348
 5502/10000 [===============>..............] - ETA: 35:51 - loss: 0.6874 - regression_loss: 0.5525 - classification_loss: 0.1348
 5503/10000 [===============>..............] - ETA: 35:50 - loss: 0.6873 - regression_loss: 0.5525 - classification_loss: 0.1348
 5504/10000 [===============>..............] - ETA: 35:50 - loss: 0.6874 - regression_loss: 0.5526 - classification_loss: 0.1348
 5505/10000 [===============>..............] - ETA: 35:49 - loss: 0.6873 - regression_loss: 0.5525 - classification_loss: 0.1348
 5506/10000 [===============>..............] - ETA: 35:49 - loss: 0.6873 - regression_loss: 0.5524 - classification_loss: 0.1348
 5507/10000 [===============>..............] - ETA: 35:48 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5508/10000 [===============>..............] - ETA: 35:48 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5509/10000 [===============>..............] - ETA: 35:47 - loss: 0.6871 - regression_loss: 0.5523 - classification_loss: 0.1348
 5510/10000 [===============>..............] - ETA: 35:46 - loss: 0.6873 - regression_loss: 0.5525 - classification_loss: 0.1348
 5511/10000 [===============>..............] - ETA: 35:46 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5512/10000 [===============>..............] - ETA: 35:45 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5513/10000 [===============>..............] - ETA: 35:45 - loss: 0.6871 - regression_loss: 0.5523 - classification_loss: 0.1348
 5514/10000 [===============>..............] - ETA: 35:44 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5515/10000 [===============>..............] - ETA: 35:44 - loss: 0.6871 - regression_loss: 0.5523 - classification_loss: 0.1348
 5516/10000 [===============>..............] - ETA: 35:43 - loss: 0.6871 - regression_loss: 0.5523 - classification_loss: 0.1348
 5517/10000 [===============>..............] - ETA: 35:43 - loss: 0.6871 - regression_loss: 0.5524 - classification_loss: 0.1348
 5518/10000 [===============>..............] - ETA: 35:42 - loss: 0.6871 - regression_loss: 0.5523 - classification_loss: 0.1348
 5519/10000 [===============>..............] - ETA: 35:42 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5520/10000 [===============>..............] - ETA: 35:42 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5521/10000 [===============>..............] - ETA: 35:41 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5522/10000 [===============>..............] - ETA: 35:41 - loss: 0.6871 - regression_loss: 0.5523 - classification_loss: 0.1348
 5523/10000 [===============>..............] - ETA: 35:40 - loss: 0.6871 - regression_loss: 0.5524 - classification_loss: 0.1348
 5524/10000 [===============>..............] - ETA: 35:40 - loss: 0.6871 - regression_loss: 0.5523 - classification_loss: 0.1347
 5525/10000 [===============>..............] - ETA: 35:39 - loss: 0.6871 - regression_loss: 0.5523 - classification_loss: 0.1347
 5526/10000 [===============>..............] - ETA: 35:39 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5527/10000 [===============>..............] - ETA: 35:38 - loss: 0.6872 - regression_loss: 0.5525 - classification_loss: 0.1348
 5528/10000 [===============>..............] - ETA: 35:38 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5529/10000 [===============>..............] - ETA: 35:37 - loss: 0.6873 - regression_loss: 0.5525 - classification_loss: 0.1348
 5530/10000 [===============>..............] - ETA: 35:37 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5531/10000 [===============>..............] - ETA: 35:36 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5532/10000 [===============>..............] - ETA: 35:36 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5533/10000 [===============>..............] - ETA: 35:35 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5534/10000 [===============>..............] - ETA: 35:35 - loss: 0.6873 - regression_loss: 0.5525 - classification_loss: 0.1348
 5535/10000 [===============>..............] - ETA: 35:34 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5536/10000 [===============>..............] - ETA: 35:34 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1349
 5537/10000 [===============>..............] - ETA: 35:33 - loss: 0.6873 - regression_loss: 0.5524 - classification_loss: 0.1349
 5538/10000 [===============>..............] - ETA: 35:33 - loss: 0.6873 - regression_loss: 0.5524 - classification_loss: 0.1349
 5539/10000 [===============>..............] - ETA: 35:32 - loss: 0.6873 - regression_loss: 0.5524 - classification_loss: 0.1349
 5540/10000 [===============>..............] - ETA: 35:32 - loss: 0.6872 - regression_loss: 0.5523 - classification_loss: 0.1348
 5541/10000 [===============>..............] - ETA: 35:31 - loss: 0.6873 - regression_loss: 0.5524 - classification_loss: 0.1349
 5542/10000 [===============>..............] - ETA: 35:31 - loss: 0.6873 - regression_loss: 0.5525 - classification_loss: 0.1349
 5543/10000 [===============>..............] - ETA: 35:30 - loss: 0.6874 - regression_loss: 0.5525 - classification_loss: 0.1349
 5544/10000 [===============>..............] - ETA: 35:30 - loss: 0.6873 - regression_loss: 0.5525 - classification_loss: 0.1349
 5545/10000 [===============>..............] - ETA: 35:29 - loss: 0.6874 - regression_loss: 0.5525 - classification_loss: 0.1349
 5546/10000 [===============>..............] - ETA: 35:29 - loss: 0.6874 - regression_loss: 0.5525 - classification_loss: 0.1349
 5547/10000 [===============>..............] - ETA: 35:28 - loss: 0.6873 - regression_loss: 0.5525 - classification_loss: 0.1349
 5548/10000 [===============>..............] - ETA: 35:28 - loss: 0.6873 - regression_loss: 0.5524 - classification_loss: 0.1349
 5549/10000 [===============>..............] - ETA: 35:27 - loss: 0.6872 - regression_loss: 0.5523 - classification_loss: 0.1348
 5550/10000 [===============>..............] - ETA: 35:27 - loss: 0.6872 - regression_loss: 0.5523 - classification_loss: 0.1349
 5551/10000 [===============>..............] - ETA: 35:26 - loss: 0.6873 - regression_loss: 0.5524 - classification_loss: 0.1349
 5552/10000 [===============>..............] - ETA: 35:26 - loss: 0.6874 - regression_loss: 0.5525 - classification_loss: 0.1349
 5553/10000 [===============>..............] - ETA: 35:25 - loss: 0.6874 - regression_loss: 0.5526 - classification_loss: 0.1349
 5554/10000 [===============>..............] - ETA: 35:25 - loss: 0.6874 - regression_loss: 0.5525 - classification_loss: 0.1349
 5555/10000 [===============>..............] - ETA: 35:24 - loss: 0.6875 - regression_loss: 0.5526 - classification_loss: 0.1349
 5556/10000 [===============>..............] - ETA: 35:24 - loss: 0.6875 - regression_loss: 0.5527 - classification_loss: 0.1349
 5557/10000 [===============>..............] - ETA: 35:23 - loss: 0.6874 - regression_loss: 0.5526 - classification_loss: 0.1348
 5558/10000 [===============>..............] - ETA: 35:23 - loss: 0.6874 - regression_loss: 0.5526 - classification_loss: 0.1348
 5559/10000 [===============>..............] - ETA: 35:22 - loss: 0.6874 - regression_loss: 0.5525 - classification_loss: 0.1349
 5560/10000 [===============>..............] - ETA: 35:22 - loss: 0.6874 - regression_loss: 0.5525 - classification_loss: 0.1348
 5561/10000 [===============>..............] - ETA: 35:21 - loss: 0.6875 - regression_loss: 0.5526 - classification_loss: 0.1349
 5562/10000 [===============>..............] - ETA: 35:21 - loss: 0.6875 - regression_loss: 0.5526 - classification_loss: 0.1349
 5563/10000 [===============>..............] - ETA: 35:20 - loss: 0.6874 - regression_loss: 0.5525 - classification_loss: 0.1349
 5564/10000 [===============>..............] - ETA: 35:20 - loss: 0.6875 - regression_loss: 0.5526 - classification_loss: 0.1349
 5565/10000 [===============>..............] - ETA: 35:19 - loss: 0.6874 - regression_loss: 0.5526 - classification_loss: 0.1349
 5566/10000 [===============>..............] - ETA: 35:19 - loss: 0.6875 - regression_loss: 0.5526 - classification_loss: 0.1349
 5567/10000 [===============>..............] - ETA: 35:18 - loss: 0.6875 - regression_loss: 0.5526 - classification_loss: 0.1349
 5568/10000 [===============>..............] - ETA: 35:18 - loss: 0.6876 - regression_loss: 0.5527 - classification_loss: 0.1349
 5569/10000 [===============>..............] - ETA: 35:17 - loss: 0.6875 - regression_loss: 0.5526 - classification_loss: 0.1349
 5570/10000 [===============>..............] - ETA: 35:17 - loss: 0.6875 - regression_loss: 0.5526 - classification_loss: 0.1349
 5571/10000 [===============>..............] - ETA: 35:16 - loss: 0.6874 - regression_loss: 0.5526 - classification_loss: 0.1349
 5572/10000 [===============>..............] - ETA: 35:16 - loss: 0.6874 - regression_loss: 0.5526 - classification_loss: 0.1349
 5573/10000 [===============>..............] - ETA: 35:15 - loss: 0.6874 - regression_loss: 0.5525 - classification_loss: 0.1349
 5574/10000 [===============>..............] - ETA: 35:15 - loss: 0.6873 - regression_loss: 0.5525 - classification_loss: 0.1348
 5575/10000 [===============>..............] - ETA: 35:14 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5576/10000 [===============>..............] - ETA: 35:14 - loss: 0.6873 - regression_loss: 0.5525 - classification_loss: 0.1348
 5577/10000 [===============>..............] - ETA: 35:13 - loss: 0.6873 - regression_loss: 0.5524 - classification_loss: 0.1348
 5578/10000 [===============>..............] - ETA: 35:13 - loss: 0.6873 - regression_loss: 0.5524 - classification_loss: 0.1349
 5579/10000 [===============>..............] - ETA: 35:12 - loss: 0.6873 - regression_loss: 0.5525 - classification_loss: 0.1349
 5580/10000 [===============>..............] - ETA: 35:12 - loss: 0.6874 - regression_loss: 0.5525 - classification_loss: 0.1349
 5581/10000 [===============>..............] - ETA: 35:11 - loss: 0.6874 - regression_loss: 0.5525 - classification_loss: 0.1349
 5582/10000 [===============>..............] - ETA: 35:11 - loss: 0.6874 - regression_loss: 0.5525 - classification_loss: 0.1349
 5583/10000 [===============>..............] - ETA: 35:10 - loss: 0.6873 - regression_loss: 0.5525 - classification_loss: 0.1349
 5584/10000 [===============>..............] - ETA: 35:10 - loss: 0.6873 - regression_loss: 0.5524 - classification_loss: 0.1348
 5585/10000 [===============>..............] - ETA: 35:09 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5586/10000 [===============>..............] - ETA: 35:09 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5587/10000 [===============>..............] - ETA: 35:08 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5588/10000 [===============>..............] - ETA: 35:08 - loss: 0.6873 - regression_loss: 0.5525 - classification_loss: 0.1348
 5589/10000 [===============>..............] - ETA: 35:07 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5590/10000 [===============>..............] - ETA: 35:07 - loss: 0.6872 - regression_loss: 0.5524 - classification_loss: 0.1348
 5591/10000 [===============>..............] - ETA: 35:06 - loss: 0.6871 - regression_loss: 0.5523 - classification_loss: 0.1348
 5592/10000 [===============>..............] - ETA: 35:06 - loss: 0.6872 - regression_loss: 0.5523 - classification_loss: 0.1348
 5593/10000 [===============>..............] - ETA: 35:05 - loss: 0.6871 - regression_loss: 0.5523 - classification_loss: 0.1348
 5594/10000 [===============>..............] - ETA: 35:05 - loss: 0.6871 - regression_loss: 0.5523 - classification_loss: 0.1348
 5595/10000 [===============>..............] - ETA: 35:04 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5596/10000 [===============>..............] - ETA: 35:04 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5597/10000 [===============>..............] - ETA: 35:04 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5598/10000 [===============>..............] - ETA: 35:03 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5599/10000 [===============>..............] - ETA: 35:03 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5600/10000 [===============>..............] - ETA: 35:02 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5601/10000 [===============>..............] - ETA: 35:02 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5602/10000 [===============>..............] - ETA: 35:01 - loss: 0.6868 - regression_loss: 0.5521 - classification_loss: 0.1348
 5603/10000 [===============>..............] - ETA: 35:01 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5604/10000 [===============>..............] - ETA: 35:00 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1347
 5605/10000 [===============>..............] - ETA: 35:00 - loss: 0.6869 - regression_loss: 0.5522 - classification_loss: 0.1347
 5606/10000 [===============>..............] - ETA: 34:59 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5607/10000 [===============>..............] - ETA: 34:59 - loss: 0.6870 - regression_loss: 0.5522 - classification_loss: 0.1348
 5608/10000 [===============>..............] - ETA: 34:58 - loss: 0.6869 - regression_loss: 0.5522 - classification_loss: 0.1348
 5609/10000 [===============>..............] - ETA: 34:58 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1347
 5610/10000 [===============>..............] - ETA: 34:57 - loss: 0.6869 - regression_loss: 0.5521 - classification_loss: 0.1348
 5611/10000 [===============>..............] - ETA: 34:57 - loss: 0.6868 - regression_loss: 0.5521 - classification_loss: 0.1347
 5612/10000 [===============>..............] - ETA: 34:56 - loss: 0.6867 - regression_loss: 0.5520 - classification_loss: 0.1347
 5613/10000 [===============>..............] - ETA: 34:56 - loss: 0.6867 - regression_loss: 0.5520 - classification_loss: 0.1347
 5614/10000 [===============>..............] - ETA: 34:55 - loss: 0.6867 - regression_loss: 0.5520 - classification_loss: 0.1347
 5615/10000 [===============>..............] - ETA: 34:55 - loss: 0.6868 - regression_loss: 0.5520 - classification_loss: 0.1347
 5616/10000 [===============>..............] - ETA: 34:54 - loss: 0.6868 - regression_loss: 0.5520 - classification_loss: 0.1347
 5617/10000 [===============>..............] - ETA: 34:54 - loss: 0.6867 - regression_loss: 0.5520 - classification_loss: 0.1347
 5618/10000 [===============>..............] - ETA: 34:53 - loss: 0.6867 - regression_loss: 0.5520 - classification_loss: 0.1347
 5619/10000 [===============>..............] - ETA: 34:53 - loss: 0.6867 - regression_loss: 0.5520 - classification_loss: 0.1347
 5620/10000 [===============>..............] - ETA: 34:52 - loss: 0.6866 - regression_loss: 0.5520 - classification_loss: 0.1347
 5621/10000 [===============>..............] - ETA: 34:52 - loss: 0.6867 - regression_loss: 0.5520 - classification_loss: 0.1347
 5622/10000 [===============>..............] - ETA: 34:51 - loss: 0.6868 - regression_loss: 0.5521 - classification_loss: 0.1347
 5623/10000 [===============>..............] - ETA: 34:51 - loss: 0.6868 - regression_loss: 0.5521 - classification_loss: 0.1347
 5624/10000 [===============>..............] - ETA: 34:50 - loss: 0.6867 - regression_loss: 0.5521 - classification_loss: 0.1347
 5625/10000 [===============>..............] - ETA: 34:50 - loss: 0.6867 - regression_loss: 0.5521 - classification_loss: 0.1347
 5626/10000 [===============>..............] - ETA: 34:49 - loss: 0.6867 - regression_loss: 0.5520 - classification_loss: 0.1347
 5627/10000 [===============>..............] - ETA: 34:49 - loss: 0.6867 - regression_loss: 0.5521 - classification_loss: 0.1346
 5628/10000 [===============>..............] - ETA: 34:48 - loss: 0.6867 - regression_loss: 0.5520 - classification_loss: 0.1346
 5629/10000 [===============>..............] - ETA: 34:48 - loss: 0.6868 - regression_loss: 0.5521 - classification_loss: 0.1347
 5630/10000 [===============>..............] - ETA: 34:47 - loss: 0.6867 - regression_loss: 0.5520 - classification_loss: 0.1346
 5631/10000 [===============>..............] - ETA: 34:47 - loss: 0.6866 - regression_loss: 0.5520 - classification_loss: 0.1346
 5632/10000 [===============>..............] - ETA: 34:46 - loss: 0.6867 - regression_loss: 0.5521 - classification_loss: 0.1346
 5633/10000 [===============>..............] - ETA: 34:46 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5634/10000 [===============>..............] - ETA: 34:45 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5635/10000 [===============>..............] - ETA: 34:45 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5636/10000 [===============>..............] - ETA: 34:44 - loss: 0.6869 - regression_loss: 0.5523 - classification_loss: 0.1346
 5637/10000 [===============>..............] - ETA: 34:44 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5638/10000 [===============>..............] - ETA: 34:43 - loss: 0.6869 - regression_loss: 0.5523 - classification_loss: 0.1346
 5639/10000 [===============>..............] - ETA: 34:43 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5640/10000 [===============>..............] - ETA: 34:42 - loss: 0.6870 - regression_loss: 0.5523 - classification_loss: 0.1346
 5641/10000 [===============>..............] - ETA: 34:42 - loss: 0.6869 - regression_loss: 0.5523 - classification_loss: 0.1346
 5642/10000 [===============>..............] - ETA: 34:41 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5643/10000 [===============>..............] - ETA: 34:41 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5644/10000 [===============>..............] - ETA: 34:40 - loss: 0.6866 - regression_loss: 0.5521 - classification_loss: 0.1346
 5645/10000 [===============>..............] - ETA: 34:40 - loss: 0.6867 - regression_loss: 0.5521 - classification_loss: 0.1346
 5646/10000 [===============>..............] - ETA: 34:39 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5647/10000 [===============>..............] - ETA: 34:39 - loss: 0.6868 - regression_loss: 0.5523 - classification_loss: 0.1346
 5648/10000 [===============>..............] - ETA: 34:38 - loss: 0.6869 - regression_loss: 0.5524 - classification_loss: 0.1346
 5649/10000 [===============>..............] - ETA: 34:38 - loss: 0.6869 - regression_loss: 0.5523 - classification_loss: 0.1346
 5650/10000 [===============>..............] - ETA: 34:37 - loss: 0.6869 - regression_loss: 0.5524 - classification_loss: 0.1346
 5651/10000 [===============>..............] - ETA: 34:37 - loss: 0.6869 - regression_loss: 0.5523 - classification_loss: 0.1346
 5652/10000 [===============>..............] - ETA: 34:36 - loss: 0.6870 - regression_loss: 0.5524 - classification_loss: 0.1346
 5653/10000 [===============>..............] - ETA: 34:36 - loss: 0.6869 - regression_loss: 0.5523 - classification_loss: 0.1346
 5654/10000 [===============>..............] - ETA: 34:35 - loss: 0.6870 - regression_loss: 0.5524 - classification_loss: 0.1346
 5655/10000 [===============>..............] - ETA: 34:35 - loss: 0.6870 - regression_loss: 0.5524 - classification_loss: 0.1346
 5656/10000 [===============>..............] - ETA: 34:34 - loss: 0.6869 - regression_loss: 0.5523 - classification_loss: 0.1345
 5657/10000 [===============>..............] - ETA: 34:34 - loss: 0.6868 - regression_loss: 0.5523 - classification_loss: 0.1346
 5658/10000 [===============>..............] - ETA: 34:33 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5659/10000 [===============>..............] - ETA: 34:33 - loss: 0.6869 - regression_loss: 0.5524 - classification_loss: 0.1346
 5660/10000 [===============>..............] - ETA: 34:32 - loss: 0.6869 - regression_loss: 0.5523 - classification_loss: 0.1345
 5661/10000 [===============>..............] - ETA: 34:32 - loss: 0.6870 - regression_loss: 0.5525 - classification_loss: 0.1346
 5662/10000 [===============>..............] - ETA: 34:31 - loss: 0.6869 - regression_loss: 0.5524 - classification_loss: 0.1345
 5663/10000 [===============>..............] - ETA: 34:31 - loss: 0.6870 - regression_loss: 0.5524 - classification_loss: 0.1345
 5664/10000 [===============>..............] - ETA: 34:30 - loss: 0.6870 - regression_loss: 0.5525 - classification_loss: 0.1345
 5665/10000 [===============>..............] - ETA: 34:30 - loss: 0.6870 - regression_loss: 0.5524 - classification_loss: 0.1345
 5666/10000 [===============>..............] - ETA: 34:29 - loss: 0.6870 - regression_loss: 0.5524 - classification_loss: 0.1345
 5667/10000 [================>.............] - ETA: 34:29 - loss: 0.6869 - regression_loss: 0.5524 - classification_loss: 0.1345
 5668/10000 [================>.............] - ETA: 34:28 - loss: 0.6870 - regression_loss: 0.5525 - classification_loss: 0.1345
 5669/10000 [================>.............] - ETA: 34:28 - loss: 0.6869 - regression_loss: 0.5524 - classification_loss: 0.1345
 5670/10000 [================>.............] - ETA: 34:27 - loss: 0.6869 - regression_loss: 0.5524 - classification_loss: 0.1345
 5671/10000 [================>.............] - ETA: 34:27 - loss: 0.6869 - regression_loss: 0.5524 - classification_loss: 0.1345
 5672/10000 [================>.............] - ETA: 34:26 - loss: 0.6870 - regression_loss: 0.5524 - classification_loss: 0.1345
 5673/10000 [================>.............] - ETA: 34:26 - loss: 0.6869 - regression_loss: 0.5524 - classification_loss: 0.1345
 5674/10000 [================>.............] - ETA: 34:25 - loss: 0.6870 - regression_loss: 0.5525 - classification_loss: 0.1345
 5675/10000 [================>.............] - ETA: 34:25 - loss: 0.6870 - regression_loss: 0.5524 - classification_loss: 0.1345
 5676/10000 [================>.............] - ETA: 34:24 - loss: 0.6869 - regression_loss: 0.5524 - classification_loss: 0.1345
 5677/10000 [================>.............] - ETA: 34:24 - loss: 0.6868 - regression_loss: 0.5523 - classification_loss: 0.1345
 5678/10000 [================>.............] - ETA: 34:23 - loss: 0.6867 - regression_loss: 0.5522 - classification_loss: 0.1345
 5679/10000 [================>.............] - ETA: 34:23 - loss: 0.6867 - regression_loss: 0.5522 - classification_loss: 0.1345
 5680/10000 [================>.............] - ETA: 34:22 - loss: 0.6869 - regression_loss: 0.5523 - classification_loss: 0.1345
 5681/10000 [================>.............] - ETA: 34:22 - loss: 0.6868 - regression_loss: 0.5523 - classification_loss: 0.1345
 5682/10000 [================>.............] - ETA: 34:21 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1345
 5683/10000 [================>.............] - ETA: 34:21 - loss: 0.6870 - regression_loss: 0.5523 - classification_loss: 0.1346
 5684/10000 [================>.............] - ETA: 34:20 - loss: 0.6870 - regression_loss: 0.5523 - classification_loss: 0.1346
 5685/10000 [================>.............] - ETA: 34:20 - loss: 0.6869 - regression_loss: 0.5523 - classification_loss: 0.1346
 5686/10000 [================>.............] - ETA: 34:19 - loss: 0.6869 - regression_loss: 0.5523 - classification_loss: 0.1346
 5687/10000 [================>.............] - ETA: 34:19 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5688/10000 [================>.............] - ETA: 34:19 - loss: 0.6870 - regression_loss: 0.5524 - classification_loss: 0.1346
 5689/10000 [================>.............] - ETA: 34:18 - loss: 0.6872 - regression_loss: 0.5525 - classification_loss: 0.1346
 5690/10000 [================>.............] - ETA: 34:18 - loss: 0.6872 - regression_loss: 0.5525 - classification_loss: 0.1346
 5691/10000 [================>.............] - ETA: 34:17 - loss: 0.6871 - regression_loss: 0.5525 - classification_loss: 0.1346
 5692/10000 [================>.............] - ETA: 34:17 - loss: 0.6871 - regression_loss: 0.5524 - classification_loss: 0.1346
 5693/10000 [================>.............] - ETA: 34:16 - loss: 0.6870 - regression_loss: 0.5524 - classification_loss: 0.1346
 5694/10000 [================>.............] - ETA: 34:16 - loss: 0.6870 - regression_loss: 0.5524 - classification_loss: 0.1346
 5695/10000 [================>.............] - ETA: 34:15 - loss: 0.6870 - regression_loss: 0.5524 - classification_loss: 0.1346
 5696/10000 [================>.............] - ETA: 34:15 - loss: 0.6871 - regression_loss: 0.5525 - classification_loss: 0.1346
 5697/10000 [================>.............] - ETA: 34:14 - loss: 0.6872 - regression_loss: 0.5525 - classification_loss: 0.1346
 5698/10000 [================>.............] - ETA: 34:14 - loss: 0.6871 - regression_loss: 0.5524 - classification_loss: 0.1346
 5699/10000 [================>.............] - ETA: 34:13 - loss: 0.6872 - regression_loss: 0.5525 - classification_loss: 0.1346
 5700/10000 [================>.............] - ETA: 34:13 - loss: 0.6871 - regression_loss: 0.5525 - classification_loss: 0.1346
 5701/10000 [================>.............] - ETA: 34:12 - loss: 0.6872 - regression_loss: 0.5525 - classification_loss: 0.1346
 5702/10000 [================>.............] - ETA: 34:12 - loss: 0.6871 - regression_loss: 0.5525 - classification_loss: 0.1346
 5703/10000 [================>.............] - ETA: 34:11 - loss: 0.6871 - regression_loss: 0.5525 - classification_loss: 0.1346
 5704/10000 [================>.............] - ETA: 34:11 - loss: 0.6871 - regression_loss: 0.5525 - classification_loss: 0.1346
 5705/10000 [================>.............] - ETA: 34:10 - loss: 0.6871 - regression_loss: 0.5525 - classification_loss: 0.1346
 5706/10000 [================>.............] - ETA: 34:10 - loss: 0.6870 - regression_loss: 0.5525 - classification_loss: 0.1346
 5707/10000 [================>.............] - ETA: 34:09 - loss: 0.6870 - regression_loss: 0.5524 - classification_loss: 0.1346
 5708/10000 [================>.............] - ETA: 34:09 - loss: 0.6869 - regression_loss: 0.5524 - classification_loss: 0.1346
 5709/10000 [================>.............] - ETA: 34:08 - loss: 0.6869 - regression_loss: 0.5523 - classification_loss: 0.1346
 5710/10000 [================>.............] - ETA: 34:08 - loss: 0.6869 - regression_loss: 0.5524 - classification_loss: 0.1346
 5711/10000 [================>.............] - ETA: 34:07 - loss: 0.6870 - regression_loss: 0.5525 - classification_loss: 0.1346
 5712/10000 [================>.............] - ETA: 34:07 - loss: 0.6870 - regression_loss: 0.5524 - classification_loss: 0.1346
 5713/10000 [================>.............] - ETA: 34:06 - loss: 0.6869 - regression_loss: 0.5524 - classification_loss: 0.1346
 5714/10000 [================>.............] - ETA: 34:06 - loss: 0.6869 - regression_loss: 0.5524 - classification_loss: 0.1346
 5715/10000 [================>.............] - ETA: 34:05 - loss: 0.6870 - regression_loss: 0.5524 - classification_loss: 0.1346
 5716/10000 [================>.............] - ETA: 34:05 - loss: 0.6871 - regression_loss: 0.5525 - classification_loss: 0.1346
 5717/10000 [================>.............] - ETA: 34:04 - loss: 0.6871 - regression_loss: 0.5524 - classification_loss: 0.1347
 5718/10000 [================>.............] - ETA: 34:04 - loss: 0.6870 - regression_loss: 0.5524 - classification_loss: 0.1347
 5719/10000 [================>.............] - ETA: 34:03 - loss: 0.6871 - regression_loss: 0.5524 - classification_loss: 0.1347
 5720/10000 [================>.............] - ETA: 34:03 - loss: 0.6871 - regression_loss: 0.5524 - classification_loss: 0.1347
 5721/10000 [================>.............] - ETA: 34:02 - loss: 0.6871 - regression_loss: 0.5524 - classification_loss: 0.1347
 5722/10000 [================>.............] - ETA: 34:02 - loss: 0.6871 - regression_loss: 0.5524 - classification_loss: 0.1347
 5723/10000 [================>.............] - ETA: 34:01 - loss: 0.6871 - regression_loss: 0.5524 - classification_loss: 0.1347
 5724/10000 [================>.............] - ETA: 34:01 - loss: 0.6872 - regression_loss: 0.5525 - classification_loss: 0.1347
 5725/10000 [================>.............] - ETA: 34:00 - loss: 0.6871 - regression_loss: 0.5524 - classification_loss: 0.1347
 5726/10000 [================>.............] - ETA: 34:00 - loss: 0.6870 - regression_loss: 0.5524 - classification_loss: 0.1347
 5727/10000 [================>.............] - ETA: 33:59 - loss: 0.6870 - regression_loss: 0.5524 - classification_loss: 0.1346
 5728/10000 [================>.............] - ETA: 33:59 - loss: 0.6869 - regression_loss: 0.5523 - classification_loss: 0.1346
 5729/10000 [================>.............] - ETA: 33:58 - loss: 0.6870 - regression_loss: 0.5523 - classification_loss: 0.1346
 5730/10000 [================>.............] - ETA: 33:58 - loss: 0.6871 - regression_loss: 0.5524 - classification_loss: 0.1346
 5731/10000 [================>.............] - ETA: 33:57 - loss: 0.6870 - regression_loss: 0.5524 - classification_loss: 0.1346
 5732/10000 [================>.............] - ETA: 33:57 - loss: 0.6871 - regression_loss: 0.5525 - classification_loss: 0.1346
 5733/10000 [================>.............] - ETA: 33:56 - loss: 0.6871 - regression_loss: 0.5525 - classification_loss: 0.1346
 5734/10000 [================>.............] - ETA: 33:56 - loss: 0.6871 - regression_loss: 0.5525 - classification_loss: 0.1346
 5735/10000 [================>.............] - ETA: 33:55 - loss: 0.6871 - regression_loss: 0.5525 - classification_loss: 0.1346
 5736/10000 [================>.............] - ETA: 33:55 - loss: 0.6872 - regression_loss: 0.5526 - classification_loss: 0.1346
 5737/10000 [================>.............] - ETA: 33:54 - loss: 0.6872 - regression_loss: 0.5526 - classification_loss: 0.1346
 5738/10000 [================>.............] - ETA: 33:54 - loss: 0.6873 - regression_loss: 0.5527 - classification_loss: 0.1346
 5739/10000 [================>.............] - ETA: 33:53 - loss: 0.6872 - regression_loss: 0.5526 - classification_loss: 0.1346
 5740/10000 [================>.............] - ETA: 33:53 - loss: 0.6872 - regression_loss: 0.5526 - classification_loss: 0.1346
 5741/10000 [================>.............] - ETA: 33:53 - loss: 0.6871 - regression_loss: 0.5525 - classification_loss: 0.1346
 5742/10000 [================>.............] - ETA: 33:52 - loss: 0.6872 - regression_loss: 0.5526 - classification_loss: 0.1346
 5743/10000 [================>.............] - ETA: 33:52 - loss: 0.6871 - regression_loss: 0.5526 - classification_loss: 0.1346
 5744/10000 [================>.............] - ETA: 33:51 - loss: 0.6872 - regression_loss: 0.5526 - classification_loss: 0.1346
 5745/10000 [================>.............] - ETA: 33:51 - loss: 0.6873 - regression_loss: 0.5527 - classification_loss: 0.1346
 5746/10000 [================>.............] - ETA: 33:50 - loss: 0.6872 - regression_loss: 0.5527 - classification_loss: 0.1346
 5747/10000 [================>.............] - ETA: 33:50 - loss: 0.6872 - regression_loss: 0.5526 - classification_loss: 0.1346
 5748/10000 [================>.............] - ETA: 33:49 - loss: 0.6873 - regression_loss: 0.5527 - classification_loss: 0.1346
 5749/10000 [================>.............] - ETA: 33:49 - loss: 0.6874 - regression_loss: 0.5528 - classification_loss: 0.1346
 5750/10000 [================>.............] - ETA: 33:48 - loss: 0.6874 - regression_loss: 0.5528 - classification_loss: 0.1346
 5751/10000 [================>.............] - ETA: 33:48 - loss: 0.6874 - regression_loss: 0.5528 - classification_loss: 0.1346
 5752/10000 [================>.............] - ETA: 33:47 - loss: 0.6873 - regression_loss: 0.5527 - classification_loss: 0.1346
 5753/10000 [================>.............] - ETA: 33:47 - loss: 0.6873 - regression_loss: 0.5527 - classification_loss: 0.1346
 5754/10000 [================>.............] - ETA: 33:46 - loss: 0.6873 - regression_loss: 0.5527 - classification_loss: 0.1346
 5755/10000 [================>.............] - ETA: 33:46 - loss: 0.6873 - regression_loss: 0.5527 - classification_loss: 0.1346
 5756/10000 [================>.............] - ETA: 33:45 - loss: 0.6874 - regression_loss: 0.5528 - classification_loss: 0.1346
 5757/10000 [================>.............] - ETA: 33:45 - loss: 0.6873 - regression_loss: 0.5527 - classification_loss: 0.1346
 5758/10000 [================>.............] - ETA: 33:44 - loss: 0.6876 - regression_loss: 0.5529 - classification_loss: 0.1346
 5759/10000 [================>.............] - ETA: 33:44 - loss: 0.6876 - regression_loss: 0.5530 - classification_loss: 0.1346
 5760/10000 [================>.............] - ETA: 33:43 - loss: 0.6876 - regression_loss: 0.5529 - classification_loss: 0.1346
 5761/10000 [================>.............] - ETA: 33:43 - loss: 0.6875 - regression_loss: 0.5529 - classification_loss: 0.1346
 5762/10000 [================>.............] - ETA: 33:42 - loss: 0.6875 - regression_loss: 0.5529 - classification_loss: 0.1346
 5763/10000 [================>.............] - ETA: 33:42 - loss: 0.6875 - regression_loss: 0.5529 - classification_loss: 0.1346
 5764/10000 [================>.............] - ETA: 33:41 - loss: 0.6874 - regression_loss: 0.5528 - classification_loss: 0.1346
 5765/10000 [================>.............] - ETA: 33:41 - loss: 0.6874 - regression_loss: 0.5528 - classification_loss: 0.1346
 5766/10000 [================>.............] - ETA: 33:40 - loss: 0.6874 - regression_loss: 0.5528 - classification_loss: 0.1346
 5767/10000 [================>.............] - ETA: 33:40 - loss: 0.6873 - regression_loss: 0.5527 - classification_loss: 0.1345
 5768/10000 [================>.............] - ETA: 33:39 - loss: 0.6873 - regression_loss: 0.5528 - classification_loss: 0.1346
 5769/10000 [================>.............] - ETA: 33:39 - loss: 0.6873 - regression_loss: 0.5527 - classification_loss: 0.1345
 5770/10000 [================>.............] - ETA: 33:38 - loss: 0.6873 - regression_loss: 0.5528 - classification_loss: 0.1346
 5771/10000 [================>.............] - ETA: 33:38 - loss: 0.6873 - regression_loss: 0.5527 - classification_loss: 0.1346
 5772/10000 [================>.............] - ETA: 33:37 - loss: 0.6873 - regression_loss: 0.5528 - classification_loss: 0.1346
 5773/10000 [================>.............] - ETA: 33:37 - loss: 0.6873 - regression_loss: 0.5527 - classification_loss: 0.1346
 5774/10000 [================>.............] - ETA: 33:36 - loss: 0.6874 - regression_loss: 0.5528 - classification_loss: 0.1346
 5775/10000 [================>.............] - ETA: 33:36 - loss: 0.6875 - regression_loss: 0.5529 - classification_loss: 0.1346
 5776/10000 [================>.............] - ETA: 33:35 - loss: 0.6875 - regression_loss: 0.5529 - classification_loss: 0.1346
 5777/10000 [================>.............] - ETA: 33:35 - loss: 0.6876 - regression_loss: 0.5530 - classification_loss: 0.1346
 5778/10000 [================>.............] - ETA: 33:34 - loss: 0.6875 - regression_loss: 0.5529 - classification_loss: 0.1346
 5779/10000 [================>.............] - ETA: 33:34 - loss: 0.6875 - regression_loss: 0.5529 - classification_loss: 0.1346
 5780/10000 [================>.............] - ETA: 33:33 - loss: 0.6874 - regression_loss: 0.5528 - classification_loss: 0.1346
 5781/10000 [================>.............] - ETA: 33:33 - loss: 0.6876 - regression_loss: 0.5530 - classification_loss: 0.1347
 5782/10000 [================>.............] - ETA: 33:32 - loss: 0.6876 - regression_loss: 0.5529 - classification_loss: 0.1347
 5783/10000 [================>.............] - ETA: 33:32 - loss: 0.6875 - regression_loss: 0.5528 - classification_loss: 0.1347
 5784/10000 [================>.............] - ETA: 33:31 - loss: 0.6875 - regression_loss: 0.5529 - classification_loss: 0.1347
 5785/10000 [================>.............] - ETA: 33:31 - loss: 0.6875 - regression_loss: 0.5529 - classification_loss: 0.1347
 5786/10000 [================>.............] - ETA: 33:30 - loss: 0.6875 - regression_loss: 0.5528 - classification_loss: 0.1346
 5787/10000 [================>.............] - ETA: 33:30 - loss: 0.6875 - regression_loss: 0.5529 - classification_loss: 0.1347
 5788/10000 [================>.............] - ETA: 33:29 - loss: 0.6875 - regression_loss: 0.5529 - classification_loss: 0.1347
 5789/10000 [================>.............] - ETA: 33:29 - loss: 0.6874 - regression_loss: 0.5528 - classification_loss: 0.1347
 5790/10000 [================>.............] - ETA: 33:28 - loss: 0.6875 - regression_loss: 0.5529 - classification_loss: 0.1346
 5791/10000 [================>.............] - ETA: 33:28 - loss: 0.6874 - regression_loss: 0.5528 - classification_loss: 0.1346
 5792/10000 [================>.............] - ETA: 33:27 - loss: 0.6874 - regression_loss: 0.5528 - classification_loss: 0.1346
 5793/10000 [================>.............] - ETA: 33:27 - loss: 0.6874 - regression_loss: 0.5528 - classification_loss: 0.1346
 5794/10000 [================>.............] - ETA: 33:26 - loss: 0.6873 - regression_loss: 0.5527 - classification_loss: 0.1346
 5795/10000 [================>.............] - ETA: 33:26 - loss: 0.6873 - regression_loss: 0.5527 - classification_loss: 0.1346
 5796/10000 [================>.............] - ETA: 33:25 - loss: 0.6873 - regression_loss: 0.5527 - classification_loss: 0.1346
 5797/10000 [================>.............] - ETA: 33:25 - loss: 0.6872 - regression_loss: 0.5526 - classification_loss: 0.1346
 5798/10000 [================>.............] - ETA: 33:24 - loss: 0.6872 - regression_loss: 0.5527 - classification_loss: 0.1346
 5799/10000 [================>.............] - ETA: 33:24 - loss: 0.6872 - regression_loss: 0.5526 - classification_loss: 0.1346
 5800/10000 [================>.............] - ETA: 33:23 - loss: 0.6873 - regression_loss: 0.5527 - classification_loss: 0.1346
 5801/10000 [================>.............] - ETA: 33:23 - loss: 0.6872 - regression_loss: 0.5526 - classification_loss: 0.1346
 5802/10000 [================>.............] - ETA: 33:22 - loss: 0.6872 - regression_loss: 0.5526 - classification_loss: 0.1346
 5803/10000 [================>.............] - ETA: 33:22 - loss: 0.6871 - regression_loss: 0.5526 - classification_loss: 0.1346
 5804/10000 [================>.............] - ETA: 33:21 - loss: 0.6870 - regression_loss: 0.5525 - classification_loss: 0.1345
 5805/10000 [================>.............] - ETA: 33:21 - loss: 0.6872 - regression_loss: 0.5526 - classification_loss: 0.1346
 5806/10000 [================>.............] - ETA: 33:20 - loss: 0.6872 - regression_loss: 0.5525 - classification_loss: 0.1346
 5807/10000 [================>.............] - ETA: 33:20 - loss: 0.6872 - regression_loss: 0.5525 - classification_loss: 0.1346
 5808/10000 [================>.............] - ETA: 33:19 - loss: 0.6871 - regression_loss: 0.5525 - classification_loss: 0.1346
 5809/10000 [================>.............] - ETA: 33:19 - loss: 0.6870 - regression_loss: 0.5524 - classification_loss: 0.1346
 5810/10000 [================>.............] - ETA: 33:18 - loss: 0.6870 - regression_loss: 0.5524 - classification_loss: 0.1346
 5811/10000 [================>.............] - ETA: 33:18 - loss: 0.6869 - regression_loss: 0.5523 - classification_loss: 0.1346
 5812/10000 [================>.............] - ETA: 33:17 - loss: 0.6869 - regression_loss: 0.5523 - classification_loss: 0.1346
 5813/10000 [================>.............] - ETA: 33:17 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5814/10000 [================>.............] - ETA: 33:16 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5815/10000 [================>.............] - ETA: 33:16 - loss: 0.6867 - regression_loss: 0.5521 - classification_loss: 0.1346
 5816/10000 [================>.............] - ETA: 33:15 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5817/10000 [================>.............] - ETA: 33:15 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5818/10000 [================>.............] - ETA: 33:14 - loss: 0.6867 - regression_loss: 0.5522 - classification_loss: 0.1346
 5819/10000 [================>.............] - ETA: 33:14 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5820/10000 [================>.............] - ETA: 33:13 - loss: 0.6867 - regression_loss: 0.5521 - classification_loss: 0.1346
 5821/10000 [================>.............] - ETA: 33:13 - loss: 0.6867 - regression_loss: 0.5522 - classification_loss: 0.1346
 5822/10000 [================>.............] - ETA: 33:12 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5823/10000 [================>.............] - ETA: 33:12 - loss: 0.6867 - regression_loss: 0.5521 - classification_loss: 0.1346
 5824/10000 [================>.............] - ETA: 33:11 - loss: 0.6867 - regression_loss: 0.5521 - classification_loss: 0.1346
 5825/10000 [================>.............] - ETA: 33:11 - loss: 0.6867 - regression_loss: 0.5521 - classification_loss: 0.1346
 5826/10000 [================>.............] - ETA: 33:10 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5827/10000 [================>.............] - ETA: 33:10 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5828/10000 [================>.............] - ETA: 33:09 - loss: 0.6867 - regression_loss: 0.5521 - classification_loss: 0.1346
 5829/10000 [================>.............] - ETA: 33:09 - loss: 0.6867 - regression_loss: 0.5521 - classification_loss: 0.1346
 5830/10000 [================>.............] - ETA: 33:09 - loss: 0.6866 - regression_loss: 0.5520 - classification_loss: 0.1346
 5831/10000 [================>.............] - ETA: 33:08 - loss: 0.6867 - regression_loss: 0.5521 - classification_loss: 0.1346
 5832/10000 [================>.............] - ETA: 33:07 - loss: 0.6867 - regression_loss: 0.5521 - classification_loss: 0.1346
 5833/10000 [================>.............] - ETA: 33:07 - loss: 0.6867 - regression_loss: 0.5521 - classification_loss: 0.1346
 5834/10000 [================>.............] - ETA: 33:06 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5835/10000 [================>.............] - ETA: 33:06 - loss: 0.6869 - regression_loss: 0.5522 - classification_loss: 0.1347
 5836/10000 [================>.............] - ETA: 33:06 - loss: 0.6869 - regression_loss: 0.5522 - classification_loss: 0.1347
 5837/10000 [================>.............] - ETA: 33:05 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5838/10000 [================>.............] - ETA: 33:05 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5839/10000 [================>.............] - ETA: 33:04 - loss: 0.6867 - regression_loss: 0.5521 - classification_loss: 0.1346
 5840/10000 [================>.............] - ETA: 33:04 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5841/10000 [================>.............] - ETA: 33:03 - loss: 0.6867 - regression_loss: 0.5521 - classification_loss: 0.1346
 5842/10000 [================>.............] - ETA: 33:03 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5843/10000 [================>.............] - ETA: 33:02 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5844/10000 [================>.............] - ETA: 33:02 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5845/10000 [================>.............] - ETA: 33:01 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5846/10000 [================>.............] - ETA: 33:01 - loss: 0.6869 - regression_loss: 0.5523 - classification_loss: 0.1346
 5847/10000 [================>.............] - ETA: 33:00 - loss: 0.6868 - regression_loss: 0.5523 - classification_loss: 0.1346
 5848/10000 [================>.............] - ETA: 33:00 - loss: 0.6869 - regression_loss: 0.5523 - classification_loss: 0.1346
 5849/10000 [================>.............] - ETA: 32:59 - loss: 0.6869 - regression_loss: 0.5523 - classification_loss: 0.1345
 5850/10000 [================>.............] - ETA: 32:59 - loss: 0.6868 - regression_loss: 0.5523 - classification_loss: 0.1345
 5851/10000 [================>.............] - ETA: 32:58 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1345
 5852/10000 [================>.............] - ETA: 32:58 - loss: 0.6867 - regression_loss: 0.5522 - classification_loss: 0.1345
 5853/10000 [================>.............] - ETA: 32:57 - loss: 0.6867 - regression_loss: 0.5522 - classification_loss: 0.1345
 5854/10000 [================>.............] - ETA: 32:57 - loss: 0.6867 - regression_loss: 0.5522 - classification_loss: 0.1345
 5855/10000 [================>.............] - ETA: 32:56 - loss: 0.6867 - regression_loss: 0.5522 - classification_loss: 0.1345
 5856/10000 [================>.............] - ETA: 32:56 - loss: 0.6868 - regression_loss: 0.5523 - classification_loss: 0.1345
 5857/10000 [================>.............] - ETA: 32:55 - loss: 0.6867 - regression_loss: 0.5523 - classification_loss: 0.1345
 5858/10000 [================>.............] - ETA: 32:55 - loss: 0.6866 - regression_loss: 0.5522 - classification_loss: 0.1345
 5859/10000 [================>.............] - ETA: 32:54 - loss: 0.6867 - regression_loss: 0.5522 - classification_loss: 0.1345
 5860/10000 [================>.............] - ETA: 32:54 - loss: 0.6866 - regression_loss: 0.5521 - classification_loss: 0.1345
 5861/10000 [================>.............] - ETA: 32:53 - loss: 0.6867 - regression_loss: 0.5523 - classification_loss: 0.1345
 5862/10000 [================>.............] - ETA: 32:53 - loss: 0.6867 - regression_loss: 0.5522 - classification_loss: 0.1345
 5863/10000 [================>.............] - ETA: 32:52 - loss: 0.6867 - regression_loss: 0.5522 - classification_loss: 0.1345
 5864/10000 [================>.............] - ETA: 32:52 - loss: 0.6867 - regression_loss: 0.5522 - classification_loss: 0.1345
 5865/10000 [================>.............] - ETA: 32:51 - loss: 0.6866 - regression_loss: 0.5522 - classification_loss: 0.1344
 5866/10000 [================>.............] - ETA: 32:51 - loss: 0.6866 - regression_loss: 0.5522 - classification_loss: 0.1345
 5867/10000 [================>.............] - ETA: 32:50 - loss: 0.6866 - regression_loss: 0.5522 - classification_loss: 0.1345
 5868/10000 [================>.............] - ETA: 32:50 - loss: 0.6866 - regression_loss: 0.5521 - classification_loss: 0.1345
 5869/10000 [================>.............] - ETA: 32:49 - loss: 0.6865 - regression_loss: 0.5520 - classification_loss: 0.1345
 5870/10000 [================>.............] - ETA: 32:49 - loss: 0.6866 - regression_loss: 0.5521 - classification_loss: 0.1345
 5871/10000 [================>.............] - ETA: 32:48 - loss: 0.6866 - regression_loss: 0.5522 - classification_loss: 0.1345
 5872/10000 [================>.............] - ETA: 32:48 - loss: 0.6866 - regression_loss: 0.5522 - classification_loss: 0.1345
 5873/10000 [================>.............] - ETA: 32:47 - loss: 0.6867 - regression_loss: 0.5522 - classification_loss: 0.1345
 5874/10000 [================>.............] - ETA: 32:47 - loss: 0.6868 - regression_loss: 0.5523 - classification_loss: 0.1345
 5875/10000 [================>.............] - ETA: 32:46 - loss: 0.6868 - regression_loss: 0.5523 - classification_loss: 0.1345
 5876/10000 [================>.............] - ETA: 32:46 - loss: 0.6867 - regression_loss: 0.5523 - classification_loss: 0.1344
 5877/10000 [================>.............] - ETA: 32:45 - loss: 0.6867 - regression_loss: 0.5523 - classification_loss: 0.1344
 5878/10000 [================>.............] - ETA: 32:45 - loss: 0.6867 - regression_loss: 0.5522 - classification_loss: 0.1344
 5879/10000 [================>.............] - ETA: 32:44 - loss: 0.6867 - regression_loss: 0.5522 - classification_loss: 0.1344
 5880/10000 [================>.............] - ETA: 32:44 - loss: 0.6866 - regression_loss: 0.5522 - classification_loss: 0.1344
 5881/10000 [================>.............] - ETA: 32:43 - loss: 0.6868 - regression_loss: 0.5523 - classification_loss: 0.1345
 5882/10000 [================>.............] - ETA: 32:43 - loss: 0.6868 - regression_loss: 0.5523 - classification_loss: 0.1345
 5883/10000 [================>.............] - ETA: 32:42 - loss: 0.6867 - regression_loss: 0.5523 - classification_loss: 0.1345
 5884/10000 [================>.............] - ETA: 32:42 - loss: 0.6869 - regression_loss: 0.5524 - classification_loss: 0.1345
 5885/10000 [================>.............] - ETA: 32:41 - loss: 0.6869 - regression_loss: 0.5524 - classification_loss: 0.1345
 5886/10000 [================>.............] - ETA: 32:41 - loss: 0.6869 - regression_loss: 0.5524 - classification_loss: 0.1345
 5887/10000 [================>.............] - ETA: 32:40 - loss: 0.6870 - regression_loss: 0.5525 - classification_loss: 0.1345
 5888/10000 [================>.............] - ETA: 32:40 - loss: 0.6870 - regression_loss: 0.5524 - classification_loss: 0.1345
 5889/10000 [================>.............] - ETA: 32:39 - loss: 0.6869 - regression_loss: 0.5524 - classification_loss: 0.1345
 5890/10000 [================>.............] - ETA: 32:39 - loss: 0.6870 - regression_loss: 0.5524 - classification_loss: 0.1345
 5891/10000 [================>.............] - ETA: 32:38 - loss: 0.6869 - regression_loss: 0.5524 - classification_loss: 0.1345
 5892/10000 [================>.............] - ETA: 32:38 - loss: 0.6869 - regression_loss: 0.5523 - classification_loss: 0.1345
 5893/10000 [================>.............] - ETA: 32:37 - loss: 0.6868 - regression_loss: 0.5523 - classification_loss: 0.1345
 5894/10000 [================>.............] - ETA: 32:37 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1345
 5895/10000 [================>.............] - ETA: 32:36 - loss: 0.6867 - regression_loss: 0.5522 - classification_loss: 0.1345
 5896/10000 [================>.............] - ETA: 32:36 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1345
 5897/10000 [================>.............] - ETA: 32:35 - loss: 0.6867 - regression_loss: 0.5522 - classification_loss: 0.1345
 5898/10000 [================>.............] - ETA: 32:35 - loss: 0.6868 - regression_loss: 0.5523 - classification_loss: 0.1345
 5899/10000 [================>.............] - ETA: 32:34 - loss: 0.6867 - regression_loss: 0.5522 - classification_loss: 0.1345
 5900/10000 [================>.............] - ETA: 32:34 - loss: 0.6867 - regression_loss: 0.5522 - classification_loss: 0.1345
 5901/10000 [================>.............] - ETA: 32:33 - loss: 0.6866 - regression_loss: 0.5521 - classification_loss: 0.1345
 5902/10000 [================>.............] - ETA: 32:33 - loss: 0.6866 - regression_loss: 0.5521 - classification_loss: 0.1346
 5903/10000 [================>.............] - ETA: 32:32 - loss: 0.6866 - regression_loss: 0.5521 - classification_loss: 0.1345
 5904/10000 [================>.............] - ETA: 32:32 - loss: 0.6865 - regression_loss: 0.5520 - classification_loss: 0.1345
 5905/10000 [================>.............] - ETA: 32:31 - loss: 0.6866 - regression_loss: 0.5520 - classification_loss: 0.1346
 5906/10000 [================>.............] - ETA: 32:31 - loss: 0.6867 - regression_loss: 0.5521 - classification_loss: 0.1346
 5907/10000 [================>.............] - ETA: 32:30 - loss: 0.6867 - regression_loss: 0.5522 - classification_loss: 0.1346
 5908/10000 [================>.............] - ETA: 32:30 - loss: 0.6867 - regression_loss: 0.5521 - classification_loss: 0.1346
 5909/10000 [================>.............] - ETA: 32:29 - loss: 0.6867 - regression_loss: 0.5521 - classification_loss: 0.1346
 5910/10000 [================>.............] - ETA: 32:29 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5911/10000 [================>.............] - ETA: 32:28 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1346
 5912/10000 [================>.............] - ETA: 32:28 - loss: 0.6867 - regression_loss: 0.5522 - classification_loss: 0.1346
 5913/10000 [================>.............] - ETA: 32:27 - loss: 0.6867 - regression_loss: 0.5522 - classification_loss: 0.1346
 5914/10000 [================>.............] - ETA: 32:27 - loss: 0.6867 - regression_loss: 0.5522 - classification_loss: 0.1345
 5915/10000 [================>.............] - ETA: 32:26 - loss: 0.6868 - regression_loss: 0.5522 - classification_loss: 0.1345
 5916/10000 [================>.............] - ETA: 32:26 - loss: 0.6868 - regression_loss: 0.5523 - classification_loss: 0.1346
 5917/10000 [================>.............] - ETA: 32:25 - loss: 0.6868 - regression_loss: 0.5523 - classification_loss: 0.1346
 5918/10000 [================>.............] - ETA: 32:25 - loss: 0.6868 - regression_loss: 0.5523 - classification_loss: 0.1345
 5919/10000 [================>.............] - ETA: 32:24 - loss: 0.6869 - regression_loss: 0.5523 - classification_loss: 0.1346
 5920/10000 [================>.............] - ETA: 32:24 - loss: 0.6868 - regression_loss: 0.5523 - classification_loss: 0.1345
 5921/10000 [================>.............] - ETA: 32:23 - loss: 0.6867 - regression_loss: 0.5522 - classification_loss: 0.1345
 5922/10000 [================>.............] - ETA: 32:23 - loss: 0.6867 - regression_loss: 0.5521 - classification_loss: 0.1345
 5923/10000 [================>.............] - ETA: 32:22 - loss: 0.6866 - regression_loss: 0.5521 - classification_loss: 0.1345
 5924/10000 [================>.............] - ETA: 32:22 - loss: 0.6866 - regression_loss: 0.5521 - classification_loss: 0.1345
 5925/10000 [================>.............] - ETA: 32:21 - loss: 0.6865 - regression_loss: 0.5520 - classification_loss: 0.1345
 5926/10000 [================>.............] - ETA: 32:21 - loss: 0.6865 - regression_loss: 0.5520 - classification_loss: 0.1345
 5927/10000 [================>.............] - ETA: 32:20 - loss: 0.6865 - regression_loss: 0.5520 - classification_loss: 0.1345
 5928/10000 [================>.............] - ETA: 32:20 - loss: 0.6864 - regression_loss: 0.5520 - classification_loss: 0.1345
 5929/10000 [================>.............] - ETA: 32:19 - loss: 0.6865 - regression_loss: 0.5520 - classification_loss: 0.1345
 5930/10000 [================>.............] - ETA: 32:19 - loss: 0.6864 - regression_loss: 0.5520 - classification_loss: 0.1345
 5931/10000 [================>.............] - ETA: 32:18 - loss: 0.6865 - regression_loss: 0.5520 - classification_loss: 0.1345
 5932/10000 [================>.............] - ETA: 32:18 - loss: 0.6866 - regression_loss: 0.5521 - classification_loss: 0.1345
 5933/10000 [================>.............] - ETA: 32:17 - loss: 0.6865 - regression_loss: 0.5520 - classification_loss: 0.1345
 5934/10000 [================>.............] - ETA: 32:17 - loss: 0.6864 - regression_loss: 0.5520 - classification_loss: 0.1344
 5935/10000 [================>.............] - ETA: 32:16 - loss: 0.6864 - regression_loss: 0.5520 - classification_loss: 0.1344
 5936/10000 [================>.............] - ETA: 32:16 - loss: 0.6865 - regression_loss: 0.5521 - classification_loss: 0.1345
 5937/10000 [================>.............] - ETA: 32:16 - loss: 0.6864 - regression_loss: 0.5520 - classification_loss: 0.1344
 5938/10000 [================>.............] - ETA: 32:15 - loss: 0.6864 - regression_loss: 0.5520 - classification_loss: 0.1344
 5939/10000 [================>.............] - ETA: 32:15 - loss: 0.6864 - regression_loss: 0.5520 - classification_loss: 0.1344
 5940/10000 [================>.............] - ETA: 32:14 - loss: 0.6864 - regression_loss: 0.5520 - classification_loss: 0.1344
 5941/10000 [================>.............] - ETA: 32:14 - loss: 0.6863 - regression_loss: 0.5519 - classification_loss: 0.1344
 5942/10000 [================>.............] - ETA: 32:13 - loss: 0.6864 - regression_loss: 0.5520 - classification_loss: 0.1344
 5943/10000 [================>.............] - ETA: 32:13 - loss: 0.6864 - regression_loss: 0.5520 - classification_loss: 0.1344
 5944/10000 [================>.............] - ETA: 32:12 - loss: 0.6863 - regression_loss: 0.5519 - classification_loss: 0.1344
 5945/10000 [================>.............] - ETA: 32:12 - loss: 0.6862 - regression_loss: 0.5518 - classification_loss: 0.1344
 5946/10000 [================>.............] - ETA: 32:11 - loss: 0.6862 - regression_loss: 0.5518 - classification_loss: 0.1344
 5947/10000 [================>.............] - ETA: 32:11 - loss: 0.6862 - regression_loss: 0.5518 - classification_loss: 0.1344
 5948/10000 [================>.............] - ETA: 32:10 - loss: 0.6862 - regression_loss: 0.5519 - classification_loss: 0.1344
 5949/10000 [================>.............] - ETA: 32:10 - loss: 0.6863 - regression_loss: 0.5519 - classification_loss: 0.1344
 5950/10000 [================>.............] - ETA: 32:09 - loss: 0.6863 - regression_loss: 0.5519 - classification_loss: 0.1344
 5951/10000 [================>.............] - ETA: 32:09 - loss: 0.6863 - regression_loss: 0.5520 - classification_loss: 0.1344
 5952/10000 [================>.............] - ETA: 32:08 - loss: 0.6863 - regression_loss: 0.5520 - classification_loss: 0.1344
 5953/10000 [================>.............] - ETA: 32:08 - loss: 0.6863 - regression_loss: 0.5519 - classification_loss: 0.1344
 5954/10000 [================>.............] - ETA: 32:07 - loss: 0.6862 - regression_loss: 0.5518 - classification_loss: 0.1343
 5955/10000 [================>.............] - ETA: 32:07 - loss: 0.6861 - regression_loss: 0.5518 - classification_loss: 0.1343
 5956/10000 [================>.............] - ETA: 32:06 - loss: 0.6862 - regression_loss: 0.5519 - classification_loss: 0.1343
 5957/10000 [================>.............] - ETA: 32:06 - loss: 0.6863 - regression_loss: 0.5519 - classification_loss: 0.1344
 5958/10000 [================>.............] - ETA: 32:05 - loss: 0.6863 - regression_loss: 0.5519 - classification_loss: 0.1344
 5959/10000 [================>.............] - ETA: 32:05 - loss: 0.6863 - regression_loss: 0.5519 - classification_loss: 0.1344
 5960/10000 [================>.............] - ETA: 32:04 - loss: 0.6862 - regression_loss: 0.5518 - classification_loss: 0.1344
 5961/10000 [================>.............] - ETA: 32:04 - loss: 0.6861 - regression_loss: 0.5518 - classification_loss: 0.1344
 5962/10000 [================>.............] - ETA: 32:03 - loss: 0.6861 - regression_loss: 0.5517 - classification_loss: 0.1344
 5963/10000 [================>.............] - ETA: 32:03 - loss: 0.6860 - regression_loss: 0.5517 - classification_loss: 0.1343
 5964/10000 [================>.............] - ETA: 32:02 - loss: 0.6859 - regression_loss: 0.5516 - classification_loss: 0.1343
 5965/10000 [================>.............] - ETA: 32:02 - loss: 0.6859 - regression_loss: 0.5516 - classification_loss: 0.1343
 5966/10000 [================>.............] - ETA: 32:01 - loss: 0.6860 - regression_loss: 0.5516 - classification_loss: 0.1343
 5967/10000 [================>.............] - ETA: 32:01 - loss: 0.6859 - regression_loss: 0.5516 - classification_loss: 0.1343
 5968/10000 [================>.............] - ETA: 32:00 - loss: 0.6860 - regression_loss: 0.5516 - classification_loss: 0.1343
 5969/10000 [================>.............] - ETA: 32:00 - loss: 0.6859 - regression_loss: 0.5516 - classification_loss: 0.1343
 5970/10000 [================>.............] - ETA: 31:59 - loss: 0.6859 - regression_loss: 0.5516 - classification_loss: 0.1343
 5971/10000 [================>.............] - ETA: 31:59 - loss: 0.6858 - regression_loss: 0.5515 - classification_loss: 0.1343
 5972/10000 [================>.............] - ETA: 31:58 - loss: 0.6857 - regression_loss: 0.5514 - classification_loss: 0.1343
 5973/10000 [================>.............] - ETA: 31:58 - loss: 0.6857 - regression_loss: 0.5514 - classification_loss: 0.1343
 5974/10000 [================>.............] - ETA: 31:57 - loss: 0.6856 - regression_loss: 0.5514 - classification_loss: 0.1343
 5975/10000 [================>.............] - ETA: 31:57 - loss: 0.6856 - regression_loss: 0.5513 - classification_loss: 0.1343
 5976/10000 [================>.............] - ETA: 31:56 - loss: 0.6855 - regression_loss: 0.5512 - classification_loss: 0.1343
 5977/10000 [================>.............] - ETA: 31:56 - loss: 0.6854 - regression_loss: 0.5512 - classification_loss: 0.1342
 5978/10000 [================>.............] - ETA: 31:55 - loss: 0.6854 - regression_loss: 0.5511 - classification_loss: 0.1342
 5979/10000 [================>.............] - ETA: 31:55 - loss: 0.6854 - regression_loss: 0.5511 - classification_loss: 0.1342
 5980/10000 [================>.............] - ETA: 31:54 - loss: 0.6855 - regression_loss: 0.5513 - classification_loss: 0.1342
 5981/10000 [================>.............] - ETA: 31:54 - loss: 0.6854 - regression_loss: 0.5512 - classification_loss: 0.1342
 5982/10000 [================>.............] - ETA: 31:53 - loss: 0.6854 - regression_loss: 0.5511 - classification_loss: 0.1342
 5983/10000 [================>.............] - ETA: 31:53 - loss: 0.6853 - regression_loss: 0.5511 - classification_loss: 0.1342
 5984/10000 [================>.............] - ETA: 31:52 - loss: 0.6852 - regression_loss: 0.5510 - classification_loss: 0.1342
 5985/10000 [================>.............] - ETA: 31:52 - loss: 0.6851 - regression_loss: 0.5510 - classification_loss: 0.1342
 5986/10000 [================>.............] - ETA: 31:52 - loss: 0.6851 - regression_loss: 0.5510 - classification_loss: 0.1342
 5987/10000 [================>.............] - ETA: 31:51 - loss: 0.6851 - regression_loss: 0.5509 - classification_loss: 0.1341
 5988/10000 [================>.............] - ETA: 31:51 - loss: 0.6850 - regression_loss: 0.5509 - classification_loss: 0.1341
 5989/10000 [================>.............] - ETA: 31:50 - loss: 0.6849 - regression_loss: 0.5508 - classification_loss: 0.1341
 5990/10000 [================>.............] - ETA: 31:50 - loss: 0.6849 - regression_loss: 0.5507 - classification_loss: 0.1341
 5991/10000 [================>.............] - ETA: 31:49 - loss: 0.6848 - regression_loss: 0.5507 - classification_loss: 0.1341
 5992/10000 [================>.............] - ETA: 31:49 - loss: 0.6850 - regression_loss: 0.5508 - classification_loss: 0.1341
 5993/10000 [================>.............] - ETA: 31:48 - loss: 0.6850 - regression_loss: 0.5508 - classification_loss: 0.1341
 5994/10000 [================>.............] - ETA: 31:48 - loss: 0.6850 - regression_loss: 0.5509 - classification_loss: 0.1341
 5995/10000 [================>.............] - ETA: 31:47 - loss: 0.6849 - regression_loss: 0.5508 - classification_loss: 0.1341
 5996/10000 [================>.............] - ETA: 31:47 - loss: 0.6850 - regression_loss: 0.5509 - classification_loss: 0.1341
 5997/10000 [================>.............] - ETA: 31:46 - loss: 0.6850 - regression_loss: 0.5509 - classification_loss: 0.1341
 5998/10000 [================>.............] - ETA: 31:46 - loss: 0.6849 - regression_loss: 0.5508 - classification_loss: 0.1341
 5999/10000 [================>.............] - ETA: 31:45 - loss: 0.6850 - regression_loss: 0.5509 - classification_loss: 0.1341
 6000/10000 [=================>............] - ETA: 31:45 - loss: 0.6850 - regression_loss: 0.5509 - classification_loss: 0.1341
 6001/10000 [=================>............] - ETA: 31:44 - loss: 0.6850 - regression_loss: 0.5509 - classification_loss: 0.1341
 6002/10000 [=================>............] - ETA: 31:44 - loss: 0.6851 - regression_loss: 0.5510 - classification_loss: 0.1341
 6003/10000 [=================>............] - ETA: 31:43 - loss: 0.6850 - regression_loss: 0.5510 - classification_loss: 0.1341
 6004/10000 [=================>............] - ETA: 31:43 - loss: 0.6850 - regression_loss: 0.5509 - classification_loss: 0.1341
 6005/10000 [=================>............] - ETA: 31:42 - loss: 0.6849 - regression_loss: 0.5508 - classification_loss: 0.1341
 6006/10000 [=================>............] - ETA: 31:42 - loss: 0.6849 - regression_loss: 0.5508 - classification_loss: 0.1341
 6007/10000 [=================>............] - ETA: 31:41 - loss: 0.6848 - regression_loss: 0.5508 - classification_loss: 0.1341
 6008/10000 [=================>............] - ETA: 31:41 - loss: 0.6849 - regression_loss: 0.5508 - classification_loss: 0.1341
 6009/10000 [=================>............] - ETA: 31:40 - loss: 0.6848 - regression_loss: 0.5508 - classification_loss: 0.1341
 6010/10000 [=================>............] - ETA: 31:40 - loss: 0.6848 - regression_loss: 0.5507 - classification_loss: 0.1341
 6011/10000 [=================>............] - ETA: 31:39 - loss: 0.6848 - regression_loss: 0.5507 - classification_loss: 0.1341
 6012/10000 [=================>............] - ETA: 31:39 - loss: 0.6850 - regression_loss: 0.5509 - classification_loss: 0.1341
 6013/10000 [=================>............] - ETA: 31:38 - loss: 0.6850 - regression_loss: 0.5509 - classification_loss: 0.1341
 6014/10000 [=================>............] - ETA: 31:38 - loss: 0.6850 - regression_loss: 0.5509 - classification_loss: 0.1341
 6015/10000 [=================>............] - ETA: 31:37 - loss: 0.6850 - regression_loss: 0.5509 - classification_loss: 0.1341
 6016/10000 [=================>............] - ETA: 31:37 - loss: 0.6849 - regression_loss: 0.5508 - classification_loss: 0.1341
 6017/10000 [=================>............] - ETA: 31:36 - loss: 0.6849 - regression_loss: 0.5508 - classification_loss: 0.1341
 6018/10000 [=================>............] - ETA: 31:36 - loss: 0.6849 - regression_loss: 0.5508 - classification_loss: 0.1341
 6019/10000 [=================>............] - ETA: 31:35 - loss: 0.6849 - regression_loss: 0.5508 - classification_loss: 0.1341
 6020/10000 [=================>............] - ETA: 31:35 - loss: 0.6848 - regression_loss: 0.5507 - classification_loss: 0.1341
 6021/10000 [=================>............] - ETA: 31:34 - loss: 0.6848 - regression_loss: 0.5507 - classification_loss: 0.1341
 6022/10000 [=================>............] - ETA: 31:34 - loss: 0.6847 - regression_loss: 0.5507 - classification_loss: 0.1341
 6023/10000 [=================>............] - ETA: 31:33 - loss: 0.6849 - regression_loss: 0.5507 - classification_loss: 0.1342
 6024/10000 [=================>............] - ETA: 31:33 - loss: 0.6848 - regression_loss: 0.5507 - classification_loss: 0.1342
 6025/10000 [=================>............] - ETA: 31:32 - loss: 0.6848 - regression_loss: 0.5507 - classification_loss: 0.1341
 6026/10000 [=================>............] - ETA: 31:32 - loss: 0.6848 - regression_loss: 0.5506 - classification_loss: 0.1341
 6027/10000 [=================>............] - ETA: 31:31 - loss: 0.6847 - regression_loss: 0.5506 - classification_loss: 0.1341
 6028/10000 [=================>............] - ETA: 31:31 - loss: 0.6846 - regression_loss: 0.5505 - classification_loss: 0.1341
 6029/10000 [=================>............] - ETA: 31:30 - loss: 0.6846 - regression_loss: 0.5505 - classification_loss: 0.1341
 6030/10000 [=================>............] - ETA: 31:30 - loss: 0.6846 - regression_loss: 0.5505 - classification_loss: 0.1341
 6031/10000 [=================>............] - ETA: 31:29 - loss: 0.6846 - regression_loss: 0.5505 - classification_loss: 0.1341
 6032/10000 [=================>............] - ETA: 31:29 - loss: 0.6845 - regression_loss: 0.5504 - classification_loss: 0.1341
 6033/10000 [=================>............] - ETA: 31:28 - loss: 0.6846 - regression_loss: 0.5505 - classification_loss: 0.1341
 6034/10000 [=================>............] - ETA: 31:28 - loss: 0.6845 - regression_loss: 0.5504 - classification_loss: 0.1341
 6035/10000 [=================>............] - ETA: 31:27 - loss: 0.6846 - regression_loss: 0.5505 - classification_loss: 0.1341
 6036/10000 [=================>............] - ETA: 31:27 - loss: 0.6847 - regression_loss: 0.5506 - classification_loss: 0.1341
 6037/10000 [=================>............] - ETA: 31:26 - loss: 0.6846 - regression_loss: 0.5505 - classification_loss: 0.1341
 6038/10000 [=================>............] - ETA: 31:26 - loss: 0.6846 - regression_loss: 0.5505 - classification_loss: 0.1341
 6039/10000 [=================>............] - ETA: 31:25 - loss: 0.6846 - regression_loss: 0.5505 - classification_loss: 0.1341
 6040/10000 [=================>............] - ETA: 31:25 - loss: 0.6847 - regression_loss: 0.5506 - classification_loss: 0.1341
 6041/10000 [=================>............] - ETA: 31:25 - loss: 0.6847 - regression_loss: 0.5506 - classification_loss: 0.1341
 6042/10000 [=================>............] - ETA: 31:24 - loss: 0.6847 - regression_loss: 0.5506 - classification_loss: 0.1341
 6043/10000 [=================>............] - ETA: 31:24 - loss: 0.6848 - regression_loss: 0.5507 - classification_loss: 0.1342
 6044/10000 [=================>............] - ETA: 31:23 - loss: 0.6848 - regression_loss: 0.5506 - classification_loss: 0.1342
 6045/10000 [=================>............] - ETA: 31:23 - loss: 0.6847 - regression_loss: 0.5505 - classification_loss: 0.1341
 6046/10000 [=================>............] - ETA: 31:22 - loss: 0.6846 - regression_loss: 0.5505 - classification_loss: 0.1341
 6047/10000 [=================>............] - ETA: 31:22 - loss: 0.6846 - regression_loss: 0.5505 - classification_loss: 0.1341
 6048/10000 [=================>............] - ETA: 31:21 - loss: 0.6847 - regression_loss: 0.5506 - classification_loss: 0.1341
 6049/10000 [=================>............] - ETA: 31:21 - loss: 0.6847 - regression_loss: 0.5506 - classification_loss: 0.1341
 6050/10000 [=================>............] - ETA: 31:20 - loss: 0.6847 - regression_loss: 0.5506 - classification_loss: 0.1341
 6051/10000 [=================>............] - ETA: 31:20 - loss: 0.6847 - regression_loss: 0.5505 - classification_loss: 0.1341
 6052/10000 [=================>............] - ETA: 31:19 - loss: 0.6847 - regression_loss: 0.5506 - classification_loss: 0.1341
 6053/10000 [=================>............] - ETA: 31:19 - loss: 0.6846 - regression_loss: 0.5505 - classification_loss: 0.1341
 6054/10000 [=================>............] - ETA: 31:18 - loss: 0.6845 - regression_loss: 0.5504 - classification_loss: 0.1341
 6055/10000 [=================>............] - ETA: 31:18 - loss: 0.6846 - regression_loss: 0.5505 - classification_loss: 0.1341
 6056/10000 [=================>............] - ETA: 31:17 - loss: 0.6845 - regression_loss: 0.5504 - classification_loss: 0.1341
 6057/10000 [=================>............] - ETA: 31:17 - loss: 0.6845 - regression_loss: 0.5504 - classification_loss: 0.1341
 6058/10000 [=================>............] - ETA: 31:16 - loss: 0.6846 - regression_loss: 0.5505 - classification_loss: 0.1341
 6059/10000 [=================>............] - ETA: 31:16 - loss: 0.6845 - regression_loss: 0.5504 - classification_loss: 0.1341
 6060/10000 [=================>............] - ETA: 31:15 - loss: 0.6844 - regression_loss: 0.5504 - classification_loss: 0.1341
 6061/10000 [=================>............] - ETA: 31:15 - loss: 0.6844 - regression_loss: 0.5503 - classification_loss: 0.1341
 6062/10000 [=================>............] - ETA: 31:14 - loss: 0.6844 - regression_loss: 0.5503 - classification_loss: 0.1341
 6063/10000 [=================>............] - ETA: 31:14 - loss: 0.6843 - regression_loss: 0.5503 - classification_loss: 0.1341
 6064/10000 [=================>............] - ETA: 31:13 - loss: 0.6843 - regression_loss: 0.5502 - classification_loss: 0.1341
 6065/10000 [=================>............] - ETA: 31:13 - loss: 0.6844 - regression_loss: 0.5503 - classification_loss: 0.1341
 6066/10000 [=================>............] - ETA: 31:12 - loss: 0.6843 - regression_loss: 0.5502 - classification_loss: 0.1341
 6067/10000 [=================>............] - ETA: 31:12 - loss: 0.6843 - regression_loss: 0.5502 - classification_loss: 0.1341
 6068/10000 [=================>............] - ETA: 31:11 - loss: 0.6843 - regression_loss: 0.5502 - classification_loss: 0.1341
 6069/10000 [=================>............] - ETA: 31:11 - loss: 0.6843 - regression_loss: 0.5502 - classification_loss: 0.1341
 6070/10000 [=================>............] - ETA: 31:10 - loss: 0.6842 - regression_loss: 0.5501 - classification_loss: 0.1341
 6071/10000 [=================>............] - ETA: 31:10 - loss: 0.6843 - regression_loss: 0.5502 - classification_loss: 0.1341
 6072/10000 [=================>............] - ETA: 31:09 - loss: 0.6843 - regression_loss: 0.5502 - classification_loss: 0.1341
 6073/10000 [=================>............] - ETA: 31:09 - loss: 0.6842 - regression_loss: 0.5502 - classification_loss: 0.1341
 6074/10000 [=================>............] - ETA: 31:08 - loss: 0.6842 - regression_loss: 0.5501 - classification_loss: 0.1340
 6075/10000 [=================>............] - ETA: 31:08 - loss: 0.6842 - regression_loss: 0.5501 - classification_loss: 0.1340
 6076/10000 [=================>............] - ETA: 31:07 - loss: 0.6842 - regression_loss: 0.5502 - classification_loss: 0.1340
 6077/10000 [=================>............] - ETA: 31:07 - loss: 0.6841 - regression_loss: 0.5501 - classification_loss: 0.1340
 6078/10000 [=================>............] - ETA: 31:07 - loss: 0.6842 - regression_loss: 0.5502 - classification_loss: 0.1340
 6079/10000 [=================>............] - ETA: 31:06 - loss: 0.6843 - regression_loss: 0.5502 - classification_loss: 0.1341
 6080/10000 [=================>............] - ETA: 31:06 - loss: 0.6843 - regression_loss: 0.5502 - classification_loss: 0.1341
 6081/10000 [=================>............] - ETA: 31:05 - loss: 0.6843 - regression_loss: 0.5502 - classification_loss: 0.1340
 6082/10000 [=================>............] - ETA: 31:05 - loss: 0.6843 - regression_loss: 0.5502 - classification_loss: 0.1340
 6083/10000 [=================>............] - ETA: 31:04 - loss: 0.6842 - regression_loss: 0.5501 - classification_loss: 0.1340
 6084/10000 [=================>............] - ETA: 31:04 - loss: 0.6842 - regression_loss: 0.5501 - classification_loss: 0.1340
 6085/10000 [=================>............] - ETA: 31:03 - loss: 0.6841 - regression_loss: 0.5501 - classification_loss: 0.1340
 6086/10000 [=================>............] - ETA: 31:03 - loss: 0.6842 - regression_loss: 0.5502 - classification_loss: 0.1340
 6087/10000 [=================>............] - ETA: 31:02 - loss: 0.6842 - regression_loss: 0.5502 - classification_loss: 0.1340
 6088/10000 [=================>............] - ETA: 31:02 - loss: 0.6842 - regression_loss: 0.5501 - classification_loss: 0.1340
 6089/10000 [=================>............] - ETA: 31:01 - loss: 0.6842 - regression_loss: 0.5501 - classification_loss: 0.1340
 6090/10000 [=================>............] - ETA: 31:01 - loss: 0.6841 - regression_loss: 0.5501 - classification_loss: 0.1340
 6091/10000 [=================>............] - ETA: 31:00 - loss: 0.6841 - regression_loss: 0.5501 - classification_loss: 0.1340
 6092/10000 [=================>............] - ETA: 31:00 - loss: 0.6841 - regression_loss: 0.5501 - classification_loss: 0.1340
 6093/10000 [=================>............] - ETA: 30:59 - loss: 0.6840 - regression_loss: 0.5500 - classification_loss: 0.1340
 6094/10000 [=================>............] - ETA: 30:59 - loss: 0.6841 - regression_loss: 0.5501 - classification_loss: 0.1340
 6095/10000 [=================>............] - ETA: 30:58 - loss: 0.6840 - regression_loss: 0.5500 - classification_loss: 0.1340
 6096/10000 [=================>............] - ETA: 30:58 - loss: 0.6840 - regression_loss: 0.5500 - classification_loss: 0.1340
 6097/10000 [=================>............] - ETA: 30:57 - loss: 0.6841 - regression_loss: 0.5501 - classification_loss: 0.1340
 6098/10000 [=================>............] - ETA: 30:57 - loss: 0.6841 - regression_loss: 0.5501 - classification_loss: 0.1340
 6099/10000 [=================>............] - ETA: 30:56 - loss: 0.6841 - regression_loss: 0.5501 - classification_loss: 0.1340
 6100/10000 [=================>............] - ETA: 30:56 - loss: 0.6841 - regression_loss: 0.5501 - classification_loss: 0.1340
 6101/10000 [=================>............] - ETA: 30:55 - loss: 0.6840 - regression_loss: 0.5500 - classification_loss: 0.1340
 6102/10000 [=================>............] - ETA: 30:55 - loss: 0.6840 - regression_loss: 0.5500 - classification_loss: 0.1340
 6103/10000 [=================>............] - ETA: 30:54 - loss: 0.6840 - regression_loss: 0.5500 - classification_loss: 0.1340
 6104/10000 [=================>............] - ETA: 30:54 - loss: 0.6840 - regression_loss: 0.5500 - classification_loss: 0.1340
 6105/10000 [=================>............] - ETA: 30:53 - loss: 0.6840 - regression_loss: 0.5500 - classification_loss: 0.1340
 6106/10000 [=================>............] - ETA: 30:53 - loss: 0.6840 - regression_loss: 0.5500 - classification_loss: 0.1340
 6107/10000 [=================>............] - ETA: 30:52 - loss: 0.6840 - regression_loss: 0.5500 - classification_loss: 0.1340
 6108/10000 [=================>............] - ETA: 30:52 - loss: 0.6841 - regression_loss: 0.5501 - classification_loss: 0.1340
 6109/10000 [=================>............] - ETA: 30:51 - loss: 0.6841 - regression_loss: 0.5501 - classification_loss: 0.1340
 6110/10000 [=================>............] - ETA: 30:51 - loss: 0.6841 - regression_loss: 0.5501 - classification_loss: 0.1340
 6111/10000 [=================>............] - ETA: 30:50 - loss: 0.6840 - regression_loss: 0.5501 - classification_loss: 0.1340
 6112/10000 [=================>............] - ETA: 30:50 - loss: 0.6841 - regression_loss: 0.5501 - classification_loss: 0.1340
 6113/10000 [=================>............] - ETA: 30:49 - loss: 0.6842 - regression_loss: 0.5502 - classification_loss: 0.1340
 6114/10000 [=================>............] - ETA: 30:49 - loss: 0.6843 - regression_loss: 0.5503 - classification_loss: 0.1340
 6115/10000 [=================>............] - ETA: 30:49 - loss: 0.6843 - regression_loss: 0.5503 - classification_loss: 0.1340
 6116/10000 [=================>............] - ETA: 30:48 - loss: 0.6843 - regression_loss: 0.5503 - classification_loss: 0.1340
 6117/10000 [=================>............] - ETA: 30:48 - loss: 0.6843 - regression_loss: 0.5503 - classification_loss: 0.1340
 6118/10000 [=================>............] - ETA: 30:47 - loss: 0.6843 - regression_loss: 0.5503 - classification_loss: 0.1340
 6119/10000 [=================>............] - ETA: 30:47 - loss: 0.6842 - regression_loss: 0.5502 - classification_loss: 0.1340
 6120/10000 [=================>............] - ETA: 30:46 - loss: 0.6843 - regression_loss: 0.5503 - classification_loss: 0.1340
 6121/10000 [=================>............] - ETA: 30:46 - loss: 0.6844 - regression_loss: 0.5504 - classification_loss: 0.1340
 6122/10000 [=================>............] - ETA: 30:45 - loss: 0.6844 - regression_loss: 0.5503 - classification_loss: 0.1340
 6123/10000 [=================>............] - ETA: 30:45 - loss: 0.6843 - regression_loss: 0.5503 - classification_loss: 0.1340
 6124/10000 [=================>............] - ETA: 30:44 - loss: 0.6843 - regression_loss: 0.5503 - classification_loss: 0.1340
 6125/10000 [=================>............] - ETA: 30:44 - loss: 0.6844 - regression_loss: 0.5504 - classification_loss: 0.1340
 6126/10000 [=================>............] - ETA: 30:43 - loss: 0.6844 - regression_loss: 0.5504 - classification_loss: 0.1340
 6127/10000 [=================>............] - ETA: 30:43 - loss: 0.6844 - regression_loss: 0.5504 - classification_loss: 0.1340
 6128/10000 [=================>............] - ETA: 30:42 - loss: 0.6845 - regression_loss: 0.5505 - classification_loss: 0.1340
 6129/10000 [=================>............] - ETA: 30:42 - loss: 0.6844 - regression_loss: 0.5504 - classification_loss: 0.1340
 6130/10000 [=================>............] - ETA: 30:41 - loss: 0.6843 - regression_loss: 0.5503 - classification_loss: 0.1340
 6131/10000 [=================>............] - ETA: 30:41 - loss: 0.6844 - regression_loss: 0.5504 - classification_loss: 0.1340
 6132/10000 [=================>............] - ETA: 30:40 - loss: 0.6843 - regression_loss: 0.5504 - classification_loss: 0.1340
 6133/10000 [=================>............] - ETA: 30:40 - loss: 0.6843 - regression_loss: 0.5503 - classification_loss: 0.1340
 6134/10000 [=================>............] - ETA: 30:39 - loss: 0.6843 - regression_loss: 0.5504 - classification_loss: 0.1340
 6135/10000 [=================>............] - ETA: 30:39 - loss: 0.6843 - regression_loss: 0.5503 - classification_loss: 0.1340
 6136/10000 [=================>............] - ETA: 30:38 - loss: 0.6844 - regression_loss: 0.5504 - classification_loss: 0.1340
 6137/10000 [=================>............] - ETA: 30:38 - loss: 0.6843 - regression_loss: 0.5503 - classification_loss: 0.1340
 6138/10000 [=================>............] - ETA: 30:37 - loss: 0.6842 - regression_loss: 0.5503 - classification_loss: 0.1340
 6139/10000 [=================>............] - ETA: 30:37 - loss: 0.6843 - regression_loss: 0.5503 - classification_loss: 0.1340
 6140/10000 [=================>............] - ETA: 30:36 - loss: 0.6842 - regression_loss: 0.5503 - classification_loss: 0.1340
 6141/10000 [=================>............] - ETA: 30:36 - loss: 0.6843 - regression_loss: 0.5503 - classification_loss: 0.1340
 6142/10000 [=================>............] - ETA: 30:35 - loss: 0.6843 - regression_loss: 0.5504 - classification_loss: 0.1340
 6143/10000 [=================>............] - ETA: 30:35 - loss: 0.6844 - regression_loss: 0.5504 - classification_loss: 0.1340
 6144/10000 [=================>............] - ETA: 30:34 - loss: 0.6843 - regression_loss: 0.5504 - classification_loss: 0.1340
 6145/10000 [=================>............] - ETA: 30:34 - loss: 0.6843 - regression_loss: 0.5504 - classification_loss: 0.1340
 6146/10000 [=================>............] - ETA: 30:33 - loss: 0.6843 - regression_loss: 0.5503 - classification_loss: 0.1340
 6147/10000 [=================>............] - ETA: 30:33 - loss: 0.6843 - regression_loss: 0.5504 - classification_loss: 0.1340
 6148/10000 [=================>............] - ETA: 30:32 - loss: 0.6845 - regression_loss: 0.5505 - classification_loss: 0.1340
 6149/10000 [=================>............] - ETA: 30:32 - loss: 0.6844 - regression_loss: 0.5504 - classification_loss: 0.1340
 6150/10000 [=================>............] - ETA: 30:32 - loss: 0.6844 - regression_loss: 0.5504 - classification_loss: 0.1339
 6151/10000 [=================>............] - ETA: 30:31 - loss: 0.6843 - regression_loss: 0.5504 - classification_loss: 0.1339
 6152/10000 [=================>............] - ETA: 30:31 - loss: 0.6843 - regression_loss: 0.5504 - classification_loss: 0.1339
 6153/10000 [=================>............] - ETA: 30:30 - loss: 0.6844 - regression_loss: 0.5504 - classification_loss: 0.1340
 6154/10000 [=================>............] - ETA: 30:30 - loss: 0.6844 - regression_loss: 0.5505 - classification_loss: 0.1340
 6155/10000 [=================>............] - ETA: 30:29 - loss: 0.6844 - regression_loss: 0.5504 - classification_loss: 0.1340
 6156/10000 [=================>............] - ETA: 30:29 - loss: 0.6844 - regression_loss: 0.5504 - classification_loss: 0.1340
 6157/10000 [=================>............] - ETA: 30:28 - loss: 0.6843 - regression_loss: 0.5503 - classification_loss: 0.1339
 6158/10000 [=================>............] - ETA: 30:28 - loss: 0.6844 - regression_loss: 0.5504 - classification_loss: 0.1339
 6159/10000 [=================>............] - ETA: 30:27 - loss: 0.6843 - regression_loss: 0.5504 - classification_loss: 0.1339
 6160/10000 [=================>............] - ETA: 30:27 - loss: 0.6842 - regression_loss: 0.5503 - classification_loss: 0.1339
 6161/10000 [=================>............] - ETA: 30:26 - loss: 0.6843 - regression_loss: 0.5503 - classification_loss: 0.1339
 6162/10000 [=================>............] - ETA: 30:26 - loss: 0.6842 - regression_loss: 0.5502 - classification_loss: 0.1339
 6163/10000 [=================>............] - ETA: 30:25 - loss: 0.6841 - regression_loss: 0.5502 - classification_loss: 0.1339
 6164/10000 [=================>............] - ETA: 30:25 - loss: 0.6840 - regression_loss: 0.5501 - classification_loss: 0.1339
 6165/10000 [=================>............] - ETA: 30:24 - loss: 0.6840 - regression_loss: 0.5501 - classification_loss: 0.1339
 6166/10000 [=================>............] - ETA: 30:24 - loss: 0.6840 - regression_loss: 0.5501 - classification_loss: 0.1339
 6167/10000 [=================>............] - ETA: 30:23 - loss: 0.6839 - regression_loss: 0.5501 - classification_loss: 0.1338
 6168/10000 [=================>............] - ETA: 30:23 - loss: 0.6839 - regression_loss: 0.5501 - classification_loss: 0.1339
 6169/10000 [=================>............] - ETA: 30:22 - loss: 0.6839 - regression_loss: 0.5500 - classification_loss: 0.1338
 6170/10000 [=================>............] - ETA: 30:22 - loss: 0.6838 - regression_loss: 0.5500 - classification_loss: 0.1338
 6171/10000 [=================>............] - ETA: 30:21 - loss: 0.6839 - regression_loss: 0.5500 - classification_loss: 0.1338
 6172/10000 [=================>............] - ETA: 30:21 - loss: 0.6838 - regression_loss: 0.5500 - classification_loss: 0.1338
 6173/10000 [=================>............] - ETA: 30:20 - loss: 0.6838 - regression_loss: 0.5500 - classification_loss: 0.1338
 6174/10000 [=================>............] - ETA: 30:20 - loss: 0.6838 - regression_loss: 0.5500 - classification_loss: 0.1338
 6175/10000 [=================>............] - ETA: 30:19 - loss: 0.6838 - regression_loss: 0.5500 - classification_loss: 0.1338
 6176/10000 [=================>............] - ETA: 30:19 - loss: 0.6837 - regression_loss: 0.5499 - classification_loss: 0.1338
 6177/10000 [=================>............] - ETA: 30:18 - loss: 0.6837 - regression_loss: 0.5499 - classification_loss: 0.1338
 6178/10000 [=================>............] - ETA: 30:18 - loss: 0.6837 - regression_loss: 0.5498 - classification_loss: 0.1338
 6179/10000 [=================>............] - ETA: 30:17 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6180/10000 [=================>............] - ETA: 30:17 - loss: 0.6837 - regression_loss: 0.5498 - classification_loss: 0.1338
 6181/10000 [=================>............] - ETA: 30:16 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6182/10000 [=================>............] - ETA: 30:16 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6183/10000 [=================>............] - ETA: 30:15 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6184/10000 [=================>............] - ETA: 30:15 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6185/10000 [=================>............] - ETA: 30:15 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6186/10000 [=================>............] - ETA: 30:14 - loss: 0.6837 - regression_loss: 0.5498 - classification_loss: 0.1338
 6187/10000 [=================>............] - ETA: 30:14 - loss: 0.6837 - regression_loss: 0.5499 - classification_loss: 0.1339
 6188/10000 [=================>............] - ETA: 30:13 - loss: 0.6837 - regression_loss: 0.5498 - classification_loss: 0.1339
 6189/10000 [=================>............] - ETA: 30:13 - loss: 0.6837 - regression_loss: 0.5498 - classification_loss: 0.1339
 6190/10000 [=================>............] - ETA: 30:12 - loss: 0.6837 - regression_loss: 0.5498 - classification_loss: 0.1339
 6191/10000 [=================>............] - ETA: 30:12 - loss: 0.6837 - regression_loss: 0.5498 - classification_loss: 0.1339
 6192/10000 [=================>............] - ETA: 30:11 - loss: 0.6837 - regression_loss: 0.5499 - classification_loss: 0.1339
 6193/10000 [=================>............] - ETA: 30:11 - loss: 0.6838 - regression_loss: 0.5499 - classification_loss: 0.1339
 6194/10000 [=================>............] - ETA: 30:10 - loss: 0.6839 - regression_loss: 0.5500 - classification_loss: 0.1339
 6195/10000 [=================>............] - ETA: 30:10 - loss: 0.6840 - regression_loss: 0.5500 - classification_loss: 0.1339
 6196/10000 [=================>............] - ETA: 30:09 - loss: 0.6840 - regression_loss: 0.5501 - classification_loss: 0.1339
 6197/10000 [=================>............] - ETA: 30:09 - loss: 0.6841 - regression_loss: 0.5501 - classification_loss: 0.1340
 6198/10000 [=================>............] - ETA: 30:08 - loss: 0.6841 - regression_loss: 0.5501 - classification_loss: 0.1339
 6199/10000 [=================>............] - ETA: 30:08 - loss: 0.6840 - regression_loss: 0.5501 - classification_loss: 0.1339
 6200/10000 [=================>............] - ETA: 30:07 - loss: 0.6839 - regression_loss: 0.5500 - classification_loss: 0.1339
 6201/10000 [=================>............] - ETA: 30:07 - loss: 0.6839 - regression_loss: 0.5500 - classification_loss: 0.1339
 6202/10000 [=================>............] - ETA: 30:06 - loss: 0.6840 - regression_loss: 0.5500 - classification_loss: 0.1339
 6203/10000 [=================>............] - ETA: 30:06 - loss: 0.6840 - regression_loss: 0.5501 - classification_loss: 0.1339
 6204/10000 [=================>............] - ETA: 30:05 - loss: 0.6840 - regression_loss: 0.5501 - classification_loss: 0.1339
 6205/10000 [=================>............] - ETA: 30:05 - loss: 0.6841 - regression_loss: 0.5502 - classification_loss: 0.1339
 6206/10000 [=================>............] - ETA: 30:04 - loss: 0.6841 - regression_loss: 0.5502 - classification_loss: 0.1339
 6207/10000 [=================>............] - ETA: 30:04 - loss: 0.6841 - regression_loss: 0.5502 - classification_loss: 0.1339
 6208/10000 [=================>............] - ETA: 30:03 - loss: 0.6842 - regression_loss: 0.5502 - classification_loss: 0.1340
 6209/10000 [=================>............] - ETA: 30:03 - loss: 0.6842 - regression_loss: 0.5502 - classification_loss: 0.1339
 6210/10000 [=================>............] - ETA: 30:02 - loss: 0.6841 - regression_loss: 0.5502 - classification_loss: 0.1339
 6211/10000 [=================>............] - ETA: 30:02 - loss: 0.6842 - regression_loss: 0.5502 - classification_loss: 0.1339
 6212/10000 [=================>............] - ETA: 30:01 - loss: 0.6841 - regression_loss: 0.5501 - classification_loss: 0.1339
 6213/10000 [=================>............] - ETA: 30:01 - loss: 0.6841 - regression_loss: 0.5501 - classification_loss: 0.1339
 6214/10000 [=================>............] - ETA: 30:00 - loss: 0.6840 - regression_loss: 0.5501 - classification_loss: 0.1339
 6215/10000 [=================>............] - ETA: 30:00 - loss: 0.6840 - regression_loss: 0.5501 - classification_loss: 0.1339
 6216/10000 [=================>............] - ETA: 29:59 - loss: 0.6840 - regression_loss: 0.5501 - classification_loss: 0.1339
 6217/10000 [=================>............] - ETA: 29:59 - loss: 0.6840 - regression_loss: 0.5501 - classification_loss: 0.1339
 6218/10000 [=================>............] - ETA: 29:58 - loss: 0.6840 - regression_loss: 0.5501 - classification_loss: 0.1339
 6219/10000 [=================>............] - ETA: 29:58 - loss: 0.6840 - regression_loss: 0.5501 - classification_loss: 0.1339
 6220/10000 [=================>............] - ETA: 29:57 - loss: 0.6839 - regression_loss: 0.5501 - classification_loss: 0.1339
 6221/10000 [=================>............] - ETA: 29:57 - loss: 0.6838 - regression_loss: 0.5500 - classification_loss: 0.1338
 6222/10000 [=================>............] - ETA: 29:56 - loss: 0.6839 - regression_loss: 0.5500 - classification_loss: 0.1338
 6223/10000 [=================>............] - ETA: 29:56 - loss: 0.6839 - regression_loss: 0.5500 - classification_loss: 0.1338
 6224/10000 [=================>............] - ETA: 29:55 - loss: 0.6840 - regression_loss: 0.5501 - classification_loss: 0.1339
 6225/10000 [=================>............] - ETA: 29:55 - loss: 0.6839 - regression_loss: 0.5500 - classification_loss: 0.1339
 6226/10000 [=================>............] - ETA: 29:54 - loss: 0.6838 - regression_loss: 0.5500 - classification_loss: 0.1338
 6227/10000 [=================>............] - ETA: 29:54 - loss: 0.6838 - regression_loss: 0.5500 - classification_loss: 0.1339
 6228/10000 [=================>............] - ETA: 29:53 - loss: 0.6838 - regression_loss: 0.5499 - classification_loss: 0.1339
 6229/10000 [=================>............] - ETA: 29:53 - loss: 0.6838 - regression_loss: 0.5499 - classification_loss: 0.1339
 6230/10000 [=================>............] - ETA: 29:52 - loss: 0.6839 - regression_loss: 0.5500 - classification_loss: 0.1339
 6231/10000 [=================>............] - ETA: 29:52 - loss: 0.6839 - regression_loss: 0.5500 - classification_loss: 0.1339
 6232/10000 [=================>............] - ETA: 29:51 - loss: 0.6839 - regression_loss: 0.5500 - classification_loss: 0.1339
 6233/10000 [=================>............] - ETA: 29:51 - loss: 0.6839 - regression_loss: 0.5500 - classification_loss: 0.1339
 6234/10000 [=================>............] - ETA: 29:51 - loss: 0.6839 - regression_loss: 0.5500 - classification_loss: 0.1339
 6235/10000 [=================>............] - ETA: 29:50 - loss: 0.6839 - regression_loss: 0.5500 - classification_loss: 0.1339
 6236/10000 [=================>............] - ETA: 29:49 - loss: 0.6838 - regression_loss: 0.5499 - classification_loss: 0.1339
 6237/10000 [=================>............] - ETA: 29:49 - loss: 0.6837 - regression_loss: 0.5498 - classification_loss: 0.1339
 6238/10000 [=================>............] - ETA: 29:49 - loss: 0.6837 - regression_loss: 0.5498 - classification_loss: 0.1339
 6239/10000 [=================>............] - ETA: 29:48 - loss: 0.6837 - regression_loss: 0.5498 - classification_loss: 0.1339
 6240/10000 [=================>............] - ETA: 29:48 - loss: 0.6837 - regression_loss: 0.5498 - classification_loss: 0.1339
 6241/10000 [=================>............] - ETA: 29:47 - loss: 0.6838 - regression_loss: 0.5498 - classification_loss: 0.1339
 6242/10000 [=================>............] - ETA: 29:47 - loss: 0.6837 - regression_loss: 0.5498 - classification_loss: 0.1339
 6243/10000 [=================>............] - ETA: 29:46 - loss: 0.6837 - regression_loss: 0.5498 - classification_loss: 0.1339
 6244/10000 [=================>............] - ETA: 29:46 - loss: 0.6836 - regression_loss: 0.5497 - classification_loss: 0.1339
 6245/10000 [=================>............] - ETA: 29:45 - loss: 0.6837 - regression_loss: 0.5497 - classification_loss: 0.1339
 6246/10000 [=================>............] - ETA: 29:45 - loss: 0.6836 - regression_loss: 0.5497 - classification_loss: 0.1339
 6247/10000 [=================>............] - ETA: 29:44 - loss: 0.6837 - regression_loss: 0.5498 - classification_loss: 0.1339
 6248/10000 [=================>............] - ETA: 29:44 - loss: 0.6836 - regression_loss: 0.5497 - classification_loss: 0.1339
 6249/10000 [=================>............] - ETA: 29:43 - loss: 0.6836 - regression_loss: 0.5497 - classification_loss: 0.1339
 6250/10000 [=================>............] - ETA: 29:43 - loss: 0.6836 - regression_loss: 0.5497 - classification_loss: 0.1339
 6251/10000 [=================>............] - ETA: 29:42 - loss: 0.6836 - regression_loss: 0.5497 - classification_loss: 0.1339
 6252/10000 [=================>............] - ETA: 29:42 - loss: 0.6836 - regression_loss: 0.5497 - classification_loss: 0.1339
 6253/10000 [=================>............] - ETA: 29:41 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1339
 6254/10000 [=================>............] - ETA: 29:41 - loss: 0.6837 - regression_loss: 0.5498 - classification_loss: 0.1339
 6255/10000 [=================>............] - ETA: 29:40 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1339
 6256/10000 [=================>............] - ETA: 29:40 - loss: 0.6836 - regression_loss: 0.5497 - classification_loss: 0.1339
 6257/10000 [=================>............] - ETA: 29:39 - loss: 0.6836 - regression_loss: 0.5497 - classification_loss: 0.1339
 6258/10000 [=================>............] - ETA: 29:39 - loss: 0.6836 - regression_loss: 0.5497 - classification_loss: 0.1339
 6259/10000 [=================>............] - ETA: 29:38 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6260/10000 [=================>............] - ETA: 29:38 - loss: 0.6835 - regression_loss: 0.5496 - classification_loss: 0.1338
 6261/10000 [=================>............] - ETA: 29:37 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6262/10000 [=================>............] - ETA: 29:37 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6263/10000 [=================>............] - ETA: 29:36 - loss: 0.6833 - regression_loss: 0.5495 - classification_loss: 0.1338
 6264/10000 [=================>............] - ETA: 29:36 - loss: 0.6832 - regression_loss: 0.5495 - classification_loss: 0.1338
 6265/10000 [=================>............] - ETA: 29:35 - loss: 0.6832 - regression_loss: 0.5494 - classification_loss: 0.1338
 6266/10000 [=================>............] - ETA: 29:35 - loss: 0.6831 - regression_loss: 0.5494 - classification_loss: 0.1338
 6267/10000 [=================>............] - ETA: 29:34 - loss: 0.6832 - regression_loss: 0.5495 - classification_loss: 0.1338
 6268/10000 [=================>............] - ETA: 29:34 - loss: 0.6832 - regression_loss: 0.5494 - classification_loss: 0.1338
 6269/10000 [=================>............] - ETA: 29:33 - loss: 0.6832 - regression_loss: 0.5494 - classification_loss: 0.1337
 6270/10000 [=================>............] - ETA: 29:33 - loss: 0.6831 - regression_loss: 0.5494 - classification_loss: 0.1337
 6271/10000 [=================>............] - ETA: 29:33 - loss: 0.6831 - regression_loss: 0.5493 - classification_loss: 0.1337
 6272/10000 [=================>............] - ETA: 29:32 - loss: 0.6831 - regression_loss: 0.5494 - classification_loss: 0.1337
 6273/10000 [=================>............] - ETA: 29:32 - loss: 0.6831 - regression_loss: 0.5494 - classification_loss: 0.1337
 6274/10000 [=================>............] - ETA: 29:31 - loss: 0.6831 - regression_loss: 0.5494 - classification_loss: 0.1337
 6275/10000 [=================>............] - ETA: 29:31 - loss: 0.6832 - regression_loss: 0.5495 - classification_loss: 0.1337
 6276/10000 [=================>............] - ETA: 29:30 - loss: 0.6832 - regression_loss: 0.5494 - classification_loss: 0.1337
 6277/10000 [=================>............] - ETA: 29:30 - loss: 0.6831 - regression_loss: 0.5494 - classification_loss: 0.1337
 6278/10000 [=================>............] - ETA: 29:29 - loss: 0.6832 - regression_loss: 0.5495 - classification_loss: 0.1337
 6279/10000 [=================>............] - ETA: 29:29 - loss: 0.6833 - regression_loss: 0.5496 - classification_loss: 0.1337
 6280/10000 [=================>............] - ETA: 29:28 - loss: 0.6833 - regression_loss: 0.5495 - classification_loss: 0.1337
 6281/10000 [=================>............] - ETA: 29:28 - loss: 0.6832 - regression_loss: 0.5495 - classification_loss: 0.1337
 6282/10000 [=================>............] - ETA: 29:27 - loss: 0.6833 - regression_loss: 0.5496 - classification_loss: 0.1337
 6283/10000 [=================>............] - ETA: 29:27 - loss: 0.6834 - regression_loss: 0.5497 - classification_loss: 0.1338
 6284/10000 [=================>............] - ETA: 29:26 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6285/10000 [=================>............] - ETA: 29:26 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6286/10000 [=================>............] - ETA: 29:25 - loss: 0.6835 - regression_loss: 0.5498 - classification_loss: 0.1338
 6287/10000 [=================>............] - ETA: 29:25 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6288/10000 [=================>............] - ETA: 29:24 - loss: 0.6834 - regression_loss: 0.5497 - classification_loss: 0.1337
 6289/10000 [=================>............] - ETA: 29:24 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6290/10000 [=================>............] - ETA: 29:23 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6291/10000 [=================>............] - ETA: 29:23 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6292/10000 [=================>............] - ETA: 29:22 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6293/10000 [=================>............] - ETA: 29:22 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6294/10000 [=================>............] - ETA: 29:21 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6295/10000 [=================>............] - ETA: 29:21 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6296/10000 [=================>............] - ETA: 29:20 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6297/10000 [=================>............] - ETA: 29:20 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6298/10000 [=================>............] - ETA: 29:19 - loss: 0.6837 - regression_loss: 0.5499 - classification_loss: 0.1338
 6299/10000 [=================>............] - ETA: 29:19 - loss: 0.6837 - regression_loss: 0.5499 - classification_loss: 0.1338
 6300/10000 [=================>............] - ETA: 29:18 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6301/10000 [=================>............] - ETA: 29:18 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6302/10000 [=================>............] - ETA: 29:18 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6303/10000 [=================>............] - ETA: 29:17 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6304/10000 [=================>............] - ETA: 29:17 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6305/10000 [=================>............] - ETA: 29:16 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6306/10000 [=================>............] - ETA: 29:16 - loss: 0.6833 - regression_loss: 0.5496 - classification_loss: 0.1338
 6307/10000 [=================>............] - ETA: 29:15 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6308/10000 [=================>............] - ETA: 29:15 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6309/10000 [=================>............] - ETA: 29:14 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6310/10000 [=================>............] - ETA: 29:14 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6311/10000 [=================>............] - ETA: 29:13 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6312/10000 [=================>............] - ETA: 29:13 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6313/10000 [=================>............] - ETA: 29:12 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6314/10000 [=================>............] - ETA: 29:12 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6315/10000 [=================>............] - ETA: 29:11 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6316/10000 [=================>............] - ETA: 29:11 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6317/10000 [=================>............] - ETA: 29:10 - loss: 0.6835 - regression_loss: 0.5498 - classification_loss: 0.1338
 6318/10000 [=================>............] - ETA: 29:10 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6319/10000 [=================>............] - ETA: 29:09 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6320/10000 [=================>............] - ETA: 29:09 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6321/10000 [=================>............] - ETA: 29:08 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6322/10000 [=================>............] - ETA: 29:08 - loss: 0.6834 - regression_loss: 0.5497 - classification_loss: 0.1338
 6323/10000 [=================>............] - ETA: 29:07 - loss: 0.6834 - regression_loss: 0.5497 - classification_loss: 0.1338
 6324/10000 [=================>............] - ETA: 29:07 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6325/10000 [=================>............] - ETA: 29:06 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6326/10000 [=================>............] - ETA: 29:06 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6327/10000 [=================>............] - ETA: 29:05 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6328/10000 [=================>............] - ETA: 29:05 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6329/10000 [=================>............] - ETA: 29:04 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6330/10000 [=================>............] - ETA: 29:04 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6331/10000 [=================>............] - ETA: 29:03 - loss: 0.6837 - regression_loss: 0.5499 - classification_loss: 0.1338
 6332/10000 [=================>............] - ETA: 29:03 - loss: 0.6837 - regression_loss: 0.5499 - classification_loss: 0.1338
 6333/10000 [=================>............] - ETA: 29:02 - loss: 0.6837 - regression_loss: 0.5498 - classification_loss: 0.1338
 6334/10000 [==================>...........] - ETA: 29:02 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6335/10000 [==================>...........] - ETA: 29:01 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6336/10000 [==================>...........] - ETA: 29:01 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6337/10000 [==================>...........] - ETA: 29:00 - loss: 0.6834 - regression_loss: 0.5497 - classification_loss: 0.1338
 6338/10000 [==================>...........] - ETA: 29:00 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6339/10000 [==================>...........] - ETA: 28:59 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6340/10000 [==================>...........] - ETA: 28:59 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6341/10000 [==================>...........] - ETA: 28:58 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6342/10000 [==================>...........] - ETA: 28:58 - loss: 0.6834 - regression_loss: 0.5497 - classification_loss: 0.1338
 6343/10000 [==================>...........] - ETA: 28:57 - loss: 0.6833 - regression_loss: 0.5496 - classification_loss: 0.1337
 6344/10000 [==================>...........] - ETA: 28:57 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6345/10000 [==================>...........] - ETA: 28:56 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6346/10000 [==================>...........] - ETA: 28:56 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6347/10000 [==================>...........] - ETA: 28:55 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6348/10000 [==================>...........] - ETA: 28:55 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6349/10000 [==================>...........] - ETA: 28:55 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6350/10000 [==================>...........] - ETA: 28:54 - loss: 0.6833 - regression_loss: 0.5495 - classification_loss: 0.1338
 6351/10000 [==================>...........] - ETA: 28:54 - loss: 0.6833 - regression_loss: 0.5495 - classification_loss: 0.1338
 6352/10000 [==================>...........] - ETA: 28:53 - loss: 0.6833 - regression_loss: 0.5496 - classification_loss: 0.1338
 6353/10000 [==================>...........] - ETA: 28:53 - loss: 0.6833 - regression_loss: 0.5495 - classification_loss: 0.1338
 6354/10000 [==================>...........] - ETA: 28:52 - loss: 0.6832 - regression_loss: 0.5495 - classification_loss: 0.1338
 6355/10000 [==================>...........] - ETA: 28:52 - loss: 0.6833 - regression_loss: 0.5496 - classification_loss: 0.1338
 6356/10000 [==================>...........] - ETA: 28:51 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6357/10000 [==================>...........] - ETA: 28:51 - loss: 0.6833 - regression_loss: 0.5496 - classification_loss: 0.1338
 6358/10000 [==================>...........] - ETA: 28:50 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6359/10000 [==================>...........] - ETA: 28:50 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6360/10000 [==================>...........] - ETA: 28:49 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6361/10000 [==================>...........] - ETA: 28:49 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6362/10000 [==================>...........] - ETA: 28:48 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6363/10000 [==================>...........] - ETA: 28:48 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6364/10000 [==================>...........] - ETA: 28:47 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6365/10000 [==================>...........] - ETA: 28:47 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6366/10000 [==================>...........] - ETA: 28:46 - loss: 0.6833 - regression_loss: 0.5495 - classification_loss: 0.1338
 6367/10000 [==================>...........] - ETA: 28:46 - loss: 0.6833 - regression_loss: 0.5495 - classification_loss: 0.1338
 6368/10000 [==================>...........] - ETA: 28:45 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6369/10000 [==================>...........] - ETA: 28:45 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6370/10000 [==================>...........] - ETA: 28:44 - loss: 0.6833 - regression_loss: 0.5495 - classification_loss: 0.1338
 6371/10000 [==================>...........] - ETA: 28:44 - loss: 0.6833 - regression_loss: 0.5496 - classification_loss: 0.1338
 6372/10000 [==================>...........] - ETA: 28:43 - loss: 0.6833 - regression_loss: 0.5495 - classification_loss: 0.1338
 6373/10000 [==================>...........] - ETA: 28:43 - loss: 0.6833 - regression_loss: 0.5495 - classification_loss: 0.1338
 6374/10000 [==================>...........] - ETA: 28:42 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6375/10000 [==================>...........] - ETA: 28:42 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6376/10000 [==================>...........] - ETA: 28:41 - loss: 0.6833 - regression_loss: 0.5495 - classification_loss: 0.1338
 6377/10000 [==================>...........] - ETA: 28:41 - loss: 0.6833 - regression_loss: 0.5496 - classification_loss: 0.1337
 6378/10000 [==================>...........] - ETA: 28:40 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6379/10000 [==================>...........] - ETA: 28:40 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6380/10000 [==================>...........] - ETA: 28:39 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6381/10000 [==================>...........] - ETA: 28:39 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6382/10000 [==================>...........] - ETA: 28:38 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6383/10000 [==================>...........] - ETA: 28:38 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6384/10000 [==================>...........] - ETA: 28:38 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6385/10000 [==================>...........] - ETA: 28:37 - loss: 0.6836 - regression_loss: 0.5497 - classification_loss: 0.1338
 6386/10000 [==================>...........] - ETA: 28:37 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6387/10000 [==================>...........] - ETA: 28:36 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6388/10000 [==================>...........] - ETA: 28:36 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6389/10000 [==================>...........] - ETA: 28:35 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6390/10000 [==================>...........] - ETA: 28:35 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6391/10000 [==================>...........] - ETA: 28:34 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6392/10000 [==================>...........] - ETA: 28:34 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6393/10000 [==================>...........] - ETA: 28:33 - loss: 0.6835 - regression_loss: 0.5498 - classification_loss: 0.1338
 6394/10000 [==================>...........] - ETA: 28:33 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6395/10000 [==================>...........] - ETA: 28:32 - loss: 0.6835 - regression_loss: 0.5498 - classification_loss: 0.1338
 6396/10000 [==================>...........] - ETA: 28:32 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6397/10000 [==================>...........] - ETA: 28:31 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6398/10000 [==================>...........] - ETA: 28:31 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6399/10000 [==================>...........] - ETA: 28:30 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6400/10000 [==================>...........] - ETA: 28:30 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6401/10000 [==================>...........] - ETA: 28:29 - loss: 0.6836 - regression_loss: 0.5498 - classification_loss: 0.1338
 6402/10000 [==================>...........] - ETA: 28:29 - loss: 0.6836 - regression_loss: 0.5497 - classification_loss: 0.1338
 6403/10000 [==================>...........] - ETA: 28:28 - loss: 0.6836 - regression_loss: 0.5497 - classification_loss: 0.1338
 6404/10000 [==================>...........] - ETA: 28:28 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6405/10000 [==================>...........] - ETA: 28:27 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6406/10000 [==================>...........] - ETA: 28:27 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6407/10000 [==================>...........] - ETA: 28:26 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6408/10000 [==================>...........] - ETA: 28:26 - loss: 0.6834 - regression_loss: 0.5496 - classification_loss: 0.1338
 6409/10000 [==================>...........] - ETA: 28:25 - loss: 0.6834 - regression_loss: 0.5497 - classification_loss: 0.1338
 6410/10000 [==================>...........] - ETA: 28:25 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6411/10000 [==================>...........] - ETA: 28:24 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6412/10000 [==================>...........] - ETA: 28:24 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6413/10000 [==================>...........] - ETA: 28:23 - loss: 0.6835 - regression_loss: 0.5497 - classification_loss: 0.1338
 6414/10000 [==================>...........] - ETA: 28:23 - loss: 0.6834 - regression_loss: 0.5497 - classification_loss: 0.1338
 6415/10000 [==================>...........] - ETA: 28:22 - loss: 0.6833 - regression_loss: 0.5496 - classification_loss: 0.1337
 6416/10000 [==================>...........] - ETA: 28:22 - loss: 0.6833 - regression_loss: 0.5496 - classification_loss: 0.1338
 6417/10000 [==================>...........] - ETA: 28:21 - loss: 0.6833 - regression_loss: 0.5495 - classification_loss: 0.1337
 6418/10000 [==================>...........] - ETA: 28:21 - loss: 0.6832 - regression_loss: 0.5495 - classification_loss: 0.1337
 6419/10000 [==================>...........] - ETA: 28:20 - loss: 0.6832 - regression_loss: 0.5494 - classification_loss: 0.1337
 6420/10000 [==================>...........] - ETA: 28:20 - loss: 0.6831 - regression_loss: 0.5494 - classification_loss: 0.1337
 6421/10000 [==================>...........] - ETA: 28:19 - loss: 0.6830 - regression_loss: 0.5493 - classification_loss: 0.1337
 6422/10000 [==================>...........] - ETA: 28:19 - loss: 0.6831 - regression_loss: 0.5493 - classification_loss: 0.1337
 6423/10000 [==================>...........] - ETA: 28:18 - loss: 0.6830 - regression_loss: 0.5493 - classification_loss: 0.1337
 6424/10000 [==================>...........] - ETA: 28:18 - loss: 0.6831 - regression_loss: 0.5493 - classification_loss: 0.1337
 6425/10000 [==================>...........] - ETA: 28:18 - loss: 0.6831 - regression_loss: 0.5493 - classification_loss: 0.1337
 6426/10000 [==================>...........] - ETA: 28:17 - loss: 0.6830 - regression_loss: 0.5493 - classification_loss: 0.1337
 6427/10000 [==================>...........] - ETA: 28:17 - loss: 0.6829 - regression_loss: 0.5492 - classification_loss: 0.1337
 6428/10000 [==================>...........] - ETA: 28:16 - loss: 0.6831 - regression_loss: 0.5493 - classification_loss: 0.1337
 6429/10000 [==================>...........] - ETA: 28:16 - loss: 0.6831 - regression_loss: 0.5494 - classification_loss: 0.1337
 6430/10000 [==================>...........] - ETA: 28:15 - loss: 0.6831 - regression_loss: 0.5494 - classification_loss: 0.1337
 6431/10000 [==================>...........] - ETA: 28:15 - loss: 0.6833 - regression_loss: 0.5496 - classification_loss: 0.1337
 6432/10000 [==================>...........] - ETA: 28:14 - loss: 0.6832 - regression_loss: 0.5495 - classification_loss: 0.1337
 6433/10000 [==================>...........] - ETA: 28:14 - loss: 0.6833 - regression_loss: 0.5495 - classification_loss: 0.1337
 6434/10000 [==================>...........] - ETA: 28:13 - loss: 0.6832 - regression_loss: 0.5495 - classification_loss: 0.1337
 6435/10000 [==================>...........] - ETA: 28:13 - loss: 0.6832 - regression_loss: 0.5495 - classification_loss: 0.1337
 6436/10000 [==================>...........] - ETA: 28:12 - loss: 0.6832 - regression_loss: 0.5494 - classification_loss: 0.1337
 6437/10000 [==================>...........] - ETA: 28:12 - loss: 0.6831 - regression_loss: 0.5494 - classification_loss: 0.1337
 6438/10000 [==================>...........] - ETA: 28:11 - loss: 0.6830 - regression_loss: 0.5493 - classification_loss: 0.1337
 6439/10000 [==================>...........] - ETA: 28:11 - loss: 0.6830 - regression_loss: 0.5493 - classification_loss: 0.1337
 6440/10000 [==================>...........] - ETA: 28:10 - loss: 0.6831 - regression_loss: 0.5494 - classification_loss: 0.1337
 6441/10000 [==================>...........] - ETA: 28:10 - loss: 0.6831 - regression_loss: 0.5494 - classification_loss: 0.1337
 6442/10000 [==================>...........] - ETA: 28:09 - loss: 0.6830 - regression_loss: 0.5494 - classification_loss: 0.1337
 6443/10000 [==================>...........] - ETA: 28:09 - loss: 0.6830 - regression_loss: 0.5493 - classification_loss: 0.1336
 6444/10000 [==================>...........] - ETA: 28:08 - loss: 0.6830 - regression_loss: 0.5493 - classification_loss: 0.1336
 6445/10000 [==================>...........] - ETA: 28:08 - loss: 0.6830 - regression_loss: 0.5494 - classification_loss: 0.1336
 6446/10000 [==================>...........] - ETA: 28:07 - loss: 0.6830 - regression_loss: 0.5494 - classification_loss: 0.1336
 6447/10000 [==================>...........] - ETA: 28:07 - loss: 0.6830 - regression_loss: 0.5494 - classification_loss: 0.1336
 6448/10000 [==================>...........] - ETA: 28:06 - loss: 0.6830 - regression_loss: 0.5494 - classification_loss: 0.1336
 6449/10000 [==================>...........] - ETA: 28:06 - loss: 0.6830 - regression_loss: 0.5493 - classification_loss: 0.1336
 6450/10000 [==================>...........] - ETA: 28:06 - loss: 0.6829 - regression_loss: 0.5493 - classification_loss: 0.1336
 6451/10000 [==================>...........] - ETA: 28:05 - loss: 0.6831 - regression_loss: 0.5494 - classification_loss: 0.1337
 6452/10000 [==================>...........] - ETA: 28:05 - loss: 0.6831 - regression_loss: 0.5494 - classification_loss: 0.1336
 6453/10000 [==================>...........] - ETA: 28:04 - loss: 0.6830 - regression_loss: 0.5494 - classification_loss: 0.1336
 6454/10000 [==================>...........] - ETA: 28:04 - loss: 0.6830 - regression_loss: 0.5494 - classification_loss: 0.1336
 6455/10000 [==================>...........] - ETA: 28:03 - loss: 0.6829 - regression_loss: 0.5493 - classification_loss: 0.1336
 6456/10000 [==================>...........] - ETA: 28:03 - loss: 0.6829 - regression_loss: 0.5493 - classification_loss: 0.1336
 6457/10000 [==================>...........] - ETA: 28:02 - loss: 0.6829 - regression_loss: 0.5493 - classification_loss: 0.1336
 6458/10000 [==================>...........] - ETA: 28:02 - loss: 0.6830 - regression_loss: 0.5494 - classification_loss: 0.1336
 6459/10000 [==================>...........] - ETA: 28:01 - loss: 0.6830 - regression_loss: 0.5493 - classification_loss: 0.1336
 6460/10000 [==================>...........] - ETA: 28:01 - loss: 0.6832 - regression_loss: 0.5495 - classification_loss: 0.1337
 6461/10000 [==================>...........] - ETA: 28:00 - loss: 0.6831 - regression_loss: 0.5494 - classification_loss: 0.1337
 6462/10000 [==================>...........] - ETA: 28:00 - loss: 0.6831 - regression_loss: 0.5494 - classification_loss: 0.1336
 6463/10000 [==================>...........] - ETA: 27:59 - loss: 0.6831 - regression_loss: 0.5495 - classification_loss: 0.1336
 6464/10000 [==================>...........] - ETA: 27:59 - loss: 0.6831 - regression_loss: 0.5495 - classification_loss: 0.1336
 6465/10000 [==================>...........] - ETA: 27:58 - loss: 0.6831 - regression_loss: 0.5495 - classification_loss: 0.1336
 6466/10000 [==================>...........] - ETA: 27:58 - loss: 0.6830 - regression_loss: 0.5494 - classification_loss: 0.1336
 6467/10000 [==================>...........] - ETA: 27:57 - loss: 0.6830 - regression_loss: 0.5494 - classification_loss: 0.1336
 6468/10000 [==================>...........] - ETA: 27:57 - loss: 0.6829 - regression_loss: 0.5493 - classification_loss: 0.1336
 6469/10000 [==================>...........] - ETA: 27:56 - loss: 0.6829 - regression_loss: 0.5493 - classification_loss: 0.1336
 6470/10000 [==================>...........] - ETA: 27:56 - loss: 0.6828 - regression_loss: 0.5493 - classification_loss: 0.1336
 6471/10000 [==================>...........] - ETA: 27:55 - loss: 0.6828 - regression_loss: 0.5492 - classification_loss: 0.1336
 6472/10000 [==================>...........] - ETA: 27:55 - loss: 0.6828 - regression_loss: 0.5492 - classification_loss: 0.1336
 6473/10000 [==================>...........] - ETA: 27:54 - loss: 0.6829 - regression_loss: 0.5493 - classification_loss: 0.1336
 6474/10000 [==================>...........] - ETA: 27:54 - loss: 0.6828 - regression_loss: 0.5493 - classification_loss: 0.1336
 6475/10000 [==================>...........] - ETA: 27:53 - loss: 0.6830 - regression_loss: 0.5494 - classification_loss: 0.1336
 6476/10000 [==================>...........] - ETA: 27:53 - loss: 0.6831 - regression_loss: 0.5495 - classification_loss: 0.1336
 6477/10000 [==================>...........] - ETA: 27:52 - loss: 0.6830 - regression_loss: 0.5494 - classification_loss: 0.1336
 6478/10000 [==================>...........] - ETA: 27:52 - loss: 0.6831 - regression_loss: 0.5495 - classification_loss: 0.1336
 6479/10000 [==================>...........] - ETA: 27:51 - loss: 0.6830 - regression_loss: 0.5494 - classification_loss: 0.1336
 6480/10000 [==================>...........] - ETA: 27:51 - loss: 0.6830 - regression_loss: 0.5494 - classification_loss: 0.1336
 6481/10000 [==================>...........] - ETA: 27:50 - loss: 0.6829 - regression_loss: 0.5493 - classification_loss: 0.1336
 6482/10000 [==================>...........] - ETA: 27:50 - loss: 0.6829 - regression_loss: 0.5493 - classification_loss: 0.1336
 6483/10000 [==================>...........] - ETA: 27:50 - loss: 0.6830 - regression_loss: 0.5494 - classification_loss: 0.1336
 6484/10000 [==================>...........] - ETA: 27:49 - loss: 0.6829 - regression_loss: 0.5494 - classification_loss: 0.1336
 6485/10000 [==================>...........] - ETA: 27:49 - loss: 0.6828 - regression_loss: 0.5493 - classification_loss: 0.1335
 6486/10000 [==================>...........] - ETA: 27:48 - loss: 0.6828 - regression_loss: 0.5493 - classification_loss: 0.1335
 6487/10000 [==================>...........] - ETA: 27:48 - loss: 0.6828 - regression_loss: 0.5492 - classification_loss: 0.1335
 6488/10000 [==================>...........] - ETA: 27:47 - loss: 0.6827 - regression_loss: 0.5492 - classification_loss: 0.1335
 6489/10000 [==================>...........] - ETA: 27:47 - loss: 0.6827 - regression_loss: 0.5491 - classification_loss: 0.1336
 6490/10000 [==================>...........] - ETA: 27:46 - loss: 0.6827 - regression_loss: 0.5492 - classification_loss: 0.1336
 6491/10000 [==================>...........] - ETA: 27:46 - loss: 0.6828 - regression_loss: 0.5492 - classification_loss: 0.1336
 6492/10000 [==================>...........] - ETA: 27:45 - loss: 0.6827 - regression_loss: 0.5492 - classification_loss: 0.1335
 6493/10000 [==================>...........] - ETA: 27:45 - loss: 0.6826 - regression_loss: 0.5491 - classification_loss: 0.1335
 6494/10000 [==================>...........] - ETA: 27:44 - loss: 0.6826 - regression_loss: 0.5491 - classification_loss: 0.1335
 6495/10000 [==================>...........] - ETA: 27:44 - loss: 0.6826 - regression_loss: 0.5491 - classification_loss: 0.1335
 6496/10000 [==================>...........] - ETA: 27:43 - loss: 0.6825 - regression_loss: 0.5490 - classification_loss: 0.1335
 6497/10000 [==================>...........] - ETA: 27:43 - loss: 0.6825 - regression_loss: 0.5490 - classification_loss: 0.1335
 6498/10000 [==================>...........] - ETA: 27:42 - loss: 0.6825 - regression_loss: 0.5490 - classification_loss: 0.1335
 6499/10000 [==================>...........] - ETA: 27:42 - loss: 0.6825 - regression_loss: 0.5490 - classification_loss: 0.1335
 6500/10000 [==================>...........] - ETA: 27:41 - loss: 0.6825 - regression_loss: 0.5490 - classification_loss: 0.1335
 6501/10000 [==================>...........] - ETA: 27:41 - loss: 0.6825 - regression_loss: 0.5490 - classification_loss: 0.1335
 6502/10000 [==================>...........] - ETA: 27:40 - loss: 0.6825 - regression_loss: 0.5490 - classification_loss: 0.1335
 6503/10000 [==================>...........] - ETA: 27:40 - loss: 0.6825 - regression_loss: 0.5491 - classification_loss: 0.1335
 6504/10000 [==================>...........] - ETA: 27:39 - loss: 0.6825 - regression_loss: 0.5490 - classification_loss: 0.1335
 6505/10000 [==================>...........] - ETA: 27:39 - loss: 0.6825 - regression_loss: 0.5490 - classification_loss: 0.1334
 6506/10000 [==================>...........] - ETA: 27:38 - loss: 0.6826 - regression_loss: 0.5491 - classification_loss: 0.1335
 6507/10000 [==================>...........] - ETA: 27:38 - loss: 0.6826 - regression_loss: 0.5491 - classification_loss: 0.1335
 6508/10000 [==================>...........] - ETA: 27:37 - loss: 0.6826 - regression_loss: 0.5491 - classification_loss: 0.1335
 6509/10000 [==================>...........] - ETA: 27:37 - loss: 0.6825 - regression_loss: 0.5490 - classification_loss: 0.1335
 6510/10000 [==================>...........] - ETA: 27:36 - loss: 0.6826 - regression_loss: 0.5491 - classification_loss: 0.1335
 6511/10000 [==================>...........] - ETA: 27:36 - loss: 0.6829 - regression_loss: 0.5494 - classification_loss: 0.1335
 6512/10000 [==================>...........] - ETA: 27:35 - loss: 0.6829 - regression_loss: 0.5494 - classification_loss: 0.1335
 6513/10000 [==================>...........] - ETA: 27:35 - loss: 0.6830 - regression_loss: 0.5494 - classification_loss: 0.1335
 6514/10000 [==================>...........] - ETA: 27:34 - loss: 0.6829 - regression_loss: 0.5494 - classification_loss: 0.1335
 6515/10000 [==================>...........] - ETA: 27:34 - loss: 0.6829 - regression_loss: 0.5494 - classification_loss: 0.1335
 6516/10000 [==================>...........] - ETA: 27:34 - loss: 0.6829 - regression_loss: 0.5494 - classification_loss: 0.1335
 6517/10000 [==================>...........] - ETA: 27:33 - loss: 0.6829 - regression_loss: 0.5494 - classification_loss: 0.1335
 6518/10000 [==================>...........] - ETA: 27:33 - loss: 0.6829 - regression_loss: 0.5494 - classification_loss: 0.1335
 6519/10000 [==================>...........] - ETA: 27:32 - loss: 0.6829 - regression_loss: 0.5493 - classification_loss: 0.1335
 6520/10000 [==================>...........] - ETA: 27:32 - loss: 0.6829 - regression_loss: 0.5494 - classification_loss: 0.1335
 6521/10000 [==================>...........] - ETA: 27:31 - loss: 0.6829 - regression_loss: 0.5494 - classification_loss: 0.1336
 6522/10000 [==================>...........] - ETA: 27:31 - loss: 0.6829 - regression_loss: 0.5494 - classification_loss: 0.1336
 6523/10000 [==================>...........] - ETA: 27:30 - loss: 0.6829 - regression_loss: 0.5493 - classification_loss: 0.1336
 6524/10000 [==================>...........] - ETA: 27:30 - loss: 0.6829 - regression_loss: 0.5493 - classification_loss: 0.1336
 6525/10000 [==================>...........] - ETA: 27:29 - loss: 0.6828 - regression_loss: 0.5492 - classification_loss: 0.1336
 6526/10000 [==================>...........] - ETA: 27:29 - loss: 0.6828 - regression_loss: 0.5492 - classification_loss: 0.1336
 6527/10000 [==================>...........] - ETA: 27:28 - loss: 0.6828 - regression_loss: 0.5492 - classification_loss: 0.1336
 6528/10000 [==================>...........] - ETA: 27:28 - loss: 0.6827 - regression_loss: 0.5492 - classification_loss: 0.1336
 6529/10000 [==================>...........] - ETA: 27:27 - loss: 0.6827 - regression_loss: 0.5491 - classification_loss: 0.1336
 6530/10000 [==================>...........] - ETA: 27:27 - loss: 0.6826 - regression_loss: 0.5491 - classification_loss: 0.1336
 6531/10000 [==================>...........] - ETA: 27:26 - loss: 0.6827 - regression_loss: 0.5491 - classification_loss: 0.1336
 6532/10000 [==================>...........] - ETA: 27:26 - loss: 0.6827 - regression_loss: 0.5491 - classification_loss: 0.1336
 6533/10000 [==================>...........] - ETA: 27:25 - loss: 0.6827 - regression_loss: 0.5491 - classification_loss: 0.1336
 6534/10000 [==================>...........] - ETA: 27:25 - loss: 0.6827 - regression_loss: 0.5490 - classification_loss: 0.1337
 6535/10000 [==================>...........] - ETA: 27:24 - loss: 0.6828 - regression_loss: 0.5492 - classification_loss: 0.1337
 6536/10000 [==================>...........] - ETA: 27:24 - loss: 0.6828 - regression_loss: 0.5491 - classification_loss: 0.1337
 6537/10000 [==================>...........] - ETA: 27:23 - loss: 0.6828 - regression_loss: 0.5491 - classification_loss: 0.1337
 6538/10000 [==================>...........] - ETA: 27:23 - loss: 0.6828 - regression_loss: 0.5491 - classification_loss: 0.1337
 6539/10000 [==================>...........] - ETA: 27:22 - loss: 0.6828 - regression_loss: 0.5491 - classification_loss: 0.1336
 6540/10000 [==================>...........] - ETA: 27:22 - loss: 0.6828 - regression_loss: 0.5491 - classification_loss: 0.1337
 6541/10000 [==================>...........] - ETA: 27:21 - loss: 0.6828 - regression_loss: 0.5491 - classification_loss: 0.1336
 6542/10000 [==================>...........] - ETA: 27:21 - loss: 0.6827 - regression_loss: 0.5491 - classification_loss: 0.1336
 6543/10000 [==================>...........] - ETA: 27:20 - loss: 0.6827 - regression_loss: 0.5491 - classification_loss: 0.1336
 6544/10000 [==================>...........] - ETA: 27:20 - loss: 0.6827 - regression_loss: 0.5491 - classification_loss: 0.1336
 6545/10000 [==================>...........] - ETA: 27:19 - loss: 0.6829 - regression_loss: 0.5492 - classification_loss: 0.1337
 6546/10000 [==================>...........] - ETA: 27:19 - loss: 0.6829 - regression_loss: 0.5492 - classification_loss: 0.1337
 6547/10000 [==================>...........] - ETA: 27:19 - loss: 0.6828 - regression_loss: 0.5492 - classification_loss: 0.1336
 6548/10000 [==================>...........] - ETA: 27:18 - loss: 0.6829 - regression_loss: 0.5492 - classification_loss: 0.1337
 6549/10000 [==================>...........] - ETA: 27:18 - loss: 0.6828 - regression_loss: 0.5492 - classification_loss: 0.1337
 6550/10000 [==================>...........] - ETA: 27:17 - loss: 0.6828 - regression_loss: 0.5492 - classification_loss: 0.1337
 6551/10000 [==================>...........] - ETA: 27:17 - loss: 0.6828 - regression_loss: 0.5491 - classification_loss: 0.1337
 6552/10000 [==================>...........] - ETA: 27:16 - loss: 0.6828 - regression_loss: 0.5492 - classification_loss: 0.1337
 6553/10000 [==================>...........] - ETA: 27:16 - loss: 0.6829 - regression_loss: 0.5492 - classification_loss: 0.1337
 6554/10000 [==================>...........] - ETA: 27:15 - loss: 0.6830 - regression_loss: 0.5493 - classification_loss: 0.1337
 6555/10000 [==================>...........] - ETA: 27:15 - loss: 0.6832 - regression_loss: 0.5495 - classification_loss: 0.1337
 6556/10000 [==================>...........] - ETA: 27:14 - loss: 0.6831 - regression_loss: 0.5494 - classification_loss: 0.1337
 6557/10000 [==================>...........] - ETA: 27:14 - loss: 0.6831 - regression_loss: 0.5494 - classification_loss: 0.1337
 6558/10000 [==================>...........] - ETA: 27:13 - loss: 0.6831 - regression_loss: 0.5494 - classification_loss: 0.1337
 6559/10000 [==================>...........] - ETA: 27:13 - loss: 0.6831 - regression_loss: 0.5494 - classification_loss: 0.1337
 6560/10000 [==================>...........] - ETA: 27:12 - loss: 0.6831 - regression_loss: 0.5494 - classification_loss: 0.1337
 6561/10000 [==================>...........] - ETA: 27:12 - loss: 0.6830 - regression_loss: 0.5494 - classification_loss: 0.1337
 6562/10000 [==================>...........] - ETA: 27:11 - loss: 0.6830 - regression_loss: 0.5493 - classification_loss: 0.1337
 6563/10000 [==================>...........] - ETA: 27:11 - loss: 0.6830 - regression_loss: 0.5493 - classification_loss: 0.1337
 6564/10000 [==================>...........] - ETA: 27:10 - loss: 0.6831 - regression_loss: 0.5493 - classification_loss: 0.1338
 6565/10000 [==================>...........] - ETA: 27:10 - loss: 0.6831 - regression_loss: 0.5493 - classification_loss: 0.1338
 6566/10000 [==================>...........] - ETA: 27:09 - loss: 0.6830 - regression_loss: 0.5493 - classification_loss: 0.1338
 6567/10000 [==================>...........] - ETA: 27:09 - loss: 0.6830 - regression_loss: 0.5492 - classification_loss: 0.1337
 6568/10000 [==================>...........] - ETA: 27:08 - loss: 0.6829 - regression_loss: 0.5492 - classification_loss: 0.1337
 6569/10000 [==================>...........] - ETA: 27:08 - loss: 0.6829 - regression_loss: 0.5492 - classification_loss: 0.1337
 6570/10000 [==================>...........] - ETA: 27:07 - loss: 0.6829 - regression_loss: 0.5492 - classification_loss: 0.1337
 6571/10000 [==================>...........] - ETA: 27:07 - loss: 0.6830 - regression_loss: 0.5492 - classification_loss: 0.1337
 6572/10000 [==================>...........] - ETA: 27:06 - loss: 0.6830 - regression_loss: 0.5492 - classification_loss: 0.1338
 6573/10000 [==================>...........] - ETA: 27:06 - loss: 0.6830 - regression_loss: 0.5492 - classification_loss: 0.1338
 6574/10000 [==================>...........] - ETA: 27:05 - loss: 0.6830 - regression_loss: 0.5492 - classification_loss: 0.1338
 6575/10000 [==================>...........] - ETA: 27:05 - loss: 0.6831 - regression_loss: 0.5493 - classification_loss: 0.1338
 6576/10000 [==================>...........] - ETA: 27:04 - loss: 0.6831 - regression_loss: 0.5493 - classification_loss: 0.1338
 6577/10000 [==================>...........] - ETA: 27:04 - loss: 0.6830 - regression_loss: 0.5492 - classification_loss: 0.1338
 6578/10000 [==================>...........] - ETA: 27:03 - loss: 0.6829 - regression_loss: 0.5492 - classification_loss: 0.1338
 6579/10000 [==================>...........] - ETA: 27:03 - loss: 0.6829 - regression_loss: 0.5491 - classification_loss: 0.1338
 6580/10000 [==================>...........] - ETA: 27:02 - loss: 0.6830 - regression_loss: 0.5492 - classification_loss: 0.1338
 6581/10000 [==================>...........] - ETA: 27:02 - loss: 0.6829 - regression_loss: 0.5491 - classification_loss: 0.1338
 6582/10000 [==================>...........] - ETA: 27:01 - loss: 0.6829 - regression_loss: 0.5491 - classification_loss: 0.1338
 6583/10000 [==================>...........] - ETA: 27:01 - loss: 0.6829 - regression_loss: 0.5491 - classification_loss: 0.1338
 6584/10000 [==================>...........] - ETA: 27:01 - loss: 0.6829 - regression_loss: 0.5491 - classification_loss: 0.1338
 6585/10000 [==================>...........] - ETA: 27:00 - loss: 0.6830 - regression_loss: 0.5492 - classification_loss: 0.1338
 6586/10000 [==================>...........] - ETA: 27:00 - loss: 0.6829 - regression_loss: 0.5491 - classification_loss: 0.1338
 6587/10000 [==================>...........] - ETA: 26:59 - loss: 0.6830 - regression_loss: 0.5492 - classification_loss: 0.1338
 6588/10000 [==================>...........] - ETA: 26:59 - loss: 0.6831 - regression_loss: 0.5493 - classification_loss: 0.1338
 6589/10000 [==================>...........] - ETA: 26:58 - loss: 0.6830 - regression_loss: 0.5492 - classification_loss: 0.1338
 6590/10000 [==================>...........] - ETA: 26:58 - loss: 0.6830 - regression_loss: 0.5491 - classification_loss: 0.1338
 6591/10000 [==================>...........] - ETA: 26:57 - loss: 0.6829 - regression_loss: 0.5491 - classification_loss: 0.1338
 6592/10000 [==================>...........] - ETA: 26:57 - loss: 0.6829 - regression_loss: 0.5491 - classification_loss: 0.1338
 6593/10000 [==================>...........] - ETA: 26:56 - loss: 0.6830 - regression_loss: 0.5491 - classification_loss: 0.1338
 6594/10000 [==================>...........] - ETA: 26:56 - loss: 0.6829 - regression_loss: 0.5491 - classification_loss: 0.1338
 6595/10000 [==================>...........] - ETA: 26:55 - loss: 0.6829 - regression_loss: 0.5491 - classification_loss: 0.1338
 6596/10000 [==================>...........] - ETA: 26:55 - loss: 0.6829 - regression_loss: 0.5491 - classification_loss: 0.1338
 6597/10000 [==================>...........] - ETA: 26:54 - loss: 0.6829 - regression_loss: 0.5491 - classification_loss: 0.1338
 6598/10000 [==================>...........] - ETA: 26:54 - loss: 0.6828 - regression_loss: 0.5490 - classification_loss: 0.1338
 6599/10000 [==================>...........] - ETA: 26:53 - loss: 0.6829 - regression_loss: 0.5490 - classification_loss: 0.1338
 6600/10000 [==================>...........] - ETA: 26:53 - loss: 0.6828 - regression_loss: 0.5490 - classification_loss: 0.1338
 6601/10000 [==================>...........] - ETA: 26:52 - loss: 0.6828 - regression_loss: 0.5489 - classification_loss: 0.1338
 6602/10000 [==================>...........] - ETA: 26:52 - loss: 0.6828 - regression_loss: 0.5490 - classification_loss: 0.1338
 6603/10000 [==================>...........] - ETA: 26:51 - loss: 0.6828 - regression_loss: 0.5490 - classification_loss: 0.1338
 6604/10000 [==================>...........] - ETA: 26:51 - loss: 0.6827 - regression_loss: 0.5489 - classification_loss: 0.1338
 6605/10000 [==================>...........] - ETA: 26:50 - loss: 0.6828 - regression_loss: 0.5489 - classification_loss: 0.1338
 6606/10000 [==================>...........] - ETA: 26:50 - loss: 0.6828 - regression_loss: 0.5490 - classification_loss: 0.1338
 6607/10000 [==================>...........] - ETA: 26:49 - loss: 0.6828 - regression_loss: 0.5490 - classification_loss: 0.1338
 6608/10000 [==================>...........] - ETA: 26:49 - loss: 0.6828 - regression_loss: 0.5490 - classification_loss: 0.1338
 6609/10000 [==================>...........] - ETA: 26:48 - loss: 0.6828 - regression_loss: 0.5490 - classification_loss: 0.1338
 6610/10000 [==================>...........] - ETA: 26:48 - loss: 0.6827 - regression_loss: 0.5490 - classification_loss: 0.1338
 6611/10000 [==================>...........] - ETA: 26:47 - loss: 0.6826 - regression_loss: 0.5489 - classification_loss: 0.1338
 6612/10000 [==================>...........] - ETA: 26:47 - loss: 0.6827 - regression_loss: 0.5489 - classification_loss: 0.1338
 6613/10000 [==================>...........] - ETA: 26:46 - loss: 0.6827 - regression_loss: 0.5490 - classification_loss: 0.1338
 6614/10000 [==================>...........] - ETA: 26:46 - loss: 0.6829 - regression_loss: 0.5491 - classification_loss: 0.1338
 6615/10000 [==================>...........] - ETA: 26:46 - loss: 0.6829 - regression_loss: 0.5491 - classification_loss: 0.1338
 6616/10000 [==================>...........] - ETA: 26:45 - loss: 0.6829 - regression_loss: 0.5491 - classification_loss: 0.1338
 6617/10000 [==================>...........] - ETA: 26:45 - loss: 0.6829 - regression_loss: 0.5492 - classification_loss: 0.1338
 6618/10000 [==================>...........] - ETA: 26:44 - loss: 0.6830 - regression_loss: 0.5492 - classification_loss: 0.1338
 6619/10000 [==================>...........] - ETA: 26:44 - loss: 0.6830 - regression_loss: 0.5492 - classification_loss: 0.1338
 6620/10000 [==================>...........] - ETA: 26:43 - loss: 0.6829 - regression_loss: 0.5491 - classification_loss: 0.1338
 6621/10000 [==================>...........] - ETA: 26:43 - loss: 0.6829 - regression_loss: 0.5491 - classification_loss: 0.1338
 6622/10000 [==================>...........] - ETA: 26:42 - loss: 0.6828 - regression_loss: 0.5490 - classification_loss: 0.1338
 6623/10000 [==================>...........] - ETA: 26:42 - loss: 0.6828 - regression_loss: 0.5490 - classification_loss: 0.1338
 6624/10000 [==================>...........] - ETA: 26:41 - loss: 0.6828 - regression_loss: 0.5490 - classification_loss: 0.1338
 6625/10000 [==================>...........] - ETA: 26:41 - loss: 0.6827 - regression_loss: 0.5489 - classification_loss: 0.1338
 6626/10000 [==================>...........] - ETA: 26:40 - loss: 0.6826 - regression_loss: 0.5488 - classification_loss: 0.1338
 6627/10000 [==================>...........] - ETA: 26:40 - loss: 0.6826 - regression_loss: 0.5488 - classification_loss: 0.1338
 6628/10000 [==================>...........] - ETA: 26:39 - loss: 0.6827 - regression_loss: 0.5489 - classification_loss: 0.1338
 6629/10000 [==================>...........] - ETA: 26:39 - loss: 0.6826 - regression_loss: 0.5488 - classification_loss: 0.1338
 6630/10000 [==================>...........] - ETA: 26:38 - loss: 0.6827 - regression_loss: 0.5490 - classification_loss: 0.1338
 6631/10000 [==================>...........] - ETA: 26:38 - loss: 0.6828 - regression_loss: 0.5490 - classification_loss: 0.1338
 6632/10000 [==================>...........] - ETA: 26:37 - loss: 0.6828 - regression_loss: 0.5490 - classification_loss: 0.1338
 6633/10000 [==================>...........] - ETA: 26:37 - loss: 0.6829 - regression_loss: 0.5491 - classification_loss: 0.1338
 6634/10000 [==================>...........] - ETA: 26:36 - loss: 0.6829 - regression_loss: 0.5491 - classification_loss: 0.1338
 6635/10000 [==================>...........] - ETA: 26:36 - loss: 0.6828 - regression_loss: 0.5491 - classification_loss: 0.1338
 6636/10000 [==================>...........] - ETA: 26:36 - loss: 0.6828 - regression_loss: 0.5490 - classification_loss: 0.1338
 6637/10000 [==================>...........] - ETA: 26:35 - loss: 0.6828 - regression_loss: 0.5490 - classification_loss: 0.1338
 6638/10000 [==================>...........] - ETA: 26:35 - loss: 0.6827 - regression_loss: 0.5490 - classification_loss: 0.1338
 6639/10000 [==================>...........] - ETA: 26:34 - loss: 0.6827 - regression_loss: 0.5490 - classification_loss: 0.1337
 6640/10000 [==================>...........] - ETA: 26:34 - loss: 0.6828 - regression_loss: 0.5490 - classification_loss: 0.1337
 6641/10000 [==================>...........] - ETA: 26:33 - loss: 0.6828 - regression_loss: 0.5491 - classification_loss: 0.1337
 6642/10000 [==================>...........] - ETA: 26:33 - loss: 0.6827 - regression_loss: 0.5490 - classification_loss: 0.1337
 6643/10000 [==================>...........] - ETA: 26:32 - loss: 0.6827 - regression_loss: 0.5490 - classification_loss: 0.1337
 6644/10000 [==================>...........] - ETA: 26:32 - loss: 0.6826 - regression_loss: 0.5489 - classification_loss: 0.1337
 6645/10000 [==================>...........] - ETA: 26:31 - loss: 0.6826 - regression_loss: 0.5489 - classification_loss: 0.1337
 6646/10000 [==================>...........] - ETA: 26:31 - loss: 0.6826 - regression_loss: 0.5489 - classification_loss: 0.1337
 6647/10000 [==================>...........] - ETA: 26:30 - loss: 0.6827 - regression_loss: 0.5490 - classification_loss: 0.1337
 6648/10000 [==================>...........] - ETA: 26:30 - loss: 0.6826 - regression_loss: 0.5489 - classification_loss: 0.1337
 6649/10000 [==================>...........] - ETA: 26:29 - loss: 0.6825 - regression_loss: 0.5489 - classification_loss: 0.1337
 6650/10000 [==================>...........] - ETA: 26:29 - loss: 0.6825 - regression_loss: 0.5488 - classification_loss: 0.1337
 6651/10000 [==================>...........] - ETA: 26:28 - loss: 0.6825 - regression_loss: 0.5489 - classification_loss: 0.1337
 6652/10000 [==================>...........] - ETA: 26:28 - loss: 0.6825 - regression_loss: 0.5489 - classification_loss: 0.1337
 6653/10000 [==================>...........] - ETA: 26:27 - loss: 0.6825 - regression_loss: 0.5488 - classification_loss: 0.1336
 6654/10000 [==================>...........] - ETA: 26:27 - loss: 0.6824 - regression_loss: 0.5488 - classification_loss: 0.1336
 6655/10000 [==================>...........] - ETA: 26:26 - loss: 0.6824 - regression_loss: 0.5488 - classification_loss: 0.1336
 6656/10000 [==================>...........] - ETA: 26:26 - loss: 0.6824 - regression_loss: 0.5488 - classification_loss: 0.1336
 6657/10000 [==================>...........] - ETA: 26:25 - loss: 0.6823 - regression_loss: 0.5488 - classification_loss: 0.1336
 6658/10000 [==================>...........] - ETA: 26:25 - loss: 0.6823 - regression_loss: 0.5487 - classification_loss: 0.1336
 6659/10000 [==================>...........] - ETA: 26:24 - loss: 0.6823 - regression_loss: 0.5487 - classification_loss: 0.1336
 6660/10000 [==================>...........] - ETA: 26:24 - loss: 0.6823 - regression_loss: 0.5487 - classification_loss: 0.1336
 6661/10000 [==================>...........] - ETA: 26:23 - loss: 0.6823 - regression_loss: 0.5487 - classification_loss: 0.1336
 6662/10000 [==================>...........] - ETA: 26:23 - loss: 0.6822 - regression_loss: 0.5487 - classification_loss: 0.1335
 6663/10000 [==================>...........] - ETA: 26:22 - loss: 0.6823 - regression_loss: 0.5487 - classification_loss: 0.1336
 6664/10000 [==================>...........] - ETA: 26:22 - loss: 0.6824 - regression_loss: 0.5488 - classification_loss: 0.1336
 6665/10000 [==================>...........] - ETA: 26:21 - loss: 0.6823 - regression_loss: 0.5488 - classification_loss: 0.1336
 6666/10000 [==================>...........] - ETA: 26:21 - loss: 0.6823 - regression_loss: 0.5488 - classification_loss: 0.1335
 6667/10000 [===================>..........] - ETA: 26:20 - loss: 0.6823 - regression_loss: 0.5487 - classification_loss: 0.1335
 6668/10000 [===================>..........] - ETA: 26:20 - loss: 0.6822 - regression_loss: 0.5487 - classification_loss: 0.1335
 6669/10000 [===================>..........] - ETA: 26:19 - loss: 0.6823 - regression_loss: 0.5488 - classification_loss: 0.1335
 6670/10000 [===================>..........] - ETA: 26:19 - loss: 0.6824 - regression_loss: 0.5488 - classification_loss: 0.1335
 6671/10000 [===================>..........] - ETA: 26:18 - loss: 0.6824 - regression_loss: 0.5489 - classification_loss: 0.1335
 6672/10000 [===================>..........] - ETA: 26:18 - loss: 0.6824 - regression_loss: 0.5488 - classification_loss: 0.1335
 6673/10000 [===================>..........] - ETA: 26:17 - loss: 0.6824 - regression_loss: 0.5488 - classification_loss: 0.1336
 6674/10000 [===================>..........] - ETA: 26:17 - loss: 0.6823 - regression_loss: 0.5488 - classification_loss: 0.1336
 6675/10000 [===================>..........] - ETA: 26:16 - loss: 0.6823 - regression_loss: 0.5488 - classification_loss: 0.1335
 6676/10000 [===================>..........] - ETA: 26:16 - loss: 0.6822 - regression_loss: 0.5487 - classification_loss: 0.1335
 6677/10000 [===================>..........] - ETA: 26:16 - loss: 0.6824 - regression_loss: 0.5488 - classification_loss: 0.1336
 6678/10000 [===================>..........] - ETA: 26:15 - loss: 0.6823 - regression_loss: 0.5488 - classification_loss: 0.1336
 6679/10000 [===================>..........] - ETA: 26:15 - loss: 0.6823 - regression_loss: 0.5487 - classification_loss: 0.1336
 6680/10000 [===================>..........] - ETA: 26:14 - loss: 0.6823 - regression_loss: 0.5487 - classification_loss: 0.1336
 6681/10000 [===================>..........] - ETA: 26:14 - loss: 0.6823 - regression_loss: 0.5488 - classification_loss: 0.1336
 6682/10000 [===================>..........] - ETA: 26:13 - loss: 0.6823 - regression_loss: 0.5487 - classification_loss: 0.1336
 6683/10000 [===================>..........] - ETA: 26:13 - loss: 0.6823 - regression_loss: 0.5487 - classification_loss: 0.1335
 6684/10000 [===================>..........] - ETA: 26:12 - loss: 0.6822 - regression_loss: 0.5487 - classification_loss: 0.1335
 6685/10000 [===================>..........] - ETA: 26:12 - loss: 0.6822 - regression_loss: 0.5487 - classification_loss: 0.1335
 6686/10000 [===================>..........] - ETA: 26:11 - loss: 0.6822 - regression_loss: 0.5487 - classification_loss: 0.1335
 6687/10000 [===================>..........] - ETA: 26:11 - loss: 0.6821 - regression_loss: 0.5486 - classification_loss: 0.1335
 6688/10000 [===================>..........] - ETA: 26:10 - loss: 0.6821 - regression_loss: 0.5486 - classification_loss: 0.1335
 6689/10000 [===================>..........] - ETA: 26:10 - loss: 0.6820 - regression_loss: 0.5485 - classification_loss: 0.1335
 6690/10000 [===================>..........] - ETA: 26:09 - loss: 0.6819 - regression_loss: 0.5485 - classification_loss: 0.1335
 6691/10000 [===================>..........] - ETA: 26:09 - loss: 0.6819 - regression_loss: 0.5484 - classification_loss: 0.1334
 6692/10000 [===================>..........] - ETA: 26:08 - loss: 0.6818 - regression_loss: 0.5484 - classification_loss: 0.1334
 6693/10000 [===================>..........] - ETA: 26:08 - loss: 0.6819 - regression_loss: 0.5485 - classification_loss: 0.1334
 6694/10000 [===================>..........] - ETA: 26:07 - loss: 0.6819 - regression_loss: 0.5485 - classification_loss: 0.1335
 6695/10000 [===================>..........] - ETA: 26:07 - loss: 0.6819 - regression_loss: 0.5484 - classification_loss: 0.1335
 6696/10000 [===================>..........] - ETA: 26:06 - loss: 0.6818 - regression_loss: 0.5483 - classification_loss: 0.1334
 6697/10000 [===================>..........] - ETA: 26:06 - loss: 0.6817 - regression_loss: 0.5483 - classification_loss: 0.1334
 6698/10000 [===================>..........] - ETA: 26:05 - loss: 0.6816 - regression_loss: 0.5482 - classification_loss: 0.1334
 6699/10000 [===================>..........] - ETA: 26:05 - loss: 0.6816 - regression_loss: 0.5482 - classification_loss: 0.1334
 6700/10000 [===================>..........] - ETA: 26:04 - loss: 0.6816 - regression_loss: 0.5482 - classification_loss: 0.1334
 6701/10000 [===================>..........] - ETA: 26:04 - loss: 0.6816 - regression_loss: 0.5482 - classification_loss: 0.1334
 6702/10000 [===================>..........] - ETA: 26:03 - loss: 0.6816 - regression_loss: 0.5482 - classification_loss: 0.1334
 6703/10000 [===================>..........] - ETA: 26:03 - loss: 0.6816 - regression_loss: 0.5482 - classification_loss: 0.1334
 6704/10000 [===================>..........] - ETA: 26:02 - loss: 0.6817 - regression_loss: 0.5483 - classification_loss: 0.1334
 6705/10000 [===================>..........] - ETA: 26:02 - loss: 0.6816 - regression_loss: 0.5482 - classification_loss: 0.1334
 6706/10000 [===================>..........] - ETA: 26:01 - loss: 0.6817 - regression_loss: 0.5483 - classification_loss: 0.1334
 6707/10000 [===================>..........] - ETA: 26:01 - loss: 0.6817 - regression_loss: 0.5483 - classification_loss: 0.1334
 6708/10000 [===================>..........] - ETA: 26:00 - loss: 0.6817 - regression_loss: 0.5483 - classification_loss: 0.1334
 6709/10000 [===================>..........] - ETA: 26:00 - loss: 0.6817 - regression_loss: 0.5482 - classification_loss: 0.1334
 6710/10000 [===================>..........] - ETA: 25:59 - loss: 0.6816 - regression_loss: 0.5482 - classification_loss: 0.1334
 6711/10000 [===================>..........] - ETA: 25:59 - loss: 0.6816 - regression_loss: 0.5482 - classification_loss: 0.1334
 6712/10000 [===================>..........] - ETA: 25:58 - loss: 0.6816 - regression_loss: 0.5482 - classification_loss: 0.1334
 6713/10000 [===================>..........] - ETA: 25:58 - loss: 0.6815 - regression_loss: 0.5481 - classification_loss: 0.1334
 6714/10000 [===================>..........] - ETA: 25:57 - loss: 0.6815 - regression_loss: 0.5481 - classification_loss: 0.1334
 6715/10000 [===================>..........] - ETA: 25:57 - loss: 0.6814 - regression_loss: 0.5480 - classification_loss: 0.1334
 6716/10000 [===================>..........] - ETA: 25:56 - loss: 0.6815 - regression_loss: 0.5481 - classification_loss: 0.1334
 6717/10000 [===================>..........] - ETA: 25:56 - loss: 0.6814 - regression_loss: 0.5480 - classification_loss: 0.1334
 6718/10000 [===================>..........] - ETA: 25:56 - loss: 0.6814 - regression_loss: 0.5480 - classification_loss: 0.1334
 6719/10000 [===================>..........] - ETA: 25:55 - loss: 0.6813 - regression_loss: 0.5480 - classification_loss: 0.1334
 6720/10000 [===================>..........] - ETA: 25:55 - loss: 0.6813 - regression_loss: 0.5480 - classification_loss: 0.1334
 6721/10000 [===================>..........] - ETA: 25:54 - loss: 0.6813 - regression_loss: 0.5479 - classification_loss: 0.1334
 6722/10000 [===================>..........] - ETA: 25:54 - loss: 0.6813 - regression_loss: 0.5479 - classification_loss: 0.1334
 6723/10000 [===================>..........] - ETA: 25:53 - loss: 0.6812 - regression_loss: 0.5478 - classification_loss: 0.1334
 6724/10000 [===================>..........] - ETA: 25:53 - loss: 0.6812 - regression_loss: 0.5478 - classification_loss: 0.1334
 6725/10000 [===================>..........] - ETA: 25:52 - loss: 0.6812 - regression_loss: 0.5478 - classification_loss: 0.1334
 6726/10000 [===================>..........] - ETA: 25:52 - loss: 0.6811 - regression_loss: 0.5477 - classification_loss: 0.1334
 6727/10000 [===================>..........] - ETA: 25:51 - loss: 0.6811 - regression_loss: 0.5478 - classification_loss: 0.1334
 6728/10000 [===================>..........] - ETA: 25:51 - loss: 0.6812 - regression_loss: 0.5478 - classification_loss: 0.1334
 6729/10000 [===================>..........] - ETA: 25:50 - loss: 0.6811 - regression_loss: 0.5478 - classification_loss: 0.1334
 6730/10000 [===================>..........] - ETA: 25:50 - loss: 0.6812 - regression_loss: 0.5478 - classification_loss: 0.1334
 6731/10000 [===================>..........] - ETA: 25:49 - loss: 0.6812 - regression_loss: 0.5478 - classification_loss: 0.1334
 6732/10000 [===================>..........] - ETA: 25:49 - loss: 0.6811 - regression_loss: 0.5478 - classification_loss: 0.1333
 6733/10000 [===================>..........] - ETA: 25:48 - loss: 0.6811 - regression_loss: 0.5478 - classification_loss: 0.1334
 6734/10000 [===================>..........] - ETA: 25:48 - loss: 0.6811 - regression_loss: 0.5477 - classification_loss: 0.1334
 6735/10000 [===================>..........] - ETA: 25:47 - loss: 0.6810 - regression_loss: 0.5477 - classification_loss: 0.1334
 6736/10000 [===================>..........] - ETA: 25:47 - loss: 0.6810 - regression_loss: 0.5476 - classification_loss: 0.1333
 6737/10000 [===================>..........] - ETA: 25:46 - loss: 0.6811 - regression_loss: 0.5477 - classification_loss: 0.1334
 6738/10000 [===================>..........] - ETA: 25:46 - loss: 0.6810 - regression_loss: 0.5477 - classification_loss: 0.1333
 6739/10000 [===================>..........] - ETA: 25:45 - loss: 0.6810 - regression_loss: 0.5477 - classification_loss: 0.1333
 6740/10000 [===================>..........] - ETA: 25:45 - loss: 0.6811 - regression_loss: 0.5478 - classification_loss: 0.1334
 6741/10000 [===================>..........] - ETA: 25:44 - loss: 0.6811 - regression_loss: 0.5478 - classification_loss: 0.1334
 6742/10000 [===================>..........] - ETA: 25:44 - loss: 0.6811 - regression_loss: 0.5478 - classification_loss: 0.1334
 6743/10000 [===================>..........] - ETA: 25:43 - loss: 0.6811 - regression_loss: 0.5477 - classification_loss: 0.1334
 6744/10000 [===================>..........] - ETA: 25:43 - loss: 0.6811 - regression_loss: 0.5478 - classification_loss: 0.1333
 6745/10000 [===================>..........] - ETA: 25:43 - loss: 0.6811 - regression_loss: 0.5478 - classification_loss: 0.1334
 6746/10000 [===================>..........] - ETA: 25:42 - loss: 0.6811 - regression_loss: 0.5477 - classification_loss: 0.1334
 6747/10000 [===================>..........] - ETA: 25:42 - loss: 0.6810 - regression_loss: 0.5477 - classification_loss: 0.1334
 6748/10000 [===================>..........] - ETA: 25:41 - loss: 0.6811 - regression_loss: 0.5477 - classification_loss: 0.1334
 6749/10000 [===================>..........] - ETA: 25:41 - loss: 0.6811 - regression_loss: 0.5477 - classification_loss: 0.1334
 6750/10000 [===================>..........] - ETA: 25:40 - loss: 0.6811 - regression_loss: 0.5478 - classification_loss: 0.1334
 6751/10000 [===================>..........] - ETA: 25:40 - loss: 0.6812 - regression_loss: 0.5478 - classification_loss: 0.1334
 6752/10000 [===================>..........] - ETA: 25:39 - loss: 0.6813 - regression_loss: 0.5479 - classification_loss: 0.1334
 6753/10000 [===================>..........] - ETA: 25:39 - loss: 0.6814 - regression_loss: 0.5479 - classification_loss: 0.1334
 6754/10000 [===================>..........] - ETA: 25:38 - loss: 0.6813 - regression_loss: 0.5479 - classification_loss: 0.1334
 6755/10000 [===================>..........] - ETA: 25:38 - loss: 0.6814 - regression_loss: 0.5480 - classification_loss: 0.1334
 6756/10000 [===================>..........] - ETA: 25:37 - loss: 0.6813 - regression_loss: 0.5479 - classification_loss: 0.1334
 6757/10000 [===================>..........] - ETA: 25:37 - loss: 0.6814 - regression_loss: 0.5480 - classification_loss: 0.1334
 6758/10000 [===================>..........] - ETA: 25:36 - loss: 0.6813 - regression_loss: 0.5479 - classification_loss: 0.1334
 6759/10000 [===================>..........] - ETA: 25:36 - loss: 0.6813 - regression_loss: 0.5479 - classification_loss: 0.1334
 6760/10000 [===================>..........] - ETA: 25:35 - loss: 0.6812 - regression_loss: 0.5478 - classification_loss: 0.1334
 6761/10000 [===================>..........] - ETA: 25:35 - loss: 0.6811 - regression_loss: 0.5478 - classification_loss: 0.1334
 6762/10000 [===================>..........] - ETA: 25:34 - loss: 0.6811 - regression_loss: 0.5478 - classification_loss: 0.1333
 6763/10000 [===================>..........] - ETA: 25:34 - loss: 0.6811 - regression_loss: 0.5477 - classification_loss: 0.1333
 6764/10000 [===================>..........] - ETA: 25:33 - loss: 0.6811 - regression_loss: 0.5477 - classification_loss: 0.1334
 6765/10000 [===================>..........] - ETA: 25:33 - loss: 0.6810 - regression_loss: 0.5477 - classification_loss: 0.1333
 6766/10000 [===================>..........] - ETA: 25:32 - loss: 0.6810 - regression_loss: 0.5477 - classification_loss: 0.1333
 6767/10000 [===================>..........] - ETA: 25:32 - loss: 0.6809 - regression_loss: 0.5476 - classification_loss: 0.1333
 6768/10000 [===================>..........] - ETA: 25:31 - loss: 0.6809 - regression_loss: 0.5476 - classification_loss: 0.1333
 6769/10000 [===================>..........] - ETA: 25:31 - loss: 0.6809 - regression_loss: 0.5476 - classification_loss: 0.1333
 6770/10000 [===================>..........] - ETA: 25:31 - loss: 0.6810 - regression_loss: 0.5477 - classification_loss: 0.1333
 6771/10000 [===================>..........] - ETA: 25:30 - loss: 0.6810 - regression_loss: 0.5477 - classification_loss: 0.1333
 6772/10000 [===================>..........] - ETA: 25:30 - loss: 0.6810 - regression_loss: 0.5476 - classification_loss: 0.1333
 6773/10000 [===================>..........] - ETA: 25:29 - loss: 0.6809 - regression_loss: 0.5476 - classification_loss: 0.1333
 6774/10000 [===================>..........] - ETA: 25:29 - loss: 0.6808 - regression_loss: 0.5475 - classification_loss: 0.1333
 6775/10000 [===================>..........] - ETA: 25:28 - loss: 0.6808 - regression_loss: 0.5475 - classification_loss: 0.1333
 6776/10000 [===================>..........] - ETA: 25:28 - loss: 0.6808 - regression_loss: 0.5475 - classification_loss: 0.1333
 6777/10000 [===================>..........] - ETA: 25:27 - loss: 0.6807 - regression_loss: 0.5474 - classification_loss: 0.1333
 6778/10000 [===================>..........] - ETA: 25:27 - loss: 0.6808 - regression_loss: 0.5474 - classification_loss: 0.1333
 6779/10000 [===================>..........] - ETA: 25:26 - loss: 0.6808 - regression_loss: 0.5475 - classification_loss: 0.1333
 6780/10000 [===================>..........] - ETA: 25:26 - loss: 0.6808 - regression_loss: 0.5475 - classification_loss: 0.1333
 6781/10000 [===================>..........] - ETA: 25:25 - loss: 0.6808 - regression_loss: 0.5475 - classification_loss: 0.1333
 6782/10000 [===================>..........] - ETA: 25:25 - loss: 0.6809 - regression_loss: 0.5476 - classification_loss: 0.1333
 6783/10000 [===================>..........] - ETA: 25:24 - loss: 0.6810 - regression_loss: 0.5476 - classification_loss: 0.1333
 6784/10000 [===================>..........] - ETA: 25:24 - loss: 0.6810 - regression_loss: 0.5476 - classification_loss: 0.1333
 6785/10000 [===================>..........] - ETA: 25:23 - loss: 0.6810 - regression_loss: 0.5476 - classification_loss: 0.1334
 6786/10000 [===================>..........] - ETA: 25:23 - loss: 0.6809 - regression_loss: 0.5476 - classification_loss: 0.1334
 6787/10000 [===================>..........] - ETA: 25:22 - loss: 0.6810 - regression_loss: 0.5477 - classification_loss: 0.1334
 6788/10000 [===================>..........] - ETA: 25:22 - loss: 0.6811 - regression_loss: 0.5477 - classification_loss: 0.1334
 6789/10000 [===================>..........] - ETA: 25:21 - loss: 0.6811 - regression_loss: 0.5477 - classification_loss: 0.1334
 6790/10000 [===================>..........] - ETA: 25:21 - loss: 0.6810 - regression_loss: 0.5476 - classification_loss: 0.1334
 6791/10000 [===================>..........] - ETA: 25:20 - loss: 0.6810 - regression_loss: 0.5477 - classification_loss: 0.1334
 6792/10000 [===================>..........] - ETA: 25:20 - loss: 0.6811 - regression_loss: 0.5477 - classification_loss: 0.1334
 6793/10000 [===================>..........] - ETA: 25:19 - loss: 0.6810 - regression_loss: 0.5477 - classification_loss: 0.1334
 6794/10000 [===================>..........] - ETA: 25:19 - loss: 0.6810 - regression_loss: 0.5476 - classification_loss: 0.1334
 6795/10000 [===================>..........] - ETA: 25:18 - loss: 0.6810 - regression_loss: 0.5476 - classification_loss: 0.1334
 6796/10000 [===================>..........] - ETA: 25:18 - loss: 0.6809 - regression_loss: 0.5476 - classification_loss: 0.1334
 6797/10000 [===================>..........] - ETA: 25:17 - loss: 0.6809 - regression_loss: 0.5476 - classification_loss: 0.1334
 6798/10000 [===================>..........] - ETA: 25:17 - loss: 0.6809 - regression_loss: 0.5475 - classification_loss: 0.1334
 6799/10000 [===================>..........] - ETA: 25:16 - loss: 0.6808 - regression_loss: 0.5475 - classification_loss: 0.1334
 6800/10000 [===================>..........] - ETA: 25:16 - loss: 0.6808 - regression_loss: 0.5474 - classification_loss: 0.1334
 6801/10000 [===================>..........] - ETA: 25:16 - loss: 0.6808 - regression_loss: 0.5474 - classification_loss: 0.1334
 6802/10000 [===================>..........] - ETA: 25:15 - loss: 0.6807 - regression_loss: 0.5474 - classification_loss: 0.1334
 6803/10000 [===================>..........] - ETA: 25:15 - loss: 0.6807 - regression_loss: 0.5473 - classification_loss: 0.1334
 6804/10000 [===================>..........] - ETA: 25:14 - loss: 0.6806 - regression_loss: 0.5473 - classification_loss: 0.1333
 6805/10000 [===================>..........] - ETA: 25:14 - loss: 0.6807 - regression_loss: 0.5473 - classification_loss: 0.1333
 6806/10000 [===================>..........] - ETA: 25:13 - loss: 0.6806 - regression_loss: 0.5473 - classification_loss: 0.1334
 6807/10000 [===================>..........] - ETA: 25:13 - loss: 0.6806 - regression_loss: 0.5472 - classification_loss: 0.1333
 6808/10000 [===================>..........] - ETA: 25:12 - loss: 0.6805 - regression_loss: 0.5472 - classification_loss: 0.1333
 6809/10000 [===================>..........] - ETA: 25:12 - loss: 0.6805 - regression_loss: 0.5472 - classification_loss: 0.1333
 6810/10000 [===================>..........] - ETA: 25:11 - loss: 0.6805 - regression_loss: 0.5472 - classification_loss: 0.1333
 6811/10000 [===================>..........] - ETA: 25:11 - loss: 0.6805 - regression_loss: 0.5472 - classification_loss: 0.1333
 6812/10000 [===================>..........] - ETA: 25:10 - loss: 0.6804 - regression_loss: 0.5471 - classification_loss: 0.1333
 6813/10000 [===================>..........] - ETA: 25:10 - loss: 0.6805 - regression_loss: 0.5472 - classification_loss: 0.1333
 6814/10000 [===================>..........] - ETA: 25:09 - loss: 0.6805 - regression_loss: 0.5472 - classification_loss: 0.1333
 6815/10000 [===================>..........] - ETA: 25:09 - loss: 0.6805 - regression_loss: 0.5472 - classification_loss: 0.1333
 6816/10000 [===================>..........] - ETA: 25:08 - loss: 0.6804 - regression_loss: 0.5472 - classification_loss: 0.1333
 6817/10000 [===================>..........] - ETA: 25:08 - loss: 0.6804 - regression_loss: 0.5471 - classification_loss: 0.1333
 6818/10000 [===================>..........] - ETA: 25:07 - loss: 0.6804 - regression_loss: 0.5471 - classification_loss: 0.1333
 6819/10000 [===================>..........] - ETA: 25:07 - loss: 0.6804 - regression_loss: 0.5471 - classification_loss: 0.1333
 6820/10000 [===================>..........] - ETA: 25:06 - loss: 0.6805 - regression_loss: 0.5472 - classification_loss: 0.1333
 6821/10000 [===================>..........] - ETA: 25:06 - loss: 0.6804 - regression_loss: 0.5472 - classification_loss: 0.1333
 6822/10000 [===================>..........] - ETA: 25:05 - loss: 0.6805 - regression_loss: 0.5472 - classification_loss: 0.1333
 6823/10000 [===================>..........] - ETA: 25:05 - loss: 0.6804 - regression_loss: 0.5472 - classification_loss: 0.1333
 6824/10000 [===================>..........] - ETA: 25:04 - loss: 0.6805 - regression_loss: 0.5472 - classification_loss: 0.1333
 6825/10000 [===================>..........] - ETA: 25:04 - loss: 0.6805 - regression_loss: 0.5472 - classification_loss: 0.1333
 6826/10000 [===================>..........] - ETA: 25:03 - loss: 0.6805 - regression_loss: 0.5472 - classification_loss: 0.1333
 6827/10000 [===================>..........] - ETA: 25:03 - loss: 0.6805 - regression_loss: 0.5472 - classification_loss: 0.1333
 6828/10000 [===================>..........] - ETA: 25:02 - loss: 0.6805 - regression_loss: 0.5472 - classification_loss: 0.1333
 6829/10000 [===================>..........] - ETA: 25:02 - loss: 0.6804 - regression_loss: 0.5471 - classification_loss: 0.1333
 6830/10000 [===================>..........] - ETA: 25:02 - loss: 0.6805 - regression_loss: 0.5472 - classification_loss: 0.1333
 6831/10000 [===================>..........] - ETA: 25:01 - loss: 0.6805 - regression_loss: 0.5471 - classification_loss: 0.1333
 6832/10000 [===================>..........] - ETA: 25:01 - loss: 0.6804 - regression_loss: 0.5471 - classification_loss: 0.1333
 6833/10000 [===================>..........] - ETA: 25:00 - loss: 0.6805 - regression_loss: 0.5472 - classification_loss: 0.1333
 6834/10000 [===================>..........] - ETA: 25:00 - loss: 0.6805 - regression_loss: 0.5472 - classification_loss: 0.1333
 6835/10000 [===================>..........] - ETA: 24:59 - loss: 0.6804 - regression_loss: 0.5471 - classification_loss: 0.1333
 6836/10000 [===================>..........] - ETA: 24:59 - loss: 0.6804 - regression_loss: 0.5471 - classification_loss: 0.1333
 6837/10000 [===================>..........] - ETA: 24:58 - loss: 0.6803 - regression_loss: 0.5470 - classification_loss: 0.1333
 6838/10000 [===================>..........] - ETA: 24:58 - loss: 0.6802 - regression_loss: 0.5470 - classification_loss: 0.1333
 6839/10000 [===================>..........] - ETA: 24:57 - loss: 0.6804 - regression_loss: 0.5471 - classification_loss: 0.1333
 6840/10000 [===================>..........] - ETA: 24:57 - loss: 0.6804 - regression_loss: 0.5471 - classification_loss: 0.1333
 6841/10000 [===================>..........] - ETA: 24:56 - loss: 0.6804 - regression_loss: 0.5471 - classification_loss: 0.1333
 6842/10000 [===================>..........] - ETA: 24:56 - loss: 0.6803 - regression_loss: 0.5470 - classification_loss: 0.1333
 6843/10000 [===================>..........] - ETA: 24:55 - loss: 0.6803 - regression_loss: 0.5470 - classification_loss: 0.1333
 6844/10000 [===================>..........] - ETA: 24:55 - loss: 0.6802 - regression_loss: 0.5469 - classification_loss: 0.1333
 6845/10000 [===================>..........] - ETA: 24:54 - loss: 0.6802 - regression_loss: 0.5469 - classification_loss: 0.1333
 6846/10000 [===================>..........] - ETA: 24:54 - loss: 0.6802 - regression_loss: 0.5469 - classification_loss: 0.1333
 6847/10000 [===================>..........] - ETA: 24:53 - loss: 0.6802 - regression_loss: 0.5469 - classification_loss: 0.1333
 6848/10000 [===================>..........] - ETA: 24:53 - loss: 0.6802 - regression_loss: 0.5469 - classification_loss: 0.1333
 6849/10000 [===================>..........] - ETA: 24:52 - loss: 0.6802 - regression_loss: 0.5469 - classification_loss: 0.1333
 6850/10000 [===================>..........] - ETA: 24:52 - loss: 0.6802 - regression_loss: 0.5469 - classification_loss: 0.1333
 6851/10000 [===================>..........] - ETA: 24:51 - loss: 0.6802 - regression_loss: 0.5469 - classification_loss: 0.1333
 6852/10000 [===================>..........] - ETA: 24:51 - loss: 0.6802 - regression_loss: 0.5469 - classification_loss: 0.1333
 6853/10000 [===================>..........] - ETA: 24:50 - loss: 0.6801 - regression_loss: 0.5468 - classification_loss: 0.1333
 6854/10000 [===================>..........] - ETA: 24:50 - loss: 0.6800 - regression_loss: 0.5468 - classification_loss: 0.1333
 6855/10000 [===================>..........] - ETA: 24:50 - loss: 0.6801 - regression_loss: 0.5468 - classification_loss: 0.1333
 6856/10000 [===================>..........] - ETA: 24:49 - loss: 0.6801 - regression_loss: 0.5468 - classification_loss: 0.1333
 6857/10000 [===================>..........] - ETA: 24:49 - loss: 0.6802 - regression_loss: 0.5469 - classification_loss: 0.1333
 6858/10000 [===================>..........] - ETA: 24:48 - loss: 0.6801 - regression_loss: 0.5468 - classification_loss: 0.1333
 6859/10000 [===================>..........] - ETA: 24:48 - loss: 0.6801 - regression_loss: 0.5468 - classification_loss: 0.1333
 6860/10000 [===================>..........] - ETA: 24:47 - loss: 0.6800 - regression_loss: 0.5468 - classification_loss: 0.1333
 6861/10000 [===================>..........] - ETA: 24:47 - loss: 0.6800 - regression_loss: 0.5467 - classification_loss: 0.1333
 6862/10000 [===================>..........] - ETA: 24:46 - loss: 0.6799 - regression_loss: 0.5467 - classification_loss: 0.1332
 6863/10000 [===================>..........] - ETA: 24:46 - loss: 0.6799 - regression_loss: 0.5467 - classification_loss: 0.1332
 6864/10000 [===================>..........] - ETA: 24:45 - loss: 0.6798 - regression_loss: 0.5466 - classification_loss: 0.1332
 6865/10000 [===================>..........] - ETA: 24:45 - loss: 0.6799 - regression_loss: 0.5467 - classification_loss: 0.1332
 6866/10000 [===================>..........] - ETA: 24:44 - loss: 0.6798 - regression_loss: 0.5466 - classification_loss: 0.1332
 6867/10000 [===================>..........] - ETA: 24:44 - loss: 0.6799 - regression_loss: 0.5467 - classification_loss: 0.1332
 6868/10000 [===================>..........] - ETA: 24:43 - loss: 0.6798 - regression_loss: 0.5466 - classification_loss: 0.1332
 6869/10000 [===================>..........] - ETA: 24:43 - loss: 0.6799 - regression_loss: 0.5467 - classification_loss: 0.1332
 6870/10000 [===================>..........] - ETA: 24:42 - loss: 0.6800 - regression_loss: 0.5468 - classification_loss: 0.1332
 6871/10000 [===================>..........] - ETA: 24:42 - loss: 0.6800 - regression_loss: 0.5468 - classification_loss: 0.1332
 6872/10000 [===================>..........] - ETA: 24:41 - loss: 0.6801 - regression_loss: 0.5469 - classification_loss: 0.1332
 6873/10000 [===================>..........] - ETA: 24:41 - loss: 0.6801 - regression_loss: 0.5469 - classification_loss: 0.1332
 6874/10000 [===================>..........] - ETA: 24:40 - loss: 0.6802 - regression_loss: 0.5469 - classification_loss: 0.1332
 6875/10000 [===================>..........] - ETA: 24:40 - loss: 0.6801 - regression_loss: 0.5469 - classification_loss: 0.1332
 6876/10000 [===================>..........] - ETA: 24:39 - loss: 0.6801 - regression_loss: 0.5469 - classification_loss: 0.1332
 6877/10000 [===================>..........] - ETA: 24:39 - loss: 0.6800 - regression_loss: 0.5468 - classification_loss: 0.1332
 6878/10000 [===================>..........] - ETA: 24:38 - loss: 0.6800 - regression_loss: 0.5468 - classification_loss: 0.1332
 6879/10000 [===================>..........] - ETA: 24:38 - loss: 0.6802 - regression_loss: 0.5470 - classification_loss: 0.1332
 6880/10000 [===================>..........] - ETA: 24:37 - loss: 0.6802 - regression_loss: 0.5470 - classification_loss: 0.1332
 6881/10000 [===================>..........] - ETA: 24:37 - loss: 0.6802 - regression_loss: 0.5470 - classification_loss: 0.1332
 6882/10000 [===================>..........] - ETA: 24:37 - loss: 0.6803 - regression_loss: 0.5471 - classification_loss: 0.1332
 6883/10000 [===================>..........] - ETA: 24:36 - loss: 0.6804 - regression_loss: 0.5471 - classification_loss: 0.1332
 6884/10000 [===================>..........] - ETA: 24:36 - loss: 0.6804 - regression_loss: 0.5472 - classification_loss: 0.1332
 6885/10000 [===================>..........] - ETA: 24:35 - loss: 0.6803 - regression_loss: 0.5471 - classification_loss: 0.1332
 6886/10000 [===================>..........] - ETA: 24:35 - loss: 0.6803 - regression_loss: 0.5471 - classification_loss: 0.1332
 6887/10000 [===================>..........] - ETA: 24:34 - loss: 0.6803 - regression_loss: 0.5471 - classification_loss: 0.1332
 6888/10000 [===================>..........] - ETA: 24:34 - loss: 0.6803 - regression_loss: 0.5471 - classification_loss: 0.1332
 6889/10000 [===================>..........] - ETA: 24:33 - loss: 0.6802 - regression_loss: 0.5470 - classification_loss: 0.1331
 6890/10000 [===================>..........] - ETA: 24:33 - loss: 0.6801 - regression_loss: 0.5470 - classification_loss: 0.1331
 6891/10000 [===================>..........] - ETA: 24:32 - loss: 0.6801 - regression_loss: 0.5469 - classification_loss: 0.1331
 6892/10000 [===================>..........] - ETA: 24:32 - loss: 0.6800 - regression_loss: 0.5469 - classification_loss: 0.1331
 6893/10000 [===================>..........] - ETA: 24:31 - loss: 0.6801 - regression_loss: 0.5470 - classification_loss: 0.1331
 6894/10000 [===================>..........] - ETA: 24:31 - loss: 0.6802 - regression_loss: 0.5470 - classification_loss: 0.1331
 6895/10000 [===================>..........] - ETA: 24:30 - loss: 0.6803 - regression_loss: 0.5471 - classification_loss: 0.1332
 6896/10000 [===================>..........] - ETA: 24:30 - loss: 0.6802 - regression_loss: 0.5471 - classification_loss: 0.1331
 6897/10000 [===================>..........] - ETA: 24:29 - loss: 0.6802 - regression_loss: 0.5470 - classification_loss: 0.1331
 6898/10000 [===================>..........] - ETA: 24:29 - loss: 0.6802 - regression_loss: 0.5470 - classification_loss: 0.1331
 6899/10000 [===================>..........] - ETA: 24:28 - loss: 0.6801 - regression_loss: 0.5470 - classification_loss: 0.1331
 6900/10000 [===================>..........] - ETA: 24:28 - loss: 0.6801 - regression_loss: 0.5469 - classification_loss: 0.1331
 6901/10000 [===================>..........] - ETA: 24:27 - loss: 0.6801 - regression_loss: 0.5470 - classification_loss: 0.1331
 6902/10000 [===================>..........] - ETA: 24:27 - loss: 0.6801 - regression_loss: 0.5469 - classification_loss: 0.1331
 6903/10000 [===================>..........] - ETA: 24:26 - loss: 0.6800 - regression_loss: 0.5469 - classification_loss: 0.1331
 6904/10000 [===================>..........] - ETA: 24:26 - loss: 0.6800 - regression_loss: 0.5469 - classification_loss: 0.1331
 6905/10000 [===================>..........] - ETA: 24:25 - loss: 0.6800 - regression_loss: 0.5469 - classification_loss: 0.1331
 6906/10000 [===================>..........] - ETA: 24:25 - loss: 0.6801 - regression_loss: 0.5470 - classification_loss: 0.1331
 6907/10000 [===================>..........] - ETA: 24:24 - loss: 0.6802 - regression_loss: 0.5470 - classification_loss: 0.1332
 6908/10000 [===================>..........] - ETA: 24:24 - loss: 0.6802 - regression_loss: 0.5470 - classification_loss: 0.1332
 6909/10000 [===================>..........] - ETA: 24:23 - loss: 0.6801 - regression_loss: 0.5470 - classification_loss: 0.1332
 6910/10000 [===================>..........] - ETA: 24:23 - loss: 0.6801 - regression_loss: 0.5469 - classification_loss: 0.1332
 6911/10000 [===================>..........] - ETA: 24:22 - loss: 0.6801 - regression_loss: 0.5469 - classification_loss: 0.1332
 6912/10000 [===================>..........] - ETA: 24:22 - loss: 0.6801 - regression_loss: 0.5469 - classification_loss: 0.1332
 6913/10000 [===================>..........] - ETA: 24:21 - loss: 0.6801 - regression_loss: 0.5469 - classification_loss: 0.1332
 6914/10000 [===================>..........] - ETA: 24:21 - loss: 0.6801 - regression_loss: 0.5469 - classification_loss: 0.1332
 6915/10000 [===================>..........] - ETA: 24:21 - loss: 0.6800 - regression_loss: 0.5468 - classification_loss: 0.1332
 6916/10000 [===================>..........] - ETA: 24:20 - loss: 0.6801 - regression_loss: 0.5469 - classification_loss: 0.1332
 6917/10000 [===================>..........] - ETA: 24:20 - loss: 0.6801 - regression_loss: 0.5469 - classification_loss: 0.1332
 6918/10000 [===================>..........] - ETA: 24:19 - loss: 0.6800 - regression_loss: 0.5468 - classification_loss: 0.1332
 6919/10000 [===================>..........] - ETA: 24:19 - loss: 0.6801 - regression_loss: 0.5469 - classification_loss: 0.1332
 6920/10000 [===================>..........] - ETA: 24:18 - loss: 0.6802 - regression_loss: 0.5470 - classification_loss: 0.1332
 6921/10000 [===================>..........] - ETA: 24:18 - loss: 0.6801 - regression_loss: 0.5469 - classification_loss: 0.1332
 6922/10000 [===================>..........] - ETA: 24:17 - loss: 0.6801 - regression_loss: 0.5469 - classification_loss: 0.1332
 6923/10000 [===================>..........] - ETA: 24:17 - loss: 0.6800 - regression_loss: 0.5469 - classification_loss: 0.1332
 6924/10000 [===================>..........] - ETA: 24:16 - loss: 0.6800 - regression_loss: 0.5468 - classification_loss: 0.1332
 6925/10000 [===================>..........] - ETA: 24:16 - loss: 0.6800 - regression_loss: 0.5468 - classification_loss: 0.1332
 6926/10000 [===================>..........] - ETA: 24:15 - loss: 0.6799 - regression_loss: 0.5467 - classification_loss: 0.1332
 6927/10000 [===================>..........] - ETA: 24:15 - loss: 0.6800 - regression_loss: 0.5468 - classification_loss: 0.1332
 6928/10000 [===================>..........] - ETA: 24:14 - loss: 0.6800 - regression_loss: 0.5469 - classification_loss: 0.1332
 6929/10000 [===================>..........] - ETA: 24:14 - loss: 0.6800 - regression_loss: 0.5468 - classification_loss: 0.1332
 6930/10000 [===================>..........] - ETA: 24:13 - loss: 0.6800 - regression_loss: 0.5468 - classification_loss: 0.1332
 6931/10000 [===================>..........] - ETA: 24:13 - loss: 0.6800 - regression_loss: 0.5469 - classification_loss: 0.1332
 6932/10000 [===================>..........] - ETA: 24:12 - loss: 0.6800 - regression_loss: 0.5469 - classification_loss: 0.1332
 6933/10000 [===================>..........] - ETA: 24:12 - loss: 0.6800 - regression_loss: 0.5469 - classification_loss: 0.1332
 6934/10000 [===================>..........] - ETA: 24:11 - loss: 0.6799 - regression_loss: 0.5468 - classification_loss: 0.1331
 6935/10000 [===================>..........] - ETA: 24:11 - loss: 0.6799 - regression_loss: 0.5468 - classification_loss: 0.1331
 6936/10000 [===================>..........] - ETA: 24:10 - loss: 0.6798 - regression_loss: 0.5467 - classification_loss: 0.1331
 6937/10000 [===================>..........] - ETA: 24:10 - loss: 0.6798 - regression_loss: 0.5467 - classification_loss: 0.1331
 6938/10000 [===================>..........] - ETA: 24:10 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 6939/10000 [===================>..........] - ETA: 24:09 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 6940/10000 [===================>..........] - ETA: 24:09 - loss: 0.6798 - regression_loss: 0.5466 - classification_loss: 0.1331
 6941/10000 [===================>..........] - ETA: 24:08 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 6942/10000 [===================>..........] - ETA: 24:08 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 6943/10000 [===================>..........] - ETA: 24:07 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 6944/10000 [===================>..........] - ETA: 24:07 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 6945/10000 [===================>..........] - ETA: 24:06 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 6946/10000 [===================>..........] - ETA: 24:06 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 6947/10000 [===================>..........] - ETA: 24:05 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 6948/10000 [===================>..........] - ETA: 24:05 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 6949/10000 [===================>..........] - ETA: 24:04 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 6950/10000 [===================>..........] - ETA: 24:04 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 6951/10000 [===================>..........] - ETA: 24:03 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 6952/10000 [===================>..........] - ETA: 24:03 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 6953/10000 [===================>..........] - ETA: 24:02 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 6954/10000 [===================>..........] - ETA: 24:02 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 6955/10000 [===================>..........] - ETA: 24:01 - loss: 0.6798 - regression_loss: 0.5467 - classification_loss: 0.1331
 6956/10000 [===================>..........] - ETA: 24:01 - loss: 0.6798 - regression_loss: 0.5467 - classification_loss: 0.1331
 6957/10000 [===================>..........] - ETA: 24:00 - loss: 0.6798 - regression_loss: 0.5467 - classification_loss: 0.1331
 6958/10000 [===================>..........] - ETA: 24:00 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 6959/10000 [===================>..........] - ETA: 23:59 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 6960/10000 [===================>..........] - ETA: 23:59 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 6961/10000 [===================>..........] - ETA: 23:58 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 6962/10000 [===================>..........] - ETA: 23:58 - loss: 0.6798 - regression_loss: 0.5466 - classification_loss: 0.1331
 6963/10000 [===================>..........] - ETA: 23:57 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 6964/10000 [===================>..........] - ETA: 23:57 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 6965/10000 [===================>..........] - ETA: 23:56 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 6966/10000 [===================>..........] - ETA: 23:56 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1331
 6967/10000 [===================>..........] - ETA: 23:56 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1331
 6968/10000 [===================>..........] - ETA: 23:55 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1331
 6969/10000 [===================>..........] - ETA: 23:55 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 6970/10000 [===================>..........] - ETA: 23:54 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 6971/10000 [===================>..........] - ETA: 23:54 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 6972/10000 [===================>..........] - ETA: 23:53 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 6973/10000 [===================>..........] - ETA: 23:53 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 6974/10000 [===================>..........] - ETA: 23:52 - loss: 0.6796 - regression_loss: 0.5466 - classification_loss: 0.1331
 6975/10000 [===================>..........] - ETA: 23:52 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 6976/10000 [===================>..........] - ETA: 23:51 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1330
 6977/10000 [===================>..........] - ETA: 23:51 - loss: 0.6796 - regression_loss: 0.5466 - classification_loss: 0.1330
 6978/10000 [===================>..........] - ETA: 23:50 - loss: 0.6796 - regression_loss: 0.5466 - classification_loss: 0.1330
 6979/10000 [===================>..........] - ETA: 23:50 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1330
 6980/10000 [===================>..........] - ETA: 23:49 - loss: 0.6796 - regression_loss: 0.5466 - classification_loss: 0.1330
 6981/10000 [===================>..........] - ETA: 23:49 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1330
 6982/10000 [===================>..........] - ETA: 23:48 - loss: 0.6796 - regression_loss: 0.5466 - classification_loss: 0.1330
 6983/10000 [===================>..........] - ETA: 23:48 - loss: 0.6796 - regression_loss: 0.5466 - classification_loss: 0.1330
 6984/10000 [===================>..........] - ETA: 23:47 - loss: 0.6797 - regression_loss: 0.5467 - classification_loss: 0.1330
 6985/10000 [===================>..........] - ETA: 23:47 - loss: 0.6797 - regression_loss: 0.5467 - classification_loss: 0.1330
 6986/10000 [===================>..........] - ETA: 23:46 - loss: 0.6797 - regression_loss: 0.5467 - classification_loss: 0.1330
 6987/10000 [===================>..........] - ETA: 23:46 - loss: 0.6796 - regression_loss: 0.5466 - classification_loss: 0.1330
 6988/10000 [===================>..........] - ETA: 23:45 - loss: 0.6796 - regression_loss: 0.5466 - classification_loss: 0.1330
 6989/10000 [===================>..........] - ETA: 23:45 - loss: 0.6796 - regression_loss: 0.5466 - classification_loss: 0.1330
 6990/10000 [===================>..........] - ETA: 23:45 - loss: 0.6796 - regression_loss: 0.5466 - classification_loss: 0.1330
 6991/10000 [===================>..........] - ETA: 23:44 - loss: 0.6796 - regression_loss: 0.5466 - classification_loss: 0.1330
 6992/10000 [===================>..........] - ETA: 23:44 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1330
 6993/10000 [===================>..........] - ETA: 23:43 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1330
 6994/10000 [===================>..........] - ETA: 23:43 - loss: 0.6794 - regression_loss: 0.5465 - classification_loss: 0.1330
 6995/10000 [===================>..........] - ETA: 23:42 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1330
 6996/10000 [===================>..........] - ETA: 23:42 - loss: 0.6794 - regression_loss: 0.5465 - classification_loss: 0.1330
 6997/10000 [===================>..........] - ETA: 23:41 - loss: 0.6794 - regression_loss: 0.5465 - classification_loss: 0.1330
 6998/10000 [===================>..........] - ETA: 23:41 - loss: 0.6794 - regression_loss: 0.5465 - classification_loss: 0.1330
 6999/10000 [===================>..........] - ETA: 23:40 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1330
 7000/10000 [====================>.........] - ETA: 23:40 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1329
 7001/10000 [====================>.........] - ETA: 23:39 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1329
 7002/10000 [====================>.........] - ETA: 23:39 - loss: 0.6793 - regression_loss: 0.5464 - classification_loss: 0.1329
 7003/10000 [====================>.........] - ETA: 23:38 - loss: 0.6793 - regression_loss: 0.5464 - classification_loss: 0.1330
 7004/10000 [====================>.........] - ETA: 23:38 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1329
 7005/10000 [====================>.........] - ETA: 23:37 - loss: 0.6792 - regression_loss: 0.5463 - classification_loss: 0.1329
 7006/10000 [====================>.........] - ETA: 23:37 - loss: 0.6792 - regression_loss: 0.5463 - classification_loss: 0.1329
 7007/10000 [====================>.........] - ETA: 23:36 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 7008/10000 [====================>.........] - ETA: 23:36 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 7009/10000 [====================>.........] - ETA: 23:35 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 7010/10000 [====================>.........] - ETA: 23:35 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 7011/10000 [====================>.........] - ETA: 23:34 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 7012/10000 [====================>.........] - ETA: 23:34 - loss: 0.6792 - regression_loss: 0.5462 - classification_loss: 0.1329
 7013/10000 [====================>.........] - ETA: 23:34 - loss: 0.6792 - regression_loss: 0.5463 - classification_loss: 0.1330
 7014/10000 [====================>.........] - ETA: 23:33 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 7015/10000 [====================>.........] - ETA: 23:33 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 7016/10000 [====================>.........] - ETA: 23:32 - loss: 0.6792 - regression_loss: 0.5463 - classification_loss: 0.1329
 7017/10000 [====================>.........] - ETA: 23:32 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1330
 7018/10000 [====================>.........] - ETA: 23:31 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1330
 7019/10000 [====================>.........] - ETA: 23:31 - loss: 0.6792 - regression_loss: 0.5463 - classification_loss: 0.1330
 7020/10000 [====================>.........] - ETA: 23:30 - loss: 0.6792 - regression_loss: 0.5463 - classification_loss: 0.1329
 7021/10000 [====================>.........] - ETA: 23:30 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1330
 7022/10000 [====================>.........] - ETA: 23:29 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1330
 7023/10000 [====================>.........] - ETA: 23:29 - loss: 0.6793 - regression_loss: 0.5464 - classification_loss: 0.1330
 7024/10000 [====================>.........] - ETA: 23:28 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1330
 7025/10000 [====================>.........] - ETA: 23:28 - loss: 0.6794 - regression_loss: 0.5465 - classification_loss: 0.1330
 7026/10000 [====================>.........] - ETA: 23:27 - loss: 0.6795 - regression_loss: 0.5464 - classification_loss: 0.1330
 7027/10000 [====================>.........] - ETA: 23:27 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1330
 7028/10000 [====================>.........] - ETA: 23:26 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1330
 7029/10000 [====================>.........] - ETA: 23:26 - loss: 0.6795 - regression_loss: 0.5464 - classification_loss: 0.1330
 7030/10000 [====================>.........] - ETA: 23:25 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1330
 7031/10000 [====================>.........] - ETA: 23:25 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1330
 7032/10000 [====================>.........] - ETA: 23:24 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1330
 7033/10000 [====================>.........] - ETA: 23:24 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1330
 7034/10000 [====================>.........] - ETA: 23:23 - loss: 0.6793 - regression_loss: 0.5462 - classification_loss: 0.1330
 7035/10000 [====================>.........] - ETA: 23:23 - loss: 0.6792 - regression_loss: 0.5462 - classification_loss: 0.1330
 7036/10000 [====================>.........] - ETA: 23:23 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1330
 7037/10000 [====================>.........] - ETA: 23:22 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1330
 7038/10000 [====================>.........] - ETA: 23:22 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1330
 7039/10000 [====================>.........] - ETA: 23:21 - loss: 0.6792 - regression_loss: 0.5462 - classification_loss: 0.1330
 7040/10000 [====================>.........] - ETA: 23:21 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1330
 7041/10000 [====================>.........] - ETA: 23:20 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 7042/10000 [====================>.........] - ETA: 23:20 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 7043/10000 [====================>.........] - ETA: 23:19 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1329
 7044/10000 [====================>.........] - ETA: 23:19 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 7045/10000 [====================>.........] - ETA: 23:18 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 7046/10000 [====================>.........] - ETA: 23:18 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 7047/10000 [====================>.........] - ETA: 23:17 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1329
 7048/10000 [====================>.........] - ETA: 23:17 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 7049/10000 [====================>.........] - ETA: 23:16 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 7050/10000 [====================>.........] - ETA: 23:16 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 7051/10000 [====================>.........] - ETA: 23:15 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1329
 7052/10000 [====================>.........] - ETA: 23:15 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 7053/10000 [====================>.........] - ETA: 23:14 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 7054/10000 [====================>.........] - ETA: 23:14 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 7055/10000 [====================>.........] - ETA: 23:13 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1329
 7056/10000 [====================>.........] - ETA: 23:13 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7057/10000 [====================>.........] - ETA: 23:12 - loss: 0.6793 - regression_loss: 0.5462 - classification_loss: 0.1330
 7058/10000 [====================>.........] - ETA: 23:12 - loss: 0.6792 - regression_loss: 0.5462 - classification_loss: 0.1330
 7059/10000 [====================>.........] - ETA: 23:12 - loss: 0.6793 - regression_loss: 0.5462 - classification_loss: 0.1330
 7060/10000 [====================>.........] - ETA: 23:11 - loss: 0.6792 - regression_loss: 0.5462 - classification_loss: 0.1330
 7061/10000 [====================>.........] - ETA: 23:11 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1330
 7062/10000 [====================>.........] - ETA: 23:10 - loss: 0.6793 - regression_loss: 0.5462 - classification_loss: 0.1330
 7063/10000 [====================>.........] - ETA: 23:10 - loss: 0.6792 - regression_loss: 0.5462 - classification_loss: 0.1330
 7064/10000 [====================>.........] - ETA: 23:09 - loss: 0.6792 - regression_loss: 0.5462 - classification_loss: 0.1330
 7065/10000 [====================>.........] - ETA: 23:09 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1330
 7066/10000 [====================>.........] - ETA: 23:08 - loss: 0.6792 - regression_loss: 0.5462 - classification_loss: 0.1330
 7067/10000 [====================>.........] - ETA: 23:08 - loss: 0.6792 - regression_loss: 0.5462 - classification_loss: 0.1330
 7068/10000 [====================>.........] - ETA: 23:07 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1330
 7069/10000 [====================>.........] - ETA: 23:07 - loss: 0.6794 - regression_loss: 0.5463 - classification_loss: 0.1330
 7070/10000 [====================>.........] - ETA: 23:06 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1330
 7071/10000 [====================>.........] - ETA: 23:06 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1330
 7072/10000 [====================>.........] - ETA: 23:05 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1330
 7073/10000 [====================>.........] - ETA: 23:05 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1330
 7074/10000 [====================>.........] - ETA: 23:04 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1330
 7075/10000 [====================>.........] - ETA: 23:04 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1330
 7076/10000 [====================>.........] - ETA: 23:03 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1330
 7077/10000 [====================>.........] - ETA: 23:03 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1330
 7078/10000 [====================>.........] - ETA: 23:02 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1331
 7079/10000 [====================>.........] - ETA: 23:02 - loss: 0.6795 - regression_loss: 0.5464 - classification_loss: 0.1331
 7080/10000 [====================>.........] - ETA: 23:01 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1331
 7081/10000 [====================>.........] - ETA: 23:01 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 7082/10000 [====================>.........] - ETA: 23:01 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 7083/10000 [====================>.........] - ETA: 23:00 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 7084/10000 [====================>.........] - ETA: 23:00 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 7085/10000 [====================>.........] - ETA: 22:59 - loss: 0.6798 - regression_loss: 0.5467 - classification_loss: 0.1331
 7086/10000 [====================>.........] - ETA: 22:59 - loss: 0.6798 - regression_loss: 0.5467 - classification_loss: 0.1331
 7087/10000 [====================>.........] - ETA: 22:58 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 7088/10000 [====================>.........] - ETA: 22:58 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 7089/10000 [====================>.........] - ETA: 22:57 - loss: 0.6796 - regression_loss: 0.5466 - classification_loss: 0.1331
 7090/10000 [====================>.........] - ETA: 22:57 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 7091/10000 [====================>.........] - ETA: 22:56 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 7092/10000 [====================>.........] - ETA: 22:56 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 7093/10000 [====================>.........] - ETA: 22:55 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 7094/10000 [====================>.........] - ETA: 22:55 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 7095/10000 [====================>.........] - ETA: 22:54 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 7096/10000 [====================>.........] - ETA: 22:54 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 7097/10000 [====================>.........] - ETA: 22:53 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 7098/10000 [====================>.........] - ETA: 22:53 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 7099/10000 [====================>.........] - ETA: 22:52 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1331
 7100/10000 [====================>.........] - ETA: 22:52 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 7101/10000 [====================>.........] - ETA: 22:51 - loss: 0.6795 - regression_loss: 0.5464 - classification_loss: 0.1331
 7102/10000 [====================>.........] - ETA: 22:51 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 7103/10000 [====================>.........] - ETA: 22:50 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1332
 7104/10000 [====================>.........] - ETA: 22:50 - loss: 0.6797 - regression_loss: 0.5465 - classification_loss: 0.1332
 7105/10000 [====================>.........] - ETA: 22:49 - loss: 0.6797 - regression_loss: 0.5465 - classification_loss: 0.1332
 7106/10000 [====================>.........] - ETA: 22:49 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1332
 7107/10000 [====================>.........] - ETA: 22:48 - loss: 0.6798 - regression_loss: 0.5467 - classification_loss: 0.1332
 7108/10000 [====================>.........] - ETA: 22:48 - loss: 0.6798 - regression_loss: 0.5466 - classification_loss: 0.1332
 7109/10000 [====================>.........] - ETA: 22:48 - loss: 0.6798 - regression_loss: 0.5466 - classification_loss: 0.1332
 7110/10000 [====================>.........] - ETA: 22:47 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 7111/10000 [====================>.........] - ETA: 22:47 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 7112/10000 [====================>.........] - ETA: 22:46 - loss: 0.6798 - regression_loss: 0.5466 - classification_loss: 0.1331
 7113/10000 [====================>.........] - ETA: 22:46 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 7114/10000 [====================>.........] - ETA: 22:45 - loss: 0.6797 - regression_loss: 0.5465 - classification_loss: 0.1331
 7115/10000 [====================>.........] - ETA: 22:45 - loss: 0.6797 - regression_loss: 0.5465 - classification_loss: 0.1331
 7116/10000 [====================>.........] - ETA: 22:44 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 7117/10000 [====================>.........] - ETA: 22:44 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 7118/10000 [====================>.........] - ETA: 22:43 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 7119/10000 [====================>.........] - ETA: 22:43 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 7120/10000 [====================>.........] - ETA: 22:42 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 7121/10000 [====================>.........] - ETA: 22:42 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 7122/10000 [====================>.........] - ETA: 22:41 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 7123/10000 [====================>.........] - ETA: 22:41 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 7124/10000 [====================>.........] - ETA: 22:40 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 7125/10000 [====================>.........] - ETA: 22:40 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 7126/10000 [====================>.........] - ETA: 22:39 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 7127/10000 [====================>.........] - ETA: 22:39 - loss: 0.6796 - regression_loss: 0.5466 - classification_loss: 0.1331
 7128/10000 [====================>.........] - ETA: 22:38 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 7129/10000 [====================>.........] - ETA: 22:38 - loss: 0.6796 - regression_loss: 0.5466 - classification_loss: 0.1331
 7130/10000 [====================>.........] - ETA: 22:37 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 7131/10000 [====================>.........] - ETA: 22:37 - loss: 0.6796 - regression_loss: 0.5466 - classification_loss: 0.1331
 7132/10000 [====================>.........] - ETA: 22:36 - loss: 0.6796 - regression_loss: 0.5466 - classification_loss: 0.1331
 7133/10000 [====================>.........] - ETA: 22:36 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1330
 7134/10000 [====================>.........] - ETA: 22:36 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 7135/10000 [====================>.........] - ETA: 22:35 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 7136/10000 [====================>.........] - ETA: 22:35 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 7137/10000 [====================>.........] - ETA: 22:34 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1331
 7138/10000 [====================>.........] - ETA: 22:34 - loss: 0.6795 - regression_loss: 0.5464 - classification_loss: 0.1331
 7139/10000 [====================>.........] - ETA: 22:33 - loss: 0.6795 - regression_loss: 0.5464 - classification_loss: 0.1331
 7140/10000 [====================>.........] - ETA: 22:33 - loss: 0.6795 - regression_loss: 0.5464 - classification_loss: 0.1331
 7141/10000 [====================>.........] - ETA: 22:32 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1331
 7142/10000 [====================>.........] - ETA: 22:32 - loss: 0.6795 - regression_loss: 0.5464 - classification_loss: 0.1330
 7143/10000 [====================>.........] - ETA: 22:31 - loss: 0.6795 - regression_loss: 0.5464 - classification_loss: 0.1331
 7144/10000 [====================>.........] - ETA: 22:31 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1330
 7145/10000 [====================>.........] - ETA: 22:30 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 7146/10000 [====================>.........] - ETA: 22:30 - loss: 0.6795 - regression_loss: 0.5464 - classification_loss: 0.1330
 7147/10000 [====================>.........] - ETA: 22:29 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1330
 7148/10000 [====================>.........] - ETA: 22:29 - loss: 0.6794 - regression_loss: 0.5463 - classification_loss: 0.1330
 7149/10000 [====================>.........] - ETA: 22:28 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1330
 7150/10000 [====================>.........] - ETA: 22:28 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1330
 7151/10000 [====================>.........] - ETA: 22:27 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1330
 7152/10000 [====================>.........] - ETA: 22:27 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1330
 7153/10000 [====================>.........] - ETA: 22:26 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1330
 7154/10000 [====================>.........] - ETA: 22:26 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1330
 7155/10000 [====================>.........] - ETA: 22:25 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1330
 7156/10000 [====================>.........] - ETA: 22:25 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1330
 7157/10000 [====================>.........] - ETA: 22:24 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1330
 7158/10000 [====================>.........] - ETA: 22:24 - loss: 0.6795 - regression_loss: 0.5464 - classification_loss: 0.1330
 7159/10000 [====================>.........] - ETA: 22:24 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1330
 7160/10000 [====================>.........] - ETA: 22:23 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1330
 7161/10000 [====================>.........] - ETA: 22:23 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1330
 7162/10000 [====================>.........] - ETA: 22:22 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1330
 7163/10000 [====================>.........] - ETA: 22:22 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1330
 7164/10000 [====================>.........] - ETA: 22:21 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1330
 7165/10000 [====================>.........] - ETA: 22:21 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1330
 7166/10000 [====================>.........] - ETA: 22:20 - loss: 0.6796 - regression_loss: 0.5466 - classification_loss: 0.1330
 7167/10000 [====================>.........] - ETA: 22:20 - loss: 0.6796 - regression_loss: 0.5466 - classification_loss: 0.1330
 7168/10000 [====================>.........] - ETA: 22:19 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 7169/10000 [====================>.........] - ETA: 22:19 - loss: 0.6798 - regression_loss: 0.5466 - classification_loss: 0.1332
 7170/10000 [====================>.........] - ETA: 22:18 - loss: 0.6798 - regression_loss: 0.5466 - classification_loss: 0.1332
 7171/10000 [====================>.........] - ETA: 22:18 - loss: 0.6798 - regression_loss: 0.5467 - classification_loss: 0.1332
 7172/10000 [====================>.........] - ETA: 22:17 - loss: 0.6799 - regression_loss: 0.5467 - classification_loss: 0.1332
 7173/10000 [====================>.........] - ETA: 22:17 - loss: 0.6799 - regression_loss: 0.5467 - classification_loss: 0.1332
 7174/10000 [====================>.........] - ETA: 22:16 - loss: 0.6798 - regression_loss: 0.5466 - classification_loss: 0.1332
 7175/10000 [====================>.........] - ETA: 22:16 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1332
 7176/10000 [====================>.........] - ETA: 22:15 - loss: 0.6797 - regression_loss: 0.5465 - classification_loss: 0.1331
 7177/10000 [====================>.........] - ETA: 22:15 - loss: 0.6797 - regression_loss: 0.5466 - classification_loss: 0.1331
 7178/10000 [====================>.........] - ETA: 22:14 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 7179/10000 [====================>.........] - ETA: 22:14 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 7180/10000 [====================>.........] - ETA: 22:13 - loss: 0.6796 - regression_loss: 0.5465 - classification_loss: 0.1331
 7181/10000 [====================>.........] - ETA: 22:13 - loss: 0.6796 - regression_loss: 0.5464 - classification_loss: 0.1331
 7182/10000 [====================>.........] - ETA: 22:13 - loss: 0.6795 - regression_loss: 0.5464 - classification_loss: 0.1331
 7183/10000 [====================>.........] - ETA: 22:12 - loss: 0.6795 - regression_loss: 0.5464 - classification_loss: 0.1331
 7184/10000 [====================>.........] - ETA: 22:12 - loss: 0.6795 - regression_loss: 0.5464 - classification_loss: 0.1331
 7185/10000 [====================>.........] - ETA: 22:11 - loss: 0.6795 - regression_loss: 0.5463 - classification_loss: 0.1331
 7186/10000 [====================>.........] - ETA: 22:11 - loss: 0.6795 - regression_loss: 0.5463 - classification_loss: 0.1331
 7187/10000 [====================>.........] - ETA: 22:10 - loss: 0.6795 - regression_loss: 0.5464 - classification_loss: 0.1331
 7188/10000 [====================>.........] - ETA: 22:10 - loss: 0.6794 - regression_loss: 0.5463 - classification_loss: 0.1331
 7189/10000 [====================>.........] - ETA: 22:09 - loss: 0.6794 - regression_loss: 0.5463 - classification_loss: 0.1331
 7190/10000 [====================>.........] - ETA: 22:09 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1331
 7191/10000 [====================>.........] - ETA: 22:08 - loss: 0.6794 - regression_loss: 0.5463 - classification_loss: 0.1331
 7192/10000 [====================>.........] - ETA: 22:08 - loss: 0.6794 - regression_loss: 0.5463 - classification_loss: 0.1331
 7193/10000 [====================>.........] - ETA: 22:07 - loss: 0.6794 - regression_loss: 0.5463 - classification_loss: 0.1331
 7194/10000 [====================>.........] - ETA: 22:07 - loss: 0.6793 - regression_loss: 0.5462 - classification_loss: 0.1331
 7195/10000 [====================>.........] - ETA: 22:06 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1331
 7196/10000 [====================>.........] - ETA: 22:06 - loss: 0.6793 - regression_loss: 0.5462 - classification_loss: 0.1331
 7197/10000 [====================>.........] - ETA: 22:05 - loss: 0.6792 - regression_loss: 0.5461 - classification_loss: 0.1331
 7198/10000 [====================>.........] - ETA: 22:05 - loss: 0.6792 - regression_loss: 0.5461 - classification_loss: 0.1331
 7199/10000 [====================>.........] - ETA: 22:04 - loss: 0.6792 - regression_loss: 0.5461 - classification_loss: 0.1331
 7200/10000 [====================>.........] - ETA: 22:04 - loss: 0.6792 - regression_loss: 0.5462 - classification_loss: 0.1331
 7201/10000 [====================>.........] - ETA: 22:03 - loss: 0.6792 - regression_loss: 0.5461 - classification_loss: 0.1331
 7202/10000 [====================>.........] - ETA: 22:03 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7203/10000 [====================>.........] - ETA: 22:02 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1331
 7204/10000 [====================>.........] - ETA: 22:02 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7205/10000 [====================>.........] - ETA: 22:01 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7206/10000 [====================>.........] - ETA: 22:01 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7207/10000 [====================>.........] - ETA: 22:01 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7208/10000 [====================>.........] - ETA: 22:00 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7209/10000 [====================>.........] - ETA: 22:00 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7210/10000 [====================>.........] - ETA: 21:59 - loss: 0.6791 - regression_loss: 0.5460 - classification_loss: 0.1330
 7211/10000 [====================>.........] - ETA: 21:59 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7212/10000 [====================>.........] - ETA: 21:58 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7213/10000 [====================>.........] - ETA: 21:58 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7214/10000 [====================>.........] - ETA: 21:57 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7215/10000 [====================>.........] - ETA: 21:57 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7216/10000 [====================>.........] - ETA: 21:56 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7217/10000 [====================>.........] - ETA: 21:56 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7218/10000 [====================>.........] - ETA: 21:55 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7219/10000 [====================>.........] - ETA: 21:55 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7220/10000 [====================>.........] - ETA: 21:54 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7221/10000 [====================>.........] - ETA: 21:54 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7222/10000 [====================>.........] - ETA: 21:53 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7223/10000 [====================>.........] - ETA: 21:53 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7224/10000 [====================>.........] - ETA: 21:52 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7225/10000 [====================>.........] - ETA: 21:52 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7226/10000 [====================>.........] - ETA: 21:51 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7227/10000 [====================>.........] - ETA: 21:51 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7228/10000 [====================>.........] - ETA: 21:50 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7229/10000 [====================>.........] - ETA: 21:50 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7230/10000 [====================>.........] - ETA: 21:49 - loss: 0.6792 - regression_loss: 0.5461 - classification_loss: 0.1330
 7231/10000 [====================>.........] - ETA: 21:49 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7232/10000 [====================>.........] - ETA: 21:49 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7233/10000 [====================>.........] - ETA: 21:48 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7234/10000 [====================>.........] - ETA: 21:48 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7235/10000 [====================>.........] - ETA: 21:47 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1330
 7236/10000 [====================>.........] - ETA: 21:47 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7237/10000 [====================>.........] - ETA: 21:46 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7238/10000 [====================>.........] - ETA: 21:46 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7239/10000 [====================>.........] - ETA: 21:45 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7240/10000 [====================>.........] - ETA: 21:45 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7241/10000 [====================>.........] - ETA: 21:44 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7242/10000 [====================>.........] - ETA: 21:44 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7243/10000 [====================>.........] - ETA: 21:43 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7244/10000 [====================>.........] - ETA: 21:43 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7245/10000 [====================>.........] - ETA: 21:42 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7246/10000 [====================>.........] - ETA: 21:42 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7247/10000 [====================>.........] - ETA: 21:41 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7248/10000 [====================>.........] - ETA: 21:41 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7249/10000 [====================>.........] - ETA: 21:40 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7250/10000 [====================>.........] - ETA: 21:40 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7251/10000 [====================>.........] - ETA: 21:39 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 7252/10000 [====================>.........] - ETA: 21:39 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 7253/10000 [====================>.........] - ETA: 21:39 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1329
 7254/10000 [====================>.........] - ETA: 21:38 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7255/10000 [====================>.........] - ETA: 21:38 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7256/10000 [====================>.........] - ETA: 21:37 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7257/10000 [====================>.........] - ETA: 21:37 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7258/10000 [====================>.........] - ETA: 21:36 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7259/10000 [====================>.........] - ETA: 21:36 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7260/10000 [====================>.........] - ETA: 21:35 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1330
 7261/10000 [====================>.........] - ETA: 21:35 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7262/10000 [====================>.........] - ETA: 21:34 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7263/10000 [====================>.........] - ETA: 21:34 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7264/10000 [====================>.........] - ETA: 21:33 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1330
 7265/10000 [====================>.........] - ETA: 21:33 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7266/10000 [====================>.........] - ETA: 21:32 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7267/10000 [====================>.........] - ETA: 21:32 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1330
 7268/10000 [====================>.........] - ETA: 21:31 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1329
 7269/10000 [====================>.........] - ETA: 21:31 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 7270/10000 [====================>.........] - ETA: 21:30 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 7271/10000 [====================>.........] - ETA: 21:30 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1329
 7272/10000 [====================>.........] - ETA: 21:29 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 7273/10000 [====================>.........] - ETA: 21:29 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 7274/10000 [====================>.........] - ETA: 21:28 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 7275/10000 [====================>.........] - ETA: 21:28 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1329
 7276/10000 [====================>.........] - ETA: 21:27 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1329
 7277/10000 [====================>.........] - ETA: 21:27 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 7278/10000 [====================>.........] - ETA: 21:27 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 7279/10000 [====================>.........] - ETA: 21:26 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 7280/10000 [====================>.........] - ETA: 21:26 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 7281/10000 [====================>.........] - ETA: 21:25 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7282/10000 [====================>.........] - ETA: 21:25 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7283/10000 [====================>.........] - ETA: 21:24 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1329
 7284/10000 [====================>.........] - ETA: 21:24 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7285/10000 [====================>.........] - ETA: 21:23 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7286/10000 [====================>.........] - ETA: 21:23 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7287/10000 [====================>.........] - ETA: 21:22 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7288/10000 [====================>.........] - ETA: 21:22 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7289/10000 [====================>.........] - ETA: 21:21 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7290/10000 [====================>.........] - ETA: 21:21 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7291/10000 [====================>.........] - ETA: 21:20 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7292/10000 [====================>.........] - ETA: 21:20 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 7293/10000 [====================>.........] - ETA: 21:19 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1329
 7294/10000 [====================>.........] - ETA: 21:19 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 7295/10000 [====================>.........] - ETA: 21:18 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 7296/10000 [====================>.........] - ETA: 21:18 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 7297/10000 [====================>.........] - ETA: 21:17 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1330
 7298/10000 [====================>.........] - ETA: 21:17 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7299/10000 [====================>.........] - ETA: 21:17 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7300/10000 [====================>.........] - ETA: 21:16 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7301/10000 [====================>.........] - ETA: 21:16 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7302/10000 [====================>.........] - ETA: 21:15 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7303/10000 [====================>.........] - ETA: 21:15 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1330
 7304/10000 [====================>.........] - ETA: 21:14 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 7305/10000 [====================>.........] - ETA: 21:14 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1330
 7306/10000 [====================>.........] - ETA: 21:13 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7307/10000 [====================>.........] - ETA: 21:13 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1330
 7308/10000 [====================>.........] - ETA: 21:12 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1329
 7309/10000 [====================>.........] - ETA: 21:12 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 7310/10000 [====================>.........] - ETA: 21:11 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 7311/10000 [====================>.........] - ETA: 21:11 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1329
 7312/10000 [====================>.........] - ETA: 21:10 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 7313/10000 [====================>.........] - ETA: 21:10 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7314/10000 [====================>.........] - ETA: 21:09 - loss: 0.6786 - regression_loss: 0.5458 - classification_loss: 0.1329
 7315/10000 [====================>.........] - ETA: 21:09 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1329
 7316/10000 [====================>.........] - ETA: 21:08 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7317/10000 [====================>.........] - ETA: 21:08 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7318/10000 [====================>.........] - ETA: 21:07 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7319/10000 [====================>.........] - ETA: 21:07 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7320/10000 [====================>.........] - ETA: 21:06 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7321/10000 [====================>.........] - ETA: 21:06 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7322/10000 [====================>.........] - ETA: 21:05 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7323/10000 [====================>.........] - ETA: 21:05 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7324/10000 [====================>.........] - ETA: 21:04 - loss: 0.6785 - regression_loss: 0.5457 - classification_loss: 0.1329
 7325/10000 [====================>.........] - ETA: 21:04 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7326/10000 [====================>.........] - ETA: 21:03 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1328
 7327/10000 [====================>.........] - ETA: 21:03 - loss: 0.6785 - regression_loss: 0.5457 - classification_loss: 0.1329
 7328/10000 [====================>.........] - ETA: 21:03 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7329/10000 [====================>.........] - ETA: 21:02 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7330/10000 [====================>.........] - ETA: 21:02 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7331/10000 [====================>.........] - ETA: 21:01 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7332/10000 [====================>.........] - ETA: 21:01 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7333/10000 [====================>.........] - ETA: 21:00 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7334/10000 [=====================>........] - ETA: 21:00 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7335/10000 [=====================>........] - ETA: 20:59 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7336/10000 [=====================>........] - ETA: 20:59 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7337/10000 [=====================>........] - ETA: 20:58 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1330
 7338/10000 [=====================>........] - ETA: 20:58 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1330
 7339/10000 [=====================>........] - ETA: 20:57 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1330
 7340/10000 [=====================>........] - ETA: 20:57 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7341/10000 [=====================>........] - ETA: 20:56 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1330
 7342/10000 [=====================>........] - ETA: 20:56 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7343/10000 [=====================>........] - ETA: 20:55 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1330
 7344/10000 [=====================>........] - ETA: 20:55 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1330
 7345/10000 [=====================>........] - ETA: 20:54 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7346/10000 [=====================>........] - ETA: 20:54 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7347/10000 [=====================>........] - ETA: 20:53 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7348/10000 [=====================>........] - ETA: 20:53 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1330
 7349/10000 [=====================>........] - ETA: 20:53 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7350/10000 [=====================>........] - ETA: 20:52 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7351/10000 [=====================>........] - ETA: 20:52 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1330
 7352/10000 [=====================>........] - ETA: 20:51 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 7353/10000 [=====================>........] - ETA: 20:51 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7354/10000 [=====================>........] - ETA: 20:50 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7355/10000 [=====================>........] - ETA: 20:50 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1330
 7356/10000 [=====================>........] - ETA: 20:49 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1329
 7357/10000 [=====================>........] - ETA: 20:49 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7358/10000 [=====================>........] - ETA: 20:48 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7359/10000 [=====================>........] - ETA: 20:48 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1329
 7360/10000 [=====================>........] - ETA: 20:47 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7361/10000 [=====================>........] - ETA: 20:47 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7362/10000 [=====================>........] - ETA: 20:46 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1329
 7363/10000 [=====================>........] - ETA: 20:46 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7364/10000 [=====================>........] - ETA: 20:45 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7365/10000 [=====================>........] - ETA: 20:45 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 7366/10000 [=====================>........] - ETA: 20:44 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 7367/10000 [=====================>........] - ETA: 20:44 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7368/10000 [=====================>........] - ETA: 20:44 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7369/10000 [=====================>........] - ETA: 20:43 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7370/10000 [=====================>........] - ETA: 20:43 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7371/10000 [=====================>........] - ETA: 20:42 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7372/10000 [=====================>........] - ETA: 20:42 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7373/10000 [=====================>........] - ETA: 20:41 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1329
 7374/10000 [=====================>........] - ETA: 20:41 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7375/10000 [=====================>........] - ETA: 20:40 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7376/10000 [=====================>........] - ETA: 20:40 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7377/10000 [=====================>........] - ETA: 20:39 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7378/10000 [=====================>........] - ETA: 20:39 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7379/10000 [=====================>........] - ETA: 20:38 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7380/10000 [=====================>........] - ETA: 20:38 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7381/10000 [=====================>........] - ETA: 20:37 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7382/10000 [=====================>........] - ETA: 20:37 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7383/10000 [=====================>........] - ETA: 20:36 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7384/10000 [=====================>........] - ETA: 20:36 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7385/10000 [=====================>........] - ETA: 20:35 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7386/10000 [=====================>........] - ETA: 20:35 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7387/10000 [=====================>........] - ETA: 20:34 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7388/10000 [=====================>........] - ETA: 20:34 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7389/10000 [=====================>........] - ETA: 20:34 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1330
 7390/10000 [=====================>........] - ETA: 20:33 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1330
 7391/10000 [=====================>........] - ETA: 20:33 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1330
 7392/10000 [=====================>........] - ETA: 20:32 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7393/10000 [=====================>........] - ETA: 20:32 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1330
 7394/10000 [=====================>........] - ETA: 20:31 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7395/10000 [=====================>........] - ETA: 20:31 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1330
 7396/10000 [=====================>........] - ETA: 20:30 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1330
 7397/10000 [=====================>........] - ETA: 20:30 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1330
 7398/10000 [=====================>........] - ETA: 20:29 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1330
 7399/10000 [=====================>........] - ETA: 20:29 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1330
 7400/10000 [=====================>........] - ETA: 20:28 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7401/10000 [=====================>........] - ETA: 20:28 - loss: 0.6786 - regression_loss: 0.5456 - classification_loss: 0.1329
 7402/10000 [=====================>........] - ETA: 20:27 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7403/10000 [=====================>........] - ETA: 20:27 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7404/10000 [=====================>........] - ETA: 20:26 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7405/10000 [=====================>........] - ETA: 20:26 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7406/10000 [=====================>........] - ETA: 20:25 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7407/10000 [=====================>........] - ETA: 20:25 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7408/10000 [=====================>........] - ETA: 20:24 - loss: 0.6786 - regression_loss: 0.5456 - classification_loss: 0.1329
 7409/10000 [=====================>........] - ETA: 20:24 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7410/10000 [=====================>........] - ETA: 20:23 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7411/10000 [=====================>........] - ETA: 20:23 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7412/10000 [=====================>........] - ETA: 20:23 - loss: 0.6785 - regression_loss: 0.5455 - classification_loss: 0.1329
 7413/10000 [=====================>........] - ETA: 20:22 - loss: 0.6786 - regression_loss: 0.5456 - classification_loss: 0.1330
 7414/10000 [=====================>........] - ETA: 20:22 - loss: 0.6786 - regression_loss: 0.5456 - classification_loss: 0.1330
 7415/10000 [=====================>........] - ETA: 20:21 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1330
 7416/10000 [=====================>........] - ETA: 20:21 - loss: 0.6786 - regression_loss: 0.5456 - classification_loss: 0.1330
 7417/10000 [=====================>........] - ETA: 20:20 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1330
 7418/10000 [=====================>........] - ETA: 20:20 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7419/10000 [=====================>........] - ETA: 20:19 - loss: 0.6785 - regression_loss: 0.5455 - classification_loss: 0.1329
 7420/10000 [=====================>........] - ETA: 20:19 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7421/10000 [=====================>........] - ETA: 20:18 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7422/10000 [=====================>........] - ETA: 20:18 - loss: 0.6784 - regression_loss: 0.5454 - classification_loss: 0.1329
 7423/10000 [=====================>........] - ETA: 20:17 - loss: 0.6784 - regression_loss: 0.5454 - classification_loss: 0.1329
 7424/10000 [=====================>........] - ETA: 20:17 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1329
 7425/10000 [=====================>........] - ETA: 20:16 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1329
 7426/10000 [=====================>........] - ETA: 20:16 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1329
 7427/10000 [=====================>........] - ETA: 20:15 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1329
 7428/10000 [=====================>........] - ETA: 20:15 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1329
 7429/10000 [=====================>........] - ETA: 20:14 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7430/10000 [=====================>........] - ETA: 20:14 - loss: 0.6785 - regression_loss: 0.5455 - classification_loss: 0.1329
 7431/10000 [=====================>........] - ETA: 20:13 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7432/10000 [=====================>........] - ETA: 20:13 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7433/10000 [=====================>........] - ETA: 20:13 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1329
 7434/10000 [=====================>........] - ETA: 20:12 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1329
 7435/10000 [=====================>........] - ETA: 20:12 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1329
 7436/10000 [=====================>........] - ETA: 20:11 - loss: 0.6782 - regression_loss: 0.5454 - classification_loss: 0.1329
 7437/10000 [=====================>........] - ETA: 20:11 - loss: 0.6782 - regression_loss: 0.5454 - classification_loss: 0.1329
 7438/10000 [=====================>........] - ETA: 20:10 - loss: 0.6782 - regression_loss: 0.5453 - classification_loss: 0.1329
 7439/10000 [=====================>........] - ETA: 20:10 - loss: 0.6782 - regression_loss: 0.5453 - classification_loss: 0.1329
 7440/10000 [=====================>........] - ETA: 20:09 - loss: 0.6782 - regression_loss: 0.5453 - classification_loss: 0.1328
 7441/10000 [=====================>........] - ETA: 20:09 - loss: 0.6782 - regression_loss: 0.5453 - classification_loss: 0.1328
 7442/10000 [=====================>........] - ETA: 20:08 - loss: 0.6781 - regression_loss: 0.5453 - classification_loss: 0.1328
 7443/10000 [=====================>........] - ETA: 20:08 - loss: 0.6782 - regression_loss: 0.5453 - classification_loss: 0.1328
 7444/10000 [=====================>........] - ETA: 20:07 - loss: 0.6782 - regression_loss: 0.5454 - classification_loss: 0.1328
 7445/10000 [=====================>........] - ETA: 20:07 - loss: 0.6782 - regression_loss: 0.5454 - classification_loss: 0.1328
 7446/10000 [=====================>........] - ETA: 20:06 - loss: 0.6782 - regression_loss: 0.5454 - classification_loss: 0.1328
 7447/10000 [=====================>........] - ETA: 20:06 - loss: 0.6782 - regression_loss: 0.5454 - classification_loss: 0.1328
 7448/10000 [=====================>........] - ETA: 20:05 - loss: 0.6782 - regression_loss: 0.5454 - classification_loss: 0.1328
 7449/10000 [=====================>........] - ETA: 20:05 - loss: 0.6782 - regression_loss: 0.5453 - classification_loss: 0.1328
 7450/10000 [=====================>........] - ETA: 20:04 - loss: 0.6781 - regression_loss: 0.5453 - classification_loss: 0.1328
 7451/10000 [=====================>........] - ETA: 20:04 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1328
 7452/10000 [=====================>........] - ETA: 20:03 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1328
 7453/10000 [=====================>........] - ETA: 20:03 - loss: 0.6782 - regression_loss: 0.5454 - classification_loss: 0.1328
 7454/10000 [=====================>........] - ETA: 20:03 - loss: 0.6782 - regression_loss: 0.5454 - classification_loss: 0.1328
 7455/10000 [=====================>........] - ETA: 20:02 - loss: 0.6781 - regression_loss: 0.5453 - classification_loss: 0.1328
 7456/10000 [=====================>........] - ETA: 20:02 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1328
 7457/10000 [=====================>........] - ETA: 20:01 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1328
 7458/10000 [=====================>........] - ETA: 20:01 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1328
 7459/10000 [=====================>........] - ETA: 20:00 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1328
 7460/10000 [=====================>........] - ETA: 20:00 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1328
 7461/10000 [=====================>........] - ETA: 19:59 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1328
 7462/10000 [=====================>........] - ETA: 19:59 - loss: 0.6782 - regression_loss: 0.5454 - classification_loss: 0.1328
 7463/10000 [=====================>........] - ETA: 19:58 - loss: 0.6782 - regression_loss: 0.5454 - classification_loss: 0.1328
 7464/10000 [=====================>........] - ETA: 19:58 - loss: 0.6781 - regression_loss: 0.5453 - classification_loss: 0.1328
 7465/10000 [=====================>........] - ETA: 19:57 - loss: 0.6782 - regression_loss: 0.5454 - classification_loss: 0.1328
 7466/10000 [=====================>........] - ETA: 19:57 - loss: 0.6782 - regression_loss: 0.5454 - classification_loss: 0.1328
 7467/10000 [=====================>........] - ETA: 19:56 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1329
 7468/10000 [=====================>........] - ETA: 19:56 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7469/10000 [=====================>........] - ETA: 19:55 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7470/10000 [=====================>........] - ETA: 19:55 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7471/10000 [=====================>........] - ETA: 19:54 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7472/10000 [=====================>........] - ETA: 19:54 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7473/10000 [=====================>........] - ETA: 19:53 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7474/10000 [=====================>........] - ETA: 19:53 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7475/10000 [=====================>........] - ETA: 19:53 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7476/10000 [=====================>........] - ETA: 19:52 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7477/10000 [=====================>........] - ETA: 19:52 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7478/10000 [=====================>........] - ETA: 19:51 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7479/10000 [=====================>........] - ETA: 19:51 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7480/10000 [=====================>........] - ETA: 19:50 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7481/10000 [=====================>........] - ETA: 19:50 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7482/10000 [=====================>........] - ETA: 19:49 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7483/10000 [=====================>........] - ETA: 19:49 - loss: 0.6785 - regression_loss: 0.5457 - classification_loss: 0.1329
 7484/10000 [=====================>........] - ETA: 19:48 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7485/10000 [=====================>........] - ETA: 19:48 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7486/10000 [=====================>........] - ETA: 19:47 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7487/10000 [=====================>........] - ETA: 19:47 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1329
 7488/10000 [=====================>........] - ETA: 19:46 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7489/10000 [=====================>........] - ETA: 19:46 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7490/10000 [=====================>........] - ETA: 19:45 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7491/10000 [=====================>........] - ETA: 19:45 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7492/10000 [=====================>........] - ETA: 19:44 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7493/10000 [=====================>........] - ETA: 19:44 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7494/10000 [=====================>........] - ETA: 19:43 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7495/10000 [=====================>........] - ETA: 19:43 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7496/10000 [=====================>........] - ETA: 19:42 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7497/10000 [=====================>........] - ETA: 19:42 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7498/10000 [=====================>........] - ETA: 19:41 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7499/10000 [=====================>........] - ETA: 19:41 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7500/10000 [=====================>........] - ETA: 19:40 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7501/10000 [=====================>........] - ETA: 19:40 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7502/10000 [=====================>........] - ETA: 19:40 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7503/10000 [=====================>........] - ETA: 19:39 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7504/10000 [=====================>........] - ETA: 19:39 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7505/10000 [=====================>........] - ETA: 19:38 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7506/10000 [=====================>........] - ETA: 19:38 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1330
 7507/10000 [=====================>........] - ETA: 19:37 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1330
 7508/10000 [=====================>........] - ETA: 19:37 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7509/10000 [=====================>........] - ETA: 19:36 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7510/10000 [=====================>........] - ETA: 19:36 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7511/10000 [=====================>........] - ETA: 19:35 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7512/10000 [=====================>........] - ETA: 19:35 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7513/10000 [=====================>........] - ETA: 19:34 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7514/10000 [=====================>........] - ETA: 19:34 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7515/10000 [=====================>........] - ETA: 19:33 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7516/10000 [=====================>........] - ETA: 19:33 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7517/10000 [=====================>........] - ETA: 19:32 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1329
 7518/10000 [=====================>........] - ETA: 19:32 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1330
 7519/10000 [=====================>........] - ETA: 19:31 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1329
 7520/10000 [=====================>........] - ETA: 19:31 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7521/10000 [=====================>........] - ETA: 19:30 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1330
 7522/10000 [=====================>........] - ETA: 19:30 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7523/10000 [=====================>........] - ETA: 19:29 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1330
 7524/10000 [=====================>........] - ETA: 19:29 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1330
 7525/10000 [=====================>........] - ETA: 19:29 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1330
 7526/10000 [=====================>........] - ETA: 19:28 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7527/10000 [=====================>........] - ETA: 19:28 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7528/10000 [=====================>........] - ETA: 19:27 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7529/10000 [=====================>........] - ETA: 19:27 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7530/10000 [=====================>........] - ETA: 19:26 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7531/10000 [=====================>........] - ETA: 19:26 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7532/10000 [=====================>........] - ETA: 19:25 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7533/10000 [=====================>........] - ETA: 19:25 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7534/10000 [=====================>........] - ETA: 19:24 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1330
 7535/10000 [=====================>........] - ETA: 19:24 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1330
 7536/10000 [=====================>........] - ETA: 19:23 - loss: 0.6786 - regression_loss: 0.5456 - classification_loss: 0.1330
 7537/10000 [=====================>........] - ETA: 19:23 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1330
 7538/10000 [=====================>........] - ETA: 19:22 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1330
 7539/10000 [=====================>........] - ETA: 19:22 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1330
 7540/10000 [=====================>........] - ETA: 19:21 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1330
 7541/10000 [=====================>........] - ETA: 19:21 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1330
 7542/10000 [=====================>........] - ETA: 19:20 - loss: 0.6786 - regression_loss: 0.5456 - classification_loss: 0.1329
 7543/10000 [=====================>........] - ETA: 19:20 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7544/10000 [=====================>........] - ETA: 19:20 - loss: 0.6785 - regression_loss: 0.5455 - classification_loss: 0.1329
 7545/10000 [=====================>........] - ETA: 19:19 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7546/10000 [=====================>........] - ETA: 19:19 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1329
 7547/10000 [=====================>........] - ETA: 19:18 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7548/10000 [=====================>........] - ETA: 19:18 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1329
 7549/10000 [=====================>........] - ETA: 19:17 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1329
 7550/10000 [=====================>........] - ETA: 19:17 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1329
 7551/10000 [=====================>........] - ETA: 19:16 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7552/10000 [=====================>........] - ETA: 19:16 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7553/10000 [=====================>........] - ETA: 19:15 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7554/10000 [=====================>........] - ETA: 19:15 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1329
 7555/10000 [=====================>........] - ETA: 19:14 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7556/10000 [=====================>........] - ETA: 19:14 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7557/10000 [=====================>........] - ETA: 19:13 - loss: 0.6785 - regression_loss: 0.5455 - classification_loss: 0.1329
 7558/10000 [=====================>........] - ETA: 19:13 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7559/10000 [=====================>........] - ETA: 19:12 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7560/10000 [=====================>........] - ETA: 19:12 - loss: 0.6785 - regression_loss: 0.5457 - classification_loss: 0.1329
 7561/10000 [=====================>........] - ETA: 19:11 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7562/10000 [=====================>........] - ETA: 19:11 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7563/10000 [=====================>........] - ETA: 19:10 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7564/10000 [=====================>........] - ETA: 19:10 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1329
 7565/10000 [=====================>........] - ETA: 19:09 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7566/10000 [=====================>........] - ETA: 19:09 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7567/10000 [=====================>........] - ETA: 19:09 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7568/10000 [=====================>........] - ETA: 19:08 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1330
 7569/10000 [=====================>........] - ETA: 19:08 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1329
 7570/10000 [=====================>........] - ETA: 19:07 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1329
 7571/10000 [=====================>........] - ETA: 19:07 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1330
 7572/10000 [=====================>........] - ETA: 19:06 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7573/10000 [=====================>........] - ETA: 19:06 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7574/10000 [=====================>........] - ETA: 19:05 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7575/10000 [=====================>........] - ETA: 19:05 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7576/10000 [=====================>........] - ETA: 19:04 - loss: 0.6786 - regression_loss: 0.5456 - classification_loss: 0.1329
 7577/10000 [=====================>........] - ETA: 19:04 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7578/10000 [=====================>........] - ETA: 19:03 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7579/10000 [=====================>........] - ETA: 19:03 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7580/10000 [=====================>........] - ETA: 19:02 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7581/10000 [=====================>........] - ETA: 19:02 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7582/10000 [=====================>........] - ETA: 19:01 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7583/10000 [=====================>........] - ETA: 19:01 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7584/10000 [=====================>........] - ETA: 19:00 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7585/10000 [=====================>........] - ETA: 19:00 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7586/10000 [=====================>........] - ETA: 18:59 - loss: 0.6786 - regression_loss: 0.5456 - classification_loss: 0.1329
 7587/10000 [=====================>........] - ETA: 18:59 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1330
 7588/10000 [=====================>........] - ETA: 18:58 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7589/10000 [=====================>........] - ETA: 18:58 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1330
 7590/10000 [=====================>........] - ETA: 18:58 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7591/10000 [=====================>........] - ETA: 18:57 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1329
 7592/10000 [=====================>........] - ETA: 18:57 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7593/10000 [=====================>........] - ETA: 18:56 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7594/10000 [=====================>........] - ETA: 18:56 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7595/10000 [=====================>........] - ETA: 18:55 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1329
 7596/10000 [=====================>........] - ETA: 18:55 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7597/10000 [=====================>........] - ETA: 18:54 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7598/10000 [=====================>........] - ETA: 18:54 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7599/10000 [=====================>........] - ETA: 18:53 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1329
 7600/10000 [=====================>........] - ETA: 18:53 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7601/10000 [=====================>........] - ETA: 18:52 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7602/10000 [=====================>........] - ETA: 18:52 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7603/10000 [=====================>........] - ETA: 18:51 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7604/10000 [=====================>........] - ETA: 18:51 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1329
 7605/10000 [=====================>........] - ETA: 18:50 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7606/10000 [=====================>........] - ETA: 18:50 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7607/10000 [=====================>........] - ETA: 18:49 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7608/10000 [=====================>........] - ETA: 18:49 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7609/10000 [=====================>........] - ETA: 18:48 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7610/10000 [=====================>........] - ETA: 18:48 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7611/10000 [=====================>........] - ETA: 18:48 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7612/10000 [=====================>........] - ETA: 18:47 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7613/10000 [=====================>........] - ETA: 18:47 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7614/10000 [=====================>........] - ETA: 18:46 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7615/10000 [=====================>........] - ETA: 18:46 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7616/10000 [=====================>........] - ETA: 18:45 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1329
 7617/10000 [=====================>........] - ETA: 18:45 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7618/10000 [=====================>........] - ETA: 18:44 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7619/10000 [=====================>........] - ETA: 18:44 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7620/10000 [=====================>........] - ETA: 18:43 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7621/10000 [=====================>........] - ETA: 18:43 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 7622/10000 [=====================>........] - ETA: 18:42 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1329
 7623/10000 [=====================>........] - ETA: 18:42 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 7624/10000 [=====================>........] - ETA: 18:41 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7625/10000 [=====================>........] - ETA: 18:41 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7626/10000 [=====================>........] - ETA: 18:40 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1329
 7627/10000 [=====================>........] - ETA: 18:40 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 7628/10000 [=====================>........] - ETA: 18:39 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 7629/10000 [=====================>........] - ETA: 18:39 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 7630/10000 [=====================>........] - ETA: 18:38 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 7631/10000 [=====================>........] - ETA: 18:38 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 7632/10000 [=====================>........] - ETA: 18:37 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 7633/10000 [=====================>........] - ETA: 18:37 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 7634/10000 [=====================>........] - ETA: 18:36 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1329
 7635/10000 [=====================>........] - ETA: 18:36 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 7636/10000 [=====================>........] - ETA: 18:36 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1329
 7637/10000 [=====================>........] - ETA: 18:35 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1329
 7638/10000 [=====================>........] - ETA: 18:35 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1329
 7639/10000 [=====================>........] - ETA: 18:34 - loss: 0.6792 - regression_loss: 0.5462 - classification_loss: 0.1330
 7640/10000 [=====================>........] - ETA: 18:34 - loss: 0.6792 - regression_loss: 0.5462 - classification_loss: 0.1330
 7641/10000 [=====================>........] - ETA: 18:33 - loss: 0.6792 - regression_loss: 0.5462 - classification_loss: 0.1329
 7642/10000 [=====================>........] - ETA: 18:33 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 7643/10000 [=====================>........] - ETA: 18:32 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1329
 7644/10000 [=====================>........] - ETA: 18:32 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 7645/10000 [=====================>........] - ETA: 18:31 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 7646/10000 [=====================>........] - ETA: 18:31 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 7647/10000 [=====================>........] - ETA: 18:30 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 7648/10000 [=====================>........] - ETA: 18:30 - loss: 0.6789 - regression_loss: 0.5461 - classification_loss: 0.1329
 7649/10000 [=====================>........] - ETA: 18:29 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 7650/10000 [=====================>........] - ETA: 18:29 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 7651/10000 [=====================>........] - ETA: 18:28 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 7652/10000 [=====================>........] - ETA: 18:28 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 7653/10000 [=====================>........] - ETA: 18:27 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 7654/10000 [=====================>........] - ETA: 18:27 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 7655/10000 [=====================>........] - ETA: 18:26 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 7656/10000 [=====================>........] - ETA: 18:26 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 7657/10000 [=====================>........] - ETA: 18:26 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 7658/10000 [=====================>........] - ETA: 18:25 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 7659/10000 [=====================>........] - ETA: 18:25 - loss: 0.6789 - regression_loss: 0.5460 - classification_loss: 0.1329
 7660/10000 [=====================>........] - ETA: 18:24 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 7661/10000 [=====================>........] - ETA: 18:24 - loss: 0.6792 - regression_loss: 0.5462 - classification_loss: 0.1329
 7662/10000 [=====================>........] - ETA: 18:23 - loss: 0.6792 - regression_loss: 0.5463 - classification_loss: 0.1329
 7663/10000 [=====================>........] - ETA: 18:23 - loss: 0.6793 - regression_loss: 0.5464 - classification_loss: 0.1330
 7664/10000 [=====================>........] - ETA: 18:22 - loss: 0.6793 - regression_loss: 0.5463 - classification_loss: 0.1329
 7665/10000 [=====================>........] - ETA: 18:22 - loss: 0.6792 - regression_loss: 0.5463 - classification_loss: 0.1329
 7666/10000 [=====================>........] - ETA: 18:21 - loss: 0.6793 - regression_loss: 0.5464 - classification_loss: 0.1330
 7667/10000 [======================>.......] - ETA: 18:21 - loss: 0.6794 - regression_loss: 0.5465 - classification_loss: 0.1330
 7668/10000 [======================>.......] - ETA: 18:20 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1330
 7669/10000 [======================>.......] - ETA: 18:20 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1330
 7670/10000 [======================>.......] - ETA: 18:19 - loss: 0.6794 - regression_loss: 0.5465 - classification_loss: 0.1330
 7671/10000 [======================>.......] - ETA: 18:19 - loss: 0.6793 - regression_loss: 0.5464 - classification_loss: 0.1329
 7672/10000 [======================>.......] - ETA: 18:18 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1329
 7673/10000 [======================>.......] - ETA: 18:18 - loss: 0.6795 - regression_loss: 0.5465 - classification_loss: 0.1330
 7674/10000 [======================>.......] - ETA: 18:17 - loss: 0.6794 - regression_loss: 0.5465 - classification_loss: 0.1330
 7675/10000 [======================>.......] - ETA: 18:17 - loss: 0.6794 - regression_loss: 0.5464 - classification_loss: 0.1329
 7676/10000 [======================>.......] - ETA: 18:17 - loss: 0.6793 - regression_loss: 0.5464 - classification_loss: 0.1329
 7677/10000 [======================>.......] - ETA: 18:16 - loss: 0.6792 - regression_loss: 0.5463 - classification_loss: 0.1329
 7678/10000 [======================>.......] - ETA: 18:16 - loss: 0.6792 - regression_loss: 0.5463 - classification_loss: 0.1329
 7679/10000 [======================>.......] - ETA: 18:15 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 7680/10000 [======================>.......] - ETA: 18:15 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 7681/10000 [======================>.......] - ETA: 18:14 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 7682/10000 [======================>.......] - ETA: 18:14 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 7683/10000 [======================>.......] - ETA: 18:13 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 7684/10000 [======================>.......] - ETA: 18:13 - loss: 0.6790 - regression_loss: 0.5462 - classification_loss: 0.1329
 7685/10000 [======================>.......] - ETA: 18:12 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 7686/10000 [======================>.......] - ETA: 18:12 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 7687/10000 [======================>.......] - ETA: 18:11 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 7688/10000 [======================>.......] - ETA: 18:11 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 7689/10000 [======================>.......] - ETA: 18:10 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 7690/10000 [======================>.......] - ETA: 18:10 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 7691/10000 [======================>.......] - ETA: 18:09 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 7692/10000 [======================>.......] - ETA: 18:09 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 7693/10000 [======================>.......] - ETA: 18:08 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 7694/10000 [======================>.......] - ETA: 18:08 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1330
 7695/10000 [======================>.......] - ETA: 18:07 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1329
 7696/10000 [======================>.......] - ETA: 18:07 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7697/10000 [======================>.......] - ETA: 18:07 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1330
 7698/10000 [======================>.......] - ETA: 18:06 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7699/10000 [======================>.......] - ETA: 18:06 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1330
 7700/10000 [======================>.......] - ETA: 18:05 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1329
 7701/10000 [======================>.......] - ETA: 18:05 - loss: 0.6792 - regression_loss: 0.5462 - classification_loss: 0.1330
 7702/10000 [======================>.......] - ETA: 18:04 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1330
 7703/10000 [======================>.......] - ETA: 18:04 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 7704/10000 [======================>.......] - ETA: 18:03 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1330
 7705/10000 [======================>.......] - ETA: 18:03 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7706/10000 [======================>.......] - ETA: 18:02 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7707/10000 [======================>.......] - ETA: 18:02 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7708/10000 [======================>.......] - ETA: 18:01 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7709/10000 [======================>.......] - ETA: 18:01 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 7710/10000 [======================>.......] - ETA: 18:00 - loss: 0.6791 - regression_loss: 0.5462 - classification_loss: 0.1330
 7711/10000 [======================>.......] - ETA: 18:00 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1329
 7712/10000 [======================>.......] - ETA: 17:59 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1329
 7713/10000 [======================>.......] - ETA: 17:59 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7714/10000 [======================>.......] - ETA: 17:58 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1330
 7715/10000 [======================>.......] - ETA: 17:58 - loss: 0.6790 - regression_loss: 0.5461 - classification_loss: 0.1330
 7716/10000 [======================>.......] - ETA: 17:58 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7717/10000 [======================>.......] - ETA: 17:57 - loss: 0.6792 - regression_loss: 0.5461 - classification_loss: 0.1330
 7718/10000 [======================>.......] - ETA: 17:57 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7719/10000 [======================>.......] - ETA: 17:56 - loss: 0.6791 - regression_loss: 0.5460 - classification_loss: 0.1330
 7720/10000 [======================>.......] - ETA: 17:56 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7721/10000 [======================>.......] - ETA: 17:55 - loss: 0.6791 - regression_loss: 0.5461 - classification_loss: 0.1330
 7722/10000 [======================>.......] - ETA: 17:55 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7723/10000 [======================>.......] - ETA: 17:54 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7724/10000 [======================>.......] - ETA: 17:54 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7725/10000 [======================>.......] - ETA: 17:53 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7726/10000 [======================>.......] - ETA: 17:53 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1330
 7727/10000 [======================>.......] - ETA: 17:52 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7728/10000 [======================>.......] - ETA: 17:52 - loss: 0.6788 - regression_loss: 0.5459 - classification_loss: 0.1330
 7729/10000 [======================>.......] - ETA: 17:51 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7730/10000 [======================>.......] - ETA: 17:51 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7731/10000 [======================>.......] - ETA: 17:50 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7732/10000 [======================>.......] - ETA: 17:50 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7733/10000 [======================>.......] - ETA: 17:49 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7734/10000 [======================>.......] - ETA: 17:49 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7735/10000 [======================>.......] - ETA: 17:48 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 7736/10000 [======================>.......] - ETA: 17:48 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1329
 7737/10000 [======================>.......] - ETA: 17:48 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1329
 7738/10000 [======================>.......] - ETA: 17:47 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7739/10000 [======================>.......] - ETA: 17:47 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7740/10000 [======================>.......] - ETA: 17:46 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7741/10000 [======================>.......] - ETA: 17:46 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7742/10000 [======================>.......] - ETA: 17:45 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7743/10000 [======================>.......] - ETA: 17:45 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7744/10000 [======================>.......] - ETA: 17:44 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7745/10000 [======================>.......] - ETA: 17:44 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7746/10000 [======================>.......] - ETA: 17:43 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1329
 7747/10000 [======================>.......] - ETA: 17:43 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7748/10000 [======================>.......] - ETA: 17:42 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7749/10000 [======================>.......] - ETA: 17:42 - loss: 0.6786 - regression_loss: 0.5456 - classification_loss: 0.1329
 7750/10000 [======================>.......] - ETA: 17:41 - loss: 0.6786 - regression_loss: 0.5456 - classification_loss: 0.1329
 7751/10000 [======================>.......] - ETA: 17:41 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7752/10000 [======================>.......] - ETA: 17:40 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1329
 7753/10000 [======================>.......] - ETA: 17:40 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7754/10000 [======================>.......] - ETA: 17:39 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7755/10000 [======================>.......] - ETA: 17:39 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7756/10000 [======================>.......] - ETA: 17:38 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7757/10000 [======================>.......] - ETA: 17:38 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7758/10000 [======================>.......] - ETA: 17:38 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7759/10000 [======================>.......] - ETA: 17:37 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7760/10000 [======================>.......] - ETA: 17:37 - loss: 0.6786 - regression_loss: 0.5457 - classification_loss: 0.1329
 7761/10000 [======================>.......] - ETA: 17:36 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7762/10000 [======================>.......] - ETA: 17:36 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7763/10000 [======================>.......] - ETA: 17:35 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7764/10000 [======================>.......] - ETA: 17:35 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7765/10000 [======================>.......] - ETA: 17:34 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1329
 7766/10000 [======================>.......] - ETA: 17:34 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7767/10000 [======================>.......] - ETA: 17:33 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7768/10000 [======================>.......] - ETA: 17:33 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1329
 7769/10000 [======================>.......] - ETA: 17:32 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1329
 7770/10000 [======================>.......] - ETA: 17:32 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7771/10000 [======================>.......] - ETA: 17:31 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7772/10000 [======================>.......] - ETA: 17:31 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7773/10000 [======================>.......] - ETA: 17:30 - loss: 0.6784 - regression_loss: 0.5456 - classification_loss: 0.1329
 7774/10000 [======================>.......] - ETA: 17:30 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7775/10000 [======================>.......] - ETA: 17:29 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7776/10000 [======================>.......] - ETA: 17:29 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1328
 7777/10000 [======================>.......] - ETA: 17:28 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1328
 7778/10000 [======================>.......] - ETA: 17:28 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1329
 7779/10000 [======================>.......] - ETA: 17:28 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1328
 7780/10000 [======================>.......] - ETA: 17:27 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1328
 7781/10000 [======================>.......] - ETA: 17:27 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1328
 7782/10000 [======================>.......] - ETA: 17:26 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1329
 7783/10000 [======================>.......] - ETA: 17:26 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1329
 7784/10000 [======================>.......] - ETA: 17:25 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1328
 7785/10000 [======================>.......] - ETA: 17:25 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1328
 7786/10000 [======================>.......] - ETA: 17:24 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7787/10000 [======================>.......] - ETA: 17:24 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1329
 7788/10000 [======================>.......] - ETA: 17:23 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1329
 7789/10000 [======================>.......] - ETA: 17:23 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1329
 7790/10000 [======================>.......] - ETA: 17:22 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1328
 7791/10000 [======================>.......] - ETA: 17:22 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1328
 7792/10000 [======================>.......] - ETA: 17:21 - loss: 0.6782 - regression_loss: 0.5454 - classification_loss: 0.1328
 7793/10000 [======================>.......] - ETA: 17:21 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1328
 7794/10000 [======================>.......] - ETA: 17:20 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1328
 7795/10000 [======================>.......] - ETA: 17:20 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1328
 7796/10000 [======================>.......] - ETA: 17:19 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1328
 7797/10000 [======================>.......] - ETA: 17:19 - loss: 0.6783 - regression_loss: 0.5455 - classification_loss: 0.1328
 7798/10000 [======================>.......] - ETA: 17:19 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1328
 7799/10000 [======================>.......] - ETA: 17:18 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7800/10000 [======================>.......] - ETA: 17:18 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7801/10000 [======================>.......] - ETA: 17:17 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7802/10000 [======================>.......] - ETA: 17:17 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7803/10000 [======================>.......] - ETA: 17:16 - loss: 0.6784 - regression_loss: 0.5455 - classification_loss: 0.1329
 7804/10000 [======================>.......] - ETA: 17:16 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7805/10000 [======================>.......] - ETA: 17:15 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7806/10000 [======================>.......] - ETA: 17:15 - loss: 0.6785 - regression_loss: 0.5456 - classification_loss: 0.1329
 7807/10000 [======================>.......] - ETA: 17:14 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1329
 7808/10000 [======================>.......] - ETA: 17:14 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7809/10000 [======================>.......] - ETA: 17:13 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1329
 7810/10000 [======================>.......] - ETA: 17:13 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1330
 7811/10000 [======================>.......] - ETA: 17:12 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1330
 7812/10000 [======================>.......] - ETA: 17:12 - loss: 0.6787 - regression_loss: 0.5458 - classification_loss: 0.1330
 7813/10000 [======================>.......] - ETA: 17:11 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7814/10000 [======================>.......] - ETA: 17:11 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7815/10000 [======================>.......] - ETA: 17:10 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7816/10000 [======================>.......] - ETA: 17:10 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7817/10000 [======================>.......] - ETA: 17:09 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7818/10000 [======================>.......] - ETA: 17:09 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7819/10000 [======================>.......] - ETA: 17:09 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7820/10000 [======================>.......] - ETA: 17:08 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7821/10000 [======================>.......] - ETA: 17:08 - loss: 0.6790 - regression_loss: 0.5460 - classification_loss: 0.1330
 7822/10000 [======================>.......] - ETA: 17:07 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7823/10000 [======================>.......] - ETA: 17:07 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7824/10000 [======================>.......] - ETA: 17:06 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1330
 7825/10000 [======================>.......] - ETA: 17:06 - loss: 0.6790 - regression_loss: 0.5458 - classification_loss: 0.1332
 7826/10000 [======================>.......] - ETA: 17:05 - loss: 0.6790 - regression_loss: 0.5458 - classification_loss: 0.1332
 7827/10000 [======================>.......] - ETA: 17:05 - loss: 0.6790 - regression_loss: 0.5458 - classification_loss: 0.1332
 7828/10000 [======================>.......] - ETA: 17:04 - loss: 0.6790 - regression_loss: 0.5458 - classification_loss: 0.1332
 7829/10000 [======================>.......] - ETA: 17:04 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 7830/10000 [======================>.......] - ETA: 17:03 - loss: 0.6789 - regression_loss: 0.5458 - classification_loss: 0.1332
 7831/10000 [======================>.......] - ETA: 17:03 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 7832/10000 [======================>.......] - ETA: 17:02 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 7833/10000 [======================>.......] - ETA: 17:02 - loss: 0.6788 - regression_loss: 0.5457 - classification_loss: 0.1332
 7834/10000 [======================>.......] - ETA: 17:01 - loss: 0.6788 - regression_loss: 0.5457 - classification_loss: 0.1332
 7835/10000 [======================>.......] - ETA: 17:01 - loss: 0.6788 - regression_loss: 0.5456 - classification_loss: 0.1332
 7836/10000 [======================>.......] - ETA: 17:00 - loss: 0.6787 - regression_loss: 0.5456 - classification_loss: 0.1331
 7837/10000 [======================>.......] - ETA: 17:00 - loss: 0.6788 - regression_loss: 0.5456 - classification_loss: 0.1332
 7838/10000 [======================>.......] - ETA: 16:59 - loss: 0.6787 - regression_loss: 0.5456 - classification_loss: 0.1331
 7839/10000 [======================>.......] - ETA: 16:59 - loss: 0.6787 - regression_loss: 0.5456 - classification_loss: 0.1332
 7840/10000 [======================>.......] - ETA: 16:59 - loss: 0.6787 - regression_loss: 0.5455 - classification_loss: 0.1332
 7841/10000 [======================>.......] - ETA: 16:58 - loss: 0.6786 - regression_loss: 0.5455 - classification_loss: 0.1332
 7842/10000 [======================>.......] - ETA: 16:58 - loss: 0.6786 - regression_loss: 0.5455 - classification_loss: 0.1332
 7843/10000 [======================>.......] - ETA: 16:57 - loss: 0.6786 - regression_loss: 0.5454 - classification_loss: 0.1332
 7844/10000 [======================>.......] - ETA: 16:57 - loss: 0.6786 - regression_loss: 0.5455 - classification_loss: 0.1331
 7845/10000 [======================>.......] - ETA: 16:56 - loss: 0.6786 - regression_loss: 0.5454 - classification_loss: 0.1331
 7846/10000 [======================>.......] - ETA: 16:56 - loss: 0.6786 - regression_loss: 0.5455 - classification_loss: 0.1332
 7847/10000 [======================>.......] - ETA: 16:55 - loss: 0.6786 - regression_loss: 0.5454 - classification_loss: 0.1332
 7848/10000 [======================>.......] - ETA: 16:55 - loss: 0.6786 - regression_loss: 0.5454 - classification_loss: 0.1332
 7849/10000 [======================>.......] - ETA: 16:54 - loss: 0.6786 - regression_loss: 0.5455 - classification_loss: 0.1332
 7850/10000 [======================>.......] - ETA: 16:54 - loss: 0.6786 - regression_loss: 0.5454 - classification_loss: 0.1331
 7851/10000 [======================>.......] - ETA: 16:53 - loss: 0.6786 - regression_loss: 0.5454 - classification_loss: 0.1332
 7852/10000 [======================>.......] - ETA: 16:53 - loss: 0.6786 - regression_loss: 0.5454 - classification_loss: 0.1332
 7853/10000 [======================>.......] - ETA: 16:52 - loss: 0.6785 - regression_loss: 0.5454 - classification_loss: 0.1331
 7854/10000 [======================>.......] - ETA: 16:52 - loss: 0.6785 - regression_loss: 0.5453 - classification_loss: 0.1331
 7855/10000 [======================>.......] - ETA: 16:51 - loss: 0.6785 - regression_loss: 0.5453 - classification_loss: 0.1332
 7856/10000 [======================>.......] - ETA: 16:51 - loss: 0.6784 - regression_loss: 0.5453 - classification_loss: 0.1331
 7857/10000 [======================>.......] - ETA: 16:50 - loss: 0.6784 - regression_loss: 0.5452 - classification_loss: 0.1331
 7858/10000 [======================>.......] - ETA: 16:50 - loss: 0.6784 - regression_loss: 0.5452 - classification_loss: 0.1331
 7859/10000 [======================>.......] - ETA: 16:49 - loss: 0.6783 - regression_loss: 0.5452 - classification_loss: 0.1331
 7860/10000 [======================>.......] - ETA: 16:49 - loss: 0.6783 - regression_loss: 0.5451 - classification_loss: 0.1331
 7861/10000 [======================>.......] - ETA: 16:49 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7862/10000 [======================>.......] - ETA: 16:48 - loss: 0.6783 - regression_loss: 0.5452 - classification_loss: 0.1331
 7863/10000 [======================>.......] - ETA: 16:48 - loss: 0.6783 - regression_loss: 0.5452 - classification_loss: 0.1331
 7864/10000 [======================>.......] - ETA: 16:47 - loss: 0.6784 - regression_loss: 0.5452 - classification_loss: 0.1331
 7865/10000 [======================>.......] - ETA: 16:47 - loss: 0.6783 - regression_loss: 0.5452 - classification_loss: 0.1331
 7866/10000 [======================>.......] - ETA: 16:46 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7867/10000 [======================>.......] - ETA: 16:46 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7868/10000 [======================>.......] - ETA: 16:45 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7869/10000 [======================>.......] - ETA: 16:45 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7870/10000 [======================>.......] - ETA: 16:44 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7871/10000 [======================>.......] - ETA: 16:44 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7872/10000 [======================>.......] - ETA: 16:43 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7873/10000 [======================>.......] - ETA: 16:43 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7874/10000 [======================>.......] - ETA: 16:42 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7875/10000 [======================>.......] - ETA: 16:42 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7876/10000 [======================>.......] - ETA: 16:41 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7877/10000 [======================>.......] - ETA: 16:41 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7878/10000 [======================>.......] - ETA: 16:40 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1331
 7879/10000 [======================>.......] - ETA: 16:40 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7880/10000 [======================>.......] - ETA: 16:39 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7881/10000 [======================>.......] - ETA: 16:39 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7882/10000 [======================>.......] - ETA: 16:39 - loss: 0.6783 - regression_loss: 0.5452 - classification_loss: 0.1331
 7883/10000 [======================>.......] - ETA: 16:38 - loss: 0.6783 - regression_loss: 0.5452 - classification_loss: 0.1331
 7884/10000 [======================>.......] - ETA: 16:38 - loss: 0.6783 - regression_loss: 0.5452 - classification_loss: 0.1331
 7885/10000 [======================>.......] - ETA: 16:37 - loss: 0.6783 - regression_loss: 0.5452 - classification_loss: 0.1331
 7886/10000 [======================>.......] - ETA: 16:37 - loss: 0.6783 - regression_loss: 0.5452 - classification_loss: 0.1331
 7887/10000 [======================>.......] - ETA: 16:36 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7888/10000 [======================>.......] - ETA: 16:36 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7889/10000 [======================>.......] - ETA: 16:35 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7890/10000 [======================>.......] - ETA: 16:35 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7891/10000 [======================>.......] - ETA: 16:34 - loss: 0.6781 - regression_loss: 0.5450 - classification_loss: 0.1331
 7892/10000 [======================>.......] - ETA: 16:34 - loss: 0.6781 - regression_loss: 0.5450 - classification_loss: 0.1331
 7893/10000 [======================>.......] - ETA: 16:33 - loss: 0.6781 - regression_loss: 0.5450 - classification_loss: 0.1331
 7894/10000 [======================>.......] - ETA: 16:33 - loss: 0.6781 - regression_loss: 0.5450 - classification_loss: 0.1331
 7895/10000 [======================>.......] - ETA: 16:32 - loss: 0.6781 - regression_loss: 0.5450 - classification_loss: 0.1331
 7896/10000 [======================>.......] - ETA: 16:32 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1331
 7897/10000 [======================>.......] - ETA: 16:31 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1331
 7898/10000 [======================>.......] - ETA: 16:31 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1331
 7899/10000 [======================>.......] - ETA: 16:30 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7900/10000 [======================>.......] - ETA: 16:30 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1331
 7901/10000 [======================>.......] - ETA: 16:29 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7902/10000 [======================>.......] - ETA: 16:29 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7903/10000 [======================>.......] - ETA: 16:29 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1331
 7904/10000 [======================>.......] - ETA: 16:28 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7905/10000 [======================>.......] - ETA: 16:28 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1331
 7906/10000 [======================>.......] - ETA: 16:27 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1331
 7907/10000 [======================>.......] - ETA: 16:27 - loss: 0.6781 - regression_loss: 0.5450 - classification_loss: 0.1331
 7908/10000 [======================>.......] - ETA: 16:26 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1331
 7909/10000 [======================>.......] - ETA: 16:26 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 7910/10000 [======================>.......] - ETA: 16:25 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 7911/10000 [======================>.......] - ETA: 16:25 - loss: 0.6781 - regression_loss: 0.5450 - classification_loss: 0.1330
 7912/10000 [======================>.......] - ETA: 16:24 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 7913/10000 [======================>.......] - ETA: 16:24 - loss: 0.6781 - regression_loss: 0.5450 - classification_loss: 0.1331
 7914/10000 [======================>.......] - ETA: 16:23 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1331
 7915/10000 [======================>.......] - ETA: 16:23 - loss: 0.6781 - regression_loss: 0.5450 - classification_loss: 0.1331
 7916/10000 [======================>.......] - ETA: 16:22 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 7917/10000 [======================>.......] - ETA: 16:22 - loss: 0.6781 - regression_loss: 0.5450 - classification_loss: 0.1331
 7918/10000 [======================>.......] - ETA: 16:21 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 7919/10000 [======================>.......] - ETA: 16:21 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7920/10000 [======================>.......] - ETA: 16:21 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7921/10000 [======================>.......] - ETA: 16:20 - loss: 0.6781 - regression_loss: 0.5450 - classification_loss: 0.1331
 7922/10000 [======================>.......] - ETA: 16:20 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1331
 7923/10000 [======================>.......] - ETA: 16:19 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7924/10000 [======================>.......] - ETA: 16:19 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1331
 7925/10000 [======================>.......] - ETA: 16:18 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7926/10000 [======================>.......] - ETA: 16:18 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1331
 7927/10000 [======================>.......] - ETA: 16:17 - loss: 0.6781 - regression_loss: 0.5450 - classification_loss: 0.1331
 7928/10000 [======================>.......] - ETA: 16:17 - loss: 0.6781 - regression_loss: 0.5450 - classification_loss: 0.1331
 7929/10000 [======================>.......] - ETA: 16:16 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1331
 7930/10000 [======================>.......] - ETA: 16:16 - loss: 0.6781 - regression_loss: 0.5450 - classification_loss: 0.1331
 7931/10000 [======================>.......] - ETA: 16:15 - loss: 0.6781 - regression_loss: 0.5450 - classification_loss: 0.1331
 7932/10000 [======================>.......] - ETA: 16:15 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7933/10000 [======================>.......] - ETA: 16:14 - loss: 0.6783 - regression_loss: 0.5452 - classification_loss: 0.1331
 7934/10000 [======================>.......] - ETA: 16:14 - loss: 0.6783 - regression_loss: 0.5452 - classification_loss: 0.1331
 7935/10000 [======================>.......] - ETA: 16:13 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7936/10000 [======================>.......] - ETA: 16:13 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7937/10000 [======================>.......] - ETA: 16:12 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7938/10000 [======================>.......] - ETA: 16:12 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7939/10000 [======================>.......] - ETA: 16:11 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7940/10000 [======================>.......] - ETA: 16:11 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1331
 7941/10000 [======================>.......] - ETA: 16:10 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1331
 7942/10000 [======================>.......] - ETA: 16:10 - loss: 0.6783 - regression_loss: 0.5452 - classification_loss: 0.1331
 7943/10000 [======================>.......] - ETA: 16:10 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1331
 7944/10000 [======================>.......] - ETA: 16:09 - loss: 0.6783 - regression_loss: 0.5452 - classification_loss: 0.1331
 7945/10000 [======================>.......] - ETA: 16:09 - loss: 0.6784 - regression_loss: 0.5453 - classification_loss: 0.1331
 7946/10000 [======================>.......] - ETA: 16:08 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1331
 7947/10000 [======================>.......] - ETA: 16:08 - loss: 0.6784 - regression_loss: 0.5453 - classification_loss: 0.1331
 7948/10000 [======================>.......] - ETA: 16:07 - loss: 0.6784 - regression_loss: 0.5453 - classification_loss: 0.1331
 7949/10000 [======================>.......] - ETA: 16:07 - loss: 0.6783 - regression_loss: 0.5452 - classification_loss: 0.1331
 7950/10000 [======================>.......] - ETA: 16:06 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1331
 7951/10000 [======================>.......] - ETA: 16:06 - loss: 0.6784 - regression_loss: 0.5453 - classification_loss: 0.1331
 7952/10000 [======================>.......] - ETA: 16:05 - loss: 0.6784 - regression_loss: 0.5453 - classification_loss: 0.1331
 7953/10000 [======================>.......] - ETA: 16:05 - loss: 0.6784 - regression_loss: 0.5453 - classification_loss: 0.1331
 7954/10000 [======================>.......] - ETA: 16:04 - loss: 0.6784 - regression_loss: 0.5453 - classification_loss: 0.1331
 7955/10000 [======================>.......] - ETA: 16:04 - loss: 0.6786 - regression_loss: 0.5455 - classification_loss: 0.1331
 7956/10000 [======================>.......] - ETA: 16:03 - loss: 0.6786 - regression_loss: 0.5454 - classification_loss: 0.1331
 7957/10000 [======================>.......] - ETA: 16:03 - loss: 0.6786 - regression_loss: 0.5454 - classification_loss: 0.1331
 7958/10000 [======================>.......] - ETA: 16:02 - loss: 0.6786 - regression_loss: 0.5454 - classification_loss: 0.1332
 7959/10000 [======================>.......] - ETA: 16:02 - loss: 0.6785 - regression_loss: 0.5454 - classification_loss: 0.1331
 7960/10000 [======================>.......] - ETA: 16:01 - loss: 0.6785 - regression_loss: 0.5454 - classification_loss: 0.1331
 7961/10000 [======================>.......] - ETA: 16:01 - loss: 0.6784 - regression_loss: 0.5453 - classification_loss: 0.1331
 7962/10000 [======================>.......] - ETA: 16:01 - loss: 0.6784 - regression_loss: 0.5453 - classification_loss: 0.1331
 7963/10000 [======================>.......] - ETA: 16:00 - loss: 0.6784 - regression_loss: 0.5453 - classification_loss: 0.1331
 7964/10000 [======================>.......] - ETA: 16:00 - loss: 0.6783 - regression_loss: 0.5452 - classification_loss: 0.1331
 7965/10000 [======================>.......] - ETA: 15:59 - loss: 0.6784 - regression_loss: 0.5452 - classification_loss: 0.1331
 7966/10000 [======================>.......] - ETA: 15:59 - loss: 0.6784 - regression_loss: 0.5452 - classification_loss: 0.1331
 7967/10000 [======================>.......] - ETA: 15:58 - loss: 0.6783 - regression_loss: 0.5452 - classification_loss: 0.1331
 7968/10000 [======================>.......] - ETA: 15:58 - loss: 0.6784 - regression_loss: 0.5452 - classification_loss: 0.1331
 7969/10000 [======================>.......] - ETA: 15:57 - loss: 0.6784 - regression_loss: 0.5453 - classification_loss: 0.1331
 7970/10000 [======================>.......] - ETA: 15:57 - loss: 0.6783 - regression_loss: 0.5452 - classification_loss: 0.1331
 7971/10000 [======================>.......] - ETA: 15:56 - loss: 0.6783 - regression_loss: 0.5452 - classification_loss: 0.1331
 7972/10000 [======================>.......] - ETA: 15:56 - loss: 0.6784 - regression_loss: 0.5453 - classification_loss: 0.1331
 7973/10000 [======================>.......] - ETA: 15:55 - loss: 0.6784 - regression_loss: 0.5453 - classification_loss: 0.1331
 7974/10000 [======================>.......] - ETA: 15:55 - loss: 0.6784 - regression_loss: 0.5452 - classification_loss: 0.1331
 7975/10000 [======================>.......] - ETA: 15:54 - loss: 0.6784 - regression_loss: 0.5452 - classification_loss: 0.1331
 7976/10000 [======================>.......] - ETA: 15:54 - loss: 0.6784 - regression_loss: 0.5453 - classification_loss: 0.1331
 7977/10000 [======================>.......] - ETA: 15:53 - loss: 0.6784 - regression_loss: 0.5453 - classification_loss: 0.1331
 7978/10000 [======================>.......] - ETA: 15:53 - loss: 0.6784 - regression_loss: 0.5453 - classification_loss: 0.1331
 7979/10000 [======================>.......] - ETA: 15:52 - loss: 0.6784 - regression_loss: 0.5452 - classification_loss: 0.1331
 7980/10000 [======================>.......] - ETA: 15:52 - loss: 0.6785 - regression_loss: 0.5453 - classification_loss: 0.1331
 7981/10000 [======================>.......] - ETA: 15:52 - loss: 0.6785 - regression_loss: 0.5453 - classification_loss: 0.1331
 7982/10000 [======================>.......] - ETA: 15:51 - loss: 0.6786 - regression_loss: 0.5454 - classification_loss: 0.1332
 7983/10000 [======================>.......] - ETA: 15:51 - loss: 0.6785 - regression_loss: 0.5454 - classification_loss: 0.1331
 7984/10000 [======================>.......] - ETA: 15:50 - loss: 0.6785 - regression_loss: 0.5454 - classification_loss: 0.1331
 7985/10000 [======================>.......] - ETA: 15:50 - loss: 0.6785 - regression_loss: 0.5454 - classification_loss: 0.1331
 7986/10000 [======================>.......] - ETA: 15:49 - loss: 0.6785 - regression_loss: 0.5454 - classification_loss: 0.1331
 7987/10000 [======================>.......] - ETA: 15:49 - loss: 0.6786 - regression_loss: 0.5455 - classification_loss: 0.1331
 7988/10000 [======================>.......] - ETA: 15:48 - loss: 0.6788 - regression_loss: 0.5456 - classification_loss: 0.1332
 7989/10000 [======================>.......] - ETA: 15:48 - loss: 0.6787 - regression_loss: 0.5456 - classification_loss: 0.1332
 7990/10000 [======================>.......] - ETA: 15:47 - loss: 0.6787 - regression_loss: 0.5455 - classification_loss: 0.1331
 7991/10000 [======================>.......] - ETA: 15:47 - loss: 0.6787 - regression_loss: 0.5456 - classification_loss: 0.1332
 7992/10000 [======================>.......] - ETA: 15:46 - loss: 0.6788 - regression_loss: 0.5456 - classification_loss: 0.1332
 7993/10000 [======================>.......] - ETA: 15:46 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 7994/10000 [======================>.......] - ETA: 15:45 - loss: 0.6788 - regression_loss: 0.5456 - classification_loss: 0.1332
 7995/10000 [======================>.......] - ETA: 15:45 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 7996/10000 [======================>.......] - ETA: 15:44 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 7997/10000 [======================>.......] - ETA: 15:44 - loss: 0.6788 - regression_loss: 0.5456 - classification_loss: 0.1332
 7998/10000 [======================>.......] - ETA: 15:43 - loss: 0.6787 - regression_loss: 0.5455 - classification_loss: 0.1332
 7999/10000 [======================>.......] - ETA: 15:43 - loss: 0.6787 - regression_loss: 0.5455 - classification_loss: 0.1332
 8000/10000 [=======================>......] - ETA: 15:42 - loss: 0.6787 - regression_loss: 0.5455 - classification_loss: 0.1332
 8001/10000 [=======================>......] - ETA: 15:42 - loss: 0.6787 - regression_loss: 0.5455 - classification_loss: 0.1332
 8002/10000 [=======================>......] - ETA: 15:42 - loss: 0.6786 - regression_loss: 0.5455 - classification_loss: 0.1332
 8003/10000 [=======================>......] - ETA: 15:41 - loss: 0.6786 - regression_loss: 0.5454 - classification_loss: 0.1332
 8004/10000 [=======================>......] - ETA: 15:41 - loss: 0.6785 - regression_loss: 0.5454 - classification_loss: 0.1332
 8005/10000 [=======================>......] - ETA: 15:40 - loss: 0.6785 - regression_loss: 0.5453 - classification_loss: 0.1332
 8006/10000 [=======================>......] - ETA: 15:40 - loss: 0.6786 - regression_loss: 0.5454 - classification_loss: 0.1332
 8007/10000 [=======================>......] - ETA: 15:39 - loss: 0.6785 - regression_loss: 0.5453 - classification_loss: 0.1332
 8008/10000 [=======================>......] - ETA: 15:39 - loss: 0.6785 - regression_loss: 0.5453 - classification_loss: 0.1333
 8009/10000 [=======================>......] - ETA: 15:38 - loss: 0.6785 - regression_loss: 0.5453 - classification_loss: 0.1332
 8010/10000 [=======================>......] - ETA: 15:38 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1332
 8011/10000 [=======================>......] - ETA: 15:37 - loss: 0.6784 - regression_loss: 0.5452 - classification_loss: 0.1333
 8012/10000 [=======================>......] - ETA: 15:37 - loss: 0.6784 - regression_loss: 0.5451 - classification_loss: 0.1332
 8013/10000 [=======================>......] - ETA: 15:36 - loss: 0.6783 - regression_loss: 0.5451 - classification_loss: 0.1332
 8014/10000 [=======================>......] - ETA: 15:36 - loss: 0.6783 - regression_loss: 0.5451 - classification_loss: 0.1332
 8015/10000 [=======================>......] - ETA: 15:35 - loss: 0.6782 - regression_loss: 0.5450 - classification_loss: 0.1332
 8016/10000 [=======================>......] - ETA: 15:35 - loss: 0.6782 - regression_loss: 0.5450 - classification_loss: 0.1332
 8017/10000 [=======================>......] - ETA: 15:34 - loss: 0.6782 - regression_loss: 0.5450 - classification_loss: 0.1332
 8018/10000 [=======================>......] - ETA: 15:34 - loss: 0.6782 - regression_loss: 0.5450 - classification_loss: 0.1332
 8019/10000 [=======================>......] - ETA: 15:33 - loss: 0.6782 - regression_loss: 0.5450 - classification_loss: 0.1332
 8020/10000 [=======================>......] - ETA: 15:33 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1332
 8021/10000 [=======================>......] - ETA: 15:33 - loss: 0.6782 - regression_loss: 0.5450 - classification_loss: 0.1332
 8022/10000 [=======================>......] - ETA: 15:32 - loss: 0.6782 - regression_loss: 0.5450 - classification_loss: 0.1332
 8023/10000 [=======================>......] - ETA: 15:32 - loss: 0.6783 - regression_loss: 0.5451 - classification_loss: 0.1332
 8024/10000 [=======================>......] - ETA: 15:31 - loss: 0.6783 - regression_loss: 0.5451 - classification_loss: 0.1332
 8025/10000 [=======================>......] - ETA: 15:31 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1332
 8026/10000 [=======================>......] - ETA: 15:30 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1332
 8027/10000 [=======================>......] - ETA: 15:30 - loss: 0.6785 - regression_loss: 0.5453 - classification_loss: 0.1332
 8028/10000 [=======================>......] - ETA: 15:29 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1332
 8029/10000 [=======================>......] - ETA: 15:29 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1332
 8030/10000 [=======================>......] - ETA: 15:28 - loss: 0.6786 - regression_loss: 0.5454 - classification_loss: 0.1333
 8031/10000 [=======================>......] - ETA: 15:28 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8032/10000 [=======================>......] - ETA: 15:27 - loss: 0.6787 - regression_loss: 0.5455 - classification_loss: 0.1333
 8033/10000 [=======================>......] - ETA: 15:27 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 8034/10000 [=======================>......] - ETA: 15:26 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 8035/10000 [=======================>......] - ETA: 15:26 - loss: 0.6787 - regression_loss: 0.5455 - classification_loss: 0.1333
 8036/10000 [=======================>......] - ETA: 15:25 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 8037/10000 [=======================>......] - ETA: 15:25 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 8038/10000 [=======================>......] - ETA: 15:24 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 8039/10000 [=======================>......] - ETA: 15:24 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 8040/10000 [=======================>......] - ETA: 15:23 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8041/10000 [=======================>......] - ETA: 15:23 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1333
 8042/10000 [=======================>......] - ETA: 15:23 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1333
 8043/10000 [=======================>......] - ETA: 15:22 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8044/10000 [=======================>......] - ETA: 15:22 - loss: 0.6791 - regression_loss: 0.5457 - classification_loss: 0.1333
 8045/10000 [=======================>......] - ETA: 15:21 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1333
 8046/10000 [=======================>......] - ETA: 15:21 - loss: 0.6791 - regression_loss: 0.5457 - classification_loss: 0.1333
 8047/10000 [=======================>......] - ETA: 15:20 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8048/10000 [=======================>......] - ETA: 15:20 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1334
 8049/10000 [=======================>......] - ETA: 15:19 - loss: 0.6791 - regression_loss: 0.5457 - classification_loss: 0.1334
 8050/10000 [=======================>......] - ETA: 15:19 - loss: 0.6791 - regression_loss: 0.5457 - classification_loss: 0.1334
 8051/10000 [=======================>......] - ETA: 15:18 - loss: 0.6791 - regression_loss: 0.5457 - classification_loss: 0.1334
 8052/10000 [=======================>......] - ETA: 15:18 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1334
 8053/10000 [=======================>......] - ETA: 15:17 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1334
 8054/10000 [=======================>......] - ETA: 15:17 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1334
 8055/10000 [=======================>......] - ETA: 15:16 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1334
 8056/10000 [=======================>......] - ETA: 15:16 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1334
 8057/10000 [=======================>......] - ETA: 15:15 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1334
 8058/10000 [=======================>......] - ETA: 15:15 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1334
 8059/10000 [=======================>......] - ETA: 15:14 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1334
 8060/10000 [=======================>......] - ETA: 15:14 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1334
 8061/10000 [=======================>......] - ETA: 15:14 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8062/10000 [=======================>......] - ETA: 15:13 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1334
 8063/10000 [=======================>......] - ETA: 15:13 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8064/10000 [=======================>......] - ETA: 15:12 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8065/10000 [=======================>......] - ETA: 15:12 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1334
 8066/10000 [=======================>......] - ETA: 15:11 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1333
 8067/10000 [=======================>......] - ETA: 15:11 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8068/10000 [=======================>......] - ETA: 15:10 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8069/10000 [=======================>......] - ETA: 15:10 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8070/10000 [=======================>......] - ETA: 15:09 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8071/10000 [=======================>......] - ETA: 15:09 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8072/10000 [=======================>......] - ETA: 15:08 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8073/10000 [=======================>......] - ETA: 15:08 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1334
 8074/10000 [=======================>......] - ETA: 15:07 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8075/10000 [=======================>......] - ETA: 15:07 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8076/10000 [=======================>......] - ETA: 15:06 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8077/10000 [=======================>......] - ETA: 15:06 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8078/10000 [=======================>......] - ETA: 15:05 - loss: 0.6789 - regression_loss: 0.5454 - classification_loss: 0.1334
 8079/10000 [=======================>......] - ETA: 15:05 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8080/10000 [=======================>......] - ETA: 15:04 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8081/10000 [=======================>......] - ETA: 15:04 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8082/10000 [=======================>......] - ETA: 15:04 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8083/10000 [=======================>......] - ETA: 15:03 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8084/10000 [=======================>......] - ETA: 15:03 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1334
 8085/10000 [=======================>......] - ETA: 15:02 - loss: 0.6791 - regression_loss: 0.5457 - classification_loss: 0.1334
 8086/10000 [=======================>......] - ETA: 15:02 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1334
 8087/10000 [=======================>......] - ETA: 15:01 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1334
 8088/10000 [=======================>......] - ETA: 15:01 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1334
 8089/10000 [=======================>......] - ETA: 15:00 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1334
 8090/10000 [=======================>......] - ETA: 15:00 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1334
 8091/10000 [=======================>......] - ETA: 14:59 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8092/10000 [=======================>......] - ETA: 14:59 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8093/10000 [=======================>......] - ETA: 14:58 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8094/10000 [=======================>......] - ETA: 14:58 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8095/10000 [=======================>......] - ETA: 14:57 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8096/10000 [=======================>......] - ETA: 14:57 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1334
 8097/10000 [=======================>......] - ETA: 14:56 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8098/10000 [=======================>......] - ETA: 14:56 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8099/10000 [=======================>......] - ETA: 14:55 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1334
 8100/10000 [=======================>......] - ETA: 14:55 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1334
 8101/10000 [=======================>......] - ETA: 14:54 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1334
 8102/10000 [=======================>......] - ETA: 14:54 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8103/10000 [=======================>......] - ETA: 14:54 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8104/10000 [=======================>......] - ETA: 14:53 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8105/10000 [=======================>......] - ETA: 14:53 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8106/10000 [=======================>......] - ETA: 14:52 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1333
 8107/10000 [=======================>......] - ETA: 14:52 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8108/10000 [=======================>......] - ETA: 14:51 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1333
 8109/10000 [=======================>......] - ETA: 14:51 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8110/10000 [=======================>......] - ETA: 14:50 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8111/10000 [=======================>......] - ETA: 14:50 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8112/10000 [=======================>......] - ETA: 14:49 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1333
 8113/10000 [=======================>......] - ETA: 14:49 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8114/10000 [=======================>......] - ETA: 14:48 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1333
 8115/10000 [=======================>......] - ETA: 14:48 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8116/10000 [=======================>......] - ETA: 14:47 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8117/10000 [=======================>......] - ETA: 14:47 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1333
 8118/10000 [=======================>......] - ETA: 14:46 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1333
 8119/10000 [=======================>......] - ETA: 14:46 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1333
 8120/10000 [=======================>......] - ETA: 14:45 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1333
 8121/10000 [=======================>......] - ETA: 14:45 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8122/10000 [=======================>......] - ETA: 14:45 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8123/10000 [=======================>......] - ETA: 14:44 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8124/10000 [=======================>......] - ETA: 14:44 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1334
 8125/10000 [=======================>......] - ETA: 14:43 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1334
 8126/10000 [=======================>......] - ETA: 14:43 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1334
 8127/10000 [=======================>......] - ETA: 14:42 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1334
 8128/10000 [=======================>......] - ETA: 14:42 - loss: 0.6791 - regression_loss: 0.5456 - classification_loss: 0.1334
 8129/10000 [=======================>......] - ETA: 14:41 - loss: 0.6791 - regression_loss: 0.5456 - classification_loss: 0.1334
 8130/10000 [=======================>......] - ETA: 14:41 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1334
 8131/10000 [=======================>......] - ETA: 14:40 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1334
 8132/10000 [=======================>......] - ETA: 14:40 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1334
 8133/10000 [=======================>......] - ETA: 14:39 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8134/10000 [=======================>......] - ETA: 14:39 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8135/10000 [=======================>......] - ETA: 14:38 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8136/10000 [=======================>......] - ETA: 14:38 - loss: 0.6790 - regression_loss: 0.5455 - classification_loss: 0.1334
 8137/10000 [=======================>......] - ETA: 14:37 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1334
 8138/10000 [=======================>......] - ETA: 14:37 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8139/10000 [=======================>......] - ETA: 14:36 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8140/10000 [=======================>......] - ETA: 14:36 - loss: 0.6790 - regression_loss: 0.5455 - classification_loss: 0.1334
 8141/10000 [=======================>......] - ETA: 14:36 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1334
 8142/10000 [=======================>......] - ETA: 14:35 - loss: 0.6790 - regression_loss: 0.5455 - classification_loss: 0.1334
 8143/10000 [=======================>......] - ETA: 14:35 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8144/10000 [=======================>......] - ETA: 14:34 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1334
 8145/10000 [=======================>......] - ETA: 14:34 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8146/10000 [=======================>......] - ETA: 14:33 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8147/10000 [=======================>......] - ETA: 14:33 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8148/10000 [=======================>......] - ETA: 14:32 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8149/10000 [=======================>......] - ETA: 14:32 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8150/10000 [=======================>......] - ETA: 14:31 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1334
 8151/10000 [=======================>......] - ETA: 14:31 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1334
 8152/10000 [=======================>......] - ETA: 14:30 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8153/10000 [=======================>......] - ETA: 14:30 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8154/10000 [=======================>......] - ETA: 14:29 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8155/10000 [=======================>......] - ETA: 14:29 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8156/10000 [=======================>......] - ETA: 14:28 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8157/10000 [=======================>......] - ETA: 14:28 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8158/10000 [=======================>......] - ETA: 14:27 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8159/10000 [=======================>......] - ETA: 14:27 - loss: 0.6787 - regression_loss: 0.5452 - classification_loss: 0.1334
 8160/10000 [=======================>......] - ETA: 14:27 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8161/10000 [=======================>......] - ETA: 14:26 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8162/10000 [=======================>......] - ETA: 14:26 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1334
 8163/10000 [=======================>......] - ETA: 14:25 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8164/10000 [=======================>......] - ETA: 14:25 - loss: 0.6787 - regression_loss: 0.5452 - classification_loss: 0.1334
 8165/10000 [=======================>......] - ETA: 14:24 - loss: 0.6787 - regression_loss: 0.5452 - classification_loss: 0.1334
 8166/10000 [=======================>......] - ETA: 14:24 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8167/10000 [=======================>......] - ETA: 14:23 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8168/10000 [=======================>......] - ETA: 14:23 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8169/10000 [=======================>......] - ETA: 14:22 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1335
 8170/10000 [=======================>......] - ETA: 14:22 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8171/10000 [=======================>......] - ETA: 14:21 - loss: 0.6787 - regression_loss: 0.5452 - classification_loss: 0.1334
 8172/10000 [=======================>......] - ETA: 14:21 - loss: 0.6787 - regression_loss: 0.5452 - classification_loss: 0.1334
 8173/10000 [=======================>......] - ETA: 14:20 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1335
 8174/10000 [=======================>......] - ETA: 14:20 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1335
 8175/10000 [=======================>......] - ETA: 14:19 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1335
 8176/10000 [=======================>......] - ETA: 14:19 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1335
 8177/10000 [=======================>......] - ETA: 14:18 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1335
 8178/10000 [=======================>......] - ETA: 14:18 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1335
 8179/10000 [=======================>......] - ETA: 14:18 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1335
 8180/10000 [=======================>......] - ETA: 14:17 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1335
 8181/10000 [=======================>......] - ETA: 14:17 - loss: 0.6787 - regression_loss: 0.5452 - classification_loss: 0.1335
 8182/10000 [=======================>......] - ETA: 14:16 - loss: 0.6787 - regression_loss: 0.5452 - classification_loss: 0.1335
 8183/10000 [=======================>......] - ETA: 14:16 - loss: 0.6787 - regression_loss: 0.5452 - classification_loss: 0.1335
 8184/10000 [=======================>......] - ETA: 14:15 - loss: 0.6787 - regression_loss: 0.5452 - classification_loss: 0.1335
 8185/10000 [=======================>......] - ETA: 14:15 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1335
 8186/10000 [=======================>......] - ETA: 14:14 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1335
 8187/10000 [=======================>......] - ETA: 14:14 - loss: 0.6789 - regression_loss: 0.5454 - classification_loss: 0.1335
 8188/10000 [=======================>......] - ETA: 14:13 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1335
 8189/10000 [=======================>......] - ETA: 14:13 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1335
 8190/10000 [=======================>......] - ETA: 14:12 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1335
 8191/10000 [=======================>......] - ETA: 14:12 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1335
 8192/10000 [=======================>......] - ETA: 14:11 - loss: 0.6787 - regression_loss: 0.5452 - classification_loss: 0.1335
 8193/10000 [=======================>......] - ETA: 14:11 - loss: 0.6787 - regression_loss: 0.5452 - classification_loss: 0.1335
 8194/10000 [=======================>......] - ETA: 14:10 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1335
 8195/10000 [=======================>......] - ETA: 14:10 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1335
 8196/10000 [=======================>......] - ETA: 14:09 - loss: 0.6786 - regression_loss: 0.5451 - classification_loss: 0.1335
 8197/10000 [=======================>......] - ETA: 14:09 - loss: 0.6787 - regression_loss: 0.5452 - classification_loss: 0.1335
 8198/10000 [=======================>......] - ETA: 14:09 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1335
 8199/10000 [=======================>......] - ETA: 14:08 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1335
 8200/10000 [=======================>......] - ETA: 14:08 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1335
 8201/10000 [=======================>......] - ETA: 14:07 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1335
 8202/10000 [=======================>......] - ETA: 14:07 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1335
 8203/10000 [=======================>......] - ETA: 14:06 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1334
 8204/10000 [=======================>......] - ETA: 14:06 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8205/10000 [=======================>......] - ETA: 14:05 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1335
 8206/10000 [=======================>......] - ETA: 14:05 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1335
 8207/10000 [=======================>......] - ETA: 14:04 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1335
 8208/10000 [=======================>......] - ETA: 14:04 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1335
 8209/10000 [=======================>......] - ETA: 14:03 - loss: 0.6787 - regression_loss: 0.5452 - classification_loss: 0.1335
 8210/10000 [=======================>......] - ETA: 14:03 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8211/10000 [=======================>......] - ETA: 14:02 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8212/10000 [=======================>......] - ETA: 14:02 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8213/10000 [=======================>......] - ETA: 14:01 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1335
 8214/10000 [=======================>......] - ETA: 14:01 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1335
 8215/10000 [=======================>......] - ETA: 14:00 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1335
 8216/10000 [=======================>......] - ETA: 14:00 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1335
 8217/10000 [=======================>......] - ETA: 14:00 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1335
 8218/10000 [=======================>......] - ETA: 13:59 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1335
 8219/10000 [=======================>......] - ETA: 13:59 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1335
 8220/10000 [=======================>......] - ETA: 13:58 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1335
 8221/10000 [=======================>......] - ETA: 13:58 - loss: 0.6787 - regression_loss: 0.5452 - classification_loss: 0.1334
 8222/10000 [=======================>......] - ETA: 13:57 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8223/10000 [=======================>......] - ETA: 13:57 - loss: 0.6787 - regression_loss: 0.5452 - classification_loss: 0.1335
 8224/10000 [=======================>......] - ETA: 13:56 - loss: 0.6787 - regression_loss: 0.5452 - classification_loss: 0.1335
 8225/10000 [=======================>......] - ETA: 13:56 - loss: 0.6787 - regression_loss: 0.5452 - classification_loss: 0.1335
 8226/10000 [=======================>......] - ETA: 13:55 - loss: 0.6787 - regression_loss: 0.5452 - classification_loss: 0.1335
 8227/10000 [=======================>......] - ETA: 13:55 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8228/10000 [=======================>......] - ETA: 13:54 - loss: 0.6786 - regression_loss: 0.5451 - classification_loss: 0.1334
 8229/10000 [=======================>......] - ETA: 13:54 - loss: 0.6787 - regression_loss: 0.5452 - classification_loss: 0.1335
 8230/10000 [=======================>......] - ETA: 13:53 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1335
 8231/10000 [=======================>......] - ETA: 13:53 - loss: 0.6786 - regression_loss: 0.5451 - classification_loss: 0.1335
 8232/10000 [=======================>......] - ETA: 13:52 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1335
 8233/10000 [=======================>......] - ETA: 13:52 - loss: 0.6786 - regression_loss: 0.5451 - classification_loss: 0.1335
 8234/10000 [=======================>......] - ETA: 13:51 - loss: 0.6787 - regression_loss: 0.5452 - classification_loss: 0.1335
 8235/10000 [=======================>......] - ETA: 13:51 - loss: 0.6787 - regression_loss: 0.5452 - classification_loss: 0.1335
 8236/10000 [=======================>......] - ETA: 13:50 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1335
 8237/10000 [=======================>......] - ETA: 13:50 - loss: 0.6787 - regression_loss: 0.5452 - classification_loss: 0.1335
 8238/10000 [=======================>......] - ETA: 13:50 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1335
 8239/10000 [=======================>......] - ETA: 13:49 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1335
 8240/10000 [=======================>......] - ETA: 13:49 - loss: 0.6789 - regression_loss: 0.5454 - classification_loss: 0.1335
 8241/10000 [=======================>......] - ETA: 13:48 - loss: 0.6789 - regression_loss: 0.5454 - classification_loss: 0.1335
 8242/10000 [=======================>......] - ETA: 13:48 - loss: 0.6790 - regression_loss: 0.5455 - classification_loss: 0.1335
 8243/10000 [=======================>......] - ETA: 13:47 - loss: 0.6791 - regression_loss: 0.5456 - classification_loss: 0.1335
 8244/10000 [=======================>......] - ETA: 13:47 - loss: 0.6792 - regression_loss: 0.5456 - classification_loss: 0.1336
 8245/10000 [=======================>......] - ETA: 13:46 - loss: 0.6792 - regression_loss: 0.5456 - classification_loss: 0.1336
 8246/10000 [=======================>......] - ETA: 13:46 - loss: 0.6791 - regression_loss: 0.5455 - classification_loss: 0.1336
 8247/10000 [=======================>......] - ETA: 13:45 - loss: 0.6792 - regression_loss: 0.5456 - classification_loss: 0.1336
 8248/10000 [=======================>......] - ETA: 13:45 - loss: 0.6791 - regression_loss: 0.5455 - classification_loss: 0.1336
 8249/10000 [=======================>......] - ETA: 13:44 - loss: 0.6791 - regression_loss: 0.5455 - classification_loss: 0.1336
 8250/10000 [=======================>......] - ETA: 13:44 - loss: 0.6791 - regression_loss: 0.5455 - classification_loss: 0.1336
 8251/10000 [=======================>......] - ETA: 13:43 - loss: 0.6791 - regression_loss: 0.5455 - classification_loss: 0.1336
 8252/10000 [=======================>......] - ETA: 13:43 - loss: 0.6793 - regression_loss: 0.5456 - classification_loss: 0.1336
 8253/10000 [=======================>......] - ETA: 13:42 - loss: 0.6792 - regression_loss: 0.5456 - classification_loss: 0.1336
 8254/10000 [=======================>......] - ETA: 13:42 - loss: 0.6792 - regression_loss: 0.5456 - classification_loss: 0.1336
 8255/10000 [=======================>......] - ETA: 13:41 - loss: 0.6792 - regression_loss: 0.5456 - classification_loss: 0.1336
 8256/10000 [=======================>......] - ETA: 13:41 - loss: 0.6792 - regression_loss: 0.5456 - classification_loss: 0.1336
 8257/10000 [=======================>......] - ETA: 13:41 - loss: 0.6792 - regression_loss: 0.5456 - classification_loss: 0.1336
 8258/10000 [=======================>......] - ETA: 13:40 - loss: 0.6792 - regression_loss: 0.5456 - classification_loss: 0.1336
 8259/10000 [=======================>......] - ETA: 13:40 - loss: 0.6793 - regression_loss: 0.5456 - classification_loss: 0.1336
 8260/10000 [=======================>......] - ETA: 13:39 - loss: 0.6792 - regression_loss: 0.5456 - classification_loss: 0.1336
 8261/10000 [=======================>......] - ETA: 13:39 - loss: 0.6792 - regression_loss: 0.5456 - classification_loss: 0.1336
 8262/10000 [=======================>......] - ETA: 13:38 - loss: 0.6793 - regression_loss: 0.5456 - classification_loss: 0.1336
 8263/10000 [=======================>......] - ETA: 13:38 - loss: 0.6792 - regression_loss: 0.5456 - classification_loss: 0.1336
 8264/10000 [=======================>......] - ETA: 13:37 - loss: 0.6793 - regression_loss: 0.5456 - classification_loss: 0.1336
 8265/10000 [=======================>......] - ETA: 13:37 - loss: 0.6793 - regression_loss: 0.5456 - classification_loss: 0.1336
 8266/10000 [=======================>......] - ETA: 13:36 - loss: 0.6793 - regression_loss: 0.5457 - classification_loss: 0.1337
 8267/10000 [=======================>......] - ETA: 13:36 - loss: 0.6794 - regression_loss: 0.5458 - classification_loss: 0.1337
 8268/10000 [=======================>......] - ETA: 13:35 - loss: 0.6794 - regression_loss: 0.5457 - classification_loss: 0.1336
 8269/10000 [=======================>......] - ETA: 13:35 - loss: 0.6794 - regression_loss: 0.5457 - classification_loss: 0.1336
 8270/10000 [=======================>......] - ETA: 13:34 - loss: 0.6794 - regression_loss: 0.5457 - classification_loss: 0.1337
 8271/10000 [=======================>......] - ETA: 13:34 - loss: 0.6795 - regression_loss: 0.5458 - classification_loss: 0.1337
 8272/10000 [=======================>......] - ETA: 13:33 - loss: 0.6795 - regression_loss: 0.5459 - classification_loss: 0.1336
 8273/10000 [=======================>......] - ETA: 13:33 - loss: 0.6796 - regression_loss: 0.5460 - classification_loss: 0.1337
 8274/10000 [=======================>......] - ETA: 13:32 - loss: 0.6796 - regression_loss: 0.5459 - classification_loss: 0.1336
 8275/10000 [=======================>......] - ETA: 13:32 - loss: 0.6795 - regression_loss: 0.5459 - classification_loss: 0.1336
 8276/10000 [=======================>......] - ETA: 13:31 - loss: 0.6795 - regression_loss: 0.5459 - classification_loss: 0.1336
 8277/10000 [=======================>......] - ETA: 13:31 - loss: 0.6794 - regression_loss: 0.5458 - classification_loss: 0.1336
 8278/10000 [=======================>......] - ETA: 13:31 - loss: 0.6794 - regression_loss: 0.5458 - classification_loss: 0.1336
 8279/10000 [=======================>......] - ETA: 13:30 - loss: 0.6794 - regression_loss: 0.5458 - classification_loss: 0.1336
 8280/10000 [=======================>......] - ETA: 13:30 - loss: 0.6795 - regression_loss: 0.5458 - classification_loss: 0.1336
 8281/10000 [=======================>......] - ETA: 13:29 - loss: 0.6795 - regression_loss: 0.5458 - classification_loss: 0.1336
 8282/10000 [=======================>......] - ETA: 13:29 - loss: 0.6795 - regression_loss: 0.5459 - classification_loss: 0.1336
 8283/10000 [=======================>......] - ETA: 13:28 - loss: 0.6796 - regression_loss: 0.5460 - classification_loss: 0.1336
 8284/10000 [=======================>......] - ETA: 13:28 - loss: 0.6797 - regression_loss: 0.5461 - classification_loss: 0.1337
 8285/10000 [=======================>......] - ETA: 13:27 - loss: 0.6797 - regression_loss: 0.5461 - classification_loss: 0.1337
 8286/10000 [=======================>......] - ETA: 13:27 - loss: 0.6797 - regression_loss: 0.5461 - classification_loss: 0.1336
 8287/10000 [=======================>......] - ETA: 13:26 - loss: 0.6797 - regression_loss: 0.5460 - classification_loss: 0.1336
 8288/10000 [=======================>......] - ETA: 13:26 - loss: 0.6796 - regression_loss: 0.5460 - classification_loss: 0.1336
 8289/10000 [=======================>......] - ETA: 13:25 - loss: 0.6795 - regression_loss: 0.5459 - classification_loss: 0.1336
 8290/10000 [=======================>......] - ETA: 13:25 - loss: 0.6795 - regression_loss: 0.5459 - classification_loss: 0.1336
 8291/10000 [=======================>......] - ETA: 13:24 - loss: 0.6795 - regression_loss: 0.5459 - classification_loss: 0.1336
 8292/10000 [=======================>......] - ETA: 13:24 - loss: 0.6795 - regression_loss: 0.5459 - classification_loss: 0.1336
 8293/10000 [=======================>......] - ETA: 13:23 - loss: 0.6794 - regression_loss: 0.5459 - classification_loss: 0.1336
 8294/10000 [=======================>......] - ETA: 13:23 - loss: 0.6795 - regression_loss: 0.5459 - classification_loss: 0.1336
 8295/10000 [=======================>......] - ETA: 13:23 - loss: 0.6795 - regression_loss: 0.5459 - classification_loss: 0.1336
 8296/10000 [=======================>......] - ETA: 13:22 - loss: 0.6795 - regression_loss: 0.5459 - classification_loss: 0.1336
 8297/10000 [=======================>......] - ETA: 13:22 - loss: 0.6795 - regression_loss: 0.5458 - classification_loss: 0.1336
 8298/10000 [=======================>......] - ETA: 13:21 - loss: 0.6795 - regression_loss: 0.5459 - classification_loss: 0.1336
 8299/10000 [=======================>......] - ETA: 13:21 - loss: 0.6795 - regression_loss: 0.5458 - classification_loss: 0.1336
 8300/10000 [=======================>......] - ETA: 13:20 - loss: 0.6795 - regression_loss: 0.5459 - classification_loss: 0.1336
 8301/10000 [=======================>......] - ETA: 13:20 - loss: 0.6795 - regression_loss: 0.5459 - classification_loss: 0.1336
 8302/10000 [=======================>......] - ETA: 13:19 - loss: 0.6795 - regression_loss: 0.5459 - classification_loss: 0.1336
 8303/10000 [=======================>......] - ETA: 13:19 - loss: 0.6795 - regression_loss: 0.5459 - classification_loss: 0.1336
 8304/10000 [=======================>......] - ETA: 13:18 - loss: 0.6796 - regression_loss: 0.5460 - classification_loss: 0.1336
 8305/10000 [=======================>......] - ETA: 13:18 - loss: 0.6795 - regression_loss: 0.5459 - classification_loss: 0.1336
 8306/10000 [=======================>......] - ETA: 13:17 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1336
 8307/10000 [=======================>......] - ETA: 13:17 - loss: 0.6795 - regression_loss: 0.5459 - classification_loss: 0.1336
 8308/10000 [=======================>......] - ETA: 13:16 - loss: 0.6795 - regression_loss: 0.5459 - classification_loss: 0.1336
 8309/10000 [=======================>......] - ETA: 13:16 - loss: 0.6794 - regression_loss: 0.5459 - classification_loss: 0.1336
 8310/10000 [=======================>......] - ETA: 13:15 - loss: 0.6794 - regression_loss: 0.5459 - classification_loss: 0.1336
 8311/10000 [=======================>......] - ETA: 13:15 - loss: 0.6794 - regression_loss: 0.5458 - classification_loss: 0.1336
 8312/10000 [=======================>......] - ETA: 13:14 - loss: 0.6793 - regression_loss: 0.5458 - classification_loss: 0.1335
 8313/10000 [=======================>......] - ETA: 13:14 - loss: 0.6794 - regression_loss: 0.5458 - classification_loss: 0.1336
 8314/10000 [=======================>......] - ETA: 13:14 - loss: 0.6793 - regression_loss: 0.5458 - classification_loss: 0.1336
 8315/10000 [=======================>......] - ETA: 13:13 - loss: 0.6793 - regression_loss: 0.5457 - classification_loss: 0.1336
 8316/10000 [=======================>......] - ETA: 13:13 - loss: 0.6794 - regression_loss: 0.5458 - classification_loss: 0.1336
 8317/10000 [=======================>......] - ETA: 13:12 - loss: 0.6794 - regression_loss: 0.5458 - classification_loss: 0.1336
 8318/10000 [=======================>......] - ETA: 13:12 - loss: 0.6794 - regression_loss: 0.5458 - classification_loss: 0.1336
 8319/10000 [=======================>......] - ETA: 13:11 - loss: 0.6794 - regression_loss: 0.5458 - classification_loss: 0.1335
 8320/10000 [=======================>......] - ETA: 13:11 - loss: 0.6793 - regression_loss: 0.5458 - classification_loss: 0.1335
 8321/10000 [=======================>......] - ETA: 13:10 - loss: 0.6794 - regression_loss: 0.5458 - classification_loss: 0.1336
 8322/10000 [=======================>......] - ETA: 13:10 - loss: 0.6794 - regression_loss: 0.5458 - classification_loss: 0.1336
 8323/10000 [=======================>......] - ETA: 13:09 - loss: 0.6793 - regression_loss: 0.5458 - classification_loss: 0.1335
 8324/10000 [=======================>......] - ETA: 13:09 - loss: 0.6793 - regression_loss: 0.5458 - classification_loss: 0.1335
 8325/10000 [=======================>......] - ETA: 13:08 - loss: 0.6793 - regression_loss: 0.5458 - classification_loss: 0.1335
 8326/10000 [=======================>......] - ETA: 13:08 - loss: 0.6793 - regression_loss: 0.5457 - classification_loss: 0.1336
 8327/10000 [=======================>......] - ETA: 13:07 - loss: 0.6793 - regression_loss: 0.5458 - classification_loss: 0.1335
 8328/10000 [=======================>......] - ETA: 13:07 - loss: 0.6793 - regression_loss: 0.5458 - classification_loss: 0.1335
 8329/10000 [=======================>......] - ETA: 13:06 - loss: 0.6793 - regression_loss: 0.5457 - classification_loss: 0.1335
 8330/10000 [=======================>......] - ETA: 13:06 - loss: 0.6793 - regression_loss: 0.5458 - classification_loss: 0.1335
 8331/10000 [=======================>......] - ETA: 13:06 - loss: 0.6794 - regression_loss: 0.5459 - classification_loss: 0.1335
 8332/10000 [=======================>......] - ETA: 13:05 - loss: 0.6794 - regression_loss: 0.5459 - classification_loss: 0.1335
 8333/10000 [=======================>......] - ETA: 13:05 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8334/10000 [========================>.....] - ETA: 13:04 - loss: 0.6796 - regression_loss: 0.5460 - classification_loss: 0.1335
 8335/10000 [========================>.....] - ETA: 13:04 - loss: 0.6796 - regression_loss: 0.5460 - classification_loss: 0.1335
 8336/10000 [========================>.....] - ETA: 13:03 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8337/10000 [========================>.....] - ETA: 13:03 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8338/10000 [========================>.....] - ETA: 13:02 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8339/10000 [========================>.....] - ETA: 13:02 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8340/10000 [========================>.....] - ETA: 13:01 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8341/10000 [========================>.....] - ETA: 13:01 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8342/10000 [========================>.....] - ETA: 13:00 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8343/10000 [========================>.....] - ETA: 13:00 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8344/10000 [========================>.....] - ETA: 12:59 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8345/10000 [========================>.....] - ETA: 12:59 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8346/10000 [========================>.....] - ETA: 12:58 - loss: 0.6795 - regression_loss: 0.5459 - classification_loss: 0.1335
 8347/10000 [========================>.....] - ETA: 12:58 - loss: 0.6794 - regression_loss: 0.5459 - classification_loss: 0.1335
 8348/10000 [========================>.....] - ETA: 12:57 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8349/10000 [========================>.....] - ETA: 12:57 - loss: 0.6796 - regression_loss: 0.5460 - classification_loss: 0.1335
 8350/10000 [========================>.....] - ETA: 12:57 - loss: 0.6796 - regression_loss: 0.5460 - classification_loss: 0.1335
 8351/10000 [========================>.....] - ETA: 12:56 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8352/10000 [========================>.....] - ETA: 12:56 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8353/10000 [========================>.....] - ETA: 12:55 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8354/10000 [========================>.....] - ETA: 12:55 - loss: 0.6796 - regression_loss: 0.5461 - classification_loss: 0.1335
 8355/10000 [========================>.....] - ETA: 12:54 - loss: 0.6796 - regression_loss: 0.5461 - classification_loss: 0.1335
 8356/10000 [========================>.....] - ETA: 12:54 - loss: 0.6795 - regression_loss: 0.5461 - classification_loss: 0.1335
 8357/10000 [========================>.....] - ETA: 12:53 - loss: 0.6796 - regression_loss: 0.5461 - classification_loss: 0.1335
 8358/10000 [========================>.....] - ETA: 12:53 - loss: 0.6796 - regression_loss: 0.5461 - classification_loss: 0.1335
 8359/10000 [========================>.....] - ETA: 12:52 - loss: 0.6797 - regression_loss: 0.5462 - classification_loss: 0.1335
 8360/10000 [========================>.....] - ETA: 12:52 - loss: 0.6797 - regression_loss: 0.5462 - classification_loss: 0.1335
 8361/10000 [========================>.....] - ETA: 12:51 - loss: 0.6797 - regression_loss: 0.5461 - classification_loss: 0.1335
 8362/10000 [========================>.....] - ETA: 12:51 - loss: 0.6797 - regression_loss: 0.5461 - classification_loss: 0.1335
 8363/10000 [========================>.....] - ETA: 12:50 - loss: 0.6797 - regression_loss: 0.5461 - classification_loss: 0.1336
 8364/10000 [========================>.....] - ETA: 12:50 - loss: 0.6797 - regression_loss: 0.5461 - classification_loss: 0.1336
 8365/10000 [========================>.....] - ETA: 12:49 - loss: 0.6798 - regression_loss: 0.5462 - classification_loss: 0.1336
 8366/10000 [========================>.....] - ETA: 12:49 - loss: 0.6797 - regression_loss: 0.5461 - classification_loss: 0.1336
 8367/10000 [========================>.....] - ETA: 12:48 - loss: 0.6797 - regression_loss: 0.5462 - classification_loss: 0.1336
 8368/10000 [========================>.....] - ETA: 12:48 - loss: 0.6798 - regression_loss: 0.5462 - classification_loss: 0.1336
 8369/10000 [========================>.....] - ETA: 12:47 - loss: 0.6798 - regression_loss: 0.5462 - classification_loss: 0.1336
 8370/10000 [========================>.....] - ETA: 12:47 - loss: 0.6797 - regression_loss: 0.5461 - classification_loss: 0.1335
 8371/10000 [========================>.....] - ETA: 12:47 - loss: 0.6797 - regression_loss: 0.5461 - classification_loss: 0.1335
 8372/10000 [========================>.....] - ETA: 12:46 - loss: 0.6796 - regression_loss: 0.5461 - classification_loss: 0.1335
 8373/10000 [========================>.....] - ETA: 12:46 - loss: 0.6797 - regression_loss: 0.5461 - classification_loss: 0.1335
 8374/10000 [========================>.....] - ETA: 12:45 - loss: 0.6797 - regression_loss: 0.5462 - classification_loss: 0.1335
 8375/10000 [========================>.....] - ETA: 12:45 - loss: 0.6797 - regression_loss: 0.5462 - classification_loss: 0.1335
 8376/10000 [========================>.....] - ETA: 12:44 - loss: 0.6798 - regression_loss: 0.5462 - classification_loss: 0.1336
 8377/10000 [========================>.....] - ETA: 12:44 - loss: 0.6797 - regression_loss: 0.5462 - classification_loss: 0.1336
 8378/10000 [========================>.....] - ETA: 12:43 - loss: 0.6797 - regression_loss: 0.5461 - classification_loss: 0.1335
 8379/10000 [========================>.....] - ETA: 12:43 - loss: 0.6798 - regression_loss: 0.5462 - classification_loss: 0.1335
 8380/10000 [========================>.....] - ETA: 12:42 - loss: 0.6797 - regression_loss: 0.5462 - classification_loss: 0.1335
 8381/10000 [========================>.....] - ETA: 12:42 - loss: 0.6797 - regression_loss: 0.5462 - classification_loss: 0.1335
 8382/10000 [========================>.....] - ETA: 12:41 - loss: 0.6797 - regression_loss: 0.5461 - classification_loss: 0.1335
 8383/10000 [========================>.....] - ETA: 12:41 - loss: 0.6797 - regression_loss: 0.5462 - classification_loss: 0.1335
 8384/10000 [========================>.....] - ETA: 12:40 - loss: 0.6797 - regression_loss: 0.5462 - classification_loss: 0.1335
 8385/10000 [========================>.....] - ETA: 12:40 - loss: 0.6797 - regression_loss: 0.5462 - classification_loss: 0.1335
 8386/10000 [========================>.....] - ETA: 12:39 - loss: 0.6797 - regression_loss: 0.5462 - classification_loss: 0.1335
 8387/10000 [========================>.....] - ETA: 12:39 - loss: 0.6797 - regression_loss: 0.5462 - classification_loss: 0.1335
 8388/10000 [========================>.....] - ETA: 12:39 - loss: 0.6797 - regression_loss: 0.5462 - classification_loss: 0.1335
 8389/10000 [========================>.....] - ETA: 12:38 - loss: 0.6798 - regression_loss: 0.5463 - classification_loss: 0.1335
 8390/10000 [========================>.....] - ETA: 12:38 - loss: 0.6798 - regression_loss: 0.5462 - classification_loss: 0.1335
 8391/10000 [========================>.....] - ETA: 12:37 - loss: 0.6797 - regression_loss: 0.5462 - classification_loss: 0.1335
 8392/10000 [========================>.....] - ETA: 12:37 - loss: 0.6797 - regression_loss: 0.5462 - classification_loss: 0.1335
 8393/10000 [========================>.....] - ETA: 12:36 - loss: 0.6796 - regression_loss: 0.5461 - classification_loss: 0.1335
 8394/10000 [========================>.....] - ETA: 12:36 - loss: 0.6796 - regression_loss: 0.5461 - classification_loss: 0.1335
 8395/10000 [========================>.....] - ETA: 12:35 - loss: 0.6796 - regression_loss: 0.5461 - classification_loss: 0.1335
 8396/10000 [========================>.....] - ETA: 12:35 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8397/10000 [========================>.....] - ETA: 12:34 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8398/10000 [========================>.....] - ETA: 12:34 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8399/10000 [========================>.....] - ETA: 12:33 - loss: 0.6794 - regression_loss: 0.5460 - classification_loss: 0.1335
 8400/10000 [========================>.....] - ETA: 12:33 - loss: 0.6794 - regression_loss: 0.5459 - classification_loss: 0.1335
 8401/10000 [========================>.....] - ETA: 12:32 - loss: 0.6794 - regression_loss: 0.5459 - classification_loss: 0.1335
 8402/10000 [========================>.....] - ETA: 12:32 - loss: 0.6794 - regression_loss: 0.5459 - classification_loss: 0.1335
 8403/10000 [========================>.....] - ETA: 12:31 - loss: 0.6794 - regression_loss: 0.5459 - classification_loss: 0.1335
 8404/10000 [========================>.....] - ETA: 12:31 - loss: 0.6794 - regression_loss: 0.5459 - classification_loss: 0.1335
 8405/10000 [========================>.....] - ETA: 12:30 - loss: 0.6794 - regression_loss: 0.5459 - classification_loss: 0.1335
 8406/10000 [========================>.....] - ETA: 12:30 - loss: 0.6794 - regression_loss: 0.5459 - classification_loss: 0.1335
 8407/10000 [========================>.....] - ETA: 12:30 - loss: 0.6794 - regression_loss: 0.5459 - classification_loss: 0.1335
 8408/10000 [========================>.....] - ETA: 12:29 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8409/10000 [========================>.....] - ETA: 12:29 - loss: 0.6794 - regression_loss: 0.5459 - classification_loss: 0.1335
 8410/10000 [========================>.....] - ETA: 12:28 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8411/10000 [========================>.....] - ETA: 12:28 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8412/10000 [========================>.....] - ETA: 12:27 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8413/10000 [========================>.....] - ETA: 12:27 - loss: 0.6794 - regression_loss: 0.5460 - classification_loss: 0.1335
 8414/10000 [========================>.....] - ETA: 12:26 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8415/10000 [========================>.....] - ETA: 12:26 - loss: 0.6794 - regression_loss: 0.5460 - classification_loss: 0.1335
 8416/10000 [========================>.....] - ETA: 12:25 - loss: 0.6794 - regression_loss: 0.5460 - classification_loss: 0.1335
 8417/10000 [========================>.....] - ETA: 12:25 - loss: 0.6794 - regression_loss: 0.5460 - classification_loss: 0.1335
 8418/10000 [========================>.....] - ETA: 12:24 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8419/10000 [========================>.....] - ETA: 12:24 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8420/10000 [========================>.....] - ETA: 12:23 - loss: 0.6794 - regression_loss: 0.5459 - classification_loss: 0.1335
 8421/10000 [========================>.....] - ETA: 12:23 - loss: 0.6794 - regression_loss: 0.5459 - classification_loss: 0.1335
 8422/10000 [========================>.....] - ETA: 12:22 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8423/10000 [========================>.....] - ETA: 12:22 - loss: 0.6796 - regression_loss: 0.5461 - classification_loss: 0.1335
 8424/10000 [========================>.....] - ETA: 12:21 - loss: 0.6796 - regression_loss: 0.5461 - classification_loss: 0.1335
 8425/10000 [========================>.....] - ETA: 12:21 - loss: 0.6795 - regression_loss: 0.5461 - classification_loss: 0.1335
 8426/10000 [========================>.....] - ETA: 12:21 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1335
 8427/10000 [========================>.....] - ETA: 12:20 - loss: 0.6795 - regression_loss: 0.5461 - classification_loss: 0.1335
 8428/10000 [========================>.....] - ETA: 12:20 - loss: 0.6796 - regression_loss: 0.5461 - classification_loss: 0.1335
 8429/10000 [========================>.....] - ETA: 12:19 - loss: 0.6797 - regression_loss: 0.5462 - classification_loss: 0.1335
 8430/10000 [========================>.....] - ETA: 12:19 - loss: 0.6797 - regression_loss: 0.5462 - classification_loss: 0.1335
 8431/10000 [========================>.....] - ETA: 12:18 - loss: 0.6798 - regression_loss: 0.5463 - classification_loss: 0.1335
 8432/10000 [========================>.....] - ETA: 12:18 - loss: 0.6798 - regression_loss: 0.5463 - classification_loss: 0.1335
 8433/10000 [========================>.....] - ETA: 12:17 - loss: 0.6798 - regression_loss: 0.5463 - classification_loss: 0.1335
 8434/10000 [========================>.....] - ETA: 12:17 - loss: 0.6798 - regression_loss: 0.5463 - classification_loss: 0.1335
 8435/10000 [========================>.....] - ETA: 12:16 - loss: 0.6798 - regression_loss: 0.5463 - classification_loss: 0.1335
 8436/10000 [========================>.....] - ETA: 12:16 - loss: 0.6797 - regression_loss: 0.5463 - classification_loss: 0.1335
 8437/10000 [========================>.....] - ETA: 12:15 - loss: 0.6797 - regression_loss: 0.5463 - classification_loss: 0.1335
 8438/10000 [========================>.....] - ETA: 12:15 - loss: 0.6797 - regression_loss: 0.5463 - classification_loss: 0.1335
 8439/10000 [========================>.....] - ETA: 12:14 - loss: 0.6797 - regression_loss: 0.5462 - classification_loss: 0.1335
 8440/10000 [========================>.....] - ETA: 12:14 - loss: 0.6797 - regression_loss: 0.5462 - classification_loss: 0.1335
 8441/10000 [========================>.....] - ETA: 12:13 - loss: 0.6797 - regression_loss: 0.5462 - classification_loss: 0.1335
 8442/10000 [========================>.....] - ETA: 12:13 - loss: 0.6796 - regression_loss: 0.5462 - classification_loss: 0.1334
 8443/10000 [========================>.....] - ETA: 12:13 - loss: 0.6796 - regression_loss: 0.5462 - classification_loss: 0.1334
 8444/10000 [========================>.....] - ETA: 12:12 - loss: 0.6796 - regression_loss: 0.5461 - classification_loss: 0.1334
 8445/10000 [========================>.....] - ETA: 12:12 - loss: 0.6796 - regression_loss: 0.5461 - classification_loss: 0.1334
 8446/10000 [========================>.....] - ETA: 12:11 - loss: 0.6795 - regression_loss: 0.5461 - classification_loss: 0.1334
 8447/10000 [========================>.....] - ETA: 12:11 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1334
 8448/10000 [========================>.....] - ETA: 12:10 - loss: 0.6794 - regression_loss: 0.5460 - classification_loss: 0.1334
 8449/10000 [========================>.....] - ETA: 12:10 - loss: 0.6794 - regression_loss: 0.5460 - classification_loss: 0.1334
 8450/10000 [========================>.....] - ETA: 12:09 - loss: 0.6794 - regression_loss: 0.5460 - classification_loss: 0.1334
 8451/10000 [========================>.....] - ETA: 12:09 - loss: 0.6794 - regression_loss: 0.5460 - classification_loss: 0.1334
 8452/10000 [========================>.....] - ETA: 12:08 - loss: 0.6794 - regression_loss: 0.5460 - classification_loss: 0.1334
 8453/10000 [========================>.....] - ETA: 12:08 - loss: 0.6793 - regression_loss: 0.5459 - classification_loss: 0.1334
 8454/10000 [========================>.....] - ETA: 12:07 - loss: 0.6793 - regression_loss: 0.5459 - classification_loss: 0.1334
 8455/10000 [========================>.....] - ETA: 12:07 - loss: 0.6793 - regression_loss: 0.5459 - classification_loss: 0.1334
 8456/10000 [========================>.....] - ETA: 12:06 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8457/10000 [========================>.....] - ETA: 12:06 - loss: 0.6793 - regression_loss: 0.5458 - classification_loss: 0.1334
 8458/10000 [========================>.....] - ETA: 12:05 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8459/10000 [========================>.....] - ETA: 12:05 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8460/10000 [========================>.....] - ETA: 12:04 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8461/10000 [========================>.....] - ETA: 12:04 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8462/10000 [========================>.....] - ETA: 12:04 - loss: 0.6793 - regression_loss: 0.5459 - classification_loss: 0.1334
 8463/10000 [========================>.....] - ETA: 12:03 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1334
 8464/10000 [========================>.....] - ETA: 12:03 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8465/10000 [========================>.....] - ETA: 12:02 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8466/10000 [========================>.....] - ETA: 12:02 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8467/10000 [========================>.....] - ETA: 12:01 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8468/10000 [========================>.....] - ETA: 12:01 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8469/10000 [========================>.....] - ETA: 12:00 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1334
 8470/10000 [========================>.....] - ETA: 12:00 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1333
 8471/10000 [========================>.....] - ETA: 11:59 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1334
 8472/10000 [========================>.....] - ETA: 11:59 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8473/10000 [========================>.....] - ETA: 11:58 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1333
 8474/10000 [========================>.....] - ETA: 11:58 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1333
 8475/10000 [========================>.....] - ETA: 11:57 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1333
 8476/10000 [========================>.....] - ETA: 11:57 - loss: 0.6793 - regression_loss: 0.5460 - classification_loss: 0.1334
 8477/10000 [========================>.....] - ETA: 11:56 - loss: 0.6793 - regression_loss: 0.5460 - classification_loss: 0.1334
 8478/10000 [========================>.....] - ETA: 11:56 - loss: 0.6794 - regression_loss: 0.5460 - classification_loss: 0.1334
 8479/10000 [========================>.....] - ETA: 11:55 - loss: 0.6794 - regression_loss: 0.5460 - classification_loss: 0.1334
 8480/10000 [========================>.....] - ETA: 11:55 - loss: 0.6794 - regression_loss: 0.5460 - classification_loss: 0.1334
 8481/10000 [========================>.....] - ETA: 11:55 - loss: 0.6793 - regression_loss: 0.5460 - classification_loss: 0.1334
 8482/10000 [========================>.....] - ETA: 11:54 - loss: 0.6794 - regression_loss: 0.5460 - classification_loss: 0.1334
 8483/10000 [========================>.....] - ETA: 11:54 - loss: 0.6794 - regression_loss: 0.5461 - classification_loss: 0.1334
 8484/10000 [========================>.....] - ETA: 11:53 - loss: 0.6795 - regression_loss: 0.5461 - classification_loss: 0.1334
 8485/10000 [========================>.....] - ETA: 11:53 - loss: 0.6795 - regression_loss: 0.5461 - classification_loss: 0.1334
 8486/10000 [========================>.....] - ETA: 11:52 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1334
 8487/10000 [========================>.....] - ETA: 11:52 - loss: 0.6794 - regression_loss: 0.5460 - classification_loss: 0.1334
 8488/10000 [========================>.....] - ETA: 11:51 - loss: 0.6794 - regression_loss: 0.5460 - classification_loss: 0.1334
 8489/10000 [========================>.....] - ETA: 11:51 - loss: 0.6795 - regression_loss: 0.5461 - classification_loss: 0.1334
 8490/10000 [========================>.....] - ETA: 11:50 - loss: 0.6795 - regression_loss: 0.5460 - classification_loss: 0.1334
 8491/10000 [========================>.....] - ETA: 11:50 - loss: 0.6795 - regression_loss: 0.5461 - classification_loss: 0.1334
 8492/10000 [========================>.....] - ETA: 11:49 - loss: 0.6795 - regression_loss: 0.5461 - classification_loss: 0.1334
 8493/10000 [========================>.....] - ETA: 11:49 - loss: 0.6795 - regression_loss: 0.5461 - classification_loss: 0.1334
 8494/10000 [========================>.....] - ETA: 11:48 - loss: 0.6794 - regression_loss: 0.5460 - classification_loss: 0.1334
 8495/10000 [========================>.....] - ETA: 11:48 - loss: 0.6794 - regression_loss: 0.5460 - classification_loss: 0.1334
 8496/10000 [========================>.....] - ETA: 11:47 - loss: 0.6796 - regression_loss: 0.5461 - classification_loss: 0.1334
 8497/10000 [========================>.....] - ETA: 11:47 - loss: 0.6795 - regression_loss: 0.5461 - classification_loss: 0.1334
 8498/10000 [========================>.....] - ETA: 11:47 - loss: 0.6795 - regression_loss: 0.5461 - classification_loss: 0.1334
 8499/10000 [========================>.....] - ETA: 11:46 - loss: 0.6795 - regression_loss: 0.5461 - classification_loss: 0.1334
 8500/10000 [========================>.....] - ETA: 11:46 - loss: 0.6794 - regression_loss: 0.5460 - classification_loss: 0.1334
 8501/10000 [========================>.....] - ETA: 11:45 - loss: 0.6794 - regression_loss: 0.5460 - classification_loss: 0.1334
 8502/10000 [========================>.....] - ETA: 11:45 - loss: 0.6794 - regression_loss: 0.5460 - classification_loss: 0.1334
 8503/10000 [========================>.....] - ETA: 11:44 - loss: 0.6793 - regression_loss: 0.5460 - classification_loss: 0.1334
 8504/10000 [========================>.....] - ETA: 11:44 - loss: 0.6793 - regression_loss: 0.5459 - classification_loss: 0.1334
 8505/10000 [========================>.....] - ETA: 11:43 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1333
 8506/10000 [========================>.....] - ETA: 11:43 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1333
 8507/10000 [========================>.....] - ETA: 11:42 - loss: 0.6793 - regression_loss: 0.5459 - classification_loss: 0.1333
 8508/10000 [========================>.....] - ETA: 11:42 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1333
 8509/10000 [========================>.....] - ETA: 11:41 - loss: 0.6793 - regression_loss: 0.5459 - classification_loss: 0.1333
 8510/10000 [========================>.....] - ETA: 11:41 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1333
 8511/10000 [========================>.....] - ETA: 11:40 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1333
 8512/10000 [========================>.....] - ETA: 11:40 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1333
 8513/10000 [========================>.....] - ETA: 11:39 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1333
 8514/10000 [========================>.....] - ETA: 11:39 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1333
 8515/10000 [========================>.....] - ETA: 11:38 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1333
 8516/10000 [========================>.....] - ETA: 11:38 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1333
 8517/10000 [========================>.....] - ETA: 11:38 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1333
 8518/10000 [========================>.....] - ETA: 11:37 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8519/10000 [========================>.....] - ETA: 11:37 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8520/10000 [========================>.....] - ETA: 11:36 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1334
 8521/10000 [========================>.....] - ETA: 11:36 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1334
 8522/10000 [========================>.....] - ETA: 11:35 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1334
 8523/10000 [========================>.....] - ETA: 11:35 - loss: 0.6791 - regression_loss: 0.5457 - classification_loss: 0.1333
 8524/10000 [========================>.....] - ETA: 11:34 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8525/10000 [========================>.....] - ETA: 11:34 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8526/10000 [========================>.....] - ETA: 11:33 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8527/10000 [========================>.....] - ETA: 11:33 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8528/10000 [========================>.....] - ETA: 11:32 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8529/10000 [========================>.....] - ETA: 11:32 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1333
 8530/10000 [========================>.....] - ETA: 11:31 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1333
 8531/10000 [========================>.....] - ETA: 11:31 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1333
 8532/10000 [========================>.....] - ETA: 11:30 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1333
 8533/10000 [========================>.....] - ETA: 11:30 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1333
 8534/10000 [========================>.....] - ETA: 11:29 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8535/10000 [========================>.....] - ETA: 11:29 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8536/10000 [========================>.....] - ETA: 11:29 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1333
 8537/10000 [========================>.....] - ETA: 11:28 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1333
 8538/10000 [========================>.....] - ETA: 11:28 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1333
 8539/10000 [========================>.....] - ETA: 11:27 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1333
 8540/10000 [========================>.....] - ETA: 11:27 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 8541/10000 [========================>.....] - ETA: 11:26 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 8542/10000 [========================>.....] - ETA: 11:26 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8543/10000 [========================>.....] - ETA: 11:25 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8544/10000 [========================>.....] - ETA: 11:25 - loss: 0.6786 - regression_loss: 0.5454 - classification_loss: 0.1333
 8545/10000 [========================>.....] - ETA: 11:24 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8546/10000 [========================>.....] - ETA: 11:24 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8547/10000 [========================>.....] - ETA: 11:23 - loss: 0.6786 - regression_loss: 0.5454 - classification_loss: 0.1333
 8548/10000 [========================>.....] - ETA: 11:23 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8549/10000 [========================>.....] - ETA: 11:22 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8550/10000 [========================>.....] - ETA: 11:22 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8551/10000 [========================>.....] - ETA: 11:21 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8552/10000 [========================>.....] - ETA: 11:21 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8553/10000 [========================>.....] - ETA: 11:20 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8554/10000 [========================>.....] - ETA: 11:20 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 8555/10000 [========================>.....] - ETA: 11:20 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8556/10000 [========================>.....] - ETA: 11:19 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8557/10000 [========================>.....] - ETA: 11:19 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8558/10000 [========================>.....] - ETA: 11:18 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8559/10000 [========================>.....] - ETA: 11:18 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8560/10000 [========================>.....] - ETA: 11:17 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8561/10000 [========================>.....] - ETA: 11:17 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1333
 8562/10000 [========================>.....] - ETA: 11:16 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 8563/10000 [========================>.....] - ETA: 11:16 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 8564/10000 [========================>.....] - ETA: 11:15 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 8565/10000 [========================>.....] - ETA: 11:15 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1334
 8566/10000 [========================>.....] - ETA: 11:14 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8567/10000 [========================>.....] - ETA: 11:14 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8568/10000 [========================>.....] - ETA: 11:13 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1334
 8569/10000 [========================>.....] - ETA: 11:13 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1334
 8570/10000 [========================>.....] - ETA: 11:12 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1334
 8571/10000 [========================>.....] - ETA: 11:12 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1334
 8572/10000 [========================>.....] - ETA: 11:11 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8573/10000 [========================>.....] - ETA: 11:11 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8574/10000 [========================>.....] - ETA: 11:11 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8575/10000 [========================>.....] - ETA: 11:10 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8576/10000 [========================>.....] - ETA: 11:10 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8577/10000 [========================>.....] - ETA: 11:09 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1333
 8578/10000 [========================>.....] - ETA: 11:09 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 8579/10000 [========================>.....] - ETA: 11:08 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1333
 8580/10000 [========================>.....] - ETA: 11:08 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8581/10000 [========================>.....] - ETA: 11:07 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8582/10000 [========================>.....] - ETA: 11:07 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8583/10000 [========================>.....] - ETA: 11:06 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8584/10000 [========================>.....] - ETA: 11:06 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8585/10000 [========================>.....] - ETA: 11:05 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8586/10000 [========================>.....] - ETA: 11:05 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8587/10000 [========================>.....] - ETA: 11:04 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8588/10000 [========================>.....] - ETA: 11:04 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1333
 8589/10000 [========================>.....] - ETA: 11:03 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1333
 8590/10000 [========================>.....] - ETA: 11:03 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8591/10000 [========================>.....] - ETA: 11:02 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8592/10000 [========================>.....] - ETA: 11:02 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8593/10000 [========================>.....] - ETA: 11:02 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1334
 8594/10000 [========================>.....] - ETA: 11:01 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1334
 8595/10000 [========================>.....] - ETA: 11:01 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1334
 8596/10000 [========================>.....] - ETA: 11:00 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8597/10000 [========================>.....] - ETA: 11:00 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8598/10000 [========================>.....] - ETA: 10:59 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8599/10000 [========================>.....] - ETA: 10:59 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8600/10000 [========================>.....] - ETA: 10:58 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8601/10000 [========================>.....] - ETA: 10:58 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8602/10000 [========================>.....] - ETA: 10:57 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8603/10000 [========================>.....] - ETA: 10:57 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8604/10000 [========================>.....] - ETA: 10:56 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8605/10000 [========================>.....] - ETA: 10:56 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8606/10000 [========================>.....] - ETA: 10:55 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1334
 8607/10000 [========================>.....] - ETA: 10:55 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1333
 8608/10000 [========================>.....] - ETA: 10:54 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8609/10000 [========================>.....] - ETA: 10:54 - loss: 0.6785 - regression_loss: 0.5451 - classification_loss: 0.1333
 8610/10000 [========================>.....] - ETA: 10:54 - loss: 0.6784 - regression_loss: 0.5451 - classification_loss: 0.1333
 8611/10000 [========================>.....] - ETA: 10:53 - loss: 0.6784 - regression_loss: 0.5451 - classification_loss: 0.1333
 8612/10000 [========================>.....] - ETA: 10:53 - loss: 0.6784 - regression_loss: 0.5451 - classification_loss: 0.1333
 8613/10000 [========================>.....] - ETA: 10:52 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8614/10000 [========================>.....] - ETA: 10:52 - loss: 0.6784 - regression_loss: 0.5451 - classification_loss: 0.1333
 8615/10000 [========================>.....] - ETA: 10:51 - loss: 0.6784 - regression_loss: 0.5451 - classification_loss: 0.1333
 8616/10000 [========================>.....] - ETA: 10:51 - loss: 0.6784 - regression_loss: 0.5451 - classification_loss: 0.1333
 8617/10000 [========================>.....] - ETA: 10:50 - loss: 0.6784 - regression_loss: 0.5451 - classification_loss: 0.1333
 8618/10000 [========================>.....] - ETA: 10:50 - loss: 0.6783 - regression_loss: 0.5450 - classification_loss: 0.1333
 8619/10000 [========================>.....] - ETA: 10:49 - loss: 0.6784 - regression_loss: 0.5451 - classification_loss: 0.1333
 8620/10000 [========================>.....] - ETA: 10:49 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8621/10000 [========================>.....] - ETA: 10:48 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1333
 8622/10000 [========================>.....] - ETA: 10:48 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8623/10000 [========================>.....] - ETA: 10:47 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8624/10000 [========================>.....] - ETA: 10:47 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8625/10000 [========================>.....] - ETA: 10:46 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8626/10000 [========================>.....] - ETA: 10:46 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8627/10000 [========================>.....] - ETA: 10:45 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8628/10000 [========================>.....] - ETA: 10:45 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8629/10000 [========================>.....] - ETA: 10:45 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8630/10000 [========================>.....] - ETA: 10:44 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8631/10000 [========================>.....] - ETA: 10:44 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8632/10000 [========================>.....] - ETA: 10:43 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8633/10000 [========================>.....] - ETA: 10:43 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8634/10000 [========================>.....] - ETA: 10:42 - loss: 0.6784 - regression_loss: 0.5451 - classification_loss: 0.1333
 8635/10000 [========================>.....] - ETA: 10:42 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8636/10000 [========================>.....] - ETA: 10:41 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1334
 8637/10000 [========================>.....] - ETA: 10:41 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8638/10000 [========================>.....] - ETA: 10:40 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8639/10000 [========================>.....] - ETA: 10:40 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8640/10000 [========================>.....] - ETA: 10:39 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8641/10000 [========================>.....] - ETA: 10:39 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8642/10000 [========================>.....] - ETA: 10:38 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8643/10000 [========================>.....] - ETA: 10:38 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8644/10000 [========================>.....] - ETA: 10:37 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8645/10000 [========================>.....] - ETA: 10:37 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8646/10000 [========================>.....] - ETA: 10:36 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8647/10000 [========================>.....] - ETA: 10:36 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8648/10000 [========================>.....] - ETA: 10:36 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8649/10000 [========================>.....] - ETA: 10:35 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8650/10000 [========================>.....] - ETA: 10:35 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8651/10000 [========================>.....] - ETA: 10:34 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8652/10000 [========================>.....] - ETA: 10:34 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8653/10000 [========================>.....] - ETA: 10:33 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8654/10000 [========================>.....] - ETA: 10:33 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8655/10000 [========================>.....] - ETA: 10:32 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8656/10000 [========================>.....] - ETA: 10:32 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8657/10000 [========================>.....] - ETA: 10:31 - loss: 0.6789 - regression_loss: 0.5454 - classification_loss: 0.1334
 8658/10000 [========================>.....] - ETA: 10:31 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8659/10000 [========================>.....] - ETA: 10:30 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8660/10000 [========================>.....] - ETA: 10:30 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8661/10000 [========================>.....] - ETA: 10:29 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8662/10000 [========================>.....] - ETA: 10:29 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8663/10000 [========================>.....] - ETA: 10:28 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8664/10000 [========================>.....] - ETA: 10:28 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8665/10000 [========================>.....] - ETA: 10:28 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8666/10000 [========================>.....] - ETA: 10:27 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8667/10000 [=========================>....] - ETA: 10:27 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8668/10000 [=========================>....] - ETA: 10:26 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8669/10000 [=========================>....] - ETA: 10:26 - loss: 0.6789 - regression_loss: 0.5454 - classification_loss: 0.1334
 8670/10000 [=========================>....] - ETA: 10:25 - loss: 0.6789 - regression_loss: 0.5454 - classification_loss: 0.1334
 8671/10000 [=========================>....] - ETA: 10:25 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8672/10000 [=========================>....] - ETA: 10:24 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8673/10000 [=========================>....] - ETA: 10:24 - loss: 0.6789 - regression_loss: 0.5454 - classification_loss: 0.1334
 8674/10000 [=========================>....] - ETA: 10:23 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8675/10000 [=========================>....] - ETA: 10:23 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8676/10000 [=========================>....] - ETA: 10:22 - loss: 0.6789 - regression_loss: 0.5454 - classification_loss: 0.1334
 8677/10000 [=========================>....] - ETA: 10:22 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8678/10000 [=========================>....] - ETA: 10:21 - loss: 0.6789 - regression_loss: 0.5454 - classification_loss: 0.1334
 8679/10000 [=========================>....] - ETA: 10:21 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8680/10000 [=========================>....] - ETA: 10:20 - loss: 0.6790 - regression_loss: 0.5455 - classification_loss: 0.1334
 8681/10000 [=========================>....] - ETA: 10:20 - loss: 0.6790 - regression_loss: 0.5455 - classification_loss: 0.1334
 8682/10000 [=========================>....] - ETA: 10:19 - loss: 0.6790 - regression_loss: 0.5455 - classification_loss: 0.1334
 8683/10000 [=========================>....] - ETA: 10:19 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1335
 8684/10000 [=========================>....] - ETA: 10:19 - loss: 0.6790 - regression_loss: 0.5455 - classification_loss: 0.1335
 8685/10000 [=========================>....] - ETA: 10:18 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1335
 8686/10000 [=========================>....] - ETA: 10:18 - loss: 0.6790 - regression_loss: 0.5455 - classification_loss: 0.1335
 8687/10000 [=========================>....] - ETA: 10:17 - loss: 0.6791 - regression_loss: 0.5456 - classification_loss: 0.1335
 8688/10000 [=========================>....] - ETA: 10:17 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1335
 8689/10000 [=========================>....] - ETA: 10:16 - loss: 0.6791 - regression_loss: 0.5456 - classification_loss: 0.1335
 8690/10000 [=========================>....] - ETA: 10:16 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1335
 8691/10000 [=========================>....] - ETA: 10:15 - loss: 0.6790 - regression_loss: 0.5455 - classification_loss: 0.1335
 8692/10000 [=========================>....] - ETA: 10:15 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1335
 8693/10000 [=========================>....] - ETA: 10:14 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8694/10000 [=========================>....] - ETA: 10:14 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1334
 8695/10000 [=========================>....] - ETA: 10:13 - loss: 0.6789 - regression_loss: 0.5454 - classification_loss: 0.1334
 8696/10000 [=========================>....] - ETA: 10:13 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8697/10000 [=========================>....] - ETA: 10:12 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1335
 8698/10000 [=========================>....] - ETA: 10:12 - loss: 0.6789 - regression_loss: 0.5454 - classification_loss: 0.1335
 8699/10000 [=========================>....] - ETA: 10:11 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1335
 8700/10000 [=========================>....] - ETA: 10:11 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1335
 8701/10000 [=========================>....] - ETA: 10:11 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1335
 8702/10000 [=========================>....] - ETA: 10:10 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1334
 8703/10000 [=========================>....] - ETA: 10:10 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1334
 8704/10000 [=========================>....] - ETA: 10:09 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1334
 8705/10000 [=========================>....] - ETA: 10:09 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8706/10000 [=========================>....] - ETA: 10:08 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8707/10000 [=========================>....] - ETA: 10:08 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8708/10000 [=========================>....] - ETA: 10:07 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8709/10000 [=========================>....] - ETA: 10:07 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8710/10000 [=========================>....] - ETA: 10:06 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8711/10000 [=========================>....] - ETA: 10:06 - loss: 0.6788 - regression_loss: 0.5453 - classification_loss: 0.1334
 8712/10000 [=========================>....] - ETA: 10:05 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8713/10000 [=========================>....] - ETA: 10:05 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8714/10000 [=========================>....] - ETA: 10:04 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8715/10000 [=========================>....] - ETA: 10:04 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8716/10000 [=========================>....] - ETA: 10:03 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8717/10000 [=========================>....] - ETA: 10:03 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8718/10000 [=========================>....] - ETA: 10:02 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8719/10000 [=========================>....] - ETA: 10:02 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8720/10000 [=========================>....] - ETA: 10:02 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8721/10000 [=========================>....] - ETA: 10:01 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8722/10000 [=========================>....] - ETA: 10:01 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8723/10000 [=========================>....] - ETA: 10:00 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8724/10000 [=========================>....] - ETA: 10:00 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8725/10000 [=========================>....] - ETA: 9:59 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334 
 8726/10000 [=========================>....] - ETA: 9:59 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1334
 8727/10000 [=========================>....] - ETA: 9:58 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8728/10000 [=========================>....] - ETA: 9:58 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1334
 8729/10000 [=========================>....] - ETA: 9:57 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1333
 8730/10000 [=========================>....] - ETA: 9:57 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1333
 8731/10000 [=========================>....] - ETA: 9:56 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8732/10000 [=========================>....] - ETA: 9:56 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8733/10000 [=========================>....] - ETA: 9:55 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8734/10000 [=========================>....] - ETA: 9:55 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8735/10000 [=========================>....] - ETA: 9:54 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8736/10000 [=========================>....] - ETA: 9:54 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8737/10000 [=========================>....] - ETA: 9:53 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8738/10000 [=========================>....] - ETA: 9:53 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1334
 8739/10000 [=========================>....] - ETA: 9:53 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8740/10000 [=========================>....] - ETA: 9:52 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8741/10000 [=========================>....] - ETA: 9:52 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8742/10000 [=========================>....] - ETA: 9:51 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8743/10000 [=========================>....] - ETA: 9:51 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1333
 8744/10000 [=========================>....] - ETA: 9:50 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8745/10000 [=========================>....] - ETA: 9:50 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1333
 8746/10000 [=========================>....] - ETA: 9:49 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8747/10000 [=========================>....] - ETA: 9:49 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1333
 8748/10000 [=========================>....] - ETA: 9:48 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1333
 8749/10000 [=========================>....] - ETA: 9:48 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8750/10000 [=========================>....] - ETA: 9:47 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8751/10000 [=========================>....] - ETA: 9:47 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1333
 8752/10000 [=========================>....] - ETA: 9:46 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1333
 8753/10000 [=========================>....] - ETA: 9:46 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8754/10000 [=========================>....] - ETA: 9:45 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8755/10000 [=========================>....] - ETA: 9:45 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8756/10000 [=========================>....] - ETA: 9:45 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8757/10000 [=========================>....] - ETA: 9:44 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8758/10000 [=========================>....] - ETA: 9:44 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8759/10000 [=========================>....] - ETA: 9:43 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1333
 8760/10000 [=========================>....] - ETA: 9:43 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8761/10000 [=========================>....] - ETA: 9:42 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8762/10000 [=========================>....] - ETA: 9:42 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8763/10000 [=========================>....] - ETA: 9:41 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8764/10000 [=========================>....] - ETA: 9:41 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8765/10000 [=========================>....] - ETA: 9:40 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8766/10000 [=========================>....] - ETA: 9:40 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8767/10000 [=========================>....] - ETA: 9:39 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8768/10000 [=========================>....] - ETA: 9:39 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8769/10000 [=========================>....] - ETA: 9:38 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8770/10000 [=========================>....] - ETA: 9:38 - loss: 0.6786 - regression_loss: 0.5452 - classification_loss: 0.1333
 8771/10000 [=========================>....] - ETA: 9:37 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8772/10000 [=========================>....] - ETA: 9:37 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8773/10000 [=========================>....] - ETA: 9:36 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8774/10000 [=========================>....] - ETA: 9:36 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8775/10000 [=========================>....] - ETA: 9:36 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8776/10000 [=========================>....] - ETA: 9:35 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8777/10000 [=========================>....] - ETA: 9:35 - loss: 0.6785 - regression_loss: 0.5453 - classification_loss: 0.1333
 8778/10000 [=========================>....] - ETA: 9:34 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8779/10000 [=========================>....] - ETA: 9:34 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8780/10000 [=========================>....] - ETA: 9:33 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8781/10000 [=========================>....] - ETA: 9:33 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8782/10000 [=========================>....] - ETA: 9:32 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8783/10000 [=========================>....] - ETA: 9:32 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8784/10000 [=========================>....] - ETA: 9:31 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8785/10000 [=========================>....] - ETA: 9:31 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8786/10000 [=========================>....] - ETA: 9:30 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8787/10000 [=========================>....] - ETA: 9:30 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8788/10000 [=========================>....] - ETA: 9:29 - loss: 0.6785 - regression_loss: 0.5452 - classification_loss: 0.1333
 8789/10000 [=========================>....] - ETA: 9:29 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8790/10000 [=========================>....] - ETA: 9:28 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8791/10000 [=========================>....] - ETA: 9:28 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1333
 8792/10000 [=========================>....] - ETA: 9:28 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8793/10000 [=========================>....] - ETA: 9:27 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1334
 8794/10000 [=========================>....] - ETA: 9:27 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1334
 8795/10000 [=========================>....] - ETA: 9:26 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1334
 8796/10000 [=========================>....] - ETA: 9:26 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1334
 8797/10000 [=========================>....] - ETA: 9:25 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1334
 8798/10000 [=========================>....] - ETA: 9:25 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1334
 8799/10000 [=========================>....] - ETA: 9:24 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8800/10000 [=========================>....] - ETA: 9:24 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8801/10000 [=========================>....] - ETA: 9:23 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1334
 8802/10000 [=========================>....] - ETA: 9:23 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8803/10000 [=========================>....] - ETA: 9:22 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8804/10000 [=========================>....] - ETA: 9:22 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1334
 8805/10000 [=========================>....] - ETA: 9:21 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8806/10000 [=========================>....] - ETA: 9:21 - loss: 0.6786 - regression_loss: 0.5453 - classification_loss: 0.1333
 8807/10000 [=========================>....] - ETA: 9:20 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1333
 8808/10000 [=========================>....] - ETA: 9:20 - loss: 0.6787 - regression_loss: 0.5453 - classification_loss: 0.1333
 8809/10000 [=========================>....] - ETA: 9:20 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1333
 8810/10000 [=========================>....] - ETA: 9:19 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8811/10000 [=========================>....] - ETA: 9:19 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1333
 8812/10000 [=========================>....] - ETA: 9:18 - loss: 0.6788 - regression_loss: 0.5454 - classification_loss: 0.1333
 8813/10000 [=========================>....] - ETA: 9:18 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 8814/10000 [=========================>....] - ETA: 9:17 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1333
 8815/10000 [=========================>....] - ETA: 9:17 - loss: 0.6789 - regression_loss: 0.5455 - classification_loss: 0.1333
 8816/10000 [=========================>....] - ETA: 9:16 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1334
 8817/10000 [=========================>....] - ETA: 9:16 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1333
 8818/10000 [=========================>....] - ETA: 9:15 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1334
 8819/10000 [=========================>....] - ETA: 9:15 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1334
 8820/10000 [=========================>....] - ETA: 9:14 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1334
 8821/10000 [=========================>....] - ETA: 9:14 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1334
 8822/10000 [=========================>....] - ETA: 9:13 - loss: 0.6791 - regression_loss: 0.5457 - classification_loss: 0.1334
 8823/10000 [=========================>....] - ETA: 9:13 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8824/10000 [=========================>....] - ETA: 9:12 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8825/10000 [=========================>....] - ETA: 9:12 - loss: 0.6791 - regression_loss: 0.5457 - classification_loss: 0.1334
 8826/10000 [=========================>....] - ETA: 9:12 - loss: 0.6791 - regression_loss: 0.5457 - classification_loss: 0.1334
 8827/10000 [=========================>....] - ETA: 9:11 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8828/10000 [=========================>....] - ETA: 9:11 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1334
 8829/10000 [=========================>....] - ETA: 9:10 - loss: 0.6791 - regression_loss: 0.5457 - classification_loss: 0.1334
 8830/10000 [=========================>....] - ETA: 9:10 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8831/10000 [=========================>....] - ETA: 9:09 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8832/10000 [=========================>....] - ETA: 9:09 - loss: 0.6791 - regression_loss: 0.5457 - classification_loss: 0.1334
 8833/10000 [=========================>....] - ETA: 9:08 - loss: 0.6791 - regression_loss: 0.5457 - classification_loss: 0.1333
 8834/10000 [=========================>....] - ETA: 9:08 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8835/10000 [=========================>....] - ETA: 9:07 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8836/10000 [=========================>....] - ETA: 9:07 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1333
 8837/10000 [=========================>....] - ETA: 9:06 - loss: 0.6791 - regression_loss: 0.5457 - classification_loss: 0.1333
 8838/10000 [=========================>....] - ETA: 9:06 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8839/10000 [=========================>....] - ETA: 9:05 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8840/10000 [=========================>....] - ETA: 9:05 - loss: 0.6791 - regression_loss: 0.5457 - classification_loss: 0.1333
 8841/10000 [=========================>....] - ETA: 9:04 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8842/10000 [=========================>....] - ETA: 9:04 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8843/10000 [=========================>....] - ETA: 9:04 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8844/10000 [=========================>....] - ETA: 9:03 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8845/10000 [=========================>....] - ETA: 9:03 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8846/10000 [=========================>....] - ETA: 9:02 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8847/10000 [=========================>....] - ETA: 9:02 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8848/10000 [=========================>....] - ETA: 9:01 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8849/10000 [=========================>....] - ETA: 9:01 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8850/10000 [=========================>....] - ETA: 9:00 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8851/10000 [=========================>....] - ETA: 9:00 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1333
 8852/10000 [=========================>....] - ETA: 8:59 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1333
 8853/10000 [=========================>....] - ETA: 8:59 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1333
 8854/10000 [=========================>....] - ETA: 8:58 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1333
 8855/10000 [=========================>....] - ETA: 8:58 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1333
 8856/10000 [=========================>....] - ETA: 8:57 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1333
 8857/10000 [=========================>....] - ETA: 8:57 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1333
 8858/10000 [=========================>....] - ETA: 8:56 - loss: 0.6790 - regression_loss: 0.5458 - classification_loss: 0.1333
 8859/10000 [=========================>....] - ETA: 8:56 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8860/10000 [=========================>....] - ETA: 8:55 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8861/10000 [=========================>....] - ETA: 8:55 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1333
 8862/10000 [=========================>....] - ETA: 8:55 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8863/10000 [=========================>....] - ETA: 8:54 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8864/10000 [=========================>....] - ETA: 8:54 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1333
 8865/10000 [=========================>....] - ETA: 8:53 - loss: 0.6790 - regression_loss: 0.5458 - classification_loss: 0.1333
 8866/10000 [=========================>....] - ETA: 8:53 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1333
 8867/10000 [=========================>....] - ETA: 8:52 - loss: 0.6790 - regression_loss: 0.5458 - classification_loss: 0.1333
 8868/10000 [=========================>....] - ETA: 8:52 - loss: 0.6790 - regression_loss: 0.5458 - classification_loss: 0.1333
 8869/10000 [=========================>....] - ETA: 8:51 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8870/10000 [=========================>....] - ETA: 8:51 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8871/10000 [=========================>....] - ETA: 8:50 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8872/10000 [=========================>....] - ETA: 8:50 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 8873/10000 [=========================>....] - ETA: 8:49 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 8874/10000 [=========================>....] - ETA: 8:49 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 8875/10000 [=========================>....] - ETA: 8:48 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1333
 8876/10000 [=========================>....] - ETA: 8:48 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8877/10000 [=========================>....] - ETA: 8:47 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1334
 8878/10000 [=========================>....] - ETA: 8:47 - loss: 0.6791 - regression_loss: 0.5457 - classification_loss: 0.1333
 8879/10000 [=========================>....] - ETA: 8:47 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8880/10000 [=========================>....] - ETA: 8:46 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8881/10000 [=========================>....] - ETA: 8:46 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8882/10000 [=========================>....] - ETA: 8:45 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8883/10000 [=========================>....] - ETA: 8:45 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1333
 8884/10000 [=========================>....] - ETA: 8:44 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8885/10000 [=========================>....] - ETA: 8:44 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8886/10000 [=========================>....] - ETA: 8:43 - loss: 0.6791 - regression_loss: 0.5457 - classification_loss: 0.1333
 8887/10000 [=========================>....] - ETA: 8:43 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8888/10000 [=========================>....] - ETA: 8:42 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8889/10000 [=========================>....] - ETA: 8:42 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8890/10000 [=========================>....] - ETA: 8:41 - loss: 0.6793 - regression_loss: 0.5459 - classification_loss: 0.1334
 8891/10000 [=========================>....] - ETA: 8:41 - loss: 0.6793 - regression_loss: 0.5459 - classification_loss: 0.1334
 8892/10000 [=========================>....] - ETA: 8:40 - loss: 0.6793 - regression_loss: 0.5459 - classification_loss: 0.1334
 8893/10000 [=========================>....] - ETA: 8:40 - loss: 0.6793 - regression_loss: 0.5459 - classification_loss: 0.1334
 8894/10000 [=========================>....] - ETA: 8:39 - loss: 0.6794 - regression_loss: 0.5459 - classification_loss: 0.1334
 8895/10000 [=========================>....] - ETA: 8:39 - loss: 0.6793 - regression_loss: 0.5459 - classification_loss: 0.1334
 8896/10000 [=========================>....] - ETA: 8:39 - loss: 0.6793 - regression_loss: 0.5459 - classification_loss: 0.1334
 8897/10000 [=========================>....] - ETA: 8:38 - loss: 0.6793 - regression_loss: 0.5459 - classification_loss: 0.1334
 8898/10000 [=========================>....] - ETA: 8:38 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1334
 8899/10000 [=========================>....] - ETA: 8:37 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1334
 8900/10000 [=========================>....] - ETA: 8:37 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8901/10000 [=========================>....] - ETA: 8:36 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8902/10000 [=========================>....] - ETA: 8:36 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8903/10000 [=========================>....] - ETA: 8:35 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8904/10000 [=========================>....] - ETA: 8:35 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8905/10000 [=========================>....] - ETA: 8:34 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8906/10000 [=========================>....] - ETA: 8:34 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1334
 8907/10000 [=========================>....] - ETA: 8:33 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1334
 8908/10000 [=========================>....] - ETA: 8:33 - loss: 0.6793 - regression_loss: 0.5459 - classification_loss: 0.1334
 8909/10000 [=========================>....] - ETA: 8:32 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8910/10000 [=========================>....] - ETA: 8:32 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8911/10000 [=========================>....] - ETA: 8:31 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8912/10000 [=========================>....] - ETA: 8:31 - loss: 0.6792 - regression_loss: 0.5458 - classification_loss: 0.1334
 8913/10000 [=========================>....] - ETA: 8:30 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1333
 8914/10000 [=========================>....] - ETA: 8:30 - loss: 0.6791 - regression_loss: 0.5457 - classification_loss: 0.1334
 8915/10000 [=========================>....] - ETA: 8:30 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8916/10000 [=========================>....] - ETA: 8:29 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8917/10000 [=========================>....] - ETA: 8:29 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1333
 8918/10000 [=========================>....] - ETA: 8:28 - loss: 0.6790 - regression_loss: 0.5456 - classification_loss: 0.1333
 8919/10000 [=========================>....] - ETA: 8:28 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1333
 8920/10000 [=========================>....] - ETA: 8:27 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1333
 8921/10000 [=========================>....] - ETA: 8:27 - loss: 0.6791 - regression_loss: 0.5457 - classification_loss: 0.1333
 8922/10000 [=========================>....] - ETA: 8:26 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1333
 8923/10000 [=========================>....] - ETA: 8:26 - loss: 0.6791 - regression_loss: 0.5457 - classification_loss: 0.1333
 8924/10000 [=========================>....] - ETA: 8:25 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1333
 8925/10000 [=========================>....] - ETA: 8:25 - loss: 0.6791 - regression_loss: 0.5457 - classification_loss: 0.1333
 8926/10000 [=========================>....] - ETA: 8:24 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1333
 8927/10000 [=========================>....] - ETA: 8:24 - loss: 0.6791 - regression_loss: 0.5457 - classification_loss: 0.1333
 8928/10000 [=========================>....] - ETA: 8:23 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1333
 8929/10000 [=========================>....] - ETA: 8:23 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1333
 8930/10000 [=========================>....] - ETA: 8:22 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1333
 8931/10000 [=========================>....] - ETA: 8:22 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1333
 8932/10000 [=========================>....] - ETA: 8:22 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1333
 8933/10000 [=========================>....] - ETA: 8:21 - loss: 0.6793 - regression_loss: 0.5459 - classification_loss: 0.1333
 8934/10000 [=========================>....] - ETA: 8:21 - loss: 0.6793 - regression_loss: 0.5459 - classification_loss: 0.1333
 8935/10000 [=========================>....] - ETA: 8:20 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1333
 8936/10000 [=========================>....] - ETA: 8:20 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1333
 8937/10000 [=========================>....] - ETA: 8:19 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1333
 8938/10000 [=========================>....] - ETA: 8:19 - loss: 0.6792 - regression_loss: 0.5459 - classification_loss: 0.1333
 8939/10000 [=========================>....] - ETA: 8:18 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1333
 8940/10000 [=========================>....] - ETA: 8:18 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1333
 8941/10000 [=========================>....] - ETA: 8:17 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8942/10000 [=========================>....] - ETA: 8:17 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1333
 8943/10000 [=========================>....] - ETA: 8:16 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8944/10000 [=========================>....] - ETA: 8:16 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8945/10000 [=========================>....] - ETA: 8:15 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1333
 8946/10000 [=========================>....] - ETA: 8:15 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1333
 8947/10000 [=========================>....] - ETA: 8:14 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1332
 8948/10000 [=========================>....] - ETA: 8:14 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1332
 8949/10000 [=========================>....] - ETA: 8:14 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1332
 8950/10000 [=========================>....] - ETA: 8:13 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1333
 8951/10000 [=========================>....] - ETA: 8:13 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8952/10000 [=========================>....] - ETA: 8:12 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8953/10000 [=========================>....] - ETA: 8:12 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8954/10000 [=========================>....] - ETA: 8:11 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8955/10000 [=========================>....] - ETA: 8:11 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1333
 8956/10000 [=========================>....] - ETA: 8:10 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1333
 8957/10000 [=========================>....] - ETA: 8:10 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8958/10000 [=========================>....] - ETA: 8:09 - loss: 0.6790 - regression_loss: 0.5458 - classification_loss: 0.1333
 8959/10000 [=========================>....] - ETA: 8:09 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8960/10000 [=========================>....] - ETA: 8:08 - loss: 0.6790 - regression_loss: 0.5458 - classification_loss: 0.1333
 8961/10000 [=========================>....] - ETA: 8:08 - loss: 0.6790 - regression_loss: 0.5458 - classification_loss: 0.1333
 8962/10000 [=========================>....] - ETA: 8:07 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1333
 8963/10000 [=========================>....] - ETA: 8:07 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8964/10000 [=========================>....] - ETA: 8:06 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8965/10000 [=========================>....] - ETA: 8:06 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1332
 8966/10000 [=========================>....] - ETA: 8:05 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1333
 8967/10000 [=========================>....] - ETA: 8:05 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1333
 8968/10000 [=========================>....] - ETA: 8:05 - loss: 0.6791 - regression_loss: 0.5458 - classification_loss: 0.1333
 8969/10000 [=========================>....] - ETA: 8:04 - loss: 0.6790 - regression_loss: 0.5458 - classification_loss: 0.1333
 8970/10000 [=========================>....] - ETA: 8:04 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1332
 8971/10000 [=========================>....] - ETA: 8:03 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1332
 8972/10000 [=========================>....] - ETA: 8:03 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1332
 8973/10000 [=========================>....] - ETA: 8:02 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8974/10000 [=========================>....] - ETA: 8:02 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8975/10000 [=========================>....] - ETA: 8:01 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8976/10000 [=========================>....] - ETA: 8:01 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8977/10000 [=========================>....] - ETA: 8:00 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8978/10000 [=========================>....] - ETA: 8:00 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8979/10000 [=========================>....] - ETA: 7:59 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1333
 8980/10000 [=========================>....] - ETA: 7:59 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1333
 8981/10000 [=========================>....] - ETA: 7:58 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1333
 8982/10000 [=========================>....] - ETA: 7:58 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1333
 8983/10000 [=========================>....] - ETA: 7:57 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1333
 8984/10000 [=========================>....] - ETA: 7:57 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1333
 8985/10000 [=========================>....] - ETA: 7:57 - loss: 0.6788 - regression_loss: 0.5456 - classification_loss: 0.1332
 8986/10000 [=========================>....] - ETA: 7:56 - loss: 0.6788 - regression_loss: 0.5456 - classification_loss: 0.1333
 8987/10000 [=========================>....] - ETA: 7:56 - loss: 0.6788 - regression_loss: 0.5456 - classification_loss: 0.1333
 8988/10000 [=========================>....] - ETA: 7:55 - loss: 0.6788 - regression_loss: 0.5456 - classification_loss: 0.1332
 8989/10000 [=========================>....] - ETA: 7:55 - loss: 0.6788 - regression_loss: 0.5456 - classification_loss: 0.1332
 8990/10000 [=========================>....] - ETA: 7:54 - loss: 0.6788 - regression_loss: 0.5456 - classification_loss: 0.1333
 8991/10000 [=========================>....] - ETA: 7:54 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1333
 8992/10000 [=========================>....] - ETA: 7:53 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1333
 8993/10000 [=========================>....] - ETA: 7:53 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1333
 8994/10000 [=========================>....] - ETA: 7:52 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1333
 8995/10000 [=========================>....] - ETA: 7:52 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 8996/10000 [=========================>....] - ETA: 7:51 - loss: 0.6788 - regression_loss: 0.5456 - classification_loss: 0.1333
 8997/10000 [=========================>....] - ETA: 7:51 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 8998/10000 [=========================>....] - ETA: 7:50 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 8999/10000 [=========================>....] - ETA: 7:50 - loss: 0.6788 - regression_loss: 0.5456 - classification_loss: 0.1333
 9000/10000 [==========================>...] - ETA: 7:49 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1333
 9001/10000 [==========================>...] - ETA: 7:49 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 9002/10000 [==========================>...] - ETA: 7:48 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 9003/10000 [==========================>...] - ETA: 7:48 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 9004/10000 [==========================>...] - ETA: 7:48 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 9005/10000 [==========================>...] - ETA: 7:47 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 9006/10000 [==========================>...] - ETA: 7:47 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 9007/10000 [==========================>...] - ETA: 7:46 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 9008/10000 [==========================>...] - ETA: 7:46 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1332
 9009/10000 [==========================>...] - ETA: 7:45 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1333
 9010/10000 [==========================>...] - ETA: 7:45 - loss: 0.6787 - regression_loss: 0.5455 - classification_loss: 0.1332
 9011/10000 [==========================>...] - ETA: 7:44 - loss: 0.6787 - regression_loss: 0.5455 - classification_loss: 0.1332
 9012/10000 [==========================>...] - ETA: 7:44 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1332
 9013/10000 [==========================>...] - ETA: 7:43 - loss: 0.6787 - regression_loss: 0.5455 - classification_loss: 0.1332
 9014/10000 [==========================>...] - ETA: 7:43 - loss: 0.6787 - regression_loss: 0.5455 - classification_loss: 0.1332
 9015/10000 [==========================>...] - ETA: 7:42 - loss: 0.6786 - regression_loss: 0.5454 - classification_loss: 0.1332
 9016/10000 [==========================>...] - ETA: 7:42 - loss: 0.6786 - regression_loss: 0.5454 - classification_loss: 0.1332
 9017/10000 [==========================>...] - ETA: 7:41 - loss: 0.6787 - regression_loss: 0.5455 - classification_loss: 0.1332
 9018/10000 [==========================>...] - ETA: 7:41 - loss: 0.6787 - regression_loss: 0.5455 - classification_loss: 0.1332
 9019/10000 [==========================>...] - ETA: 7:40 - loss: 0.6787 - regression_loss: 0.5455 - classification_loss: 0.1332
 9020/10000 [==========================>...] - ETA: 7:40 - loss: 0.6786 - regression_loss: 0.5454 - classification_loss: 0.1332
 9021/10000 [==========================>...] - ETA: 7:40 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1332
 9022/10000 [==========================>...] - ETA: 7:39 - loss: 0.6787 - regression_loss: 0.5454 - classification_loss: 0.1332
 9023/10000 [==========================>...] - ETA: 7:39 - loss: 0.6788 - regression_loss: 0.5455 - classification_loss: 0.1332
 9024/10000 [==========================>...] - ETA: 7:38 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1333
 9025/10000 [==========================>...] - ETA: 7:38 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1332
 9026/10000 [==========================>...] - ETA: 7:37 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1332
 9027/10000 [==========================>...] - ETA: 7:37 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1332
 9028/10000 [==========================>...] - ETA: 7:36 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1333
 9029/10000 [==========================>...] - ETA: 7:36 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 9030/10000 [==========================>...] - ETA: 7:35 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1333
 9031/10000 [==========================>...] - ETA: 7:35 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1332
 9032/10000 [==========================>...] - ETA: 7:34 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1332
 9033/10000 [==========================>...] - ETA: 7:34 - loss: 0.6789 - regression_loss: 0.5456 - classification_loss: 0.1332
 9034/10000 [==========================>...] - ETA: 7:33 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 9035/10000 [==========================>...] - ETA: 7:33 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 9036/10000 [==========================>...] - ETA: 7:32 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1332
 9037/10000 [==========================>...] - ETA: 7:32 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 9038/10000 [==========================>...] - ETA: 7:32 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 9039/10000 [==========================>...] - ETA: 7:31 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 9040/10000 [==========================>...] - ETA: 7:31 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 9041/10000 [==========================>...] - ETA: 7:30 - loss: 0.6790 - regression_loss: 0.5457 - classification_loss: 0.1332
 9042/10000 [==========================>...] - ETA: 7:30 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 9043/10000 [==========================>...] - ETA: 7:29 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 9044/10000 [==========================>...] - ETA: 7:29 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 9045/10000 [==========================>...] - ETA: 7:28 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 9046/10000 [==========================>...] - ETA: 7:28 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 9047/10000 [==========================>...] - ETA: 7:27 - loss: 0.6788 - regression_loss: 0.5456 - classification_loss: 0.1332
 9048/10000 [==========================>...] - ETA: 7:27 - loss: 0.6788 - regression_loss: 0.5456 - classification_loss: 0.1332
 9049/10000 [==========================>...] - ETA: 7:26 - loss: 0.6788 - regression_loss: 0.5456 - classification_loss: 0.1332
 9050/10000 [==========================>...] - ETA: 7:26 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 9051/10000 [==========================>...] - ETA: 7:25 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 9052/10000 [==========================>...] - ETA: 7:25 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 9053/10000 [==========================>...] - ETA: 7:24 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 9054/10000 [==========================>...] - ETA: 7:24 - loss: 0.6788 - regression_loss: 0.5456 - classification_loss: 0.1332
 9055/10000 [==========================>...] - ETA: 7:23 - loss: 0.6788 - regression_loss: 0.5456 - classification_loss: 0.1332
 9056/10000 [==========================>...] - ETA: 7:23 - loss: 0.6788 - regression_loss: 0.5456 - classification_loss: 0.1332
 9057/10000 [==========================>...] - ETA: 7:23 - loss: 0.6788 - regression_loss: 0.5456 - classification_loss: 0.1332
 9058/10000 [==========================>...] - ETA: 7:22 - loss: 0.6788 - regression_loss: 0.5456 - classification_loss: 0.1332
 9059/10000 [==========================>...] - ETA: 7:22 - loss: 0.6787 - regression_loss: 0.5456 - classification_loss: 0.1332
 9060/10000 [==========================>...] - ETA: 7:21 - loss: 0.6787 - regression_loss: 0.5455 - classification_loss: 0.1332
 9061/10000 [==========================>...] - ETA: 7:21 - loss: 0.6787 - regression_loss: 0.5455 - classification_loss: 0.1331
 9062/10000 [==========================>...] - ETA: 7:20 - loss: 0.6786 - regression_loss: 0.5455 - classification_loss: 0.1331
 9063/10000 [==========================>...] - ETA: 7:20 - loss: 0.6786 - regression_loss: 0.5455 - classification_loss: 0.1331
 9064/10000 [==========================>...] - ETA: 7:19 - loss: 0.6787 - regression_loss: 0.5455 - classification_loss: 0.1331
 9065/10000 [==========================>...] - ETA: 7:19 - loss: 0.6787 - regression_loss: 0.5456 - classification_loss: 0.1331
 9066/10000 [==========================>...] - ETA: 7:18 - loss: 0.6787 - regression_loss: 0.5455 - classification_loss: 0.1331
 9067/10000 [==========================>...] - ETA: 7:18 - loss: 0.6787 - regression_loss: 0.5456 - classification_loss: 0.1332
 9068/10000 [==========================>...] - ETA: 7:17 - loss: 0.6787 - regression_loss: 0.5455 - classification_loss: 0.1332
 9069/10000 [==========================>...] - ETA: 7:17 - loss: 0.6787 - regression_loss: 0.5456 - classification_loss: 0.1332
 9070/10000 [==========================>...] - ETA: 7:16 - loss: 0.6788 - regression_loss: 0.5456 - classification_loss: 0.1332
 9071/10000 [==========================>...] - ETA: 7:16 - loss: 0.6788 - regression_loss: 0.5456 - classification_loss: 0.1332
 9072/10000 [==========================>...] - ETA: 7:16 - loss: 0.6789 - regression_loss: 0.5457 - classification_loss: 0.1332
 9073/10000 [==========================>...] - ETA: 7:15 - loss: 0.6789 - regression_loss: 0.5458 - classification_loss: 0.1332
 9074/10000 [==========================>...] - ETA: 7:15 - loss: 0.6789 - regression_loss: 0.5458 - classification_loss: 0.1332
 9075/10000 [==========================>...] - ETA: 7:14 - loss: 0.6790 - regression_loss: 0.5458 - classification_loss: 0.1332
 9076/10000 [==========================>...] - ETA: 7:14 - loss: 0.6790 - regression_loss: 0.5458 - classification_loss: 0.1332
 9077/10000 [==========================>...] - ETA: 7:13 - loss: 0.6790 - regression_loss: 0.5458 - classification_loss: 0.1332
 9078/10000 [==========================>...] - ETA: 7:13 - loss: 0.6790 - regression_loss: 0.5458 - classification_loss: 0.1332
 9079/10000 [==========================>...] - ETA: 7:12 - loss: 0.6790 - regression_loss: 0.5458 - classification_loss: 0.1332
 9080/10000 [==========================>...] - ETA: 7:12 - loss: 0.6791 - regression_loss: 0.5459 - classification_loss: 0.1332
 9081/10000 [==========================>...] - ETA: 7:11 - loss: 0.6790 - regression_loss: 0.5458 - classification_loss: 0.1332
 9082/10000 [==========================>...] - ETA: 7:11 - loss: 0.6791 - regression_loss: 0.5459 - classification_loss: 0.1332
 9083/10000 [==========================>...] - ETA: 7:10 - loss: 0.6791 - regression_loss: 0.5459 - classification_loss: 0.1332
 9084/10000 [==========================>...] - ETA: 7:10 - loss: 0.6792 - regression_loss: 0.5460 - classification_loss: 0.1332
 9085/10000 [==========================>...] - ETA: 7:09 - loss: 0.6793 - regression_loss: 0.5461 - classification_loss: 0.1332
 9086/10000 [==========================>...] - ETA: 7:09 - loss: 0.6793 - regression_loss: 0.5461 - classification_loss: 0.1332
 9087/10000 [==========================>...] - ETA: 7:08 - loss: 0.6792 - regression_loss: 0.5460 - classification_loss: 0.1332
 9088/10000 [==========================>...] - ETA: 7:08 - loss: 0.6792 - regression_loss: 0.5460 - classification_loss: 0.1332
 9089/10000 [==========================>...] - ETA: 7:07 - loss: 0.6791 - regression_loss: 0.5460 - classification_loss: 0.1332
 9090/10000 [==========================>...] - ETA: 7:07 - loss: 0.6791 - regression_loss: 0.5459 - classification_loss: 0.1332
 9091/10000 [==========================>...] - ETA: 7:07 - loss: 0.6791 - regression_loss: 0.5459 - classification_loss: 0.1332
 9092/10000 [==========================>...] - ETA: 7:06 - loss: 0.6791 - regression_loss: 0.5459 - classification_loss: 0.1332
 9093/10000 [==========================>...] - ETA: 7:06 - loss: 0.6793 - regression_loss: 0.5461 - classification_loss: 0.1332
 9094/10000 [==========================>...] - ETA: 7:05 - loss: 0.6792 - regression_loss: 0.5460 - classification_loss: 0.1332
 9095/10000 [==========================>...] - ETA: 7:05 - loss: 0.6792 - regression_loss: 0.5460 - classification_loss: 0.1332
 9096/10000 [==========================>...] - ETA: 7:04 - loss: 0.6791 - regression_loss: 0.5460 - classification_loss: 0.1332
 9097/10000 [==========================>...] - ETA: 7:04 - loss: 0.6791 - regression_loss: 0.5460 - classification_loss: 0.1332
 9098/10000 [==========================>...] - ETA: 7:03 - loss: 0.6791 - regression_loss: 0.5459 - classification_loss: 0.1332
 9099/10000 [==========================>...] - ETA: 7:03 - loss: 0.6791 - regression_loss: 0.5460 - classification_loss: 0.1331
 9100/10000 [==========================>...] - ETA: 7:02 - loss: 0.6791 - regression_loss: 0.5459 - classification_loss: 0.1331
 9101/10000 [==========================>...] - ETA: 7:02 - loss: 0.6790 - regression_loss: 0.5459 - classification_loss: 0.1331
 9102/10000 [==========================>...] - ETA: 7:01 - loss: 0.6791 - regression_loss: 0.5459 - classification_loss: 0.1332
 9103/10000 [==========================>...] - ETA: 7:01 - loss: 0.6791 - regression_loss: 0.5459 - classification_loss: 0.1332
 9104/10000 [==========================>...] - ETA: 7:00 - loss: 0.6791 - regression_loss: 0.5459 - classification_loss: 0.1332
 9105/10000 [==========================>...] - ETA: 7:00 - loss: 0.6791 - regression_loss: 0.5459 - classification_loss: 0.1332
 9106/10000 [==========================>...] - ETA: 6:59 - loss: 0.6791 - regression_loss: 0.5459 - classification_loss: 0.1332
 9107/10000 [==========================>...] - ETA: 6:59 - loss: 0.6790 - regression_loss: 0.5459 - classification_loss: 0.1332
 9108/10000 [==========================>...] - ETA: 6:59 - loss: 0.6791 - regression_loss: 0.5460 - classification_loss: 0.1332
 9109/10000 [==========================>...] - ETA: 6:58 - loss: 0.6791 - regression_loss: 0.5459 - classification_loss: 0.1332
 9110/10000 [==========================>...] - ETA: 6:58 - loss: 0.6792 - regression_loss: 0.5460 - classification_loss: 0.1332
 9111/10000 [==========================>...] - ETA: 6:57 - loss: 0.6792 - regression_loss: 0.5460 - classification_loss: 0.1332
 9112/10000 [==========================>...] - ETA: 6:57 - loss: 0.6792 - regression_loss: 0.5460 - classification_loss: 0.1332
 9113/10000 [==========================>...] - ETA: 6:56 - loss: 0.6791 - regression_loss: 0.5460 - classification_loss: 0.1332
 9114/10000 [==========================>...] - ETA: 6:56 - loss: 0.6791 - regression_loss: 0.5459 - classification_loss: 0.1332
 9115/10000 [==========================>...] - ETA: 6:55 - loss: 0.6791 - regression_loss: 0.5460 - classification_loss: 0.1332
 9116/10000 [==========================>...] - ETA: 6:55 - loss: 0.6791 - regression_loss: 0.5459 - classification_loss: 0.1332
 9117/10000 [==========================>...] - ETA: 6:54 - loss: 0.6790 - regression_loss: 0.5459 - classification_loss: 0.1332
 9118/10000 [==========================>...] - ETA: 6:54 - loss: 0.6791 - regression_loss: 0.5459 - classification_loss: 0.1332
 9119/10000 [==========================>...] - ETA: 6:53 - loss: 0.6790 - regression_loss: 0.5459 - classification_loss: 0.1332
 9120/10000 [==========================>...] - ETA: 6:53 - loss: 0.6790 - regression_loss: 0.5458 - classification_loss: 0.1332
 9121/10000 [==========================>...] - ETA: 6:52 - loss: 0.6790 - regression_loss: 0.5459 - classification_loss: 0.1331
 9122/10000 [==========================>...] - ETA: 6:52 - loss: 0.6790 - regression_loss: 0.5459 - classification_loss: 0.1331
 9123/10000 [==========================>...] - ETA: 6:51 - loss: 0.6791 - regression_loss: 0.5459 - classification_loss: 0.1331
 9124/10000 [==========================>...] - ETA: 6:51 - loss: 0.6790 - regression_loss: 0.5459 - classification_loss: 0.1331
 9125/10000 [==========================>...] - ETA: 6:51 - loss: 0.6790 - regression_loss: 0.5459 - classification_loss: 0.1331
 9126/10000 [==========================>...] - ETA: 6:50 - loss: 0.6791 - regression_loss: 0.5459 - classification_loss: 0.1331
 9127/10000 [==========================>...] - ETA: 6:50 - loss: 0.6791 - regression_loss: 0.5459 - classification_loss: 0.1331
 9128/10000 [==========================>...] - ETA: 6:49 - loss: 0.6791 - regression_loss: 0.5459 - classification_loss: 0.1331
 9129/10000 [==========================>...] - ETA: 6:49 - loss: 0.6791 - regression_loss: 0.5459 - classification_loss: 0.1331
 9130/10000 [==========================>...] - ETA: 6:48 - loss: 0.6790 - regression_loss: 0.5459 - classification_loss: 0.1331
 9131/10000 [==========================>...] - ETA: 6:48 - loss: 0.6790 - regression_loss: 0.5459 - classification_loss: 0.1331
 9132/10000 [==========================>...] - ETA: 6:47 - loss: 0.6790 - regression_loss: 0.5459 - classification_loss: 0.1331
 9133/10000 [==========================>...] - ETA: 6:47 - loss: 0.6790 - regression_loss: 0.5458 - classification_loss: 0.1331
 9134/10000 [==========================>...] - ETA: 6:46 - loss: 0.6790 - regression_loss: 0.5459 - classification_loss: 0.1331
 9135/10000 [==========================>...] - ETA: 6:46 - loss: 0.6789 - regression_loss: 0.5458 - classification_loss: 0.1331
 9136/10000 [==========================>...] - ETA: 6:45 - loss: 0.6789 - regression_loss: 0.5458 - classification_loss: 0.1331
 9137/10000 [==========================>...] - ETA: 6:45 - loss: 0.6789 - regression_loss: 0.5458 - classification_loss: 0.1331
 9138/10000 [==========================>...] - ETA: 6:44 - loss: 0.6789 - regression_loss: 0.5458 - classification_loss: 0.1331
 9139/10000 [==========================>...] - ETA: 6:44 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1331
 9140/10000 [==========================>...] - ETA: 6:43 - loss: 0.6789 - regression_loss: 0.5458 - classification_loss: 0.1331
 9141/10000 [==========================>...] - ETA: 6:43 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1331
 9142/10000 [==========================>...] - ETA: 6:43 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1331
 9143/10000 [==========================>...] - ETA: 6:42 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1331
 9144/10000 [==========================>...] - ETA: 6:42 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1331
 9145/10000 [==========================>...] - ETA: 6:41 - loss: 0.6789 - regression_loss: 0.5458 - classification_loss: 0.1331
 9146/10000 [==========================>...] - ETA: 6:41 - loss: 0.6789 - regression_loss: 0.5459 - classification_loss: 0.1331
 9147/10000 [==========================>...] - ETA: 6:40 - loss: 0.6789 - regression_loss: 0.5458 - classification_loss: 0.1330
 9148/10000 [==========================>...] - ETA: 6:40 - loss: 0.6789 - regression_loss: 0.5458 - classification_loss: 0.1331
 9149/10000 [==========================>...] - ETA: 6:39 - loss: 0.6789 - regression_loss: 0.5458 - classification_loss: 0.1331
 9150/10000 [==========================>...] - ETA: 6:39 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1331
 9151/10000 [==========================>...] - ETA: 6:38 - loss: 0.6788 - regression_loss: 0.5457 - classification_loss: 0.1331
 9152/10000 [==========================>...] - ETA: 6:38 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1331
 9153/10000 [==========================>...] - ETA: 6:37 - loss: 0.6788 - regression_loss: 0.5457 - classification_loss: 0.1330
 9154/10000 [==========================>...] - ETA: 6:37 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 9155/10000 [==========================>...] - ETA: 6:36 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 9156/10000 [==========================>...] - ETA: 6:36 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 9157/10000 [==========================>...] - ETA: 6:35 - loss: 0.6788 - regression_loss: 0.5458 - classification_loss: 0.1330
 9158/10000 [==========================>...] - ETA: 6:35 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1330
 9159/10000 [==========================>...] - ETA: 6:35 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1330
 9160/10000 [==========================>...] - ETA: 6:34 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1330
 9161/10000 [==========================>...] - ETA: 6:34 - loss: 0.6786 - regression_loss: 0.5456 - classification_loss: 0.1330
 9162/10000 [==========================>...] - ETA: 6:33 - loss: 0.6786 - regression_loss: 0.5456 - classification_loss: 0.1330
 9163/10000 [==========================>...] - ETA: 6:33 - loss: 0.6786 - regression_loss: 0.5456 - classification_loss: 0.1330
 9164/10000 [==========================>...] - ETA: 6:32 - loss: 0.6786 - regression_loss: 0.5456 - classification_loss: 0.1330
 9165/10000 [==========================>...] - ETA: 6:32 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1330
 9166/10000 [==========================>...] - ETA: 6:31 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1330
 9167/10000 [==========================>...] - ETA: 6:31 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1330
 9168/10000 [==========================>...] - ETA: 6:30 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1330
 9169/10000 [==========================>...] - ETA: 6:30 - loss: 0.6787 - regression_loss: 0.5457 - classification_loss: 0.1330
 9170/10000 [==========================>...] - ETA: 6:29 - loss: 0.6786 - regression_loss: 0.5456 - classification_loss: 0.1330
 9171/10000 [==========================>...] - ETA: 6:29 - loss: 0.6786 - regression_loss: 0.5456 - classification_loss: 0.1330
 9172/10000 [==========================>...] - ETA: 6:28 - loss: 0.6785 - regression_loss: 0.5455 - classification_loss: 0.1330
 9173/10000 [==========================>...] - ETA: 6:28 - loss: 0.6785 - regression_loss: 0.5455 - classification_loss: 0.1330
 9174/10000 [==========================>...] - ETA: 6:27 - loss: 0.6785 - regression_loss: 0.5455 - classification_loss: 0.1330
 9175/10000 [==========================>...] - ETA: 6:27 - loss: 0.6785 - regression_loss: 0.5455 - classification_loss: 0.1330
 9176/10000 [==========================>...] - ETA: 6:27 - loss: 0.6785 - regression_loss: 0.5455 - classification_loss: 0.1330
 9177/10000 [==========================>...] - ETA: 6:26 - loss: 0.6785 - regression_loss: 0.5455 - classification_loss: 0.1330
 9178/10000 [==========================>...] - ETA: 6:26 - loss: 0.6784 - regression_loss: 0.5454 - classification_loss: 0.1330
 9179/10000 [==========================>...] - ETA: 6:25 - loss: 0.6785 - regression_loss: 0.5455 - classification_loss: 0.1330
 9180/10000 [==========================>...] - ETA: 6:25 - loss: 0.6784 - regression_loss: 0.5454 - classification_loss: 0.1330
 9181/10000 [==========================>...] - ETA: 6:24 - loss: 0.6785 - regression_loss: 0.5454 - classification_loss: 0.1330
 9182/10000 [==========================>...] - ETA: 6:24 - loss: 0.6784 - regression_loss: 0.5454 - classification_loss: 0.1330
 9183/10000 [==========================>...] - ETA: 6:23 - loss: 0.6784 - regression_loss: 0.5454 - classification_loss: 0.1330
 9184/10000 [==========================>...] - ETA: 6:23 - loss: 0.6784 - regression_loss: 0.5454 - classification_loss: 0.1330
 9185/10000 [==========================>...] - ETA: 6:22 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1330
 9186/10000 [==========================>...] - ETA: 6:22 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9187/10000 [==========================>...] - ETA: 6:21 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9188/10000 [==========================>...] - ETA: 6:21 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9189/10000 [==========================>...] - ETA: 6:20 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9190/10000 [==========================>...] - ETA: 6:20 - loss: 0.6782 - regression_loss: 0.5453 - classification_loss: 0.1330
 9191/10000 [==========================>...] - ETA: 6:19 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9192/10000 [==========================>...] - ETA: 6:19 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9193/10000 [==========================>...] - ETA: 6:19 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9194/10000 [==========================>...] - ETA: 6:18 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9195/10000 [==========================>...] - ETA: 6:18 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9196/10000 [==========================>...] - ETA: 6:17 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1330
 9197/10000 [==========================>...] - ETA: 6:17 - loss: 0.6784 - regression_loss: 0.5454 - classification_loss: 0.1330
 9198/10000 [==========================>...] - ETA: 6:16 - loss: 0.6784 - regression_loss: 0.5454 - classification_loss: 0.1330
 9199/10000 [==========================>...] - ETA: 6:16 - loss: 0.6784 - regression_loss: 0.5454 - classification_loss: 0.1330
 9200/10000 [==========================>...] - ETA: 6:15 - loss: 0.6784 - regression_loss: 0.5454 - classification_loss: 0.1330
 9201/10000 [==========================>...] - ETA: 6:15 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1330
 9202/10000 [==========================>...] - ETA: 6:14 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1330
 9203/10000 [==========================>...] - ETA: 6:14 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1330
 9204/10000 [==========================>...] - ETA: 6:13 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1330
 9205/10000 [==========================>...] - ETA: 6:13 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1330
 9206/10000 [==========================>...] - ETA: 6:12 - loss: 0.6783 - regression_loss: 0.5454 - classification_loss: 0.1330
 9207/10000 [==========================>...] - ETA: 6:12 - loss: 0.6784 - regression_loss: 0.5454 - classification_loss: 0.1330
 9208/10000 [==========================>...] - ETA: 6:11 - loss: 0.6784 - regression_loss: 0.5454 - classification_loss: 0.1330
 9209/10000 [==========================>...] - ETA: 6:11 - loss: 0.6784 - regression_loss: 0.5454 - classification_loss: 0.1330
 9210/10000 [==========================>...] - ETA: 6:11 - loss: 0.6784 - regression_loss: 0.5454 - classification_loss: 0.1330
 9211/10000 [==========================>...] - ETA: 6:10 - loss: 0.6784 - regression_loss: 0.5454 - classification_loss: 0.1330
 9212/10000 [==========================>...] - ETA: 6:10 - loss: 0.6784 - regression_loss: 0.5454 - classification_loss: 0.1330
 9213/10000 [==========================>...] - ETA: 6:09 - loss: 0.6784 - regression_loss: 0.5454 - classification_loss: 0.1330
 9214/10000 [==========================>...] - ETA: 6:09 - loss: 0.6784 - regression_loss: 0.5454 - classification_loss: 0.1330
 9215/10000 [==========================>...] - ETA: 6:08 - loss: 0.6784 - regression_loss: 0.5454 - classification_loss: 0.1330
 9216/10000 [==========================>...] - ETA: 6:08 - loss: 0.6784 - regression_loss: 0.5454 - classification_loss: 0.1330
 9217/10000 [==========================>...] - ETA: 6:07 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9218/10000 [==========================>...] - ETA: 6:07 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9219/10000 [==========================>...] - ETA: 6:06 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9220/10000 [==========================>...] - ETA: 6:06 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9221/10000 [==========================>...] - ETA: 6:05 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9222/10000 [==========================>...] - ETA: 6:05 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9223/10000 [==========================>...] - ETA: 6:04 - loss: 0.6783 - regression_loss: 0.5452 - classification_loss: 0.1330
 9224/10000 [==========================>...] - ETA: 6:04 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9225/10000 [==========================>...] - ETA: 6:03 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9226/10000 [==========================>...] - ETA: 6:03 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9227/10000 [==========================>...] - ETA: 6:03 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9228/10000 [==========================>...] - ETA: 6:02 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9229/10000 [==========================>...] - ETA: 6:02 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9230/10000 [==========================>...] - ETA: 6:01 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9231/10000 [==========================>...] - ETA: 6:01 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9232/10000 [==========================>...] - ETA: 6:00 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9233/10000 [==========================>...] - ETA: 6:00 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9234/10000 [==========================>...] - ETA: 5:59 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9235/10000 [==========================>...] - ETA: 5:59 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9236/10000 [==========================>...] - ETA: 5:58 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9237/10000 [==========================>...] - ETA: 5:58 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9238/10000 [==========================>...] - ETA: 5:57 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9239/10000 [==========================>...] - ETA: 5:57 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9240/10000 [==========================>...] - ETA: 5:56 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9241/10000 [==========================>...] - ETA: 5:56 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9242/10000 [==========================>...] - ETA: 5:55 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9243/10000 [==========================>...] - ETA: 5:55 - loss: 0.6780 - regression_loss: 0.5451 - classification_loss: 0.1330
 9244/10000 [==========================>...] - ETA: 5:55 - loss: 0.6780 - regression_loss: 0.5451 - classification_loss: 0.1330
 9245/10000 [==========================>...] - ETA: 5:54 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9246/10000 [==========================>...] - ETA: 5:54 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9247/10000 [==========================>...] - ETA: 5:53 - loss: 0.6780 - regression_loss: 0.5451 - classification_loss: 0.1329
 9248/10000 [==========================>...] - ETA: 5:53 - loss: 0.6780 - regression_loss: 0.5451 - classification_loss: 0.1329
 9249/10000 [==========================>...] - ETA: 5:52 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1329
 9250/10000 [==========================>...] - ETA: 5:52 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1329
 9251/10000 [==========================>...] - ETA: 5:51 - loss: 0.6780 - regression_loss: 0.5451 - classification_loss: 0.1329
 9252/10000 [==========================>...] - ETA: 5:51 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1329
 9253/10000 [==========================>...] - ETA: 5:50 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1329
 9254/10000 [==========================>...] - ETA: 5:50 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1329
 9255/10000 [==========================>...] - ETA: 5:49 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1329
 9256/10000 [==========================>...] - ETA: 5:49 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9257/10000 [==========================>...] - ETA: 5:48 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9258/10000 [==========================>...] - ETA: 5:48 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9259/10000 [==========================>...] - ETA: 5:47 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9260/10000 [==========================>...] - ETA: 5:47 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9261/10000 [==========================>...] - ETA: 5:46 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9262/10000 [==========================>...] - ETA: 5:46 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9263/10000 [==========================>...] - ETA: 5:46 - loss: 0.6778 - regression_loss: 0.5448 - classification_loss: 0.1329
 9264/10000 [==========================>...] - ETA: 5:45 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9265/10000 [==========================>...] - ETA: 5:45 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9266/10000 [==========================>...] - ETA: 5:44 - loss: 0.6779 - regression_loss: 0.5449 - classification_loss: 0.1329
 9267/10000 [==========================>...] - ETA: 5:44 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9268/10000 [==========================>...] - ETA: 5:43 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1329
 9269/10000 [==========================>...] - ETA: 5:43 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1329
 9270/10000 [==========================>...] - ETA: 5:42 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1329
 9271/10000 [==========================>...] - ETA: 5:42 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1329
 9272/10000 [==========================>...] - ETA: 5:41 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1330
 9273/10000 [==========================>...] - ETA: 5:41 - loss: 0.6779 - regression_loss: 0.5449 - classification_loss: 0.1330
 9274/10000 [==========================>...] - ETA: 5:40 - loss: 0.6779 - regression_loss: 0.5449 - classification_loss: 0.1329
 9275/10000 [==========================>...] - ETA: 5:40 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9276/10000 [==========================>...] - ETA: 5:39 - loss: 0.6778 - regression_loss: 0.5448 - classification_loss: 0.1329
 9277/10000 [==========================>...] - ETA: 5:39 - loss: 0.6778 - regression_loss: 0.5448 - classification_loss: 0.1329
 9278/10000 [==========================>...] - ETA: 5:38 - loss: 0.6778 - regression_loss: 0.5448 - classification_loss: 0.1329
 9279/10000 [==========================>...] - ETA: 5:38 - loss: 0.6778 - regression_loss: 0.5448 - classification_loss: 0.1329
 9280/10000 [==========================>...] - ETA: 5:38 - loss: 0.6778 - regression_loss: 0.5448 - classification_loss: 0.1329
 9281/10000 [==========================>...] - ETA: 5:37 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9282/10000 [==========================>...] - ETA: 5:37 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9283/10000 [==========================>...] - ETA: 5:36 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9284/10000 [==========================>...] - ETA: 5:36 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9285/10000 [==========================>...] - ETA: 5:35 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9286/10000 [==========================>...] - ETA: 5:35 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9287/10000 [==========================>...] - ETA: 5:34 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9288/10000 [==========================>...] - ETA: 5:34 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9289/10000 [==========================>...] - ETA: 5:33 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9290/10000 [==========================>...] - ETA: 5:33 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9291/10000 [==========================>...] - ETA: 5:32 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9292/10000 [==========================>...] - ETA: 5:32 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9293/10000 [==========================>...] - ETA: 5:31 - loss: 0.6778 - regression_loss: 0.5448 - classification_loss: 0.1329
 9294/10000 [==========================>...] - ETA: 5:31 - loss: 0.6778 - regression_loss: 0.5448 - classification_loss: 0.1329
 9295/10000 [==========================>...] - ETA: 5:30 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9296/10000 [==========================>...] - ETA: 5:30 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1329
 9297/10000 [==========================>...] - ETA: 5:30 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1329
 9298/10000 [==========================>...] - ETA: 5:29 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1329
 9299/10000 [==========================>...] - ETA: 5:29 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1329
 9300/10000 [==========================>...] - ETA: 5:28 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1329
 9301/10000 [==========================>...] - ETA: 5:28 - loss: 0.6779 - regression_loss: 0.5449 - classification_loss: 0.1329
 9302/10000 [==========================>...] - ETA: 5:27 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9303/10000 [==========================>...] - ETA: 5:27 - loss: 0.6779 - regression_loss: 0.5449 - classification_loss: 0.1329
 9304/10000 [==========================>...] - ETA: 5:26 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9305/10000 [==========================>...] - ETA: 5:26 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9306/10000 [==========================>...] - ETA: 5:25 - loss: 0.6779 - regression_loss: 0.5449 - classification_loss: 0.1329
 9307/10000 [==========================>...] - ETA: 5:25 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1329
 9308/10000 [==========================>...] - ETA: 5:24 - loss: 0.6779 - regression_loss: 0.5449 - classification_loss: 0.1329
 9309/10000 [==========================>...] - ETA: 5:24 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9310/10000 [==========================>...] - ETA: 5:23 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9311/10000 [==========================>...] - ETA: 5:23 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9312/10000 [==========================>...] - ETA: 5:22 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9313/10000 [==========================>...] - ETA: 5:22 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9314/10000 [==========================>...] - ETA: 5:22 - loss: 0.6777 - regression_loss: 0.5449 - classification_loss: 0.1329
 9315/10000 [==========================>...] - ETA: 5:21 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9316/10000 [==========================>...] - ETA: 5:21 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9317/10000 [==========================>...] - ETA: 5:20 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9318/10000 [==========================>...] - ETA: 5:20 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9319/10000 [==========================>...] - ETA: 5:19 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9320/10000 [==========================>...] - ETA: 5:19 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9321/10000 [==========================>...] - ETA: 5:18 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9322/10000 [==========================>...] - ETA: 5:18 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9323/10000 [==========================>...] - ETA: 5:17 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9324/10000 [==========================>...] - ETA: 5:17 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9325/10000 [==========================>...] - ETA: 5:16 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9326/10000 [==========================>...] - ETA: 5:16 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9327/10000 [==========================>...] - ETA: 5:15 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9328/10000 [==========================>...] - ETA: 5:15 - loss: 0.6778 - regression_loss: 0.5448 - classification_loss: 0.1329
 9329/10000 [==========================>...] - ETA: 5:14 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9330/10000 [==========================>...] - ETA: 5:14 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9331/10000 [==========================>...] - ETA: 5:14 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9332/10000 [==========================>...] - ETA: 5:13 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9333/10000 [==========================>...] - ETA: 5:13 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9334/10000 [===========================>..] - ETA: 5:12 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9335/10000 [===========================>..] - ETA: 5:12 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9336/10000 [===========================>..] - ETA: 5:11 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9337/10000 [===========================>..] - ETA: 5:11 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9338/10000 [===========================>..] - ETA: 5:10 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9339/10000 [===========================>..] - ETA: 5:10 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9340/10000 [===========================>..] - ETA: 5:09 - loss: 0.6777 - regression_loss: 0.5447 - classification_loss: 0.1329
 9341/10000 [===========================>..] - ETA: 5:09 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9342/10000 [===========================>..] - ETA: 5:08 - loss: 0.6778 - regression_loss: 0.5448 - classification_loss: 0.1329
 9343/10000 [===========================>..] - ETA: 5:08 - loss: 0.6778 - regression_loss: 0.5448 - classification_loss: 0.1329
 9344/10000 [===========================>..] - ETA: 5:07 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9345/10000 [===========================>..] - ETA: 5:07 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9346/10000 [===========================>..] - ETA: 5:07 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9347/10000 [===========================>..] - ETA: 5:06 - loss: 0.6778 - regression_loss: 0.5448 - classification_loss: 0.1329
 9348/10000 [===========================>..] - ETA: 5:06 - loss: 0.6778 - regression_loss: 0.5448 - classification_loss: 0.1329
 9349/10000 [===========================>..] - ETA: 5:05 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9350/10000 [===========================>..] - ETA: 5:05 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9351/10000 [===========================>..] - ETA: 5:04 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9352/10000 [===========================>..] - ETA: 5:04 - loss: 0.6778 - regression_loss: 0.5448 - classification_loss: 0.1329
 9353/10000 [===========================>..] - ETA: 5:03 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9354/10000 [===========================>..] - ETA: 5:03 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9355/10000 [===========================>..] - ETA: 5:02 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9356/10000 [===========================>..] - ETA: 5:02 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9357/10000 [===========================>..] - ETA: 5:01 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9358/10000 [===========================>..] - ETA: 5:01 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9359/10000 [===========================>..] - ETA: 5:00 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9360/10000 [===========================>..] - ETA: 5:00 - loss: 0.6777 - regression_loss: 0.5448 - classification_loss: 0.1329
 9361/10000 [===========================>..] - ETA: 4:59 - loss: 0.6778 - regression_loss: 0.5449 - classification_loss: 0.1329
 9362/10000 [===========================>..] - ETA: 4:59 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1329
 9363/10000 [===========================>..] - ETA: 4:59 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1329
 9364/10000 [===========================>..] - ETA: 4:58 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1329
 9365/10000 [===========================>..] - ETA: 4:58 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1329
 9366/10000 [===========================>..] - ETA: 4:57 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1329
 9367/10000 [===========================>..] - ETA: 4:57 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1329
 9368/10000 [===========================>..] - ETA: 4:56 - loss: 0.6778 - regression_loss: 0.5450 - classification_loss: 0.1329
 9369/10000 [===========================>..] - ETA: 4:56 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1329
 9370/10000 [===========================>..] - ETA: 4:55 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1329
 9371/10000 [===========================>..] - ETA: 4:55 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1329
 9372/10000 [===========================>..] - ETA: 4:54 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1329
 9373/10000 [===========================>..] - ETA: 4:54 - loss: 0.6779 - regression_loss: 0.5450 - classification_loss: 0.1329
 9374/10000 [===========================>..] - ETA: 4:53 - loss: 0.6778 - regression_loss: 0.5450 - classification_loss: 0.1329
 9375/10000 [===========================>..] - ETA: 4:53 - loss: 0.6780 - regression_loss: 0.5451 - classification_loss: 0.1329
 9376/10000 [===========================>..] - ETA: 4:52 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9377/10000 [===========================>..] - ETA: 4:52 - loss: 0.6781 - regression_loss: 0.5452 - classification_loss: 0.1329
 9378/10000 [===========================>..] - ETA: 4:51 - loss: 0.6781 - regression_loss: 0.5452 - classification_loss: 0.1329
 9379/10000 [===========================>..] - ETA: 4:51 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1329
 9380/10000 [===========================>..] - ETA: 4:51 - loss: 0.6781 - regression_loss: 0.5452 - classification_loss: 0.1329
 9381/10000 [===========================>..] - ETA: 4:50 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1329
 9382/10000 [===========================>..] - ETA: 4:50 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9383/10000 [===========================>..] - ETA: 4:49 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9384/10000 [===========================>..] - ETA: 4:49 - loss: 0.6781 - regression_loss: 0.5452 - classification_loss: 0.1329
 9385/10000 [===========================>..] - ETA: 4:48 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1329
 9386/10000 [===========================>..] - ETA: 4:48 - loss: 0.6781 - regression_loss: 0.5452 - classification_loss: 0.1330
 9387/10000 [===========================>..] - ETA: 4:47 - loss: 0.6781 - regression_loss: 0.5452 - classification_loss: 0.1329
 9388/10000 [===========================>..] - ETA: 4:47 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1329
 9389/10000 [===========================>..] - ETA: 4:46 - loss: 0.6781 - regression_loss: 0.5452 - classification_loss: 0.1329
 9390/10000 [===========================>..] - ETA: 4:46 - loss: 0.6781 - regression_loss: 0.5452 - classification_loss: 0.1329
 9391/10000 [===========================>..] - ETA: 4:45 - loss: 0.6780 - regression_loss: 0.5451 - classification_loss: 0.1329
 9392/10000 [===========================>..] - ETA: 4:45 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1329
 9393/10000 [===========================>..] - ETA: 4:44 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1329
 9394/10000 [===========================>..] - ETA: 4:44 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9395/10000 [===========================>..] - ETA: 4:43 - loss: 0.6782 - regression_loss: 0.5453 - classification_loss: 0.1330
 9396/10000 [===========================>..] - ETA: 4:43 - loss: 0.6782 - regression_loss: 0.5453 - classification_loss: 0.1329
 9397/10000 [===========================>..] - ETA: 4:43 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9398/10000 [===========================>..] - ETA: 4:42 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9399/10000 [===========================>..] - ETA: 4:42 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9400/10000 [===========================>..] - ETA: 4:41 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9401/10000 [===========================>..] - ETA: 4:41 - loss: 0.6782 - regression_loss: 0.5453 - classification_loss: 0.1330
 9402/10000 [===========================>..] - ETA: 4:40 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9403/10000 [===========================>..] - ETA: 4:40 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9404/10000 [===========================>..] - ETA: 4:39 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9405/10000 [===========================>..] - ETA: 4:39 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9406/10000 [===========================>..] - ETA: 4:38 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9407/10000 [===========================>..] - ETA: 4:38 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9408/10000 [===========================>..] - ETA: 4:37 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9409/10000 [===========================>..] - ETA: 4:37 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9410/10000 [===========================>..] - ETA: 4:36 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9411/10000 [===========================>..] - ETA: 4:36 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9412/10000 [===========================>..] - ETA: 4:35 - loss: 0.6781 - regression_loss: 0.5452 - classification_loss: 0.1330
 9413/10000 [===========================>..] - ETA: 4:35 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9414/10000 [===========================>..] - ETA: 4:35 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9415/10000 [===========================>..] - ETA: 4:34 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9416/10000 [===========================>..] - ETA: 4:34 - loss: 0.6781 - regression_loss: 0.5452 - classification_loss: 0.1330
 9417/10000 [===========================>..] - ETA: 4:33 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9418/10000 [===========================>..] - ETA: 4:33 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9419/10000 [===========================>..] - ETA: 4:32 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9420/10000 [===========================>..] - ETA: 4:32 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9421/10000 [===========================>..] - ETA: 4:31 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9422/10000 [===========================>..] - ETA: 4:31 - loss: 0.6783 - regression_loss: 0.5452 - classification_loss: 0.1330
 9423/10000 [===========================>..] - ETA: 4:30 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9424/10000 [===========================>..] - ETA: 4:30 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9425/10000 [===========================>..] - ETA: 4:29 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9426/10000 [===========================>..] - ETA: 4:29 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9427/10000 [===========================>..] - ETA: 4:28 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9428/10000 [===========================>..] - ETA: 4:28 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9429/10000 [===========================>..] - ETA: 4:27 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9430/10000 [===========================>..] - ETA: 4:27 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9431/10000 [===========================>..] - ETA: 4:27 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9432/10000 [===========================>..] - ETA: 4:26 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9433/10000 [===========================>..] - ETA: 4:26 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9434/10000 [===========================>..] - ETA: 4:25 - loss: 0.6780 - regression_loss: 0.5451 - classification_loss: 0.1330
 9435/10000 [===========================>..] - ETA: 4:25 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9436/10000 [===========================>..] - ETA: 4:24 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 9437/10000 [===========================>..] - ETA: 4:24 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 9438/10000 [===========================>..] - ETA: 4:23 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 9439/10000 [===========================>..] - ETA: 4:23 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 9440/10000 [===========================>..] - ETA: 4:22 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 9441/10000 [===========================>..] - ETA: 4:22 - loss: 0.6780 - regression_loss: 0.5451 - classification_loss: 0.1330
 9442/10000 [===========================>..] - ETA: 4:21 - loss: 0.6780 - regression_loss: 0.5451 - classification_loss: 0.1330
 9443/10000 [===========================>..] - ETA: 4:21 - loss: 0.6780 - regression_loss: 0.5451 - classification_loss: 0.1330
 9444/10000 [===========================>..] - ETA: 4:20 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 9445/10000 [===========================>..] - ETA: 4:20 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9446/10000 [===========================>..] - ETA: 4:19 - loss: 0.6781 - regression_loss: 0.5452 - classification_loss: 0.1330
 9447/10000 [===========================>..] - ETA: 4:19 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9448/10000 [===========================>..] - ETA: 4:19 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9449/10000 [===========================>..] - ETA: 4:18 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9450/10000 [===========================>..] - ETA: 4:18 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9451/10000 [===========================>..] - ETA: 4:17 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9452/10000 [===========================>..] - ETA: 4:17 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9453/10000 [===========================>..] - ETA: 4:16 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9454/10000 [===========================>..] - ETA: 4:16 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9455/10000 [===========================>..] - ETA: 4:15 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9456/10000 [===========================>..] - ETA: 4:15 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9457/10000 [===========================>..] - ETA: 4:14 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9458/10000 [===========================>..] - ETA: 4:14 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9459/10000 [===========================>..] - ETA: 4:13 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9460/10000 [===========================>..] - ETA: 4:13 - loss: 0.6782 - regression_loss: 0.5453 - classification_loss: 0.1330
 9461/10000 [===========================>..] - ETA: 4:12 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9462/10000 [===========================>..] - ETA: 4:12 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9463/10000 [===========================>..] - ETA: 4:11 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9464/10000 [===========================>..] - ETA: 4:11 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9465/10000 [===========================>..] - ETA: 4:11 - loss: 0.6782 - regression_loss: 0.5453 - classification_loss: 0.1330
 9466/10000 [===========================>..] - ETA: 4:10 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9467/10000 [===========================>..] - ETA: 4:10 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9468/10000 [===========================>..] - ETA: 4:09 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9469/10000 [===========================>..] - ETA: 4:09 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9470/10000 [===========================>..] - ETA: 4:08 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9471/10000 [===========================>..] - ETA: 4:08 - loss: 0.6781 - regression_loss: 0.5452 - classification_loss: 0.1330
 9472/10000 [===========================>..] - ETA: 4:07 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9473/10000 [===========================>..] - ETA: 4:07 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9474/10000 [===========================>..] - ETA: 4:06 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9475/10000 [===========================>..] - ETA: 4:06 - loss: 0.6782 - regression_loss: 0.5453 - classification_loss: 0.1330
 9476/10000 [===========================>..] - ETA: 4:05 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9477/10000 [===========================>..] - ETA: 4:05 - loss: 0.6782 - regression_loss: 0.5453 - classification_loss: 0.1330
 9478/10000 [===========================>..] - ETA: 4:04 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1330
 9479/10000 [===========================>..] - ETA: 4:04 - loss: 0.6782 - regression_loss: 0.5453 - classification_loss: 0.1330
 9480/10000 [===========================>..] - ETA: 4:04 - loss: 0.6782 - regression_loss: 0.5453 - classification_loss: 0.1330
 9481/10000 [===========================>..] - ETA: 4:03 - loss: 0.6782 - regression_loss: 0.5453 - classification_loss: 0.1330
 9482/10000 [===========================>..] - ETA: 4:03 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1329
 9483/10000 [===========================>..] - ETA: 4:02 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1329
 9484/10000 [===========================>..] - ETA: 4:02 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1329
 9485/10000 [===========================>..] - ETA: 4:01 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1329
 9486/10000 [===========================>..] - ETA: 4:01 - loss: 0.6782 - regression_loss: 0.5453 - classification_loss: 0.1329
 9487/10000 [===========================>..] - ETA: 4:00 - loss: 0.6782 - regression_loss: 0.5453 - classification_loss: 0.1329
 9488/10000 [===========================>..] - ETA: 4:00 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1329
 9489/10000 [===========================>..] - ETA: 3:59 - loss: 0.6781 - regression_loss: 0.5452 - classification_loss: 0.1329
 9490/10000 [===========================>..] - ETA: 3:59 - loss: 0.6781 - regression_loss: 0.5452 - classification_loss: 0.1329
 9491/10000 [===========================>..] - ETA: 3:58 - loss: 0.6782 - regression_loss: 0.5453 - classification_loss: 0.1329
 9492/10000 [===========================>..] - ETA: 3:58 - loss: 0.6783 - regression_loss: 0.5453 - classification_loss: 0.1329
 9493/10000 [===========================>..] - ETA: 3:57 - loss: 0.6782 - regression_loss: 0.5453 - classification_loss: 0.1329
 9494/10000 [===========================>..] - ETA: 3:57 - loss: 0.6782 - regression_loss: 0.5453 - classification_loss: 0.1329
 9495/10000 [===========================>..] - ETA: 3:56 - loss: 0.6781 - regression_loss: 0.5452 - classification_loss: 0.1329
 9496/10000 [===========================>..] - ETA: 3:56 - loss: 0.6781 - regression_loss: 0.5452 - classification_loss: 0.1329
 9497/10000 [===========================>..] - ETA: 3:56 - loss: 0.6781 - regression_loss: 0.5452 - classification_loss: 0.1329
 9498/10000 [===========================>..] - ETA: 3:55 - loss: 0.6781 - regression_loss: 0.5452 - classification_loss: 0.1329
 9499/10000 [===========================>..] - ETA: 3:55 - loss: 0.6781 - regression_loss: 0.5452 - classification_loss: 0.1329
 9500/10000 [===========================>..] - ETA: 3:54 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9501/10000 [===========================>..] - ETA: 3:54 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9502/10000 [===========================>..] - ETA: 3:53 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9503/10000 [===========================>..] - ETA: 3:53 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9504/10000 [===========================>..] - ETA: 3:52 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9505/10000 [===========================>..] - ETA: 3:52 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 9506/10000 [===========================>..] - ETA: 3:51 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 9507/10000 [===========================>..] - ETA: 3:51 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 9508/10000 [===========================>..] - ETA: 3:50 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 9509/10000 [===========================>..] - ETA: 3:50 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9510/10000 [===========================>..] - ETA: 3:49 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9511/10000 [===========================>..] - ETA: 3:49 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 9512/10000 [===========================>..] - ETA: 3:48 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 9513/10000 [===========================>..] - ETA: 3:48 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 9514/10000 [===========================>..] - ETA: 3:48 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 9515/10000 [===========================>..] - ETA: 3:47 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 9516/10000 [===========================>..] - ETA: 3:47 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 9517/10000 [===========================>..] - ETA: 3:46 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 9518/10000 [===========================>..] - ETA: 3:46 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 9519/10000 [===========================>..] - ETA: 3:45 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9520/10000 [===========================>..] - ETA: 3:45 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9521/10000 [===========================>..] - ETA: 3:44 - loss: 0.6781 - regression_loss: 0.5452 - classification_loss: 0.1330
 9522/10000 [===========================>..] - ETA: 3:44 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9523/10000 [===========================>..] - ETA: 3:43 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9524/10000 [===========================>..] - ETA: 3:43 - loss: 0.6781 - regression_loss: 0.5452 - classification_loss: 0.1330
 9525/10000 [===========================>..] - ETA: 3:42 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9526/10000 [===========================>..] - ETA: 3:42 - loss: 0.6782 - regression_loss: 0.5452 - classification_loss: 0.1330
 9527/10000 [===========================>..] - ETA: 3:41 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9528/10000 [===========================>..] - ETA: 3:41 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9529/10000 [===========================>..] - ETA: 3:40 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1330
 9530/10000 [===========================>..] - ETA: 3:40 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9531/10000 [===========================>..] - ETA: 3:40 - loss: 0.6782 - regression_loss: 0.5451 - classification_loss: 0.1330
 9532/10000 [===========================>..] - ETA: 3:39 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9533/10000 [===========================>..] - ETA: 3:39 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9534/10000 [===========================>..] - ETA: 3:38 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9535/10000 [===========================>..] - ETA: 3:38 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 9536/10000 [===========================>..] - ETA: 3:37 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9537/10000 [===========================>..] - ETA: 3:37 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9538/10000 [===========================>..] - ETA: 3:36 - loss: 0.6781 - regression_loss: 0.5451 - classification_loss: 0.1330
 9539/10000 [===========================>..] - ETA: 3:36 - loss: 0.6781 - regression_loss: 0.5450 - classification_loss: 0.1330
 9540/10000 [===========================>..] - ETA: 3:35 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 9541/10000 [===========================>..] - ETA: 3:35 - loss: 0.6780 - regression_loss: 0.5450 - classification_loss: 0.1330
 9542/10000 [===========================>..] - ETA: 3:34 - loss: 0.6779 - regression_loss: 0.5449 - classification_loss: 0.1330
 9543/10000 [===========================>..] - ETA: 3:34 - loss: 0.6779 - regression_loss: 0.5449 - classification_loss: 0.1330
 9544/10000 [===========================>..] - ETA: 3:33 - loss: 0.6779 - regression_loss: 0.5449 - classification_loss: 0.1330
 9545/10000 [===========================>..] - ETA: 3:33 - loss: 0.6779 - regression_loss: 0.5449 - classification_loss: 0.1330
 9546/10000 [===========================>..] - ETA: 3:32 - loss: 0.6779 - regression_loss: 0.5449 - classification_loss: 0.1330
 9547/10000 [===========================>..] - ETA: 3:32 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9548/10000 [===========================>..] - ETA: 3:32 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9549/10000 [===========================>..] - ETA: 3:31 - loss: 0.6779 - regression_loss: 0.5449 - classification_loss: 0.1331
 9550/10000 [===========================>..] - ETA: 3:31 - loss: 0.6779 - regression_loss: 0.5449 - classification_loss: 0.1330
 9551/10000 [===========================>..] - ETA: 3:30 - loss: 0.6779 - regression_loss: 0.5449 - classification_loss: 0.1331
 9552/10000 [===========================>..] - ETA: 3:30 - loss: 0.6779 - regression_loss: 0.5448 - classification_loss: 0.1331
 9553/10000 [===========================>..] - ETA: 3:29 - loss: 0.6778 - regression_loss: 0.5448 - classification_loss: 0.1331
 9554/10000 [===========================>..] - ETA: 3:29 - loss: 0.6779 - regression_loss: 0.5448 - classification_loss: 0.1331
 9555/10000 [===========================>..] - ETA: 3:28 - loss: 0.6779 - regression_loss: 0.5449 - classification_loss: 0.1331
 9556/10000 [===========================>..] - ETA: 3:28 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9557/10000 [===========================>..] - ETA: 3:27 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9558/10000 [===========================>..] - ETA: 3:27 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9559/10000 [===========================>..] - ETA: 3:26 - loss: 0.6781 - regression_loss: 0.5450 - classification_loss: 0.1331
 9560/10000 [===========================>..] - ETA: 3:26 - loss: 0.6781 - regression_loss: 0.5450 - classification_loss: 0.1331
 9561/10000 [===========================>..] - ETA: 3:25 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9562/10000 [===========================>..] - ETA: 3:25 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9563/10000 [===========================>..] - ETA: 3:25 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9564/10000 [===========================>..] - ETA: 3:24 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9565/10000 [===========================>..] - ETA: 3:24 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9566/10000 [===========================>..] - ETA: 3:23 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9567/10000 [===========================>..] - ETA: 3:23 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9568/10000 [===========================>..] - ETA: 3:22 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9569/10000 [===========================>..] - ETA: 3:22 - loss: 0.6779 - regression_loss: 0.5448 - classification_loss: 0.1331
 9570/10000 [===========================>..] - ETA: 3:21 - loss: 0.6779 - regression_loss: 0.5448 - classification_loss: 0.1331
 9571/10000 [===========================>..] - ETA: 3:21 - loss: 0.6779 - regression_loss: 0.5448 - classification_loss: 0.1331
 9572/10000 [===========================>..] - ETA: 3:20 - loss: 0.6779 - regression_loss: 0.5448 - classification_loss: 0.1331
 9573/10000 [===========================>..] - ETA: 3:20 - loss: 0.6779 - regression_loss: 0.5448 - classification_loss: 0.1331
 9574/10000 [===========================>..] - ETA: 3:19 - loss: 0.6779 - regression_loss: 0.5448 - classification_loss: 0.1331
 9575/10000 [===========================>..] - ETA: 3:19 - loss: 0.6778 - regression_loss: 0.5447 - classification_loss: 0.1331
 9576/10000 [===========================>..] - ETA: 3:18 - loss: 0.6778 - regression_loss: 0.5448 - classification_loss: 0.1331
 9577/10000 [===========================>..] - ETA: 3:18 - loss: 0.6779 - regression_loss: 0.5448 - classification_loss: 0.1331
 9578/10000 [===========================>..] - ETA: 3:17 - loss: 0.6779 - regression_loss: 0.5448 - classification_loss: 0.1331
 9579/10000 [===========================>..] - ETA: 3:17 - loss: 0.6779 - regression_loss: 0.5449 - classification_loss: 0.1331
 9580/10000 [===========================>..] - ETA: 3:17 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9581/10000 [===========================>..] - ETA: 3:16 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9582/10000 [===========================>..] - ETA: 3:16 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9583/10000 [===========================>..] - ETA: 3:15 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9584/10000 [===========================>..] - ETA: 3:15 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9585/10000 [===========================>..] - ETA: 3:14 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9586/10000 [===========================>..] - ETA: 3:14 - loss: 0.6779 - regression_loss: 0.5449 - classification_loss: 0.1331
 9587/10000 [===========================>..] - ETA: 3:13 - loss: 0.6779 - regression_loss: 0.5448 - classification_loss: 0.1331
 9588/10000 [===========================>..] - ETA: 3:13 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9589/10000 [===========================>..] - ETA: 3:12 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9590/10000 [===========================>..] - ETA: 3:12 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9591/10000 [===========================>..] - ETA: 3:11 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9592/10000 [===========================>..] - ETA: 3:11 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9593/10000 [===========================>..] - ETA: 3:10 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9594/10000 [===========================>..] - ETA: 3:10 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9595/10000 [===========================>..] - ETA: 3:09 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9596/10000 [===========================>..] - ETA: 3:09 - loss: 0.6779 - regression_loss: 0.5449 - classification_loss: 0.1331
 9597/10000 [===========================>..] - ETA: 3:09 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9598/10000 [===========================>..] - ETA: 3:08 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9599/10000 [===========================>..] - ETA: 3:08 - loss: 0.6780 - regression_loss: 0.5449 - classification_loss: 0.1331
 9600/10000 [===========================>..] - ETA: 3:07 - loss: 0.6779 - regression_loss: 0.5449 - classification_loss: 0.1331
 9601/10000 [===========================>..] - ETA: 3:07 - loss: 0.6779 - regression_loss: 0.5448 - classification_loss: 0.1331
 9602/10000 [===========================>..] - ETA: 3:06 - loss: 0.6779 - regression_loss: 0.5448 - classification_loss: 0.1331
 9603/10000 [===========================>..] - ETA: 3:06 - loss: 0.6779 - regression_loss: 0.5448 - classification_loss: 0.1331
 9604/10000 [===========================>..] - ETA: 3:05 - loss: 0.6779 - regression_loss: 0.5448 - classification_loss: 0.1331
 9605/10000 [===========================>..] - ETA: 3:05 - loss: 0.6778 - regression_loss: 0.5448 - classification_loss: 0.1331
 9606/10000 [===========================>..] - ETA: 3:04 - loss: 0.6778 - regression_loss: 0.5447 - classification_loss: 0.1331
 9607/10000 [===========================>..] - ETA: 3:04 - loss: 0.6778 - regression_loss: 0.5447 - classification_loss: 0.1330
 9608/10000 [===========================>..] - ETA: 3:03 - loss: 0.6777 - regression_loss: 0.5447 - classification_loss: 0.1330
 9609/10000 [===========================>..] - ETA: 3:03 - loss: 0.6777 - regression_loss: 0.5447 - classification_loss: 0.1330
 9610/10000 [===========================>..] - ETA: 3:02 - loss: 0.6777 - regression_loss: 0.5446 - classification_loss: 0.1330
 9611/10000 [===========================>..] - ETA: 3:02 - loss: 0.6776 - regression_loss: 0.5446 - classification_loss: 0.1330
 9612/10000 [===========================>..] - ETA: 3:01 - loss: 0.6776 - regression_loss: 0.5446 - classification_loss: 0.1330
 9613/10000 [===========================>..] - ETA: 3:01 - loss: 0.6776 - regression_loss: 0.5446 - classification_loss: 0.1330
 9614/10000 [===========================>..] - ETA: 3:01 - loss: 0.6776 - regression_loss: 0.5446 - classification_loss: 0.1330
 9615/10000 [===========================>..] - ETA: 3:00 - loss: 0.6775 - regression_loss: 0.5445 - classification_loss: 0.1330
 9616/10000 [===========================>..] - ETA: 3:00 - loss: 0.6776 - regression_loss: 0.5446 - classification_loss: 0.1330
 9617/10000 [===========================>..] - ETA: 2:59 - loss: 0.6776 - regression_loss: 0.5446 - classification_loss: 0.1330
 9618/10000 [===========================>..] - ETA: 2:59 - loss: 0.6776 - regression_loss: 0.5446 - classification_loss: 0.1330
 9619/10000 [===========================>..] - ETA: 2:58 - loss: 0.6775 - regression_loss: 0.5445 - classification_loss: 0.1330
 9620/10000 [===========================>..] - ETA: 2:58 - loss: 0.6775 - regression_loss: 0.5445 - classification_loss: 0.1330
 9621/10000 [===========================>..] - ETA: 2:57 - loss: 0.6776 - regression_loss: 0.5446 - classification_loss: 0.1330
 9622/10000 [===========================>..] - ETA: 2:57 - loss: 0.6775 - regression_loss: 0.5445 - classification_loss: 0.1330
 9623/10000 [===========================>..] - ETA: 2:56 - loss: 0.6775 - regression_loss: 0.5445 - classification_loss: 0.1330
 9624/10000 [===========================>..] - ETA: 2:56 - loss: 0.6775 - regression_loss: 0.5445 - classification_loss: 0.1330
 9625/10000 [===========================>..] - ETA: 2:55 - loss: 0.6774 - regression_loss: 0.5444 - classification_loss: 0.1330
 9626/10000 [===========================>..] - ETA: 2:55 - loss: 0.6774 - regression_loss: 0.5444 - classification_loss: 0.1330
 9627/10000 [===========================>..] - ETA: 2:54 - loss: 0.6774 - regression_loss: 0.5444 - classification_loss: 0.1330
 9628/10000 [===========================>..] - ETA: 2:54 - loss: 0.6774 - regression_loss: 0.5444 - classification_loss: 0.1330
 9629/10000 [===========================>..] - ETA: 2:54 - loss: 0.6774 - regression_loss: 0.5444 - classification_loss: 0.1330
 9630/10000 [===========================>..] - ETA: 2:53 - loss: 0.6774 - regression_loss: 0.5444 - classification_loss: 0.1330
 9631/10000 [===========================>..] - ETA: 2:53 - loss: 0.6774 - regression_loss: 0.5444 - classification_loss: 0.1330
 9632/10000 [===========================>..] - ETA: 2:52 - loss: 0.6774 - regression_loss: 0.5444 - classification_loss: 0.1330
 9633/10000 [===========================>..] - ETA: 2:52 - loss: 0.6774 - regression_loss: 0.5444 - classification_loss: 0.1330
 9634/10000 [===========================>..] - ETA: 2:51 - loss: 0.6774 - regression_loss: 0.5444 - classification_loss: 0.1330
 9635/10000 [===========================>..] - ETA: 2:51 - loss: 0.6773 - regression_loss: 0.5444 - classification_loss: 0.1330
 9636/10000 [===========================>..] - ETA: 2:50 - loss: 0.6773 - regression_loss: 0.5443 - classification_loss: 0.1330
 9637/10000 [===========================>..] - ETA: 2:50 - loss: 0.6773 - regression_loss: 0.5443 - classification_loss: 0.1330
 9638/10000 [===========================>..] - ETA: 2:49 - loss: 0.6773 - regression_loss: 0.5443 - classification_loss: 0.1330
 9639/10000 [===========================>..] - ETA: 2:49 - loss: 0.6774 - regression_loss: 0.5444 - classification_loss: 0.1330
 9640/10000 [===========================>..] - ETA: 2:48 - loss: 0.6774 - regression_loss: 0.5444 - classification_loss: 0.1330
 9641/10000 [===========================>..] - ETA: 2:48 - loss: 0.6774 - regression_loss: 0.5444 - classification_loss: 0.1330
 9642/10000 [===========================>..] - ETA: 2:47 - loss: 0.6774 - regression_loss: 0.5444 - classification_loss: 0.1330
 9643/10000 [===========================>..] - ETA: 2:47 - loss: 0.6774 - regression_loss: 0.5444 - classification_loss: 0.1330
 9644/10000 [===========================>..] - ETA: 2:46 - loss: 0.6774 - regression_loss: 0.5444 - classification_loss: 0.1330
 9645/10000 [===========================>..] - ETA: 2:46 - loss: 0.6774 - regression_loss: 0.5444 - classification_loss: 0.1330
 9646/10000 [===========================>..] - ETA: 2:46 - loss: 0.6775 - regression_loss: 0.5445 - classification_loss: 0.1330
 9647/10000 [===========================>..] - ETA: 2:45 - loss: 0.6775 - regression_loss: 0.5445 - classification_loss: 0.1330
 9648/10000 [===========================>..] - ETA: 2:45 - loss: 0.6776 - regression_loss: 0.5445 - classification_loss: 0.1330
 9649/10000 [===========================>..] - ETA: 2:44 - loss: 0.6776 - regression_loss: 0.5445 - classification_loss: 0.1330
 9650/10000 [===========================>..] - ETA: 2:44 - loss: 0.6775 - regression_loss: 0.5445 - classification_loss: 0.1330
 9651/10000 [===========================>..] - ETA: 2:43 - loss: 0.6775 - regression_loss: 0.5445 - classification_loss: 0.1330
 9652/10000 [===========================>..] - ETA: 2:43 - loss: 0.6774 - regression_loss: 0.5444 - classification_loss: 0.1330
 9653/10000 [===========================>..] - ETA: 2:42 - loss: 0.6774 - regression_loss: 0.5445 - classification_loss: 0.1330
 9654/10000 [===========================>..] - ETA: 2:42 - loss: 0.6774 - regression_loss: 0.5444 - classification_loss: 0.1330
 9655/10000 [===========================>..] - ETA: 2:41 - loss: 0.6774 - regression_loss: 0.5445 - classification_loss: 0.1330
 9656/10000 [===========================>..] - ETA: 2:41 - loss: 0.6774 - regression_loss: 0.5444 - classification_loss: 0.1330
 9657/10000 [===========================>..] - ETA: 2:40 - loss: 0.6774 - regression_loss: 0.5444 - classification_loss: 0.1330
 9658/10000 [===========================>..] - ETA: 2:40 - loss: 0.6774 - regression_loss: 0.5444 - classification_loss: 0.1330
 9659/10000 [===========================>..] - ETA: 2:39 - loss: 0.6774 - regression_loss: 0.5444 - classification_loss: 0.1330
 9660/10000 [===========================>..] - ETA: 2:39 - loss: 0.6773 - regression_loss: 0.5444 - classification_loss: 0.1330
 9661/10000 [===========================>..] - ETA: 2:38 - loss: 0.6774 - regression_loss: 0.5444 - classification_loss: 0.1330
 9662/10000 [===========================>..] - ETA: 2:38 - loss: 0.6773 - regression_loss: 0.5444 - classification_loss: 0.1330
 9663/10000 [===========================>..] - ETA: 2:38 - loss: 0.6773 - regression_loss: 0.5443 - classification_loss: 0.1329
 9664/10000 [===========================>..] - ETA: 2:37 - loss: 0.6772 - regression_loss: 0.5443 - classification_loss: 0.1329
 9665/10000 [===========================>..] - ETA: 2:37 - loss: 0.6772 - regression_loss: 0.5443 - classification_loss: 0.1329
 9666/10000 [===========================>..] - ETA: 2:36 - loss: 0.6772 - regression_loss: 0.5443 - classification_loss: 0.1329
 9667/10000 [============================>.] - ETA: 2:36 - loss: 0.6771 - regression_loss: 0.5442 - classification_loss: 0.1329
 9668/10000 [============================>.] - ETA: 2:35 - loss: 0.6771 - regression_loss: 0.5442 - classification_loss: 0.1329
 9669/10000 [============================>.] - ETA: 2:35 - loss: 0.6771 - regression_loss: 0.5442 - classification_loss: 0.1329
 9670/10000 [============================>.] - ETA: 2:34 - loss: 0.6771 - regression_loss: 0.5442 - classification_loss: 0.1329
 9671/10000 [============================>.] - ETA: 2:34 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9672/10000 [============================>.] - ETA: 2:33 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9673/10000 [============================>.] - ETA: 2:33 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9674/10000 [============================>.] - ETA: 2:32 - loss: 0.6771 - regression_loss: 0.5441 - classification_loss: 0.1329
 9675/10000 [============================>.] - ETA: 2:32 - loss: 0.6771 - regression_loss: 0.5442 - classification_loss: 0.1330
 9676/10000 [============================>.] - ETA: 2:31 - loss: 0.6771 - regression_loss: 0.5441 - classification_loss: 0.1329
 9677/10000 [============================>.] - ETA: 2:31 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9678/10000 [============================>.] - ETA: 2:31 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9679/10000 [============================>.] - ETA: 2:30 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9680/10000 [============================>.] - ETA: 2:30 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9681/10000 [============================>.] - ETA: 2:29 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9682/10000 [============================>.] - ETA: 2:29 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9683/10000 [============================>.] - ETA: 2:28 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1329
 9684/10000 [============================>.] - ETA: 2:28 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9685/10000 [============================>.] - ETA: 2:27 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9686/10000 [============================>.] - ETA: 2:27 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9687/10000 [============================>.] - ETA: 2:26 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9688/10000 [============================>.] - ETA: 2:26 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9689/10000 [============================>.] - ETA: 2:25 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9690/10000 [============================>.] - ETA: 2:25 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9691/10000 [============================>.] - ETA: 2:24 - loss: 0.6768 - regression_loss: 0.5439 - classification_loss: 0.1329
 9692/10000 [============================>.] - ETA: 2:24 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9693/10000 [============================>.] - ETA: 2:23 - loss: 0.6768 - regression_loss: 0.5440 - classification_loss: 0.1329
 9694/10000 [============================>.] - ETA: 2:23 - loss: 0.6768 - regression_loss: 0.5439 - classification_loss: 0.1329
 9695/10000 [============================>.] - ETA: 2:23 - loss: 0.6769 - regression_loss: 0.5439 - classification_loss: 0.1329
 9696/10000 [============================>.] - ETA: 2:22 - loss: 0.6768 - regression_loss: 0.5439 - classification_loss: 0.1329
 9697/10000 [============================>.] - ETA: 2:22 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9698/10000 [============================>.] - ETA: 2:21 - loss: 0.6769 - regression_loss: 0.5439 - classification_loss: 0.1329
 9699/10000 [============================>.] - ETA: 2:21 - loss: 0.6769 - regression_loss: 0.5439 - classification_loss: 0.1329
 9700/10000 [============================>.] - ETA: 2:20 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9701/10000 [============================>.] - ETA: 2:20 - loss: 0.6768 - regression_loss: 0.5439 - classification_loss: 0.1329
 9702/10000 [============================>.] - ETA: 2:19 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9703/10000 [============================>.] - ETA: 2:19 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9704/10000 [============================>.] - ETA: 2:18 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9705/10000 [============================>.] - ETA: 2:18 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9706/10000 [============================>.] - ETA: 2:17 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9707/10000 [============================>.] - ETA: 2:17 - loss: 0.6769 - regression_loss: 0.5439 - classification_loss: 0.1329
 9708/10000 [============================>.] - ETA: 2:16 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9709/10000 [============================>.] - ETA: 2:16 - loss: 0.6768 - regression_loss: 0.5439 - classification_loss: 0.1329
 9710/10000 [============================>.] - ETA: 2:15 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9711/10000 [============================>.] - ETA: 2:15 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9712/10000 [============================>.] - ETA: 2:15 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9713/10000 [============================>.] - ETA: 2:14 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9714/10000 [============================>.] - ETA: 2:14 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9715/10000 [============================>.] - ETA: 2:13 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9716/10000 [============================>.] - ETA: 2:13 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9717/10000 [============================>.] - ETA: 2:12 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9718/10000 [============================>.] - ETA: 2:12 - loss: 0.6768 - regression_loss: 0.5439 - classification_loss: 0.1329
 9719/10000 [============================>.] - ETA: 2:11 - loss: 0.6768 - regression_loss: 0.5439 - classification_loss: 0.1329
 9720/10000 [============================>.] - ETA: 2:11 - loss: 0.6768 - regression_loss: 0.5439 - classification_loss: 0.1329
 9721/10000 [============================>.] - ETA: 2:10 - loss: 0.6768 - regression_loss: 0.5439 - classification_loss: 0.1329
 9722/10000 [============================>.] - ETA: 2:10 - loss: 0.6768 - regression_loss: 0.5439 - classification_loss: 0.1329
 9723/10000 [============================>.] - ETA: 2:09 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9724/10000 [============================>.] - ETA: 2:09 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9725/10000 [============================>.] - ETA: 2:08 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9726/10000 [============================>.] - ETA: 2:08 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9727/10000 [============================>.] - ETA: 2:08 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9728/10000 [============================>.] - ETA: 2:07 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9729/10000 [============================>.] - ETA: 2:07 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9730/10000 [============================>.] - ETA: 2:06 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9731/10000 [============================>.] - ETA: 2:06 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9732/10000 [============================>.] - ETA: 2:05 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9733/10000 [============================>.] - ETA: 2:05 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9734/10000 [============================>.] - ETA: 2:04 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9735/10000 [============================>.] - ETA: 2:04 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9736/10000 [============================>.] - ETA: 2:03 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9737/10000 [============================>.] - ETA: 2:03 - loss: 0.6769 - regression_loss: 0.5441 - classification_loss: 0.1329
 9738/10000 [============================>.] - ETA: 2:02 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9739/10000 [============================>.] - ETA: 2:02 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9740/10000 [============================>.] - ETA: 2:01 - loss: 0.6769 - regression_loss: 0.5441 - classification_loss: 0.1329
 9741/10000 [============================>.] - ETA: 2:01 - loss: 0.6769 - regression_loss: 0.5441 - classification_loss: 0.1329
 9742/10000 [============================>.] - ETA: 2:00 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9743/10000 [============================>.] - ETA: 2:00 - loss: 0.6771 - regression_loss: 0.5442 - classification_loss: 0.1329
 9744/10000 [============================>.] - ETA: 2:00 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9745/10000 [============================>.] - ETA: 1:59 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9746/10000 [============================>.] - ETA: 1:59 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9747/10000 [============================>.] - ETA: 1:58 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9748/10000 [============================>.] - ETA: 1:58 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9749/10000 [============================>.] - ETA: 1:57 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9750/10000 [============================>.] - ETA: 1:57 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9751/10000 [============================>.] - ETA: 1:56 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9752/10000 [============================>.] - ETA: 1:56 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9753/10000 [============================>.] - ETA: 1:55 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9754/10000 [============================>.] - ETA: 1:55 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9755/10000 [============================>.] - ETA: 1:54 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9756/10000 [============================>.] - ETA: 1:54 - loss: 0.6768 - regression_loss: 0.5440 - classification_loss: 0.1329
 9757/10000 [============================>.] - ETA: 1:53 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9758/10000 [============================>.] - ETA: 1:53 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9759/10000 [============================>.] - ETA: 1:53 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1329
 9760/10000 [============================>.] - ETA: 1:52 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9761/10000 [============================>.] - ETA: 1:52 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9762/10000 [============================>.] - ETA: 1:51 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1329
 9763/10000 [============================>.] - ETA: 1:51 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1329
 9764/10000 [============================>.] - ETA: 1:50 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9765/10000 [============================>.] - ETA: 1:50 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9766/10000 [============================>.] - ETA: 1:49 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9767/10000 [============================>.] - ETA: 1:49 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1329
 9768/10000 [============================>.] - ETA: 1:48 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9769/10000 [============================>.] - ETA: 1:48 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1329
 9770/10000 [============================>.] - ETA: 1:47 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9771/10000 [============================>.] - ETA: 1:47 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1329
 9772/10000 [============================>.] - ETA: 1:46 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9773/10000 [============================>.] - ETA: 1:46 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1329
 9774/10000 [============================>.] - ETA: 1:45 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9775/10000 [============================>.] - ETA: 1:45 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9776/10000 [============================>.] - ETA: 1:45 - loss: 0.6768 - regression_loss: 0.5439 - classification_loss: 0.1329
 9777/10000 [============================>.] - ETA: 1:44 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1329
 9778/10000 [============================>.] - ETA: 1:44 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1329
 9779/10000 [============================>.] - ETA: 1:43 - loss: 0.6771 - regression_loss: 0.5441 - classification_loss: 0.1330
 9780/10000 [============================>.] - ETA: 1:43 - loss: 0.6771 - regression_loss: 0.5441 - classification_loss: 0.1330
 9781/10000 [============================>.] - ETA: 1:42 - loss: 0.6771 - regression_loss: 0.5441 - classification_loss: 0.1330
 9782/10000 [============================>.] - ETA: 1:42 - loss: 0.6771 - regression_loss: 0.5441 - classification_loss: 0.1330
 9783/10000 [============================>.] - ETA: 1:41 - loss: 0.6771 - regression_loss: 0.5441 - classification_loss: 0.1330
 9784/10000 [============================>.] - ETA: 1:41 - loss: 0.6771 - regression_loss: 0.5441 - classification_loss: 0.1330
 9785/10000 [============================>.] - ETA: 1:40 - loss: 0.6771 - regression_loss: 0.5441 - classification_loss: 0.1330
 9786/10000 [============================>.] - ETA: 1:40 - loss: 0.6771 - regression_loss: 0.5441 - classification_loss: 0.1330
 9787/10000 [============================>.] - ETA: 1:39 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1330
 9788/10000 [============================>.] - ETA: 1:39 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1330
 9789/10000 [============================>.] - ETA: 1:38 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1330
 9790/10000 [============================>.] - ETA: 1:38 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1330
 9791/10000 [============================>.] - ETA: 1:37 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1330
 9792/10000 [============================>.] - ETA: 1:37 - loss: 0.6770 - regression_loss: 0.5441 - classification_loss: 0.1330
 9793/10000 [============================>.] - ETA: 1:37 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1330
 9794/10000 [============================>.] - ETA: 1:36 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1330
 9795/10000 [============================>.] - ETA: 1:36 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1330
 9796/10000 [============================>.] - ETA: 1:35 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1330
 9797/10000 [============================>.] - ETA: 1:35 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1330
 9798/10000 [============================>.] - ETA: 1:34 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1330
 9799/10000 [============================>.] - ETA: 1:34 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1330
 9800/10000 [============================>.] - ETA: 1:33 - loss: 0.6769 - regression_loss: 0.5439 - classification_loss: 0.1330
 9801/10000 [============================>.] - ETA: 1:33 - loss: 0.6769 - regression_loss: 0.5439 - classification_loss: 0.1330
 9802/10000 [============================>.] - ETA: 1:32 - loss: 0.6768 - regression_loss: 0.5439 - classification_loss: 0.1330
 9803/10000 [============================>.] - ETA: 1:32 - loss: 0.6768 - regression_loss: 0.5438 - classification_loss: 0.1330
 9804/10000 [============================>.] - ETA: 1:31 - loss: 0.6768 - regression_loss: 0.5438 - classification_loss: 0.1330
 9805/10000 [============================>.] - ETA: 1:31 - loss: 0.6767 - regression_loss: 0.5438 - classification_loss: 0.1329
 9806/10000 [============================>.] - ETA: 1:30 - loss: 0.6767 - regression_loss: 0.5437 - classification_loss: 0.1329
 9807/10000 [============================>.] - ETA: 1:30 - loss: 0.6767 - regression_loss: 0.5438 - classification_loss: 0.1329
 9808/10000 [============================>.] - ETA: 1:30 - loss: 0.6767 - regression_loss: 0.5438 - classification_loss: 0.1329
 9809/10000 [============================>.] - ETA: 1:29 - loss: 0.6767 - regression_loss: 0.5438 - classification_loss: 0.1329
 9810/10000 [============================>.] - ETA: 1:29 - loss: 0.6768 - regression_loss: 0.5438 - classification_loss: 0.1330
 9811/10000 [============================>.] - ETA: 1:28 - loss: 0.6768 - regression_loss: 0.5439 - classification_loss: 0.1330
 9812/10000 [============================>.] - ETA: 1:28 - loss: 0.6768 - regression_loss: 0.5438 - classification_loss: 0.1330
 9813/10000 [============================>.] - ETA: 1:27 - loss: 0.6768 - regression_loss: 0.5439 - classification_loss: 0.1330
 9814/10000 [============================>.] - ETA: 1:27 - loss: 0.6768 - regression_loss: 0.5438 - classification_loss: 0.1330
 9815/10000 [============================>.] - ETA: 1:26 - loss: 0.6768 - regression_loss: 0.5439 - classification_loss: 0.1330
 9816/10000 [============================>.] - ETA: 1:26 - loss: 0.6768 - regression_loss: 0.5439 - classification_loss: 0.1330
 9817/10000 [============================>.] - ETA: 1:25 - loss: 0.6768 - regression_loss: 0.5438 - classification_loss: 0.1330
 9818/10000 [============================>.] - ETA: 1:25 - loss: 0.6769 - regression_loss: 0.5439 - classification_loss: 0.1330
 9819/10000 [============================>.] - ETA: 1:24 - loss: 0.6769 - regression_loss: 0.5439 - classification_loss: 0.1330
 9820/10000 [============================>.] - ETA: 1:24 - loss: 0.6769 - regression_loss: 0.5439 - classification_loss: 0.1330
 9821/10000 [============================>.] - ETA: 1:23 - loss: 0.6769 - regression_loss: 0.5439 - classification_loss: 0.1330
 9822/10000 [============================>.] - ETA: 1:23 - loss: 0.6769 - regression_loss: 0.5439 - classification_loss: 0.1330
 9823/10000 [============================>.] - ETA: 1:22 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1330
 9824/10000 [============================>.] - ETA: 1:22 - loss: 0.6769 - regression_loss: 0.5439 - classification_loss: 0.1330
 9825/10000 [============================>.] - ETA: 1:22 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1330
 9826/10000 [============================>.] - ETA: 1:21 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1330
 9827/10000 [============================>.] - ETA: 1:21 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1330
 9828/10000 [============================>.] - ETA: 1:20 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1330
 9829/10000 [============================>.] - ETA: 1:20 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1330
 9830/10000 [============================>.] - ETA: 1:19 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1330
 9831/10000 [============================>.] - ETA: 1:19 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1330
 9832/10000 [============================>.] - ETA: 1:18 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1330
 9833/10000 [============================>.] - ETA: 1:18 - loss: 0.6769 - regression_loss: 0.5439 - classification_loss: 0.1330
 9834/10000 [============================>.] - ETA: 1:17 - loss: 0.6768 - regression_loss: 0.5439 - classification_loss: 0.1329
 9835/10000 [============================>.] - ETA: 1:17 - loss: 0.6768 - regression_loss: 0.5439 - classification_loss: 0.1329
 9836/10000 [============================>.] - ETA: 1:16 - loss: 0.6769 - regression_loss: 0.5439 - classification_loss: 0.1329
 9837/10000 [============================>.] - ETA: 1:16 - loss: 0.6769 - regression_loss: 0.5439 - classification_loss: 0.1329
 9838/10000 [============================>.] - ETA: 1:15 - loss: 0.6769 - regression_loss: 0.5439 - classification_loss: 0.1329
 9839/10000 [============================>.] - ETA: 1:15 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1330
 9840/10000 [============================>.] - ETA: 1:15 - loss: 0.6770 - regression_loss: 0.5440 - classification_loss: 0.1330
 9841/10000 [============================>.] - ETA: 1:14 - loss: 0.6769 - regression_loss: 0.5440 - classification_loss: 0.1330
 9842/10000 [============================>.] - ETA: 1:14 - loss: 0.6769 - regression_loss: 0.5439 - classification_loss: 0.1330
 9843/10000 [============================>.] - ETA: 1:13 - loss: 0.6769 - regression_loss: 0.5439 - classification_loss: 0.1330
 9844/10000 [============================>.] - ETA: 1:13 - loss: 0.6768 - regression_loss: 0.5439 - classification_loss: 0.1330
 9845/10000 [============================>.] - ETA: 1:12 - loss: 0.6768 - regression_loss: 0.5439 - classification_loss: 0.1329
 9846/10000 [============================>.] - ETA: 1:12 - loss: 0.6768 - regression_loss: 0.5438 - classification_loss: 0.1329
 9847/10000 [============================>.] - ETA: 1:11 - loss: 0.6768 - regression_loss: 0.5438 - classification_loss: 0.1329
 9848/10000 [============================>.] - ETA: 1:11 - loss: 0.6767 - regression_loss: 0.5438 - classification_loss: 0.1329
 9849/10000 [============================>.] - ETA: 1:10 - loss: 0.6767 - regression_loss: 0.5438 - classification_loss: 0.1330
 9850/10000 [============================>.] - ETA: 1:10 - loss: 0.6767 - regression_loss: 0.5437 - classification_loss: 0.1330
 9851/10000 [============================>.] - ETA: 1:09 - loss: 0.6767 - regression_loss: 0.5437 - classification_loss: 0.1330
 9852/10000 [============================>.] - ETA: 1:09 - loss: 0.6767 - regression_loss: 0.5437 - classification_loss: 0.1330
 9853/10000 [============================>.] - ETA: 1:08 - loss: 0.6766 - regression_loss: 0.5437 - classification_loss: 0.1330
 9854/10000 [============================>.] - ETA: 1:08 - loss: 0.6766 - regression_loss: 0.5436 - classification_loss: 0.1330
 9855/10000 [============================>.] - ETA: 1:07 - loss: 0.6766 - regression_loss: 0.5436 - classification_loss: 0.1330
 9856/10000 [============================>.] - ETA: 1:07 - loss: 0.6765 - regression_loss: 0.5436 - classification_loss: 0.1329
 9857/10000 [============================>.] - ETA: 1:07 - loss: 0.6766 - regression_loss: 0.5436 - classification_loss: 0.1329
 9858/10000 [============================>.] - ETA: 1:06 - loss: 0.6765 - regression_loss: 0.5436 - classification_loss: 0.1329
 9859/10000 [============================>.] - ETA: 1:06 - loss: 0.6765 - regression_loss: 0.5435 - classification_loss: 0.1329
 9860/10000 [============================>.] - ETA: 1:05 - loss: 0.6764 - regression_loss: 0.5435 - classification_loss: 0.1329
 9861/10000 [============================>.] - ETA: 1:05 - loss: 0.6764 - regression_loss: 0.5435 - classification_loss: 0.1329
 9862/10000 [============================>.] - ETA: 1:04 - loss: 0.6764 - regression_loss: 0.5435 - classification_loss: 0.1329
 9863/10000 [============================>.] - ETA: 1:04 - loss: 0.6764 - regression_loss: 0.5435 - classification_loss: 0.1329
 9864/10000 [============================>.] - ETA: 1:03 - loss: 0.6764 - regression_loss: 0.5435 - classification_loss: 0.1329
 9865/10000 [============================>.] - ETA: 1:03 - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1329
 9866/10000 [============================>.] - ETA: 1:02 - loss: 0.6764 - regression_loss: 0.5435 - classification_loss: 0.1329
 9867/10000 [============================>.] - ETA: 1:02 - loss: 0.6763 - regression_loss: 0.5434 - classification_loss: 0.1329
 9868/10000 [============================>.] - ETA: 1:01 - loss: 0.6763 - regression_loss: 0.5434 - classification_loss: 0.1329
 9869/10000 [============================>.] - ETA: 1:01 - loss: 0.6763 - regression_loss: 0.5434 - classification_loss: 0.1329
 9870/10000 [============================>.] - ETA: 1:00 - loss: 0.6764 - regression_loss: 0.5435 - classification_loss: 0.1329
 9871/10000 [============================>.] - ETA: 1:00 - loss: 0.6763 - regression_loss: 0.5434 - classification_loss: 0.1329
 9872/10000 [============================>.] - ETA: 1:00 - loss: 0.6763 - regression_loss: 0.5434 - classification_loss: 0.1329
 9873/10000 [============================>.] - ETA: 59s - loss: 0.6763 - regression_loss: 0.5434 - classification_loss: 0.1329 
 9874/10000 [============================>.] - ETA: 59s - loss: 0.6763 - regression_loss: 0.5434 - classification_loss: 0.1329
 9875/10000 [============================>.] - ETA: 58s - loss: 0.6763 - regression_loss: 0.5434 - classification_loss: 0.1329
 9876/10000 [============================>.] - ETA: 58s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1329
 9877/10000 [============================>.] - ETA: 57s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1329
 9878/10000 [============================>.] - ETA: 57s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1329
 9879/10000 [============================>.] - ETA: 56s - loss: 0.6763 - regression_loss: 0.5434 - classification_loss: 0.1329
 9880/10000 [============================>.] - ETA: 56s - loss: 0.6763 - regression_loss: 0.5433 - classification_loss: 0.1329
 9881/10000 [============================>.] - ETA: 55s - loss: 0.6762 - regression_loss: 0.5433 - classification_loss: 0.1329
 9882/10000 [============================>.] - ETA: 55s - loss: 0.6762 - regression_loss: 0.5433 - classification_loss: 0.1329
 9883/10000 [============================>.] - ETA: 54s - loss: 0.6763 - regression_loss: 0.5433 - classification_loss: 0.1329
 9884/10000 [============================>.] - ETA: 54s - loss: 0.6763 - regression_loss: 0.5434 - classification_loss: 0.1329
 9885/10000 [============================>.] - ETA: 53s - loss: 0.6763 - regression_loss: 0.5434 - classification_loss: 0.1329
 9886/10000 [============================>.] - ETA: 53s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1329
 9887/10000 [============================>.] - ETA: 52s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1329
 9888/10000 [============================>.] - ETA: 52s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9889/10000 [============================>.] - ETA: 52s - loss: 0.6763 - regression_loss: 0.5434 - classification_loss: 0.1330
 9890/10000 [============================>.] - ETA: 51s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9891/10000 [============================>.] - ETA: 51s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9892/10000 [============================>.] - ETA: 50s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9893/10000 [============================>.] - ETA: 50s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9894/10000 [============================>.] - ETA: 49s - loss: 0.6763 - regression_loss: 0.5434 - classification_loss: 0.1330
 9895/10000 [============================>.] - ETA: 49s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9896/10000 [============================>.] - ETA: 48s - loss: 0.6765 - regression_loss: 0.5434 - classification_loss: 0.1330
 9897/10000 [============================>.] - ETA: 48s - loss: 0.6765 - regression_loss: 0.5435 - classification_loss: 0.1330
 9898/10000 [============================>.] - ETA: 47s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9899/10000 [============================>.] - ETA: 47s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9900/10000 [============================>.] - ETA: 46s - loss: 0.6765 - regression_loss: 0.5434 - classification_loss: 0.1330
 9901/10000 [============================>.] - ETA: 46s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9902/10000 [============================>.] - ETA: 45s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9903/10000 [============================>.] - ETA: 45s - loss: 0.6763 - regression_loss: 0.5433 - classification_loss: 0.1330
 9904/10000 [============================>.] - ETA: 45s - loss: 0.6763 - regression_loss: 0.5433 - classification_loss: 0.1330
 9905/10000 [============================>.] - ETA: 44s - loss: 0.6763 - regression_loss: 0.5433 - classification_loss: 0.1330
 9906/10000 [============================>.] - ETA: 44s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9907/10000 [============================>.] - ETA: 43s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9908/10000 [============================>.] - ETA: 43s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9909/10000 [============================>.] - ETA: 42s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9910/10000 [============================>.] - ETA: 42s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9911/10000 [============================>.] - ETA: 41s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9912/10000 [============================>.] - ETA: 41s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9913/10000 [============================>.] - ETA: 40s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9914/10000 [============================>.] - ETA: 40s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9915/10000 [============================>.] - ETA: 39s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9916/10000 [============================>.] - ETA: 39s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9917/10000 [============================>.] - ETA: 38s - loss: 0.6763 - regression_loss: 0.5434 - classification_loss: 0.1330
 9918/10000 [============================>.] - ETA: 38s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9919/10000 [============================>.] - ETA: 37s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9920/10000 [============================>.] - ETA: 37s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9921/10000 [============================>.] - ETA: 37s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9922/10000 [============================>.] - ETA: 36s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9923/10000 [============================>.] - ETA: 36s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9924/10000 [============================>.] - ETA: 35s - loss: 0.6763 - regression_loss: 0.5434 - classification_loss: 0.1330
 9925/10000 [============================>.] - ETA: 35s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9926/10000 [============================>.] - ETA: 34s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9927/10000 [============================>.] - ETA: 34s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9928/10000 [============================>.] - ETA: 33s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9929/10000 [============================>.] - ETA: 33s - loss: 0.6763 - regression_loss: 0.5434 - classification_loss: 0.1330
 9930/10000 [============================>.] - ETA: 32s - loss: 0.6763 - regression_loss: 0.5434 - classification_loss: 0.1330
 9931/10000 [============================>.] - ETA: 32s - loss: 0.6763 - regression_loss: 0.5434 - classification_loss: 0.1330
 9932/10000 [============================>.] - ETA: 31s - loss: 0.6763 - regression_loss: 0.5433 - classification_loss: 0.1330
 9933/10000 [============================>.] - ETA: 31s - loss: 0.6762 - regression_loss: 0.5433 - classification_loss: 0.1330
 9934/10000 [============================>.] - ETA: 30s - loss: 0.6763 - regression_loss: 0.5433 - classification_loss: 0.1330
 9935/10000 [============================>.] - ETA: 30s - loss: 0.6762 - regression_loss: 0.5433 - classification_loss: 0.1330
 9936/10000 [============================>.] - ETA: 30s - loss: 0.6762 - regression_loss: 0.5432 - classification_loss: 0.1330
 9937/10000 [============================>.] - ETA: 29s - loss: 0.6762 - regression_loss: 0.5433 - classification_loss: 0.1330
 9938/10000 [============================>.] - ETA: 29s - loss: 0.6762 - regression_loss: 0.5432 - classification_loss: 0.1330
 9939/10000 [============================>.] - ETA: 28s - loss: 0.6762 - regression_loss: 0.5432 - classification_loss: 0.1330
 9940/10000 [============================>.] - ETA: 28s - loss: 0.6762 - regression_loss: 0.5432 - classification_loss: 0.1330
 9941/10000 [============================>.] - ETA: 27s - loss: 0.6763 - regression_loss: 0.5433 - classification_loss: 0.1330
 9942/10000 [============================>.] - ETA: 27s - loss: 0.6763 - regression_loss: 0.5433 - classification_loss: 0.1330
 9943/10000 [============================>.] - ETA: 26s - loss: 0.6763 - regression_loss: 0.5433 - classification_loss: 0.1330
 9944/10000 [============================>.] - ETA: 26s - loss: 0.6763 - regression_loss: 0.5433 - classification_loss: 0.1330
 9945/10000 [============================>.] - ETA: 25s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9946/10000 [============================>.] - ETA: 25s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9947/10000 [============================>.] - ETA: 24s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9948/10000 [============================>.] - ETA: 24s - loss: 0.6765 - regression_loss: 0.5435 - classification_loss: 0.1331
 9949/10000 [============================>.] - ETA: 23s - loss: 0.6766 - regression_loss: 0.5435 - classification_loss: 0.1331
 9950/10000 [============================>.] - ETA: 23s - loss: 0.6766 - regression_loss: 0.5435 - classification_loss: 0.1331
 9951/10000 [============================>.] - ETA: 22s - loss: 0.6765 - regression_loss: 0.5435 - classification_loss: 0.1331
 9952/10000 [============================>.] - ETA: 22s - loss: 0.6766 - regression_loss: 0.5436 - classification_loss: 0.1331
 9953/10000 [============================>.] - ETA: 22s - loss: 0.6767 - regression_loss: 0.5436 - classification_loss: 0.1331
 9954/10000 [============================>.] - ETA: 21s - loss: 0.6767 - regression_loss: 0.5436 - classification_loss: 0.1331
 9955/10000 [============================>.] - ETA: 21s - loss: 0.6767 - regression_loss: 0.5436 - classification_loss: 0.1331
 9956/10000 [============================>.] - ETA: 20s - loss: 0.6768 - regression_loss: 0.5437 - classification_loss: 0.1331
 9957/10000 [============================>.] - ETA: 20s - loss: 0.6768 - regression_loss: 0.5437 - classification_loss: 0.1331
 9958/10000 [============================>.] - ETA: 19s - loss: 0.6769 - regression_loss: 0.5438 - classification_loss: 0.1331
 9959/10000 [============================>.] - ETA: 19s - loss: 0.6769 - regression_loss: 0.5438 - classification_loss: 0.1331
 9960/10000 [============================>.] - ETA: 18s - loss: 0.6769 - regression_loss: 0.5438 - classification_loss: 0.1331
 9961/10000 [============================>.] - ETA: 18s - loss: 0.6769 - regression_loss: 0.5437 - classification_loss: 0.1331
 9962/10000 [============================>.] - ETA: 17s - loss: 0.6769 - regression_loss: 0.5438 - classification_loss: 0.1332
 9963/10000 [============================>.] - ETA: 17s - loss: 0.6770 - regression_loss: 0.5438 - classification_loss: 0.1331
 9964/10000 [============================>.] - ETA: 16s - loss: 0.6769 - regression_loss: 0.5438 - classification_loss: 0.1331
 9965/10000 [============================>.] - ETA: 16s - loss: 0.6769 - regression_loss: 0.5438 - classification_loss: 0.1331
 9966/10000 [============================>.] - ETA: 15s - loss: 0.6768 - regression_loss: 0.5437 - classification_loss: 0.1331
 9967/10000 [============================>.] - ETA: 15s - loss: 0.6769 - regression_loss: 0.5437 - classification_loss: 0.1331
 9968/10000 [============================>.] - ETA: 14s - loss: 0.6768 - regression_loss: 0.5437 - classification_loss: 0.1331
 9969/10000 [============================>.] - ETA: 14s - loss: 0.6768 - regression_loss: 0.5437 - classification_loss: 0.1331
 9970/10000 [============================>.] - ETA: 14s - loss: 0.6768 - regression_loss: 0.5437 - classification_loss: 0.1331
 9971/10000 [============================>.] - ETA: 13s - loss: 0.6768 - regression_loss: 0.5436 - classification_loss: 0.1331
 9972/10000 [============================>.] - ETA: 13s - loss: 0.6767 - regression_loss: 0.5436 - classification_loss: 0.1331
 9973/10000 [============================>.] - ETA: 12s - loss: 0.6767 - regression_loss: 0.5436 - classification_loss: 0.1331
 9974/10000 [============================>.] - ETA: 12s - loss: 0.6766 - regression_loss: 0.5436 - classification_loss: 0.1331
 9975/10000 [============================>.] - ETA: 11s - loss: 0.6766 - regression_loss: 0.5435 - classification_loss: 0.1331
 9976/10000 [============================>.] - ETA: 11s - loss: 0.6766 - regression_loss: 0.5435 - classification_loss: 0.1331
 9977/10000 [============================>.] - ETA: 10s - loss: 0.6766 - regression_loss: 0.5435 - classification_loss: 0.1331
 9978/10000 [============================>.] - ETA: 10s - loss: 0.6766 - regression_loss: 0.5435 - classification_loss: 0.1331
 9979/10000 [============================>.] - ETA: 9s - loss: 0.6766 - regression_loss: 0.5435 - classification_loss: 0.1330 
 9980/10000 [============================>.] - ETA: 9s - loss: 0.6765 - regression_loss: 0.5435 - classification_loss: 0.1330
 9981/10000 [============================>.] - ETA: 8s - loss: 0.6765 - regression_loss: 0.5434 - classification_loss: 0.1330
 9982/10000 [============================>.] - ETA: 8s - loss: 0.6765 - regression_loss: 0.5435 - classification_loss: 0.1330
 9983/10000 [============================>.] - ETA: 7s - loss: 0.6765 - regression_loss: 0.5434 - classification_loss: 0.1330
 9984/10000 [============================>.] - ETA: 7s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9985/10000 [============================>.] - ETA: 7s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9986/10000 [============================>.] - ETA: 6s - loss: 0.6765 - regression_loss: 0.5435 - classification_loss: 0.1330
 9987/10000 [============================>.] - ETA: 6s - loss: 0.6765 - regression_loss: 0.5434 - classification_loss: 0.1330
 9988/10000 [============================>.] - ETA: 5s - loss: 0.6765 - regression_loss: 0.5435 - classification_loss: 0.1330
 9989/10000 [============================>.] - ETA: 5s - loss: 0.6765 - regression_loss: 0.5434 - classification_loss: 0.1330
 9990/10000 [============================>.] - ETA: 4s - loss: 0.6765 - regression_loss: 0.5435 - classification_loss: 0.1330
 9991/10000 [============================>.] - ETA: 4s - loss: 0.6765 - regression_loss: 0.5434 - classification_loss: 0.1330
 9992/10000 [============================>.] - ETA: 3s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9993/10000 [============================>.] - ETA: 3s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9994/10000 [============================>.] - ETA: 2s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9995/10000 [============================>.] - ETA: 2s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9996/10000 [============================>.] - ETA: 1s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9997/10000 [============================>.] - ETA: 1s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9998/10000 [============================>.] - ETA: 0s - loss: 0.6764 - regression_loss: 0.5434 - classification_loss: 0.1330
 9999/10000 [============================>.] - ETA: 0s - loss: 0.6765 - regression_loss: 0.5435 - classification_loss: 0.1330
10000/10000 [==============================] - 4687s 469ms/step - loss: 0.6765 - regression_loss: 0.5435 - classification_loss: 0.1330
1/49522/49523/49524/49525/49526/49527/49528/49529/495210/495211/495212/495213/495214/495215/495216/495217/495218/495219/495220/495221/495222/495223/495224/495225/495226/495227/495228/495229/495230/495231/495232/495233/495234/495235/495236/495237/495238/495239/495240/495241/495242/495243/495244/495245/495246/495247/495248/495249/495250/495251/495252/495253/495254/495255/495256/495257/495258/495259/495260/495261/495262/495263/495264/495265/495266/495267/495268/495269/495270/495271/495272/495273/495274/495275/495276/495277/495278/495279/495280/495281/495282/495283/495284/495285/495286/495287/495288/495289/495290/495291/495292/495293/495294/495295/495296/495297/495298/495299/4952100/4952101/4952102/4952103/4952104/4952105/4952106/4952107/4952108/4952109/4952110/4952111/4952112/4952113/4952114/4952115/4952116/4952117/4952118/4952119/4952120/4952121/4952122/4952123/4952124/4952125/4952126/4952127/4952128/4952129/4952130/4952131/4952132/4952133/4952134/4952135/4952136/4952137/4952138/4952139/4952140/4952141/4952142/4952143/4952144/4952145/4952146/4952147/4952148/4952149/4952150/4952151/4952152/4952153/4952154/4952155/4952156/4952157/4952158/4952159/4952160/4952161/4952162/4952163/4952164/4952165/4952166/4952167/4952168/4952169/4952170/4952171/4952172/4952173/4952174/4952175/4952176/4952177/4952178/4952179/4952180/4952181/4952182/4952183/4952184/4952185/4952186/4952187/4952188/4952189/4952190/4952191/4952192/4952193/4952194/4952195/4952196/4952197/4952198/4952199/4952200/4952201/4952202/4952203/4952204/4952205/4952206/4952207/4952208/4952209/4952210/4952211/4952212/4952213/4952214/4952215/4952216/4952217/4952218/4952219/4952220/4952221/4952222/4952223/4952224/4952225/4952226/4952227/4952228/4952229/4952230/4952231/4952232/4952233/4952234/4952235/4952236/4952237/4952238/4952239/4952240/4952241/4952242/4952243/4952244/4952245/4952246/4952247/4952248/4952249/4952250/4952251/4952252/4952253/4952254/4952255/4952256/4952257/4952258/4952259/4952260/4952261/4952262/4952263/4952264/4952265/4952266/4952267/4952268/4952269/4952270/4952271/4952272/4952273/4952274/4952275/4952276/4952277/4952278/4952279/4952280/4952281/4952282/4952283/4952284/4952285/4952286/4952287/4952288/4952289/4952290/4952291/4952292/4952293/4952294/4952295/4952296/4952297/4952298/4952299/4952300/4952301/4952302/4952303/4952304/4952305/4952306/4952307/4952308/4952309/4952310/4952311/4952312/4952313/4952314/4952315/4952316/4952317/4952318/4952319/4952320/4952321/4952322/4952323/4952324/4952325/4952326/4952327/4952328/4952329/4952330/4952331/4952332/4952333/4952334/4952335/4952336/4952337/4952338/4952339/4952340/4952341/4952342/4952343/4952344/4952345/4952346/4952347/4952348/4952349/4952350/4952351/4952352/4952353/4952354/4952355/4952356/4952357/4952358/4952359/4952360/4952361/4952362/4952363/4952364/4952365/4952366/4952367/4952368/4952369/4952370/4952371/4952372/4952373/4952374/4952375/4952376/4952377/4952378/4952379/4952380/4952381/4952382/4952383/4952384/4952385/4952386/4952387/4952388/4952389/4952390/4952391/4952392/4952393/4952394/4952395/4952396/4952397/4952398/4952399/4952400/4952401/4952402/4952403/4952404/4952405/4952406/4952407/4952408/4952409/4952410/4952411/4952412/4952413/4952414/4952415/4952416/4952417/4952418/4952419/4952420/4952421/4952422/4952423/4952424/4952425/4952426/4952427/4952428/4952429/4952430/4952431/4952432/4952433/4952434/4952435/4952436/4952437/4952438/4952439/4952440/4952441/4952442/4952443/4952444/4952445/4952446/4952447/4952448/4952449/4952450/4952451/4952452/4952453/4952454/4952455/4952456/4952457/4952458/4952459/4952460/4952461/4952462/4952463/4952464/4952465/4952466/4952467/4952468/4952469/4952470/4952471/4952472/4952473/4952474/4952475/4952476/4952477/4952478/4952479/4952480/4952481/4952482/4952483/4952484/4952485/4952486/4952487/4952488/4952489/4952490/4952491/4952492/4952493/4952494/4952495/4952496/4952497/4952498/4952499/4952500/4952501/4952502/4952503/4952504/4952505/4952506/4952507/4952508/4952509/4952510/4952511/4952512/4952513/4952514/4952515/4952516/4952517/4952518/4952519/4952520/4952521/4952522/4952523/4952524/4952525/4952526/4952527/4952528/4952529/4952530/4952531/4952532/4952533/4952534/4952535/4952536/4952537/4952538/4952539/4952540/4952541/4952542/4952543/4952544/4952545/4952546/4952547/4952548/4952549/4952550/4952551/4952552/4952553/4952554/4952555/4952556/4952557/4952558/4952559/4952560/4952561/4952562/4952563/4952564/4952565/4952566/4952567/4952568/4952569/4952570/4952571/4952572/4952573/4952574/4952575/4952576/4952577/4952578/4952579/4952580/4952581/4952582/4952583/4952584/4952585/4952586/4952587/4952588/4952589/4952590/4952591/4952592/4952593/4952594/4952595/4952596/4952597/4952598/4952599/4952600/4952601/4952602/4952603/4952604/4952605/4952606/4952607/4952608/4952609/4952610/4952611/4952612/4952613/4952614/4952615/4952616/4952617/4952618/4952619/4952620/4952621/4952622/4952623/4952624/4952625/4952626/4952627/4952628/4952629/4952630/4952631/4952632/4952633/4952634/4952635/4952636/4952637/4952638/4952639/4952640/4952641/4952642/4952643/4952644/4952645/4952646/4952647/4952648/4952649/4952650/4952651/4952652/4952653/4952654/4952655/4952656/4952657/4952658/4952659/4952660/4952661/4952662/4952663/4952664/4952665/4952666/4952667/4952668/4952669/4952670/4952671/4952672/4952673/4952674/4952675/4952676/4952677/4952678/4952679/4952680/4952681/4952682/4952683/4952684/4952685/4952686/4952687/4952688/4952689/4952690/4952691/4952692/4952693/4952694/4952695/4952696/4952697/4952698/4952699/4952700/4952701/4952702/4952703/4952704/4952705/4952706/4952707/4952708/4952709/4952710/4952711/4952712/4952713/4952714/4952715/4952716/4952717/4952718/4952719/4952720/4952721/4952722/4952723/4952724/4952725/4952726/4952727/4952728/4952729/4952730/4952731/4952732/4952733/4952734/4952735/4952736/4952737/4952738/4952739/4952740/4952741/4952742/4952743/4952744/4952745/4952746/4952747/4952748/4952749/4952750/4952751/4952752/4952753/4952754/4952755/4952756/4952757/4952758/4952759/4952760/4952761/4952762/4952763/4952764/4952765/4952766/4952767/4952768/4952769/4952770/4952771/4952772/4952773/4952774/4952775/4952776/4952777/4952778/4952779/4952780/4952781/4952782/4952783/4952784/4952785/4952786/4952787/4952788/4952789/4952790/4952791/4952792/4952793/4952794/4952795/4952796/4952797/4952798/4952799/4952800/4952801/4952802/4952803/4952804/4952805/4952806/4952807/4952808/4952809/4952810/4952811/4952812/4952813/4952814/4952815/4952816/4952817/4952818/4952819/4952820/4952821/4952822/4952823/4952824/4952825/4952826/4952827/4952828/4952829/4952830/4952831/4952832/4952833/4952834/4952835/4952836/4952837/4952838/4952839/4952840/4952841/4952842/4952843/4952844/4952845/4952846/4952847/4952848/4952849/4952850/4952851/4952852/4952853/4952854/4952855/4952856/4952857/4952858/4952859/4952860/4952861/4952862/4952863/4952864/4952865/4952866/4952867/4952868/4952869/4952870/4952871/4952872/4952873/4952874/4952875/4952876/4952877/4952878/4952879/4952880/4952881/4952882/4952883/4952884/4952885/4952886/4952887/4952888/4952889/4952890/4952891/4952892/4952893/4952894/4952895/4952896/4952897/4952898/4952899/4952900/4952901/4952902/4952903/4952904/4952905/4952906/4952907/4952908/4952909/4952910/4952911/4952912/4952913/4952914/4952915/4952916/4952917/4952918/4952919/4952920/4952921/4952922/4952923/4952924/4952925/4952926/4952927/4952928/4952929/4952930/4952931/4952932/4952933/4952934/4952935/4952936/4952937/4952938/4952939/4952940/4952941/4952942/4952943/4952944/4952945/4952946/4952947/4952948/4952949/4952950/4952951/4952952/4952953/4952954/4952955/4952956/4952957/4952958/4952959/4952960/4952961/4952962/4952963/4952964/4952965/4952966/4952967/4952968/4952969/4952970/4952971/4952972/4952973/4952974/4952975/4952976/4952977/4952978/4952979/4952980/4952981/4952982/4952983/4952984/4952985/4952986/4952987/4952988/4952989/4952990/4952991/4952992/4952993/4952994/4952995/4952996/4952997/4952998/4952999/49521000/49521001/49521002/49521003/49521004/49521005/49521006/49521007/49521008/49521009/49521010/49521011/49521012/49521013/49521014/49521015/49521016/49521017/49521018/49521019/49521020/49521021/49521022/49521023/49521024/49521025/49521026/49521027/49521028/49521029/49521030/49521031/49521032/49521033/49521034/49521035/49521036/49521037/49521038/49521039/49521040/49521041/49521042/49521043/49521044/49521045/49521046/49521047/49521048/49521049/49521050/49521051/49521052/49521053/49521054/49521055/49521056/49521057/49521058/49521059/49521060/49521061/49521062/49521063/49521064/49521065/49521066/49521067/49521068/49521069/49521070/49521071/49521072/49521073/49521074/49521075/49521076/49521077/49521078/49521079/49521080/49521081/49521082/49521083/49521084/49521085/49521086/49521087/49521088/49521089/49521090/49521091/49521092/49521093/49521094/49521095/49521096/49521097/49521098/49521099/49521100/49521101/49521102/49521103/49521104/49521105/49521106/49521107/49521108/49521109/49521110/49521111/49521112/49521113/49521114/49521115/49521116/49521117/49521118/49521119/49521120/49521121/49521122/49521123/49521124/49521125/49521126/49521127/49521128/49521129/49521130/49521131/49521132/49521133/49521134/49521135/49521136/49521137/49521138/49521139/49521140/49521141/49521142/49521143/49521144/49521145/49521146/49521147/49521148/49521149/49521150/49521151/49521152/49521153/49521154/49521155/49521156/49521157/49521158/49521159/49521160/49521161/49521162/49521163/49521164/49521165/49521166/49521167/49521168/49521169/49521170/49521171/49521172/49521173/49521174/49521175/49521176/49521177/49521178/49521179/49521180/49521181/49521182/49521183/49521184/49521185/49521186/49521187/49521188/49521189/49521190/49521191/49521192/49521193/49521194/49521195/49521196/49521197/49521198/49521199/49521200/49521201/49521202/49521203/49521204/49521205/49521206/49521207/49521208/49521209/49521210/49521211/49521212/49521213/49521214/49521215/49521216/49521217/49521218/49521219/49521220/49521221/49521222/49521223/49521224/49521225/49521226/49521227/49521228/49521229/49521230/49521231/49521232/49521233/49521234/49521235/49521236/49521237/49521238/49521239/49521240/49521241/49521242/49521243/49521244/49521245/49521246/49521247/49521248/49521249/49521250/49521251/49521252/49521253/49521254/49521255/49521256/49521257/49521258/49521259/49521260/49521261/49521262/49521263/49521264/49521265/49521266/49521267/49521268/49521269/49521270/49521271/49521272/49521273/49521274/49521275/49521276/49521277/49521278/49521279/49521280/49521281/49521282/49521283/49521284/49521285/49521286/49521287/49521288/49521289/49521290/49521291/49521292/49521293/49521294/49521295/49521296/49521297/49521298/49521299/49521300/49521301/49521302/49521303/49521304/49521305/49521306/49521307/49521308/49521309/49521310/49521311/49521312/49521313/49521314/49521315/49521316/49521317/49521318/49521319/49521320/49521321/49521322/49521323/49521324/49521325/49521326/49521327/49521328/49521329/49521330/49521331/49521332/49521333/49521334/49521335/49521336/49521337/49521338/49521339/49521340/49521341/49521342/49521343/49521344/49521345/49521346/49521347/49521348/49521349/49521350/49521351/49521352/49521353/49521354/49521355/49521356/49521357/49521358/49521359/49521360/49521361/49521362/49521363/49521364/49521365/49521366/49521367/49521368/49521369/49521370/49521371/49521372/49521373/49521374/49521375/49521376/49521377/49521378/49521379/49521380/49521381/49521382/49521383/49521384/49521385/49521386/49521387/49521388/49521389/49521390/49521391/49521392/49521393/49521394/49521395/49521396/49521397/49521398/49521399/49521400/49521401/49521402/49521403/49521404/49521405/49521406/49521407/49521408/49521409/49521410/49521411/49521412/49521413/49521414/49521415/49521416/49521417/49521418/49521419/49521420/49521421/49521422/49521423/49521424/49521425/49521426/49521427/49521428/49521429/49521430/49521431/49521432/49521433/49521434/49521435/49521436/49521437/49521438/49521439/49521440/49521441/49521442/49521443/49521444/49521445/49521446/49521447/49521448/49521449/49521450/49521451/49521452/49521453/49521454/49521455/49521456/49521457/49521458/49521459/49521460/49521461/49521462/49521463/49521464/49521465/49521466/49521467/49521468/49521469/49521470/49521471/49521472/49521473/49521474/49521475/49521476/49521477/49521478/49521479/49521480/49521481/49521482/49521483/49521484/49521485/49521486/49521487/49521488/49521489/49521490/49521491/49521492/49521493/49521494/49521495/49521496/49521497/49521498/49521499/49521500/49521501/49521502/49521503/49521504/49521505/49521506/49521507/49521508/49521509/49521510/49521511/49521512/49521513/49521514/49521515/49521516/49521517/49521518/49521519/49521520/49521521/49521522/49521523/49521524/49521525/49521526/49521527/49521528/49521529/49521530/49521531/49521532/49521533/49521534/49521535/49521536/49521537/49521538/49521539/49521540/49521541/49521542/49521543/49521544/49521545/49521546/49521547/49521548/49521549/49521550/49521551/49521552/49521553/49521554/49521555/49521556/49521557/49521558/49521559/49521560/49521561/49521562/49521563/49521564/49521565/49521566/49521567/49521568/49521569/49521570/49521571/49521572/49521573/49521574/49521575/49521576/49521577/49521578/49521579/49521580/49521581/49521582/49521583/49521584/49521585/49521586/49521587/49521588/49521589/49521590/49521591/49521592/49521593/49521594/49521595/49521596/49521597/49521598/49521599/49521600/49521601/49521602/49521603/49521604/49521605/49521606/49521607/49521608/49521609/49521610/49521611/49521612/49521613/49521614/49521615/49521616/49521617/49521618/49521619/49521620/49521621/49521622/49521623/49521624/49521625/49521626/49521627/49521628/49521629/49521630/49521631/49521632/49521633/49521634/49521635/49521636/49521637/49521638/49521639/49521640/49521641/49521642/49521643/49521644/49521645/49521646/49521647/49521648/49521649/49521650/49521651/49521652/49521653/49521654/49521655/49521656/49521657/49521658/49521659/49521660/49521661/49521662/49521663/49521664/49521665/49521666/49521667/49521668/49521669/49521670/49521671/49521672/49521673/49521674/49521675/49521676/49521677/49521678/49521679/49521680/49521681/49521682/49521683/49521684/49521685/49521686/49521687/49521688/49521689/49521690/49521691/49521692/49521693/49521694/49521695/49521696/49521697/49521698/49521699/49521700/49521701/49521702/49521703/49521704/49521705/49521706/49521707/49521708/49521709/49521710/49521711/49521712/49521713/49521714/49521715/49521716/49521717/49521718/49521719/49521720/49521721/49521722/49521723/49521724/49521725/49521726/49521727/49521728/49521729/49521730/49521731/49521732/49521733/49521734/49521735/49521736/49521737/49521738/49521739/49521740/49521741/49521742/49521743/49521744/49521745/49521746/49521747/49521748/49521749/49521750/49521751/49521752/49521753/49521754/49521755/49521756/49521757/49521758/49521759/49521760/49521761/49521762/49521763/49521764/49521765/49521766/49521767/49521768/49521769/49521770/49521771/49521772/49521773/49521774/49521775/49521776/49521777/49521778/49521779/49521780/49521781/49521782/49521783/49521784/49521785/49521786/49521787/49521788/49521789/49521790/49521791/49521792/49521793/49521794/49521795/49521796/49521797/49521798/49521799/49521800/49521801/49521802/49521803/49521804/49521805/49521806/49521807/49521808/49521809/49521810/49521811/49521812/49521813/49521814/49521815/49521816/49521817/49521818/49521819/49521820/49521821/49521822/49521823/49521824/49521825/49521826/49521827/49521828/49521829/49521830/49521831/49521832/49521833/49521834/49521835/49521836/49521837/49521838/49521839/49521840/49521841/49521842/49521843/49521844/49521845/49521846/49521847/49521848/49521849/49521850/49521851/49521852/49521853/49521854/49521855/49521856/49521857/49521858/49521859/49521860/49521861/49521862/49521863/49521864/49521865/49521866/49521867/49521868/49521869/49521870/49521871/49521872/49521873/49521874/49521875/49521876/49521877/49521878/49521879/49521880/49521881/49521882/49521883/49521884/49521885/49521886/49521887/49521888/49521889/49521890/49521891/49521892/49521893/49521894/49521895/49521896/49521897/49521898/49521899/49521900/49521901/49521902/49521903/49521904/49521905/49521906/49521907/49521908/49521909/49521910/49521911/49521912/49521913/49521914/49521915/49521916/49521917/49521918/49521919/49521920/49521921/49521922/49521923/49521924/49521925/49521926/49521927/49521928/49521929/49521930/49521931/49521932/49521933/49521934/49521935/49521936/49521937/49521938/49521939/49521940/49521941/49521942/49521943/49521944/49521945/49521946/49521947/49521948/49521949/49521950/49521951/49521952/49521953/49521954/49521955/49521956/49521957/49521958/49521959/49521960/49521961/49521962/49521963/49521964/49521965/49521966/49521967/49521968/49521969/49521970/49521971/49521972/49521973/49521974/49521975/49521976/49521977/49521978/49521979/49521980/49521981/49521982/49521983/49521984/49521985/49521986/49521987/49521988/49521989/49521990/49521991/49521992/49521993/49521994/49521995/49521996/49521997/49521998/49521999/49522000/49522001/49522002/49522003/49522004/49522005/49522006/49522007/49522008/49522009/49522010/49522011/49522012/49522013/49522014/49522015/49522016/49522017/49522018/49522019/49522020/49522021/49522022/49522023/49522024/49522025/49522026/49522027/49522028/49522029/49522030/49522031/49522032/49522033/49522034/49522035/49522036/49522037/49522038/49522039/49522040/49522041/49522042/49522043/49522044/49522045/49522046/49522047/49522048/49522049/49522050/49522051/49522052/49522053/49522054/49522055/49522056/49522057/49522058/49522059/49522060/49522061/49522062/49522063/49522064/49522065/49522066/49522067/49522068/49522069/49522070/49522071/49522072/49522073/49522074/49522075/49522076/49522077/49522078/49522079/49522080/49522081/49522082/49522083/49522084/49522085/49522086/49522087/49522088/49522089/49522090/49522091/49522092/49522093/49522094/49522095/49522096/49522097/49522098/49522099/49522100/49522101/49522102/49522103/49522104/49522105/49522106/49522107/49522108/49522109/49522110/49522111/49522112/49522113/49522114/49522115/49522116/49522117/49522118/49522119/49522120/49522121/49522122/49522123/49522124/49522125/49522126/49522127/49522128/49522129/49522130/49522131/49522132/49522133/49522134/49522135/49522136/49522137/49522138/49522139/49522140/49522141/49522142/49522143/49522144/49522145/49522146/49522147/49522148/49522149/49522150/49522151/49522152/49522153/49522154/49522155/49522156/49522157/49522158/49522159/49522160/49522161/49522162/49522163/49522164/49522165/49522166/49522167/49522168/49522169/49522170/49522171/49522172/49522173/49522174/49522175/49522176/49522177/49522178/49522179/49522180/49522181/49522182/49522183/49522184/49522185/49522186/49522187/49522188/49522189/49522190/49522191/49522192/49522193/49522194/49522195/49522196/49522197/49522198/49522199/49522200/49522201/49522202/49522203/49522204/49522205/49522206/49522207/49522208/49522209/49522210/49522211/49522212/49522213/49522214/49522215/49522216/49522217/49522218/49522219/49522220/49522221/49522222/49522223/49522224/49522225/49522226/49522227/49522228/49522229/49522230/49522231/49522232/49522233/49522234/49522235/49522236/49522237/49522238/49522239/49522240/49522241/49522242/49522243/49522244/49522245/49522246/49522247/49522248/49522249/49522250/49522251/49522252/49522253/49522254/49522255/49522256/49522257/49522258/49522259/49522260/49522261/49522262/49522263/49522264/49522265/49522266/49522267/49522268/49522269/49522270/49522271/49522272/49522273/49522274/49522275/49522276/49522277/49522278/49522279/49522280/49522281/49522282/49522283/49522284/49522285/49522286/49522287/49522288/49522289/49522290/49522291/49522292/49522293/49522294/49522295/49522296/49522297/49522298/49522299/49522300/49522301/49522302/49522303/49522304/49522305/49522306/49522307/49522308/49522309/49522310/49522311/49522312/49522313/49522314/49522315/49522316/49522317/49522318/49522319/49522320/49522321/49522322/49522323/49522324/49522325/49522326/49522327/49522328/49522329/49522330/49522331/49522332/49522333/49522334/49522335/49522336/49522337/49522338/49522339/49522340/49522341/49522342/49522343/49522344/49522345/49522346/49522347/49522348/49522349/49522350/49522351/49522352/49522353/49522354/49522355/49522356/49522357/49522358/49522359/49522360/49522361/49522362/49522363/49522364/49522365/49522366/49522367/49522368/49522369/49522370/49522371/49522372/49522373/49522374/49522375/49522376/49522377/49522378/49522379/49522380/49522381/49522382/49522383/49522384/49522385/49522386/49522387/49522388/49522389/49522390/49522391/49522392/49522393/49522394/49522395/49522396/49522397/49522398/49522399/49522400/49522401/49522402/49522403/49522404/49522405/49522406/49522407/49522408/49522409/49522410/49522411/49522412/49522413/49522414/49522415/49522416/49522417/49522418/49522419/49522420/49522421/49522422/49522423/49522424/49522425/49522426/49522427/49522428/49522429/49522430/49522431/49522432/49522433/49522434/49522435/49522436/49522437/49522438/49522439/49522440/49522441/49522442/49522443/49522444/49522445/49522446/49522447/49522448/49522449/49522450/49522451/49522452/49522453/49522454/49522455/49522456/49522457/49522458/49522459/49522460/49522461/49522462/49522463/49522464/49522465/49522466/49522467/49522468/49522469/49522470/49522471/49522472/49522473/49522474/49522475/49522476/49522477/49522478/49522479/49522480/49522481/49522482/49522483/49522484/49522485/49522486/49522487/49522488/49522489/49522490/49522491/49522492/49522493/49522494/49522495/49522496/49522497/49522498/49522499/49522500/49522501/49522502/49522503/49522504/49522505/49522506/49522507/49522508/49522509/49522510/49522511/49522512/49522513/49522514/49522515/49522516/49522517/49522518/49522519/49522520/49522521/49522522/49522523/49522524/49522525/49522526/49522527/49522528/49522529/49522530/49522531/49522532/49522533/49522534/49522535/49522536/49522537/49522538/49522539/49522540/49522541/49522542/49522543/49522544/49522545/49522546/49522547/49522548/49522549/49522550/49522551/49522552/49522553/49522554/49522555/49522556/49522557/49522558/49522559/49522560/49522561/49522562/49522563/49522564/49522565/49522566/49522567/49522568/49522569/49522570/49522571/49522572/49522573/49522574/49522575/49522576/49522577/49522578/49522579/49522580/49522581/49522582/49522583/49522584/49522585/49522586/49522587/49522588/49522589/49522590/49522591/49522592/49522593/49522594/49522595/49522596/49522597/49522598/49522599/49522600/49522601/49522602/49522603/49522604/49522605/49522606/49522607/49522608/49522609/49522610/49522611/49522612/49522613/49522614/49522615/49522616/49522617/49522618/49522619/49522620/49522621/49522622/49522623/49522624/49522625/49522626/49522627/49522628/49522629/49522630/49522631/49522632/49522633/49522634/49522635/49522636/49522637/49522638/49522639/49522640/49522641/49522642/49522643/49522644/49522645/49522646/49522647/49522648/49522649/49522650/49522651/49522652/49522653/49522654/49522655/49522656/49522657/49522658/49522659/49522660/49522661/49522662/49522663/49522664/49522665/49522666/49522667/49522668/49522669/49522670/49522671/49522672/49522673/49522674/49522675/49522676/49522677/49522678/49522679/49522680/49522681/49522682/49522683/49522684/49522685/49522686/49522687/49522688/49522689/49522690/49522691/49522692/49522693/49522694/49522695/49522696/49522697/49522698/49522699/49522700/49522701/49522702/49522703/49522704/49522705/49522706/49522707/49522708/49522709/49522710/49522711/49522712/49522713/49522714/49522715/49522716/49522717/49522718/49522719/49522720/49522721/49522722/49522723/49522724/49522725/49522726/49522727/49522728/49522729/49522730/49522731/49522732/49522733/49522734/49522735/49522736/49522737/49522738/49522739/49522740/49522741/49522742/49522743/49522744/49522745/49522746/49522747/49522748/49522749/49522750/49522751/49522752/49522753/49522754/49522755/49522756/49522757/49522758/49522759/49522760/49522761/49522762/49522763/49522764/49522765/49522766/49522767/49522768/49522769/49522770/49522771/49522772/49522773/49522774/49522775/49522776/49522777/49522778/49522779/49522780/49522781/49522782/49522783/49522784/49522785/49522786/49522787/49522788/49522789/49522790/49522791/49522792/49522793/49522794/49522795/49522796/49522797/49522798/49522799/49522800/49522801/49522802/49522803/49522804/49522805/49522806/49522807/49522808/49522809/49522810/49522811/49522812/49522813/49522814/49522815/49522816/49522817/49522818/49522819/49522820/49522821/49522822/49522823/49522824/49522825/49522826/49522827/49522828/49522829/49522830/49522831/49522832/49522833/49522834/49522835/49522836/49522837/49522838/49522839/49522840/49522841/49522842/49522843/49522844/49522845/49522846/49522847/49522848/49522849/49522850/49522851/49522852/49522853/49522854/49522855/49522856/49522857/49522858/49522859/49522860/49522861/49522862/49522863/49522864/49522865/49522866/49522867/49522868/49522869/49522870/49522871/49522872/49522873/49522874/49522875/49522876/49522877/49522878/49522879/49522880/49522881/49522882/49522883/49522884/49522885/49522886/49522887/49522888/49522889/49522890/49522891/49522892/49522893/49522894/49522895/49522896/49522897/49522898/49522899/49522900/49522901/49522902/49522903/49522904/49522905/49522906/49522907/49522908/49522909/49522910/49522911/49522912/49522913/49522914/49522915/49522916/49522917/49522918/49522919/49522920/49522921/49522922/49522923/49522924/49522925/49522926/49522927/49522928/49522929/49522930/49522931/49522932/49522933/49522934/49522935/49522936/49522937/49522938/49522939/49522940/49522941/49522942/49522943/49522944/49522945/49522946/49522947/49522948/49522949/49522950/49522951/49522952/49522953/49522954/49522955/49522956/49522957/49522958/49522959/49522960/49522961/49522962/49522963/49522964/49522965/49522966/49522967/49522968/49522969/49522970/49522971/49522972/49522973/49522974/49522975/49522976/49522977/49522978/49522979/49522980/49522981/49522982/49522983/49522984/49522985/49522986/49522987/49522988/49522989/49522990/49522991/49522992/49522993/49522994/49522995/49522996/49522997/49522998/49522999/49523000/49523001/49523002/49523003/49523004/49523005/49523006/49523007/49523008/49523009/49523010/49523011/49523012/49523013/49523014/49523015/49523016/49523017/49523018/49523019/49523020/49523021/49523022/49523023/49523024/49523025/49523026/49523027/49523028/49523029/49523030/49523031/49523032/49523033/49523034/49523035/49523036/49523037/49523038/49523039/49523040/49523041/49523042/49523043/49523044/49523045/49523046/49523047/49523048/49523049/49523050/49523051/49523052/49523053/49523054/49523055/49523056/49523057/49523058/49523059/49523060/49523061/49523062/49523063/49523064/49523065/49523066/49523067/49523068/49523069/49523070/49523071/49523072/49523073/49523074/49523075/49523076/49523077/49523078/49523079/49523080/49523081/49523082/49523083/49523084/49523085/49523086/49523087/49523088/49523089/49523090/49523091/49523092/49523093/49523094/49523095/49523096/49523097/49523098/49523099/49523100/49523101/49523102/49523103/49523104/49523105/49523106/49523107/49523108/49523109/49523110/49523111/49523112/49523113/49523114/49523115/49523116/49523117/49523118/49523119/49523120/49523121/49523122/49523123/49523124/49523125/49523126/49523127/49523128/49523129/49523130/49523131/49523132/49523133/49523134/49523135/49523136/49523137/49523138/49523139/49523140/49523141/49523142/49523143/49523144/49523145/49523146/49523147/49523148/49523149/49523150/49523151/49523152/49523153/49523154/49523155/49523156/49523157/49523158/49523159/49523160/49523161/49523162/49523163/49523164/49523165/49523166/49523167/49523168/49523169/49523170/49523171/49523172/49523173/49523174/49523175/49523176/49523177/49523178/49523179/49523180/49523181/49523182/49523183/49523184/49523185/49523186/49523187/49523188/49523189/49523190/49523191/49523192/49523193/49523194/49523195/49523196/49523197/49523198/49523199/49523200/49523201/49523202/49523203/49523204/49523205/49523206/49523207/49523208/49523209/49523210/49523211/49523212/49523213/49523214/49523215/49523216/49523217/49523218/49523219/49523220/49523221/49523222/49523223/49523224/49523225/49523226/49523227/49523228/49523229/49523230/49523231/49523232/49523233/49523234/49523235/49523236/49523237/49523238/49523239/49523240/49523241/49523242/49523243/49523244/49523245/49523246/49523247/49523248/49523249/49523250/49523251/49523252/49523253/49523254/49523255/49523256/49523257/49523258/49523259/49523260/49523261/49523262/49523263/49523264/49523265/49523266/49523267/49523268/49523269/49523270/49523271/49523272/49523273/49523274/49523275/49523276/49523277/49523278/49523279/49523280/49523281/49523282/49523283/49523284/49523285/49523286/49523287/49523288/49523289/49523290/49523291/49523292/49523293/49523294/49523295/49523296/49523297/49523298/49523299/49523300/49523301/49523302/49523303/49523304/49523305/49523306/49523307/49523308/49523309/49523310/49523311/49523312/49523313/49523314/49523315/49523316/49523317/49523318/49523319/49523320/49523321/49523322/49523323/49523324/49523325/49523326/49523327/49523328/49523329/49523330/49523331/49523332/49523333/49523334/49523335/49523336/49523337/49523338/49523339/49523340/49523341/49523342/49523343/49523344/49523345/49523346/49523347/49523348/49523349/49523350/49523351/49523352/49523353/49523354/49523355/49523356/49523357/49523358/49523359/49523360/49523361/49523362/49523363/49523364/49523365/49523366/49523367/49523368/49523369/49523370/49523371/49523372/49523373/49523374/49523375/49523376/49523377/49523378/49523379/49523380/49523381/49523382/49523383/49523384/49523385/49523386/49523387/49523388/49523389/49523390/49523391/49523392/49523393/49523394/49523395/49523396/49523397/49523398/49523399/49523400/49523401/49523402/49523403/49523404/49523405/49523406/49523407/49523408/49523409/49523410/49523411/49523412/49523413/49523414/49523415/49523416/49523417/49523418/49523419/49523420/49523421/49523422/49523423/49523424/49523425/49523426/49523427/49523428/49523429/49523430/49523431/49523432/49523433/49523434/49523435/49523436/49523437/49523438/49523439/49523440/49523441/49523442/49523443/49523444/49523445/49523446/49523447/49523448/49523449/49523450/49523451/49523452/49523453/49523454/49523455/49523456/49523457/49523458/49523459/49523460/49523461/49523462/49523463/49523464/49523465/49523466/49523467/49523468/49523469/49523470/49523471/49523472/49523473/49523474/49523475/49523476/49523477/49523478/49523479/49523480/49523481/49523482/49523483/49523484/49523485/49523486/49523487/49523488/49523489/49523490/49523491/49523492/49523493/49523494/49523495/49523496/49523497/49523498/49523499/49523500/49523501/49523502/49523503/49523504/49523505/49523506/49523507/49523508/49523509/49523510/49523511/49523512/49523513/49523514/49523515/49523516/49523517/49523518/49523519/49523520/49523521/49523522/49523523/49523524/49523525/49523526/49523527/49523528/49523529/49523530/49523531/49523532/49523533/49523534/49523535/49523536/49523537/49523538/49523539/49523540/49523541/49523542/49523543/49523544/49523545/49523546/49523547/49523548/49523549/49523550/49523551/49523552/49523553/49523554/49523555/49523556/49523557/49523558/49523559/49523560/49523561/49523562/49523563/49523564/49523565/49523566/49523567/49523568/49523569/49523570/49523571/49523572/49523573/49523574/49523575/49523576/49523577/49523578/49523579/49523580/49523581/49523582/49523583/49523584/49523585/49523586/49523587/49523588/49523589/49523590/49523591/49523592/49523593/49523594/49523595/49523596/49523597/49523598/49523599/49523600/49523601/49523602/49523603/49523604/49523605/49523606/49523607/49523608/49523609/49523610/49523611/49523612/49523613/49523614/49523615/49523616/49523617/49523618/49523619/49523620/49523621/49523622/49523623/49523624/49523625/49523626/49523627/49523628/49523629/49523630/49523631/49523632/49523633/49523634/49523635/49523636/49523637/49523638/49523639/49523640/49523641/49523642/49523643/49523644/49523645/49523646/49523647/49523648/49523649/49523650/49523651/49523652/49523653/49523654/49523655/49523656/49523657/49523658/49523659/49523660/49523661/49523662/49523663/49523664/49523665/49523666/49523667/49523668/49523669/49523670/49523671/49523672/49523673/49523674/49523675/49523676/49523677/49523678/49523679/49523680/49523681/49523682/49523683/49523684/49523685/49523686/49523687/49523688/49523689/49523690/49523691/49523692/49523693/49523694/49523695/49523696/49523697/49523698/49523699/49523700/49523701/49523702/49523703/49523704/49523705/49523706/49523707/49523708/49523709/49523710/49523711/49523712/49523713/49523714/49523715/49523716/49523717/49523718/49523719/49523720/49523721/49523722/49523723/49523724/49523725/49523726/49523727/49523728/49523729/49523730/49523731/49523732/49523733/49523734/49523735/49523736/49523737/49523738/49523739/49523740/49523741/49523742/49523743/49523744/49523745/49523746/49523747/49523748/49523749/49523750/49523751/49523752/49523753/49523754/49523755/49523756/49523757/49523758/49523759/49523760/49523761/49523762/49523763/49523764/49523765/49523766/49523767/49523768/49523769/49523770/49523771/49523772/49523773/49523774/49523775/49523776/49523777/49523778/49523779/49523780/49523781/49523782/49523783/49523784/49523785/49523786/49523787/49523788/49523789/49523790/49523791/49523792/49523793/49523794/49523795/49523796/49523797/49523798/49523799/49523800/49523801/49523802/49523803/49523804/49523805/49523806/49523807/49523808/49523809/49523810/49523811/49523812/49523813/49523814/49523815/49523816/49523817/49523818/49523819/49523820/49523821/49523822/49523823/49523824/49523825/49523826/49523827/49523828/49523829/49523830/49523831/49523832/49523833/49523834/49523835/49523836/49523837/49523838/49523839/49523840/49523841/49523842/49523843/49523844/49523845/49523846/49523847/49523848/49523849/49523850/49523851/49523852/49523853/49523854/49523855/49523856/49523857/49523858/49523859/49523860/49523861/49523862/49523863/49523864/49523865/49523866/49523867/49523868/49523869/49523870/49523871/49523872/49523873/49523874/49523875/49523876/49523877/49523878/49523879/49523880/49523881/49523882/49523883/49523884/49523885/49523886/49523887/49523888/49523889/49523890/49523891/49523892/49523893/49523894/49523895/49523896/49523897/49523898/49523899/49523900/49523901/49523902/49523903/49523904/49523905/49523906/49523907/49523908/49523909/49523910/49523911/49523912/49523913/49523914/49523915/49523916/49523917/49523918/49523919/49523920/49523921/49523922/49523923/49523924/49523925/49523926/49523927/49523928/49523929/49523930/49523931/49523932/49523933/49523934/49523935/49523936/49523937/49523938/49523939/49523940/49523941/49523942/49523943/49523944/49523945/49523946/49523947/49523948/49523949/49523950/49523951/49523952/49523953/49523954/49523955/49523956/49523957/49523958/49523959/49523960/49523961/49523962/49523963/49523964/49523965/49523966/49523967/49523968/49523969/49523970/49523971/49523972/49523973/49523974/49523975/49523976/49523977/49523978/49523979/49523980/49523981/49523982/49523983/49523984/49523985/49523986/49523987/49523988/49523989/49523990/49523991/49523992/49523993/49523994/49523995/49523996/49523997/49523998/49523999/49524000/49524001/49524002/49524003/49524004/49524005/49524006/49524007/49524008/49524009/49524010/49524011/49524012/49524013/49524014/49524015/49524016/49524017/49524018/49524019/49524020/49524021/49524022/49524023/49524024/49524025/49524026/49524027/49524028/49524029/49524030/49524031/49524032/49524033/49524034/49524035/49524036/49524037/49524038/49524039/49524040/49524041/49524042/49524043/49524044/49524045/49524046/49524047/49524048/49524049/49524050/49524051/49524052/49524053/49524054/49524055/49524056/49524057/49524058/49524059/49524060/49524061/49524062/49524063/49524064/49524065/49524066/49524067/49524068/49524069/49524070/49524071/49524072/49524073/49524074/49524075/49524076/49524077/49524078/49524079/49524080/49524081/49524082/49524083/49524084/49524085/49524086/49524087/49524088/49524089/49524090/49524091/49524092/49524093/49524094/49524095/49524096/49524097/49524098/49524099/49524100/49524101/49524102/49524103/49524104/49524105/49524106/49524107/49524108/49524109/49524110/49524111/49524112/49524113/49524114/49524115/49524116/49524117/49524118/49524119/49524120/49524121/49524122/49524123/49524124/49524125/49524126/49524127/49524128/49524129/49524130/49524131/49524132/49524133/49524134/49524135/49524136/49524137/49524138/49524139/49524140/49524141/49524142/49524143/49524144/49524145/49524146/49524147/49524148/49524149/49524150/49524151/49524152/49524153/49524154/49524155/49524156/49524157/49524158/49524159/49524160/49524161/49524162/49524163/49524164/49524165/49524166/49524167/49524168/49524169/49524170/49524171/49524172/49524173/49524174/49524175/49524176/49524177/49524178/49524179/49524180/49524181/49524182/49524183/49524184/49524185/49524186/49524187/49524188/49524189/49524190/49524191/49524192/49524193/49524194/49524195/49524196/49524197/49524198/49524199/49524200/49524201/49524202/49524203/49524204/49524205/49524206/49524207/49524208/49524209/49524210/49524211/49524212/49524213/49524214/49524215/49524216/49524217/49524218/49524219/49524220/49524221/49524222/49524223/49524224/49524225/49524226/49524227/49524228/49524229/49524230/49524231/49524232/49524233/49524234/49524235/49524236/49524237/49524238/49524239/49524240/49524241/49524242/49524243/49524244/49524245/49524246/49524247/49524248/49524249/49524250/49524251/49524252/49524253/49524254/49524255/49524256/49524257/49524258/49524259/49524260/49524261/49524262/49524263/49524264/49524265/49524266/49524267/49524268/49524269/49524270/49524271/49524272/49524273/49524274/49524275/49524276/49524277/49524278/49524279/49524280/49524281/49524282/49524283/49524284/49524285/49524286/49524287/49524288/49524289/49524290/49524291/49524292/49524293/49524294/49524295/49524296/49524297/49524298/49524299/49524300/49524301/49524302/49524303/49524304/49524305/49524306/49524307/49524308/49524309/49524310/49524311/49524312/49524313/49524314/49524315/49524316/49524317/49524318/49524319/49524320/49524321/49524322/49524323/49524324/49524325/49524326/49524327/49524328/49524329/49524330/49524331/49524332/49524333/49524334/49524335/49524336/49524337/49524338/49524339/49524340/49524341/49524342/49524343/49524344/49524345/49524346/49524347/49524348/49524349/49524350/49524351/49524352/49524353/49524354/49524355/49524356/49524357/49524358/49524359/49524360/49524361/49524362/49524363/49524364/49524365/49524366/49524367/49524368/49524369/49524370/49524371/49524372/49524373/49524374/49524375/49524376/49524377/49524378/49524379/49524380/49524381/49524382/49524383/49524384/49524385/49524386/49524387/49524388/49524389/49524390/49524391/49524392/49524393/49524394/49524395/49524396/49524397/49524398/49524399/49524400/49524401/49524402/49524403/49524404/49524405/49524406/49524407/49524408/49524409/49524410/49524411/49524412/49524413/49524414/49524415/49524416/49524417/49524418/49524419/49524420/49524421/49524422/49524423/49524424/49524425/49524426/49524427/49524428/49524429/49524430/49524431/49524432/49524433/49524434/49524435/49524436/49524437/49524438/49524439/49524440/49524441/49524442/49524443/49524444/49524445/49524446/49524447/49524448/49524449/49524450/49524451/49524452/49524453/49524454/49524455/49524456/49524457/49524458/49524459/49524460/49524461/49524462/49524463/49524464/49524465/49524466/49524467/49524468/49524469/49524470/49524471/49524472/49524473/49524474/49524475/49524476/49524477/49524478/49524479/49524480/49524481/49524482/49524483/49524484/49524485/49524486/49524487/49524488/49524489/49524490/49524491/49524492/49524493/49524494/49524495/49524496/49524497/49524498/49524499/49524500/49524501/49524502/49524503/49524504/49524505/49524506/49524507/49524508/49524509/49524510/49524511/49524512/49524513/49524514/49524515/49524516/49524517/49524518/49524519/49524520/49524521/49524522/49524523/49524524/49524525/49524526/49524527/49524528/49524529/49524530/49524531/49524532/49524533/49524534/49524535/49524536/49524537/49524538/49524539/49524540/49524541/49524542/49524543/49524544/49524545/49524546/49524547/49524548/49524549/49524550/49524551/49524552/49524553/49524554/49524555/49524556/49524557/49524558/49524559/49524560/49524561/49524562/49524563/49524564/49524565/49524566/49524567/49524568/49524569/49524570/49524571/49524572/49524573/49524574/49524575/49524576/49524577/49524578/49524579/49524580/49524581/49524582/49524583/49524584/49524585/49524586/49524587/49524588/49524589/49524590/49524591/49524592/49524593/49524594/49524595/49524596/49524597/49524598/49524599/49524600/49524601/49524602/49524603/49524604/49524605/49524606/49524607/49524608/49524609/49524610/49524611/49524612/49524613/49524614/49524615/49524616/49524617/49524618/49524619/49524620/49524621/49524622/49524623/49524624/49524625/49524626/49524627/49524628/49524629/49524630/49524631/49524632/49524633/49524634/49524635/49524636/49524637/49524638/49524639/49524640/49524641/49524642/49524643/49524644/49524645/49524646/49524647/49524648/49524649/49524650/49524651/49524652/49524653/49524654/49524655/49524656/49524657/49524658/49524659/49524660/49524661/49524662/49524663/49524664/49524665/49524666/49524667/49524668/49524669/49524670/49524671/49524672/49524673/49524674/49524675/49524676/49524677/49524678/49524679/49524680/49524681/49524682/49524683/49524684/49524685/49524686/49524687/49524688/49524689/49524690/49524691/49524692/49524693/49524694/49524695/49524696/49524697/49524698/49524699/49524700/49524701/49524702/49524703/49524704/49524705/49524706/49524707/49524708/49524709/49524710/49524711/49524712/49524713/49524714/49524715/49524716/49524717/49524718/49524719/49524720/49524721/49524722/49524723/49524724/49524725/49524726/49524727/49524728/49524729/49524730/49524731/49524732/49524733/49524734/49524735/49524736/49524737/49524738/49524739/49524740/49524741/49524742/49524743/49524744/49524745/49524746/49524747/49524748/49524749/49524750/49524751/49524752/49524753/49524754/49524755/49524756/49524757/49524758/49524759/49524760/49524761/49524762/49524763/49524764/49524765/49524766/49524767/49524768/49524769/49524770/49524771/49524772/49524773/49524774/49524775/49524776/49524777/49524778/49524779/49524780/49524781/49524782/49524783/49524784/49524785/49524786/49524787/49524788/49524789/49524790/49524791/49524792/49524793/49524794/49524795/49524796/49524797/49524798/49524799/49524800/49524801/49524802/49524803/49524804/49524805/49524806/49524807/49524808/49524809/49524810/49524811/49524812/49524813/49524814/49524815/49524816/49524817/49524818/49524819/49524820/49524821/49524822/49524823/49524824/49524825/49524826/49524827/49524828/49524829/49524830/49524831/49524832/49524833/49524834/49524835/49524836/49524837/49524838/49524839/49524840/49524841/49524842/49524843/49524844/49524845/49524846/49524847/49524848/49524849/49524850/49524851/49524852/49524853/49524854/49524855/49524856/49524857/49524858/49524859/49524860/49524861/49524862/49524863/49524864/49524865/49524866/49524867/49524868/49524869/49524870/49524871/49524872/49524873/49524874/49524875/49524876/49524877/49524878/49524879/49524880/49524881/49524882/49524883/49524884/49524885/49524886/49524887/49524888/49524889/49524890/49524891/49524892/49524893/49524894/49524895/49524896/49524897/49524898/49524899/49524900/49524901/49524902/49524903/49524904/49524905/49524906/49524907/49524908/49524909/49524910/49524911/49524912/49524913/49524914/49524915/49524916/49524917/49524918/49524919/49524920/49524921/49524922/49524923/49524924/49524925/49524926/49524927/49524928/49524929/49524930/49524931/49524932/49524933/49524934/49524935/49524936/49524937/49524938/49524939/49524940/49524941/49524942/49524943/49524944/49524945/49524946/49524947/49524948/49524949/49524950/49524951/49524952/49521/49522/49523/49524/49525/49526/49527/49528/49529/495210/495211/495212/495213/495214/495215/495216/495217/495218/495219/495220/495221/495222/495223/495224/495225/495226/495227/495228/495229/495230/495231/495232/495233/495234/495235/495236/495237/495238/495239/495240/495241/495242/495243/495244/495245/495246/495247/495248/495249/495250/495251/495252/495253/495254/495255/495256/495257/495258/495259/495260/495261/495262/495263/495264/495265/495266/495267/495268/495269/495270/495271/495272/495273/495274/495275/495276/495277/495278/495279/495280/495281/495282/495283/495284/495285/495286/495287/495288/495289/495290/495291/495292/495293/495294/495295/495296/495297/495298/495299/4952100/4952101/4952102/4952103/4952104/4952105/4952106/4952107/4952108/4952109/4952110/4952111/4952112/4952113/4952114/4952115/4952116/4952117/4952118/4952119/4952120/4952121/4952122/4952123/4952124/4952125/4952126/4952127/4952128/4952129/4952130/4952131/4952132/4952133/4952134/4952135/4952136/4952137/4952138/4952139/4952140/4952141/4952142/4952143/4952144/4952145/4952146/4952147/4952148/4952149/4952150/4952151/4952152/4952153/4952154/4952155/4952156/4952157/4952158/4952159/4952160/4952161/4952162/4952163/4952164/4952165/4952166/4952167/4952168/4952169/4952170/4952171/4952172/4952173/4952174/4952175/4952176/4952177/4952178/4952179/4952180/4952181/4952182/4952183/4952184/4952185/4952186/4952187/4952188/4952189/4952190/4952191/4952192/4952193/4952194/4952195/4952196/4952197/4952198/4952199/4952200/4952201/4952202/4952203/4952204/4952205/4952206/4952207/4952208/4952209/4952210/4952211/4952212/4952213/4952214/4952215/4952216/4952217/4952218/4952219/4952220/4952221/4952222/4952223/4952224/4952225/4952226/4952227/4952228/4952229/4952230/4952231/4952232/4952233/4952234/4952235/4952236/4952237/4952238/4952239/4952240/4952241/4952242/4952243/4952244/4952245/4952246/4952247/4952248/4952249/4952250/4952251/4952252/4952253/4952254/4952255/4952256/4952257/4952258/4952259/4952260/4952261/4952262/4952263/4952264/4952265/4952266/4952267/4952268/4952269/4952270/4952271/4952272/4952273/4952274/4952275/4952276/4952277/4952278/4952279/4952280/4952281/4952282/4952283/4952284/4952285/4952286/4952287/4952288/4952289/4952290/4952291/4952292/4952293/4952294/4952295/4952296/4952297/4952298/4952299/4952300/4952301/4952302/4952303/4952304/4952305/4952306/4952307/4952308/4952309/4952310/4952311/4952312/4952313/4952314/4952315/4952316/4952317/4952318/4952319/4952320/4952321/4952322/4952323/4952324/4952325/4952326/4952327/4952328/4952329/4952330/4952331/4952332/4952333/4952334/4952335/4952336/4952337/4952338/4952339/4952340/4952341/4952342/4952343/4952344/4952345/4952346/4952347/4952348/4952349/4952350/4952351/4952352/4952353/4952354/4952355/4952356/4952357/4952358/4952359/4952360/4952361/4952362/4952363/4952364/4952365/4952366/4952367/4952368/4952369/4952370/4952371/4952372/4952373/4952374/4952375/4952376/4952377/4952378/4952379/4952380/4952381/4952382/4952383/4952384/4952385/4952386/4952387/4952388/4952389/4952390/4952391/4952392/4952393/4952394/4952395/4952396/4952397/4952398/4952399/4952400/4952401/4952402/4952403/4952404/4952405/4952406/4952407/4952408/4952409/4952410/4952411/4952412/4952413/4952414/4952415/4952416/4952417/4952418/4952419/4952420/4952421/4952422/4952423/4952424/4952425/4952426/4952427/4952428/4952429/4952430/4952431/4952432/4952433/4952434/4952435/4952436/4952437/4952438/4952439/4952440/4952441/4952442/4952443/4952444/4952445/4952446/4952447/4952448/4952449/4952450/4952451/4952452/4952453/4952454/4952455/4952456/4952457/4952458/4952459/4952460/4952461/4952462/4952463/4952464/4952465/4952466/4952467/4952468/4952469/4952470/4952471/4952472/4952473/4952474/4952475/4952476/4952477/4952478/4952479/4952480/4952481/4952482/4952483/4952484/4952485/4952486/4952487/4952488/4952489/4952490/4952491/4952492/4952493/4952494/4952495/4952496/4952497/4952498/4952499/4952500/4952501/4952502/4952503/4952504/4952505/4952506/4952507/4952508/4952509/4952510/4952511/4952512/4952513/4952514/4952515/4952516/4952517/4952518/4952519/4952520/4952521/4952522/4952523/4952524/4952525/4952526/4952527/4952528/4952529/4952530/4952531/4952532/4952533/4952534/4952535/4952536/4952537/4952538/4952539/4952540/4952541/4952542/4952543/4952544/4952545/4952546/4952547/4952548/4952549/4952550/4952551/4952552/4952553/4952554/4952555/4952556/4952557/4952558/4952559/4952560/4952561/4952562/4952563/4952564/4952565/4952566/4952567/4952568/4952569/4952570/4952571/4952572/4952573/4952574/4952575/4952576/4952577/4952578/4952579/4952580/4952581/4952582/4952583/4952584/4952585/4952586/4952587/4952588/4952589/4952590/4952591/4952592/4952593/4952594/4952595/4952596/4952597/4952598/4952599/4952600/4952601/4952602/4952603/4952604/4952605/4952606/4952607/4952608/4952609/4952610/4952611/4952612/4952613/4952614/4952615/4952616/4952617/4952618/4952619/4952620/4952621/4952622/4952623/4952624/4952625/4952626/4952627/4952628/4952629/4952630/4952631/4952632/4952633/4952634/4952635/4952636/4952637/4952638/4952639/4952640/4952641/4952642/4952643/4952644/4952645/4952646/4952647/4952648/4952649/4952650/4952651/4952652/4952653/4952654/4952655/4952656/4952657/4952658/4952659/4952660/4952661/4952662/4952663/4952664/4952665/4952666/4952667/4952668/4952669/4952670/4952671/4952672/4952673/4952674/4952675/4952676/4952677/4952678/4952679/4952680/4952681/4952682/4952683/4952684/4952685/4952686/4952687/4952688/4952689/4952690/4952691/4952692/4952693/4952694/4952695/4952696/4952697/4952698/4952699/4952700/4952701/4952702/4952703/4952704/4952705/4952706/4952707/4952708/4952709/4952710/4952711/4952712/4952713/4952714/4952715/4952716/4952717/4952718/4952719/4952720/4952721/4952722/4952723/4952724/4952725/4952726/4952727/4952728/4952729/4952730/4952731/4952732/4952733/4952734/4952735/4952736/4952737/4952738/4952739/4952740/4952741/4952742/4952743/4952744/4952745/4952746/4952747/4952748/4952749/4952750/4952751/4952752/4952753/4952754/4952755/4952756/4952757/4952758/4952759/4952760/4952761/4952762/4952763/4952764/4952765/4952766/4952767/4952768/4952769/4952770/4952771/4952772/4952773/4952774/4952775/4952776/4952777/4952778/4952779/4952780/4952781/4952782/4952783/4952784/4952785/4952786/4952787/4952788/4952789/4952790/4952791/4952792/4952793/4952794/4952795/4952796/4952797/4952798/4952799/4952800/4952801/4952802/4952803/4952804/4952805/4952806/4952807/4952808/4952809/4952810/4952811/4952812/4952813/4952814/4952815/4952816/4952817/4952818/4952819/4952820/4952821/4952822/4952823/4952824/4952825/4952826/4952827/4952828/4952829/4952830/4952831/4952832/4952833/4952834/4952835/4952836/4952837/4952838/4952839/4952840/4952841/4952842/4952843/4952844/4952845/4952846/4952847/4952848/4952849/4952850/4952851/4952852/4952853/4952854/4952855/4952856/4952857/4952858/4952859/4952860/4952861/4952862/4952863/4952864/4952865/4952866/4952867/4952868/4952869/4952870/4952871/4952872/4952873/4952874/4952875/4952876/4952877/4952878/4952879/4952880/4952881/4952882/4952883/4952884/4952885/4952886/4952887/4952888/4952889/4952890/4952891/4952892/4952893/4952894/4952895/4952896/4952897/4952898/4952899/4952900/4952901/4952902/4952903/4952904/4952905/4952906/4952907/4952908/4952909/4952910/4952911/4952912/4952913/4952914/4952915/4952916/4952917/4952918/4952919/4952920/4952921/4952922/4952923/4952924/4952925/4952926/4952927/4952928/4952929/4952930/4952931/4952932/4952933/4952934/4952935/4952936/4952937/4952938/4952939/4952940/4952941/4952942/4952943/4952944/4952945/4952946/4952947/4952948/4952949/4952950/4952951/4952952/4952953/4952954/4952955/4952956/4952957/4952958/4952959/4952960/4952961/4952962/4952963/4952964/4952965/4952966/4952967/4952968/4952969/4952970/4952971/4952972/4952973/4952974/4952975/4952976/4952977/4952978/4952979/4952980/4952981/4952982/4952983/4952984/4952985/4952986/4952987/4952988/4952989/4952990/4952991/4952992/4952993/4952994/4952995/4952996/4952997/4952998/4952999/49521000/49521001/49521002/49521003/49521004/49521005/49521006/49521007/49521008/49521009/49521010/49521011/49521012/49521013/49521014/49521015/49521016/49521017/49521018/49521019/49521020/49521021/49521022/49521023/49521024/49521025/49521026/49521027/49521028/49521029/49521030/49521031/49521032/49521033/49521034/49521035/49521036/49521037/49521038/49521039/49521040/49521041/49521042/49521043/49521044/49521045/49521046/49521047/49521048/49521049/49521050/49521051/49521052/49521053/49521054/49521055/49521056/49521057/49521058/49521059/49521060/49521061/49521062/49521063/49521064/49521065/49521066/49521067/49521068/49521069/49521070/49521071/49521072/49521073/49521074/49521075/49521076/49521077/49521078/49521079/49521080/49521081/49521082/49521083/49521084/49521085/49521086/49521087/49521088/49521089/49521090/49521091/49521092/49521093/49521094/49521095/49521096/49521097/49521098/49521099/49521100/49521101/49521102/49521103/49521104/49521105/49521106/49521107/49521108/49521109/49521110/49521111/49521112/49521113/49521114/49521115/49521116/49521117/49521118/49521119/49521120/49521121/49521122/49521123/49521124/49521125/49521126/49521127/49521128/49521129/49521130/49521131/49521132/49521133/49521134/49521135/49521136/49521137/49521138/49521139/49521140/49521141/49521142/49521143/49521144/49521145/49521146/49521147/49521148/49521149/49521150/49521151/49521152/49521153/49521154/49521155/49521156/49521157/49521158/49521159/49521160/49521161/49521162/49521163/49521164/49521165/49521166/49521167/49521168/49521169/49521170/49521171/49521172/49521173/49521174/49521175/49521176/49521177/49521178/49521179/49521180/49521181/49521182/49521183/49521184/49521185/49521186/49521187/49521188/49521189/49521190/49521191/49521192/49521193/49521194/49521195/49521196/49521197/49521198/49521199/49521200/49521201/49521202/49521203/49521204/49521205/49521206/49521207/49521208/49521209/49521210/49521211/49521212/49521213/49521214/49521215/49521216/49521217/49521218/49521219/49521220/49521221/49521222/49521223/49521224/49521225/49521226/49521227/49521228/49521229/49521230/49521231/49521232/49521233/49521234/49521235/49521236/49521237/49521238/49521239/49521240/49521241/49521242/49521243/49521244/49521245/49521246/49521247/49521248/49521249/49521250/49521251/49521252/49521253/49521254/49521255/49521256/49521257/49521258/49521259/49521260/49521261/49521262/49521263/49521264/49521265/49521266/49521267/49521268/49521269/49521270/49521271/49521272/49521273/49521274/49521275/49521276/49521277/49521278/49521279/49521280/49521281/49521282/49521283/49521284/49521285/49521286/49521287/49521288/49521289/49521290/49521291/49521292/49521293/49521294/49521295/49521296/49521297/49521298/49521299/49521300/49521301/49521302/49521303/49521304/49521305/49521306/49521307/49521308/49521309/49521310/49521311/49521312/49521313/49521314/49521315/49521316/49521317/49521318/49521319/49521320/49521321/49521322/49521323/49521324/49521325/49521326/49521327/49521328/49521329/49521330/49521331/49521332/49521333/49521334/49521335/49521336/49521337/49521338/49521339/49521340/49521341/49521342/49521343/49521344/49521345/49521346/49521347/49521348/49521349/49521350/49521351/49521352/49521353/49521354/49521355/49521356/49521357/49521358/49521359/49521360/49521361/49521362/49521363/49521364/49521365/49521366/49521367/49521368/49521369/49521370/49521371/49521372/49521373/49521374/49521375/49521376/49521377/49521378/49521379/49521380/49521381/49521382/49521383/49521384/49521385/49521386/49521387/49521388/49521389/49521390/49521391/49521392/49521393/49521394/49521395/49521396/49521397/49521398/49521399/49521400/49521401/49521402/49521403/49521404/49521405/49521406/49521407/49521408/49521409/49521410/49521411/49521412/49521413/49521414/49521415/49521416/49521417/49521418/49521419/49521420/49521421/49521422/49521423/49521424/49521425/49521426/49521427/49521428/49521429/49521430/49521431/49521432/49521433/49521434/49521435/49521436/49521437/49521438/49521439/49521440/49521441/49521442/49521443/49521444/49521445/49521446/49521447/49521448/49521449/49521450/49521451/49521452/49521453/49521454/49521455/49521456/49521457/49521458/49521459/49521460/49521461/49521462/49521463/49521464/49521465/49521466/49521467/49521468/49521469/49521470/49521471/49521472/49521473/49521474/49521475/49521476/49521477/49521478/49521479/49521480/49521481/49521482/49521483/49521484/49521485/49521486/49521487/49521488/49521489/49521490/49521491/49521492/49521493/49521494/49521495/49521496/49521497/49521498/49521499/49521500/49521501/49521502/49521503/49521504/49521505/49521506/49521507/49521508/49521509/49521510/49521511/49521512/49521513/49521514/49521515/49521516/49521517/49521518/49521519/49521520/49521521/49521522/49521523/49521524/49521525/49521526/49521527/49521528/49521529/49521530/49521531/49521532/49521533/49521534/49521535/49521536/49521537/49521538/49521539/49521540/49521541/49521542/49521543/49521544/49521545/49521546/49521547/49521548/49521549/49521550/49521551/49521552/49521553/49521554/49521555/49521556/49521557/49521558/49521559/49521560/49521561/49521562/49521563/49521564/49521565/49521566/49521567/49521568/49521569/49521570/49521571/49521572/49521573/49521574/49521575/49521576/49521577/49521578/49521579/49521580/49521581/49521582/49521583/49521584/49521585/49521586/49521587/49521588/49521589/49521590/49521591/49521592/49521593/49521594/49521595/49521596/49521597/49521598/49521599/49521600/49521601/49521602/49521603/49521604/49521605/49521606/49521607/49521608/49521609/49521610/49521611/49521612/49521613/49521614/49521615/49521616/49521617/49521618/49521619/49521620/49521621/49521622/49521623/49521624/49521625/49521626/49521627/49521628/49521629/49521630/49521631/49521632/49521633/49521634/49521635/49521636/49521637/49521638/49521639/49521640/49521641/49521642/49521643/49521644/49521645/49521646/49521647/49521648/49521649/49521650/49521651/49521652/49521653/49521654/49521655/49521656/49521657/49521658/49521659/49521660/49521661/49521662/49521663/49521664/49521665/49521666/49521667/49521668/49521669/49521670/49521671/49521672/49521673/49521674/49521675/49521676/49521677/49521678/49521679/49521680/49521681/49521682/49521683/49521684/49521685/49521686/49521687/49521688/49521689/49521690/49521691/49521692/49521693/49521694/49521695/49521696/49521697/49521698/49521699/49521700/49521701/49521702/49521703/49521704/49521705/49521706/49521707/49521708/49521709/49521710/49521711/49521712/49521713/49521714/49521715/49521716/49521717/49521718/49521719/49521720/49521721/49521722/49521723/49521724/49521725/49521726/49521727/49521728/49521729/49521730/49521731/49521732/49521733/49521734/49521735/49521736/49521737/49521738/49521739/49521740/49521741/49521742/49521743/49521744/49521745/49521746/49521747/49521748/49521749/49521750/49521751/49521752/49521753/49521754/49521755/49521756/49521757/49521758/49521759/49521760/49521761/49521762/49521763/49521764/49521765/49521766/49521767/49521768/49521769/49521770/49521771/49521772/49521773/49521774/49521775/49521776/49521777/49521778/49521779/49521780/49521781/49521782/49521783/49521784/49521785/49521786/49521787/49521788/49521789/49521790/49521791/49521792/49521793/49521794/49521795/49521796/49521797/49521798/49521799/49521800/49521801/49521802/49521803/49521804/49521805/49521806/49521807/49521808/49521809/49521810/49521811/49521812/49521813/49521814/49521815/49521816/49521817/49521818/49521819/49521820/49521821/49521822/49521823/49521824/49521825/49521826/49521827/49521828/49521829/49521830/49521831/49521832/49521833/49521834/49521835/49521836/49521837/49521838/49521839/49521840/49521841/49521842/49521843/49521844/49521845/49521846/49521847/49521848/49521849/49521850/49521851/49521852/49521853/49521854/49521855/49521856/49521857/49521858/49521859/49521860/49521861/49521862/49521863/49521864/49521865/49521866/49521867/49521868/49521869/49521870/49521871/49521872/49521873/49521874/49521875/49521876/49521877/49521878/49521879/49521880/49521881/49521882/49521883/49521884/49521885/49521886/49521887/49521888/49521889/49521890/49521891/49521892/49521893/49521894/49521895/49521896/49521897/49521898/49521899/49521900/49521901/49521902/49521903/49521904/49521905/49521906/49521907/49521908/49521909/49521910/49521911/49521912/49521913/49521914/49521915/49521916/49521917/49521918/49521919/49521920/49521921/49521922/49521923/49521924/49521925/49521926/49521927/49521928/49521929/49521930/49521931/49521932/49521933/49521934/49521935/49521936/49521937/49521938/49521939/49521940/49521941/49521942/49521943/49521944/49521945/49521946/49521947/49521948/49521949/49521950/49521951/49521952/49521953/49521954/49521955/49521956/49521957/49521958/49521959/49521960/49521961/49521962/49521963/49521964/49521965/49521966/49521967/49521968/49521969/49521970/49521971/49521972/49521973/49521974/49521975/49521976/49521977/49521978/49521979/49521980/49521981/49521982/49521983/49521984/49521985/49521986/49521987/49521988/49521989/49521990/49521991/49521992/49521993/49521994/49521995/49521996/49521997/49521998/49521999/49522000/49522001/49522002/49522003/49522004/49522005/49522006/49522007/49522008/49522009/49522010/49522011/49522012/49522013/49522014/49522015/49522016/49522017/49522018/49522019/49522020/49522021/49522022/49522023/49522024/49522025/49522026/49522027/49522028/49522029/49522030/49522031/49522032/49522033/49522034/49522035/49522036/49522037/49522038/49522039/49522040/49522041/49522042/49522043/49522044/49522045/49522046/49522047/49522048/49522049/49522050/49522051/49522052/49522053/49522054/49522055/49522056/49522057/49522058/49522059/49522060/49522061/49522062/49522063/49522064/49522065/49522066/49522067/49522068/49522069/49522070/49522071/49522072/49522073/49522074/49522075/49522076/49522077/49522078/49522079/49522080/49522081/49522082/49522083/49522084/49522085/49522086/49522087/49522088/49522089/49522090/49522091/49522092/49522093/49522094/49522095/49522096/49522097/49522098/49522099/49522100/49522101/49522102/49522103/49522104/49522105/49522106/49522107/49522108/49522109/49522110/49522111/49522112/49522113/49522114/49522115/49522116/49522117/49522118/49522119/49522120/49522121/49522122/49522123/49522124/49522125/49522126/49522127/49522128/49522129/49522130/49522131/49522132/49522133/49522134/49522135/49522136/49522137/49522138/49522139/49522140/49522141/49522142/49522143/49522144/49522145/49522146/49522147/49522148/49522149/49522150/49522151/49522152/49522153/49522154/49522155/49522156/49522157/49522158/49522159/49522160/49522161/49522162/49522163/49522164/49522165/49522166/49522167/49522168/49522169/49522170/49522171/49522172/49522173/49522174/49522175/49522176/49522177/49522178/49522179/49522180/49522181/49522182/49522183/49522184/49522185/49522186/49522187/49522188/49522189/49522190/49522191/49522192/49522193/49522194/49522195/49522196/49522197/49522198/49522199/49522200/49522201/49522202/49522203/49522204/49522205/49522206/49522207/49522208/49522209/49522210/49522211/49522212/49522213/49522214/49522215/49522216/49522217/49522218/49522219/49522220/49522221/49522222/49522223/49522224/49522225/49522226/49522227/49522228/49522229/49522230/49522231/49522232/49522233/49522234/49522235/49522236/49522237/49522238/49522239/49522240/49522241/49522242/49522243/49522244/49522245/49522246/49522247/49522248/49522249/49522250/49522251/49522252/49522253/49522254/49522255/49522256/49522257/49522258/49522259/49522260/49522261/49522262/49522263/49522264/49522265/49522266/49522267/49522268/49522269/49522270/49522271/49522272/49522273/49522274/49522275/49522276/49522277/49522278/49522279/49522280/49522281/49522282/49522283/49522284/49522285/49522286/49522287/49522288/49522289/49522290/49522291/49522292/49522293/49522294/49522295/49522296/49522297/49522298/49522299/49522300/49522301/49522302/49522303/49522304/49522305/49522306/49522307/49522308/49522309/49522310/49522311/49522312/49522313/49522314/49522315/49522316/49522317/49522318/49522319/49522320/49522321/49522322/49522323/49522324/49522325/49522326/49522327/49522328/49522329/49522330/49522331/49522332/49522333/49522334/49522335/49522336/49522337/49522338/49522339/49522340/49522341/49522342/49522343/49522344/49522345/49522346/49522347/49522348/49522349/49522350/49522351/49522352/49522353/49522354/49522355/49522356/49522357/49522358/49522359/49522360/49522361/49522362/49522363/49522364/49522365/49522366/49522367/49522368/49522369/49522370/49522371/49522372/49522373/49522374/49522375/49522376/49522377/49522378/49522379/49522380/49522381/49522382/49522383/49522384/49522385/49522386/49522387/49522388/49522389/49522390/49522391/49522392/49522393/49522394/49522395/49522396/49522397/49522398/49522399/49522400/49522401/49522402/49522403/49522404/49522405/49522406/49522407/49522408/49522409/49522410/49522411/49522412/49522413/49522414/49522415/49522416/49522417/49522418/49522419/49522420/49522421/49522422/49522423/49522424/49522425/49522426/49522427/49522428/49522429/49522430/49522431/49522432/49522433/49522434/49522435/49522436/49522437/49522438/49522439/49522440/49522441/49522442/49522443/49522444/49522445/49522446/49522447/49522448/49522449/49522450/49522451/49522452/49522453/49522454/49522455/49522456/49522457/49522458/49522459/49522460/49522461/49522462/49522463/49522464/49522465/49522466/49522467/49522468/49522469/49522470/49522471/49522472/49522473/49522474/49522475/49522476/49522477/49522478/49522479/49522480/49522481/49522482/49522483/49522484/49522485/49522486/49522487/49522488/49522489/49522490/49522491/49522492/49522493/49522494/49522495/49522496/49522497/49522498/49522499/49522500/49522501/49522502/49522503/49522504/49522505/49522506/49522507/49522508/49522509/49522510/49522511/49522512/49522513/49522514/49522515/49522516/49522517/49522518/49522519/49522520/49522521/49522522/49522523/49522524/49522525/49522526/49522527/49522528/49522529/49522530/49522531/49522532/49522533/49522534/49522535/49522536/49522537/49522538/49522539/49522540/49522541/49522542/49522543/49522544/49522545/49522546/49522547/49522548/49522549/49522550/49522551/49522552/49522553/49522554/49522555/49522556/49522557/49522558/49522559/49522560/49522561/49522562/49522563/49522564/49522565/49522566/49522567/49522568/49522569/49522570/49522571/49522572/49522573/49522574/49522575/49522576/49522577/49522578/49522579/49522580/49522581/49522582/49522583/49522584/49522585/49522586/49522587/49522588/49522589/49522590/49522591/49522592/49522593/49522594/49522595/49522596/49522597/49522598/49522599/49522600/49522601/49522602/49522603/49522604/49522605/49522606/49522607/49522608/49522609/49522610/49522611/49522612/49522613/49522614/49522615/49522616/49522617/49522618/49522619/49522620/49522621/49522622/49522623/49522624/49522625/49522626/49522627/49522628/49522629/49522630/49522631/49522632/49522633/49522634/49522635/49522636/49522637/49522638/49522639/49522640/49522641/49522642/49522643/49522644/49522645/49522646/49522647/49522648/49522649/49522650/49522651/49522652/49522653/49522654/49522655/49522656/49522657/49522658/49522659/49522660/49522661/49522662/49522663/49522664/49522665/49522666/49522667/49522668/49522669/49522670/49522671/49522672/49522673/49522674/49522675/49522676/49522677/49522678/49522679/49522680/49522681/49522682/49522683/49522684/49522685/49522686/49522687/49522688/49522689/49522690/49522691/49522692/49522693/49522694/49522695/49522696/49522697/49522698/49522699/49522700/49522701/49522702/49522703/49522704/49522705/49522706/49522707/49522708/49522709/49522710/49522711/49522712/49522713/49522714/49522715/49522716/49522717/49522718/49522719/49522720/49522721/49522722/49522723/49522724/49522725/49522726/49522727/49522728/49522729/49522730/49522731/49522732/49522733/49522734/49522735/49522736/49522737/49522738/49522739/49522740/49522741/49522742/49522743/49522744/49522745/49522746/49522747/49522748/49522749/49522750/49522751/49522752/49522753/49522754/49522755/49522756/49522757/49522758/49522759/49522760/49522761/49522762/49522763/49522764/49522765/49522766/49522767/49522768/49522769/49522770/49522771/49522772/49522773/49522774/49522775/49522776/49522777/49522778/49522779/49522780/49522781/49522782/49522783/49522784/49522785/49522786/49522787/49522788/49522789/49522790/49522791/49522792/49522793/49522794/49522795/49522796/49522797/49522798/49522799/49522800/49522801/49522802/49522803/49522804/49522805/49522806/49522807/49522808/49522809/49522810/49522811/49522812/49522813/49522814/49522815/49522816/49522817/49522818/49522819/49522820/49522821/49522822/49522823/49522824/49522825/49522826/49522827/49522828/49522829/49522830/49522831/49522832/49522833/49522834/49522835/49522836/49522837/49522838/49522839/49522840/49522841/49522842/49522843/49522844/49522845/49522846/49522847/49522848/49522849/49522850/49522851/49522852/49522853/49522854/49522855/49522856/49522857/49522858/49522859/49522860/49522861/49522862/49522863/49522864/49522865/49522866/49522867/49522868/49522869/49522870/49522871/49522872/49522873/49522874/49522875/49522876/49522877/49522878/49522879/49522880/49522881/49522882/49522883/49522884/49522885/49522886/49522887/49522888/49522889/49522890/49522891/49522892/49522893/49522894/49522895/49522896/49522897/49522898/49522899/49522900/49522901/49522902/49522903/49522904/49522905/49522906/49522907/49522908/49522909/49522910/49522911/49522912/49522913/49522914/49522915/49522916/49522917/49522918/49522919/49522920/49522921/49522922/49522923/49522924/49522925/49522926/49522927/49522928/49522929/49522930/49522931/49522932/49522933/49522934/49522935/49522936/49522937/49522938/49522939/49522940/49522941/49522942/49522943/49522944/49522945/49522946/49522947/49522948/49522949/49522950/49522951/49522952/49522953/49522954/49522955/49522956/49522957/49522958/49522959/49522960/49522961/49522962/49522963/49522964/49522965/49522966/49522967/49522968/49522969/49522970/49522971/49522972/49522973/49522974/49522975/49522976/49522977/49522978/49522979/49522980/49522981/49522982/49522983/49522984/49522985/49522986/49522987/49522988/49522989/49522990/49522991/49522992/49522993/49522994/49522995/49522996/49522997/49522998/49522999/49523000/49523001/49523002/49523003/49523004/49523005/49523006/49523007/49523008/49523009/49523010/49523011/49523012/49523013/49523014/49523015/49523016/49523017/49523018/49523019/49523020/49523021/49523022/49523023/49523024/49523025/49523026/49523027/49523028/49523029/49523030/49523031/49523032/49523033/49523034/49523035/49523036/49523037/49523038/49523039/49523040/49523041/49523042/49523043/49523044/49523045/49523046/49523047/49523048/49523049/49523050/49523051/49523052/49523053/49523054/49523055/49523056/49523057/49523058/49523059/49523060/49523061/49523062/49523063/49523064/49523065/49523066/49523067/49523068/49523069/49523070/49523071/49523072/49523073/49523074/49523075/49523076/49523077/49523078/49523079/49523080/49523081/49523082/49523083/49523084/49523085/49523086/49523087/49523088/49523089/49523090/49523091/49523092/49523093/49523094/49523095/49523096/49523097/49523098/49523099/49523100/49523101/49523102/49523103/49523104/49523105/49523106/49523107/49523108/49523109/49523110/49523111/49523112/49523113/49523114/49523115/49523116/49523117/49523118/49523119/49523120/49523121/49523122/49523123/49523124/49523125/49523126/49523127/49523128/49523129/49523130/49523131/49523132/49523133/49523134/49523135/49523136/49523137/49523138/49523139/49523140/49523141/49523142/49523143/49523144/49523145/49523146/49523147/49523148/49523149/49523150/49523151/49523152/49523153/49523154/49523155/49523156/49523157/49523158/49523159/49523160/49523161/49523162/49523163/49523164/49523165/49523166/49523167/49523168/49523169/49523170/49523171/49523172/49523173/49523174/49523175/49523176/49523177/49523178/49523179/49523180/49523181/49523182/49523183/49523184/49523185/49523186/49523187/49523188/49523189/49523190/49523191/49523192/49523193/49523194/49523195/49523196/49523197/49523198/49523199/49523200/49523201/49523202/49523203/49523204/49523205/49523206/49523207/49523208/49523209/49523210/49523211/49523212/49523213/49523214/49523215/49523216/49523217/49523218/49523219/49523220/49523221/49523222/49523223/49523224/49523225/49523226/49523227/49523228/49523229/49523230/49523231/49523232/49523233/49523234/49523235/49523236/49523237/49523238/49523239/49523240/49523241/49523242/49523243/49523244/49523245/49523246/49523247/49523248/49523249/49523250/49523251/49523252/49523253/49523254/49523255/49523256/49523257/49523258/49523259/49523260/49523261/49523262/49523263/49523264/49523265/49523266/49523267/49523268/49523269/49523270/49523271/49523272/49523273/49523274/49523275/49523276/49523277/49523278/49523279/49523280/49523281/49523282/49523283/49523284/49523285/49523286/49523287/49523288/49523289/49523290/49523291/49523292/49523293/49523294/49523295/49523296/49523297/49523298/49523299/49523300/49523301/49523302/49523303/49523304/49523305/49523306/49523307/49523308/49523309/49523310/49523311/49523312/49523313/49523314/49523315/49523316/49523317/49523318/49523319/49523320/49523321/49523322/49523323/49523324/49523325/49523326/49523327/49523328/49523329/49523330/49523331/49523332/49523333/49523334/49523335/49523336/49523337/49523338/49523339/49523340/49523341/49523342/49523343/49523344/49523345/49523346/49523347/49523348/49523349/49523350/49523351/49523352/49523353/49523354/49523355/49523356/49523357/49523358/49523359/49523360/49523361/49523362/49523363/49523364/49523365/49523366/49523367/49523368/49523369/49523370/49523371/49523372/49523373/49523374/49523375/49523376/49523377/49523378/49523379/49523380/49523381/49523382/49523383/49523384/49523385/49523386/49523387/49523388/49523389/49523390/49523391/49523392/49523393/49523394/49523395/49523396/49523397/49523398/49523399/49523400/49523401/49523402/49523403/49523404/49523405/49523406/49523407/49523408/49523409/49523410/49523411/49523412/49523413/49523414/49523415/49523416/49523417/49523418/49523419/49523420/49523421/49523422/49523423/49523424/49523425/49523426/49523427/49523428/49523429/49523430/49523431/49523432/49523433/49523434/49523435/49523436/49523437/49523438/49523439/49523440/49523441/49523442/49523443/49523444/49523445/49523446/49523447/49523448/49523449/49523450/49523451/49523452/49523453/49523454/49523455/49523456/49523457/49523458/49523459/49523460/49523461/49523462/49523463/49523464/49523465/49523466/49523467/49523468/49523469/49523470/49523471/49523472/49523473/49523474/49523475/49523476/49523477/49523478/49523479/49523480/49523481/49523482/49523483/49523484/49523485/49523486/49523487/49523488/49523489/49523490/49523491/49523492/49523493/49523494/49523495/49523496/49523497/49523498/49523499/49523500/49523501/49523502/49523503/49523504/49523505/49523506/49523507/49523508/49523509/49523510/49523511/49523512/49523513/49523514/49523515/49523516/49523517/49523518/49523519/49523520/49523521/49523522/49523523/49523524/49523525/49523526/49523527/49523528/49523529/49523530/49523531/49523532/49523533/49523534/49523535/49523536/49523537/49523538/49523539/49523540/49523541/49523542/49523543/49523544/49523545/49523546/49523547/49523548/49523549/49523550/49523551/49523552/49523553/49523554/49523555/49523556/49523557/49523558/49523559/49523560/49523561/49523562/49523563/49523564/49523565/49523566/49523567/49523568/49523569/49523570/49523571/49523572/49523573/49523574/49523575/49523576/49523577/49523578/49523579/49523580/49523581/49523582/49523583/49523584/49523585/49523586/49523587/49523588/49523589/49523590/49523591/49523592/49523593/49523594/49523595/49523596/49523597/49523598/49523599/49523600/49523601/49523602/49523603/49523604/49523605/49523606/49523607/49523608/49523609/49523610/49523611/49523612/49523613/49523614/49523615/49523616/49523617/49523618/49523619/49523620/49523621/49523622/49523623/49523624/49523625/49523626/49523627/49523628/49523629/49523630/49523631/49523632/49523633/49523634/49523635/49523636/49523637/49523638/49523639/49523640/49523641/49523642/49523643/49523644/49523645/49523646/49523647/49523648/49523649/49523650/49523651/49523652/49523653/49523654/49523655/49523656/49523657/49523658/49523659/49523660/49523661/49523662/49523663/49523664/49523665/49523666/49523667/49523668/49523669/49523670/49523671/49523672/49523673/49523674/49523675/49523676/49523677/49523678/49523679/49523680/49523681/49523682/49523683/49523684/49523685/49523686/49523687/49523688/49523689/49523690/49523691/49523692/49523693/49523694/49523695/49523696/49523697/49523698/49523699/49523700/49523701/49523702/49523703/49523704/49523705/49523706/49523707/49523708/49523709/49523710/49523711/49523712/49523713/49523714/49523715/49523716/49523717/49523718/49523719/49523720/49523721/49523722/49523723/49523724/49523725/49523726/49523727/49523728/49523729/49523730/49523731/49523732/49523733/49523734/49523735/49523736/49523737/49523738/49523739/49523740/49523741/49523742/49523743/49523744/49523745/49523746/49523747/49523748/49523749/49523750/49523751/49523752/49523753/49523754/49523755/49523756/49523757/49523758/49523759/49523760/49523761/49523762/49523763/49523764/49523765/49523766/49523767/49523768/49523769/49523770/49523771/49523772/49523773/49523774/49523775/49523776/49523777/49523778/49523779/49523780/49523781/49523782/49523783/49523784/49523785/49523786/49523787/49523788/49523789/49523790/49523791/49523792/49523793/49523794/49523795/49523796/49523797/49523798/49523799/49523800/49523801/49523802/49523803/49523804/49523805/49523806/49523807/49523808/49523809/49523810/49523811/49523812/49523813/49523814/49523815/49523816/49523817/49523818/49523819/49523820/49523821/49523822/49523823/49523824/49523825/49523826/49523827/49523828/49523829/49523830/49523831/49523832/49523833/49523834/49523835/49523836/49523837/49523838/49523839/49523840/49523841/49523842/49523843/49523844/49523845/49523846/49523847/49523848/49523849/49523850/49523851/49523852/49523853/49523854/49523855/49523856/49523857/49523858/49523859/49523860/49523861/49523862/49523863/49523864/49523865/49523866/49523867/49523868/49523869/49523870/49523871/49523872/49523873/49523874/49523875/49523876/49523877/49523878/49523879/49523880/49523881/49523882/49523883/49523884/49523885/49523886/49523887/49523888/49523889/49523890/49523891/49523892/49523893/49523894/49523895/49523896/49523897/49523898/49523899/49523900/49523901/49523902/49523903/49523904/49523905/49523906/49523907/49523908/49523909/49523910/49523911/49523912/49523913/49523914/49523915/49523916/49523917/49523918/49523919/49523920/49523921/49523922/49523923/49523924/49523925/49523926/49523927/49523928/49523929/49523930/49523931/49523932/49523933/49523934/49523935/49523936/49523937/49523938/49523939/49523940/49523941/49523942/49523943/49523944/49523945/49523946/49523947/49523948/49523949/49523950/49523951/49523952/49523953/49523954/49523955/49523956/49523957/49523958/49523959/49523960/49523961/49523962/49523963/49523964/49523965/49523966/49523967/49523968/49523969/49523970/49523971/49523972/49523973/49523974/49523975/49523976/49523977/49523978/49523979/49523980/49523981/49523982/49523983/49523984/49523985/49523986/49523987/49523988/49523989/49523990/49523991/49523992/49523993/49523994/49523995/49523996/49523997/49523998/49523999/49524000/49524001/49524002/49524003/49524004/49524005/49524006/49524007/49524008/49524009/49524010/49524011/49524012/49524013/49524014/49524015/49524016/49524017/49524018/49524019/49524020/49524021/49524022/49524023/49524024/49524025/49524026/49524027/49524028/49524029/49524030/49524031/49524032/49524033/49524034/49524035/49524036/49524037/49524038/49524039/49524040/49524041/49524042/49524043/49524044/49524045/49524046/49524047/49524048/49524049/49524050/49524051/49524052/49524053/49524054/49524055/49524056/49524057/49524058/49524059/49524060/49524061/49524062/49524063/49524064/49524065/49524066/49524067/49524068/49524069/49524070/49524071/49524072/49524073/49524074/49524075/49524076/49524077/49524078/49524079/49524080/49524081/49524082/49524083/49524084/49524085/49524086/49524087/49524088/49524089/49524090/49524091/49524092/49524093/49524094/49524095/49524096/49524097/49524098/49524099/49524100/49524101/49524102/49524103/49524104/49524105/49524106/49524107/49524108/49524109/49524110/49524111/49524112/49524113/49524114/49524115/49524116/49524117/49524118/49524119/49524120/49524121/49524122/49524123/49524124/49524125/49524126/49524127/49524128/49524129/49524130/49524131/49524132/49524133/49524134/49524135/49524136/49524137/49524138/49524139/49524140/49524141/49524142/49524143/49524144/49524145/49524146/49524147/49524148/49524149/49524150/49524151/49524152/49524153/49524154/49524155/49524156/49524157/49524158/49524159/49524160/49524161/49524162/49524163/49524164/49524165/49524166/49524167/49524168/49524169/49524170/49524171/49524172/49524173/49524174/49524175/49524176/49524177/49524178/49524179/49524180/49524181/49524182/49524183/49524184/49524185/49524186/49524187/49524188/49524189/49524190/49524191/49524192/49524193/49524194/49524195/49524196/49524197/49524198/49524199/49524200/49524201/49524202/49524203/49524204/49524205/49524206/49524207/49524208/49524209/49524210/49524211/49524212/49524213/49524214/49524215/49524216/49524217/49524218/49524219/49524220/49524221/49524222/49524223/49524224/49524225/49524226/49524227/49524228/49524229/49524230/49524231/49524232/49524233/49524234/49524235/49524236/49524237/49524238/49524239/49524240/49524241/49524242/49524243/49524244/49524245/49524246/49524247/49524248/49524249/49524250/49524251/49524252/49524253/49524254/49524255/49524256/49524257/49524258/49524259/49524260/49524261/49524262/49524263/49524264/49524265/49524266/49524267/49524268/49524269/49524270/49524271/49524272/49524273/49524274/49524275/49524276/49524277/49524278/49524279/49524280/49524281/49524282/49524283/49524284/49524285/49524286/49524287/49524288/49524289/49524290/49524291/49524292/49524293/49524294/49524295/49524296/49524297/49524298/49524299/49524300/49524301/49524302/49524303/49524304/49524305/49524306/49524307/49524308/49524309/49524310/49524311/49524312/49524313/49524314/49524315/49524316/49524317/49524318/49524319/49524320/49524321/49524322/49524323/49524324/49524325/49524326/49524327/49524328/49524329/49524330/49524331/49524332/49524333/49524334/49524335/49524336/49524337/49524338/49524339/49524340/49524341/49524342/49524343/49524344/49524345/49524346/49524347/49524348/49524349/49524350/49524351/49524352/49524353/49524354/49524355/49524356/49524357/49524358/49524359/49524360/49524361/49524362/49524363/49524364/49524365/49524366/49524367/49524368/49524369/49524370/49524371/49524372/49524373/49524374/49524375/49524376/49524377/49524378/49524379/49524380/49524381/49524382/49524383/49524384/49524385/49524386/49524387/49524388/49524389/49524390/49524391/49524392/49524393/49524394/49524395/49524396/49524397/49524398/49524399/49524400/49524401/49524402/49524403/49524404/49524405/49524406/49524407/49524408/49524409/49524410/49524411/49524412/49524413/49524414/49524415/49524416/49524417/49524418/49524419/49524420/49524421/49524422/49524423/49524424/49524425/49524426/49524427/49524428/49524429/49524430/49524431/49524432/49524433/49524434/49524435/49524436/49524437/49524438/49524439/49524440/49524441/49524442/49524443/49524444/49524445/49524446/49524447/49524448/49524449/49524450/49524451/49524452/49524453/49524454/49524455/49524456/49524457/49524458/49524459/49524460/49524461/49524462/49524463/49524464/49524465/49524466/49524467/49524468/49524469/49524470/49524471/49524472/49524473/49524474/49524475/49524476/49524477/49524478/49524479/49524480/49524481/49524482/49524483/49524484/49524485/49524486/49524487/49524488/49524489/49524490/49524491/49524492/49524493/49524494/49524495/49524496/49524497/49524498/49524499/49524500/49524501/49524502/49524503/49524504/49524505/49524506/49524507/49524508/49524509/49524510/49524511/49524512/49524513/49524514/49524515/49524516/49524517/49524518/49524519/49524520/49524521/49524522/49524523/49524524/49524525/49524526/49524527/49524528/49524529/49524530/49524531/49524532/49524533/49524534/49524535/49524536/49524537/49524538/49524539/49524540/49524541/49524542/49524543/49524544/49524545/49524546/49524547/49524548/49524549/49524550/49524551/49524552/49524553/49524554/49524555/49524556/49524557/49524558/49524559/49524560/49524561/49524562/49524563/49524564/49524565/49524566/49524567/49524568/49524569/49524570/49524571/49524572/49524573/49524574/49524575/49524576/49524577/49524578/49524579/49524580/49524581/49524582/49524583/49524584/49524585/49524586/49524587/49524588/49524589/49524590/49524591/49524592/49524593/49524594/49524595/49524596/49524597/49524598/49524599/49524600/49524601/49524602/49524603/49524604/49524605/49524606/49524607/49524608/49524609/49524610/49524611/49524612/49524613/49524614/49524615/49524616/49524617/49524618/49524619/49524620/49524621/49524622/49524623/49524624/49524625/49524626/49524627/49524628/49524629/49524630/49524631/49524632/49524633/49524634/49524635/49524636/49524637/49524638/49524639/49524640/49524641/49524642/49524643/49524644/49524645/49524646/49524647/49524648/49524649/49524650/49524651/49524652/49524653/49524654/49524655/49524656/49524657/49524658/49524659/49524660/49524661/49524662/49524663/49524664/49524665/49524666/49524667/49524668/49524669/49524670/49524671/49524672/49524673/49524674/49524675/49524676/49524677/49524678/49524679/49524680/49524681/49524682/49524683/49524684/49524685/49524686/49524687/49524688/49524689/49524690/49524691/49524692/49524693/49524694/49524695/49524696/49524697/49524698/49524699/49524700/49524701/49524702/49524703/49524704/49524705/49524706/49524707/49524708/49524709/49524710/49524711/49524712/49524713/49524714/49524715/49524716/49524717/49524718/49524719/49524720/49524721/49524722/49524723/49524724/49524725/49524726/49524727/49524728/49524729/49524730/49524731/49524732/49524733/49524734/49524735/49524736/49524737/49524738/49524739/49524740/49524741/49524742/49524743/49524744/49524745/49524746/49524747/49524748/49524749/49524750/49524751/49524752/49524753/49524754/49524755/49524756/49524757/49524758/49524759/49524760/49524761/49524762/49524763/49524764/49524765/49524766/49524767/49524768/49524769/49524770/49524771/49524772/49524773/49524774/49524775/49524776/49524777/49524778/49524779/49524780/49524781/49524782/49524783/49524784/49524785/49524786/49524787/49524788/49524789/49524790/49524791/49524792/49524793/49524794/49524795/49524796/49524797/49524798/49524799/49524800/49524801/49524802/49524803/49524804/49524805/49524806/49524807/49524808/49524809/49524810/49524811/49524812/49524813/49524814/49524815/49524816/49524817/49524818/49524819/49524820/49524821/49524822/49524823/49524824/49524825/49524826/49524827/49524828/49524829/49524830/49524831/49524832/49524833/49524834/49524835/49524836/49524837/49524838/49524839/49524840/49524841/49524842/49524843/49524844/49524845/49524846/49524847/49524848/49524849/49524850/49524851/49524852/49524853/49524854/49524855/49524856/49524857/49524858/49524859/49524860/49524861/49524862/49524863/49524864/49524865/49524866/49524867/49524868/49524869/49524870/49524871/49524872/49524873/49524874/49524875/49524876/49524877/49524878/49524879/49524880/49524881/49524882/49524883/49524884/49524885/49524886/49524887/49524888/49524889/49524890/49524891/49524892/49524893/49524894/49524895/49524896/49524897/49524898/49524899/49524900/49524901/49524902/49524903/49524904/49524905/49524906/49524907/49524908/49524909/49524910/49524911/49524912/49524913/49524914/49524915/49524916/49524917/49524918/49524919/49524920/49524921/49524922/49524923/49524924/49524925/49524926/49524927/49524928/49524929/49524930/49524931/49524932/49524933/49524934/49524935/49524936/49524937/49524938/49524939/49524940/49524941/49524942/49524943/49524944/49524945/49524946/49524947/49524948/49524949/49524950/49524951/49524952/4952aeroplane 0.6595
bicycle 0.6931
bird 0.6147
boat 0.4039
bottle 0.4324
bus 0.6672
car 0.7734
cat 0.8290
chair 0.3929
cow 0.6291
diningtable 0.4418
dog 0.7705
horse 0.7256
motorbike 0.7020
person 0.7668
pottedplant 0.3554
sheep 0.5195
sofa 0.5397
train 0.6954
tvmonitor 0.6480
mAP: 0.6130

Epoch 00001: saving model to ./snapshots\resnet50_pascal_01.h5
Epoch 2/2

    1/10000 [..............................] - ETA: 1:28:31 - loss: 0.3673 - regression_loss: 0.3379 - classification_loss: 0.0294
    2/10000 [..............................] - ETA: 1:25:54 - loss: 0.7266 - regression_loss: 0.6568 - classification_loss: 0.0698
    3/10000 [..............................] - ETA: 1:21:34 - loss: 0.8287 - regression_loss: 0.7352 - classification_loss: 0.0935
    4/10000 [..............................] - ETA: 1:21:59 - loss: 0.8307 - regression_loss: 0.7349 - classification_loss: 0.0958
    5/10000 [..............................] - ETA: 1:20:41 - loss: 0.8451 - regression_loss: 0.7323 - classification_loss: 0.1129
    6/10000 [..............................] - ETA: 1:19:22 - loss: 0.8367 - regression_loss: 0.7313 - classification_loss: 0.1054
    7/10000 [..............................] - ETA: 1:18:26 - loss: 0.7751 - regression_loss: 0.6707 - classification_loss: 0.1044
    8/10000 [..............................] - ETA: 1:17:44 - loss: 0.7653 - regression_loss: 0.6452 - classification_loss: 0.1200
    9/10000 [..............................] - ETA: 1:17:28 - loss: 0.7251 - regression_loss: 0.6054 - classification_loss: 0.1197
   10/10000 [..............................] - ETA: 1:16:13 - loss: 0.6776 - regression_loss: 0.5676 - classification_loss: 0.1100
   11/10000 [..............................] - ETA: 1:15:54 - loss: 0.7102 - regression_loss: 0.5640 - classification_loss: 0.1462
   12/10000 [..............................] - ETA: 1:16:17 - loss: 0.6741 - regression_loss: 0.5364 - classification_loss: 0.1377
   13/10000 [..............................] - ETA: 1:16:01 - loss: 0.6371 - regression_loss: 0.5075 - classification_loss: 0.1296
   14/10000 [..............................] - ETA: 1:15:58 - loss: 0.6782 - regression_loss: 0.5375 - classification_loss: 0.1407
   15/10000 [..............................] - ETA: 1:15:55 - loss: 0.7095 - regression_loss: 0.5712 - classification_loss: 0.1384
   16/10000 [..............................] - ETA: 1:15:53 - loss: 0.7326 - regression_loss: 0.5895 - classification_loss: 0.1431
   17/10000 [..............................] - ETA: 1:16:09 - loss: 0.7083 - regression_loss: 0.5709 - classification_loss: 0.1374
   18/10000 [..............................] - ETA: 1:15:49 - loss: 0.7004 - regression_loss: 0.5661 - classification_loss: 0.1343
   19/10000 [..............................] - ETA: 1:15:47 - loss: 0.6850 - regression_loss: 0.5502 - classification_loss: 0.1348
   20/10000 [..............................] - ETA: 1:16:08 - loss: 0.6909 - regression_loss: 0.5560 - classification_loss: 0.1349
   21/10000 [..............................] - ETA: 1:15:58 - loss: 0.6722 - regression_loss: 0.5402 - classification_loss: 0.1320
   22/10000 [..............................] - ETA: 1:15:56 - loss: 0.6869 - regression_loss: 0.5529 - classification_loss: 0.1340
   23/10000 [..............................] - ETA: 1:15:54 - loss: 0.6787 - regression_loss: 0.5443 - classification_loss: 0.1344
   24/10000 [..............................] - ETA: 1:15:52 - loss: 0.7204 - regression_loss: 0.5775 - classification_loss: 0.1430
   25/10000 [..............................] - ETA: 1:15:44 - loss: 0.7089 - regression_loss: 0.5690 - classification_loss: 0.1399
   26/10000 [..............................] - ETA: 1:15:43 - loss: 0.7610 - regression_loss: 0.6154 - classification_loss: 0.1456
   27/10000 [..............................] - ETA: 1:15:36 - loss: 0.7571 - regression_loss: 0.6121 - classification_loss: 0.1450
   28/10000 [..............................] - ETA: 1:15:35 - loss: 0.7444 - regression_loss: 0.6023 - classification_loss: 0.1421
   29/10000 [..............................] - ETA: 1:15:44 - loss: 0.7310 - regression_loss: 0.5912 - classification_loss: 0.1398
   30/10000 [..............................] - ETA: 1:15:38 - loss: 0.7202 - regression_loss: 0.5823 - classification_loss: 0.1379
   31/10000 [..............................] - ETA: 1:15:37 - loss: 0.7041 - regression_loss: 0.5696 - classification_loss: 0.1345
   32/10000 [..............................] - ETA: 1:15:41 - loss: 0.6993 - regression_loss: 0.5643 - classification_loss: 0.1350
   33/10000 [..............................] - ETA: 1:15:39 - loss: 0.6922 - regression_loss: 0.5569 - classification_loss: 0.1353
   34/10000 [..............................] - ETA: 1:15:34 - loss: 0.6852 - regression_loss: 0.5515 - classification_loss: 0.1337
   35/10000 [..............................] - ETA: 1:15:33 - loss: 0.6808 - regression_loss: 0.5486 - classification_loss: 0.1322
   36/10000 [..............................] - ETA: 1:15:27 - loss: 0.6957 - regression_loss: 0.5612 - classification_loss: 0.1345
   37/10000 [..............................] - ETA: 1:15:27 - loss: 0.7009 - regression_loss: 0.5658 - classification_loss: 0.1351
   38/10000 [..............................] - ETA: 1:15:30 - loss: 0.7043 - regression_loss: 0.5695 - classification_loss: 0.1348
   39/10000 [..............................] - ETA: 1:15:37 - loss: 0.7011 - regression_loss: 0.5677 - classification_loss: 0.1333
   40/10000 [..............................] - ETA: 1:15:40 - loss: 0.7056 - regression_loss: 0.5731 - classification_loss: 0.1325
   41/10000 [..............................] - ETA: 1:15:39 - loss: 0.7058 - regression_loss: 0.5724 - classification_loss: 0.1334
   42/10000 [..............................] - ETA: 1:15:38 - loss: 0.7225 - regression_loss: 0.5858 - classification_loss: 0.1368
   43/10000 [..............................] - ETA: 1:15:33 - loss: 0.7198 - regression_loss: 0.5841 - classification_loss: 0.1357
   44/10000 [..............................] - ETA: 1:15:39 - loss: 0.7120 - regression_loss: 0.5780 - classification_loss: 0.1340
   45/10000 [..............................] - ETA: 1:15:35 - loss: 0.7326 - regression_loss: 0.5974 - classification_loss: 0.1353
   46/10000 [..............................] - ETA: 1:15:40 - loss: 0.7259 - regression_loss: 0.5914 - classification_loss: 0.1345
   47/10000 [..............................] - ETA: 1:15:46 - loss: 0.7145 - regression_loss: 0.5825 - classification_loss: 0.1320
   48/10000 [..............................] - ETA: 1:15:41 - loss: 0.7081 - regression_loss: 0.5766 - classification_loss: 0.1315
   49/10000 [..............................] - ETA: 1:15:47 - loss: 0.7189 - regression_loss: 0.5873 - classification_loss: 0.1316
   50/10000 [..............................] - ETA: 1:15:55 - loss: 0.7234 - regression_loss: 0.5911 - classification_loss: 0.1322
   51/10000 [..............................] - ETA: 1:15:50 - loss: 0.7304 - regression_loss: 0.5971 - classification_loss: 0.1332
   52/10000 [..............................] - ETA: 1:15:55 - loss: 0.7329 - regression_loss: 0.6001 - classification_loss: 0.1328
   53/10000 [..............................] - ETA: 1:15:57 - loss: 0.7260 - regression_loss: 0.5948 - classification_loss: 0.1312
   54/10000 [..............................] - ETA: 1:15:47 - loss: 0.7176 - regression_loss: 0.5877 - classification_loss: 0.1298
   55/10000 [..............................] - ETA: 1:15:45 - loss: 0.7184 - regression_loss: 0.5898 - classification_loss: 0.1286
   56/10000 [..............................] - ETA: 1:15:50 - loss: 0.7203 - regression_loss: 0.5907 - classification_loss: 0.1297
   57/10000 [..............................] - ETA: 1:15:46 - loss: 0.7116 - regression_loss: 0.5833 - classification_loss: 0.1282
   58/10000 [..............................] - ETA: 1:15:50 - loss: 0.7020 - regression_loss: 0.5753 - classification_loss: 0.1266
   59/10000 [..............................] - ETA: 1:15:49 - loss: 0.6991 - regression_loss: 0.5727 - classification_loss: 0.1264
   60/10000 [..............................] - ETA: 1:15:45 - loss: 0.6945 - regression_loss: 0.5684 - classification_loss: 0.1261
   61/10000 [..............................] - ETA: 1:15:49 - loss: 0.6888 - regression_loss: 0.5644 - classification_loss: 0.1245
   62/10000 [..............................] - ETA: 1:15:55 - loss: 0.6806 - regression_loss: 0.5579 - classification_loss: 0.1228
   63/10000 [..............................] - ETA: 1:15:56 - loss: 0.6758 - regression_loss: 0.5516 - classification_loss: 0.1242
   64/10000 [..............................] - ETA: 1:16:00 - loss: 0.6685 - regression_loss: 0.5456 - classification_loss: 0.1229
   65/10000 [..............................] - ETA: 1:16:06 - loss: 0.6773 - regression_loss: 0.5520 - classification_loss: 0.1253
   66/10000 [..............................] - ETA: 1:16:02 - loss: 0.6734 - regression_loss: 0.5483 - classification_loss: 0.1251
   67/10000 [..............................] - ETA: 1:16:08 - loss: 0.6706 - regression_loss: 0.5460 - classification_loss: 0.1245
   68/10000 [..............................] - ETA: 1:16:04 - loss: 0.6690 - regression_loss: 0.5447 - classification_loss: 0.1242
   69/10000 [..............................] - ETA: 1:16:07 - loss: 0.6690 - regression_loss: 0.5456 - classification_loss: 0.1235
   70/10000 [..............................] - ETA: 1:16:06 - loss: 0.6740 - regression_loss: 0.5502 - classification_loss: 0.1238
   71/10000 [..............................] - ETA: 1:16:08 - loss: 0.6712 - regression_loss: 0.5475 - classification_loss: 0.1237
   72/10000 [..............................] - ETA: 1:16:05 - loss: 0.6667 - regression_loss: 0.5440 - classification_loss: 0.1227
   73/10000 [..............................] - ETA: 1:16:08 - loss: 0.6655 - regression_loss: 0.5437 - classification_loss: 0.1218
   74/10000 [..............................] - ETA: 1:16:11 - loss: 0.6576 - regression_loss: 0.5373 - classification_loss: 0.1202
   75/10000 [..............................] - ETA: 1:16:13 - loss: 0.6596 - regression_loss: 0.5390 - classification_loss: 0.1206
   76/10000 [..............................] - ETA: 1:16:16 - loss: 0.6571 - regression_loss: 0.5366 - classification_loss: 0.1204
   77/10000 [..............................] - ETA: 1:16:12 - loss: 0.6557 - regression_loss: 0.5360 - classification_loss: 0.1197
   78/10000 [..............................] - ETA: 1:16:11 - loss: 0.6536 - regression_loss: 0.5336 - classification_loss: 0.1200
   79/10000 [..............................] - ETA: 1:16:13 - loss: 0.6544 - regression_loss: 0.5345 - classification_loss: 0.1199
   80/10000 [..............................] - ETA: 1:16:12 - loss: 0.6512 - regression_loss: 0.5315 - classification_loss: 0.1197
   81/10000 [..............................] - ETA: 1:16:12 - loss: 0.6520 - regression_loss: 0.5329 - classification_loss: 0.1191
   82/10000 [..............................] - ETA: 1:16:09 - loss: 0.6479 - regression_loss: 0.5291 - classification_loss: 0.1188
   83/10000 [..............................] - ETA: 1:16:12 - loss: 0.6472 - regression_loss: 0.5281 - classification_loss: 0.1192
   84/10000 [..............................] - ETA: 1:16:12 - loss: 0.6511 - regression_loss: 0.5307 - classification_loss: 0.1205
   85/10000 [..............................] - ETA: 1:16:11 - loss: 0.6501 - regression_loss: 0.5292 - classification_loss: 0.1209
   86/10000 [..............................] - ETA: 1:16:07 - loss: 0.6472 - regression_loss: 0.5271 - classification_loss: 0.1200
   87/10000 [..............................] - ETA: 1:16:11 - loss: 0.6473 - regression_loss: 0.5272 - classification_loss: 0.1201
   88/10000 [..............................] - ETA: 1:16:14 - loss: 0.6467 - regression_loss: 0.5249 - classification_loss: 0.1218
   89/10000 [..............................] - ETA: 1:16:10 - loss: 0.6526 - regression_loss: 0.5304 - classification_loss: 0.1222
   90/10000 [..............................] - ETA: 1:16:09 - loss: 0.6518 - regression_loss: 0.5306 - classification_loss: 0.1212
   91/10000 [..............................] - ETA: 1:16:08 - loss: 0.6466 - regression_loss: 0.5258 - classification_loss: 0.1208
   92/10000 [..............................] - ETA: 1:16:08 - loss: 0.6424 - regression_loss: 0.5227 - classification_loss: 0.1198
   93/10000 [..............................] - ETA: 1:16:07 - loss: 0.6415 - regression_loss: 0.5222 - classification_loss: 0.1193
   94/10000 [..............................] - ETA: 1:16:04 - loss: 0.6427 - regression_loss: 0.5240 - classification_loss: 0.1187
   95/10000 [..............................] - ETA: 1:16:03 - loss: 0.6408 - regression_loss: 0.5229 - classification_loss: 0.1180
   96/10000 [..............................] - ETA: 1:16:01 - loss: 0.6386 - regression_loss: 0.5214 - classification_loss: 0.1172
   97/10000 [..............................] - ETA: 1:15:59 - loss: 0.6343 - regression_loss: 0.5182 - classification_loss: 0.1161
   98/10000 [..............................] - ETA: 1:15:57 - loss: 0.6421 - regression_loss: 0.5247 - classification_loss: 0.1174
   99/10000 [..............................] - ETA: 1:15:55 - loss: 0.6435 - regression_loss: 0.5258 - classification_loss: 0.1177
  100/10000 [..............................] - ETA: 1:15:53 - loss: 0.6388 - regression_loss: 0.5220 - classification_loss: 0.1168
  101/10000 [..............................] - ETA: 1:15:54 - loss: 0.6349 - regression_loss: 0.5188 - classification_loss: 0.1160
  102/10000 [..............................] - ETA: 1:15:53 - loss: 0.6421 - regression_loss: 0.5248 - classification_loss: 0.1172
  103/10000 [..............................] - ETA: 1:15:50 - loss: 0.6426 - regression_loss: 0.5227 - classification_loss: 0.1200
  104/10000 [..............................] - ETA: 1:15:49 - loss: 0.6431 - regression_loss: 0.5225 - classification_loss: 0.1206
  105/10000 [..............................] - ETA: 1:15:51 - loss: 0.6392 - regression_loss: 0.5195 - classification_loss: 0.1197
  106/10000 [..............................] - ETA: 1:15:53 - loss: 0.6421 - regression_loss: 0.5213 - classification_loss: 0.1208
  107/10000 [..............................] - ETA: 1:15:52 - loss: 0.6449 - regression_loss: 0.5241 - classification_loss: 0.1208
  108/10000 [..............................] - ETA: 1:15:46 - loss: 0.6428 - regression_loss: 0.5227 - classification_loss: 0.1201
  109/10000 [..............................] - ETA: 1:15:48 - loss: 0.6409 - regression_loss: 0.5216 - classification_loss: 0.1193
  110/10000 [..............................] - ETA: 1:15:50 - loss: 0.6389 - regression_loss: 0.5201 - classification_loss: 0.1188
  111/10000 [..............................] - ETA: 1:15:43 - loss: 0.6423 - regression_loss: 0.5233 - classification_loss: 0.1190
  112/10000 [..............................] - ETA: 1:15:46 - loss: 0.6407 - regression_loss: 0.5222 - classification_loss: 0.1184
  113/10000 [..............................] - ETA: 1:15:44 - loss: 0.6387 - regression_loss: 0.5207 - classification_loss: 0.1181
  114/10000 [..............................] - ETA: 1:15:43 - loss: 0.6395 - regression_loss: 0.5217 - classification_loss: 0.1179
  115/10000 [..............................] - ETA: 1:15:40 - loss: 0.6359 - regression_loss: 0.5185 - classification_loss: 0.1174
  116/10000 [..............................] - ETA: 1:15:41 - loss: 0.6367 - regression_loss: 0.5189 - classification_loss: 0.1177
  117/10000 [..............................] - ETA: 1:15:41 - loss: 0.6336 - regression_loss: 0.5165 - classification_loss: 0.1171
  118/10000 [..............................] - ETA: 1:15:41 - loss: 0.6353 - regression_loss: 0.5182 - classification_loss: 0.1172
  119/10000 [..............................] - ETA: 1:15:40 - loss: 0.6324 - regression_loss: 0.5158 - classification_loss: 0.1165
  120/10000 [..............................] - ETA: 1:15:42 - loss: 0.6296 - regression_loss: 0.5132 - classification_loss: 0.1164
  121/10000 [..............................] - ETA: 1:15:40 - loss: 0.6283 - regression_loss: 0.5117 - classification_loss: 0.1166
  122/10000 [..............................] - ETA: 1:15:35 - loss: 0.6251 - regression_loss: 0.5089 - classification_loss: 0.1161
  123/10000 [..............................] - ETA: 1:15:34 - loss: 0.6264 - regression_loss: 0.5102 - classification_loss: 0.1162
  124/10000 [..............................] - ETA: 1:15:33 - loss: 0.6273 - regression_loss: 0.5110 - classification_loss: 0.1163
  125/10000 [..............................] - ETA: 1:15:33 - loss: 0.6253 - regression_loss: 0.5094 - classification_loss: 0.1159
  126/10000 [..............................] - ETA: 1:15:36 - loss: 0.6267 - regression_loss: 0.5104 - classification_loss: 0.1163
  127/10000 [..............................] - ETA: 1:15:38 - loss: 0.6369 - regression_loss: 0.5178 - classification_loss: 0.1191
  128/10000 [..............................] - ETA: 1:15:37 - loss: 0.6354 - regression_loss: 0.5163 - classification_loss: 0.1191
  129/10000 [..............................] - ETA: 1:15:34 - loss: 0.6343 - regression_loss: 0.5156 - classification_loss: 0.1187
  130/10000 [..............................] - ETA: 1:15:36 - loss: 0.6328 - regression_loss: 0.5147 - classification_loss: 0.1181
  131/10000 [..............................] - ETA: 1:15:35 - loss: 0.6318 - regression_loss: 0.5133 - classification_loss: 0.1185
  132/10000 [..............................] - ETA: 1:15:33 - loss: 0.6335 - regression_loss: 0.5151 - classification_loss: 0.1184
  133/10000 [..............................] - ETA: 1:15:37 - loss: 0.6312 - regression_loss: 0.5132 - classification_loss: 0.1180
  134/10000 [..............................] - ETA: 1:15:36 - loss: 0.6277 - regression_loss: 0.5102 - classification_loss: 0.1175
  135/10000 [..............................] - ETA: 1:15:34 - loss: 0.6248 - regression_loss: 0.5076 - classification_loss: 0.1172
  136/10000 [..............................] - ETA: 1:15:35 - loss: 0.6225 - regression_loss: 0.5059 - classification_loss: 0.1166
  137/10000 [..............................] - ETA: 1:15:36 - loss: 0.6230 - regression_loss: 0.5059 - classification_loss: 0.1171
  138/10000 [..............................] - ETA: 1:15:37 - loss: 0.6229 - regression_loss: 0.5061 - classification_loss: 0.1168
  139/10000 [..............................] - ETA: 1:15:35 - loss: 0.6265 - regression_loss: 0.5090 - classification_loss: 0.1175
  140/10000 [..............................] - ETA: 1:15:35 - loss: 0.6288 - regression_loss: 0.5115 - classification_loss: 0.1173
  141/10000 [..............................] - ETA: 1:15:35 - loss: 0.6281 - regression_loss: 0.5107 - classification_loss: 0.1174
  142/10000 [..............................] - ETA: 1:15:34 - loss: 0.6255 - regression_loss: 0.5087 - classification_loss: 0.1168
  143/10000 [..............................] - ETA: 1:15:33 - loss: 0.6245 - regression_loss: 0.5076 - classification_loss: 0.1169
  144/10000 [..............................] - ETA: 1:15:35 - loss: 0.6305 - regression_loss: 0.5126 - classification_loss: 0.1179
  145/10000 [..............................] - ETA: 1:15:33 - loss: 0.6333 - regression_loss: 0.5150 - classification_loss: 0.1183
  146/10000 [..............................] - ETA: 1:15:30 - loss: 0.6298 - regression_loss: 0.5121 - classification_loss: 0.1177
  147/10000 [..............................] - ETA: 1:15:31 - loss: 0.6285 - regression_loss: 0.5114 - classification_loss: 0.1170
  148/10000 [..............................] - ETA: 1:15:31 - loss: 0.6264 - regression_loss: 0.5097 - classification_loss: 0.1167
  149/10000 [..............................] - ETA: 1:15:33 - loss: 0.6234 - regression_loss: 0.5072 - classification_loss: 0.1161
  150/10000 [..............................] - ETA: 1:15:32 - loss: 0.6218 - regression_loss: 0.5058 - classification_loss: 0.1159
  151/10000 [..............................] - ETA: 1:15:33 - loss: 0.6189 - regression_loss: 0.5035 - classification_loss: 0.1154
  152/10000 [..............................] - ETA: 1:15:31 - loss: 0.6172 - regression_loss: 0.5021 - classification_loss: 0.1151
  153/10000 [..............................] - ETA: 1:15:32 - loss: 0.6150 - regression_loss: 0.5005 - classification_loss: 0.1145
  154/10000 [..............................] - ETA: 1:15:31 - loss: 0.6173 - regression_loss: 0.5024 - classification_loss: 0.1150
  155/10000 [..............................] - ETA: 1:15:29 - loss: 0.6163 - regression_loss: 0.5015 - classification_loss: 0.1147
  156/10000 [..............................] - ETA: 1:15:28 - loss: 0.6143 - regression_loss: 0.5000 - classification_loss: 0.1143
  157/10000 [..............................] - ETA: 1:15:26 - loss: 0.6131 - regression_loss: 0.4989 - classification_loss: 0.1142
  158/10000 [..............................] - ETA: 1:15:25 - loss: 0.6125 - regression_loss: 0.4982 - classification_loss: 0.1143
  159/10000 [..............................] - ETA: 1:15:24 - loss: 0.6110 - regression_loss: 0.4969 - classification_loss: 0.1141
  160/10000 [..............................] - ETA: 1:15:22 - loss: 0.6084 - regression_loss: 0.4948 - classification_loss: 0.1136
  161/10000 [..............................] - ETA: 1:15:23 - loss: 0.6091 - regression_loss: 0.4955 - classification_loss: 0.1136
  162/10000 [..............................] - ETA: 1:15:24 - loss: 0.6128 - regression_loss: 0.4990 - classification_loss: 0.1138
  163/10000 [..............................] - ETA: 1:15:22 - loss: 0.6135 - regression_loss: 0.4988 - classification_loss: 0.1148
  164/10000 [..............................] - ETA: 1:15:20 - loss: 0.6132 - regression_loss: 0.4976 - classification_loss: 0.1156
  165/10000 [..............................] - ETA: 1:15:19 - loss: 0.6131 - regression_loss: 0.4974 - classification_loss: 0.1157
  166/10000 [..............................] - ETA: 1:15:16 - loss: 0.6139 - regression_loss: 0.4980 - classification_loss: 0.1159
  167/10000 [..............................] - ETA: 1:15:17 - loss: 0.6140 - regression_loss: 0.4982 - classification_loss: 0.1157
  168/10000 [..............................] - ETA: 1:15:16 - loss: 0.6155 - regression_loss: 0.4998 - classification_loss: 0.1157
  169/10000 [..............................] - ETA: 1:15:14 - loss: 0.6169 - regression_loss: 0.5011 - classification_loss: 0.1158
  170/10000 [..............................] - ETA: 1:15:16 - loss: 0.6166 - regression_loss: 0.5008 - classification_loss: 0.1157
  171/10000 [..............................] - ETA: 1:15:14 - loss: 0.6179 - regression_loss: 0.5020 - classification_loss: 0.1158
  172/10000 [..............................] - ETA: 1:15:14 - loss: 0.6188 - regression_loss: 0.5029 - classification_loss: 0.1159
  173/10000 [..............................] - ETA: 1:15:15 - loss: 0.6182 - regression_loss: 0.5026 - classification_loss: 0.1156
  174/10000 [..............................] - ETA: 1:15:12 - loss: 0.6170 - regression_loss: 0.5019 - classification_loss: 0.1151
  175/10000 [..............................] - ETA: 1:15:10 - loss: 0.6166 - regression_loss: 0.5015 - classification_loss: 0.1151
  176/10000 [..............................] - ETA: 1:15:10 - loss: 0.6172 - regression_loss: 0.5018 - classification_loss: 0.1154
  177/10000 [..............................] - ETA: 1:15:08 - loss: 0.6185 - regression_loss: 0.5032 - classification_loss: 0.1153
  178/10000 [..............................] - ETA: 1:15:06 - loss: 0.6201 - regression_loss: 0.5052 - classification_loss: 0.1150
  179/10000 [..............................] - ETA: 1:15:05 - loss: 0.6231 - regression_loss: 0.5075 - classification_loss: 0.1157
  180/10000 [..............................] - ETA: 1:15:04 - loss: 0.6220 - regression_loss: 0.5068 - classification_loss: 0.1152
  181/10000 [..............................] - ETA: 1:15:05 - loss: 0.6198 - regression_loss: 0.5050 - classification_loss: 0.1148
  182/10000 [..............................] - ETA: 1:15:06 - loss: 0.6185 - regression_loss: 0.5042 - classification_loss: 0.1143
  183/10000 [..............................] - ETA: 1:15:05 - loss: 0.6194 - regression_loss: 0.5050 - classification_loss: 0.1144
  184/10000 [..............................] - ETA: 1:15:04 - loss: 0.6203 - regression_loss: 0.5057 - classification_loss: 0.1146
  185/10000 [..............................] - ETA: 1:15:02 - loss: 0.6182 - regression_loss: 0.5041 - classification_loss: 0.1141
  186/10000 [..............................] - ETA: 1:15:01 - loss: 0.6162 - regression_loss: 0.5026 - classification_loss: 0.1137
  187/10000 [..............................] - ETA: 1:15:00 - loss: 0.6143 - regression_loss: 0.5012 - classification_loss: 0.1131
  188/10000 [..............................] - ETA: 1:15:01 - loss: 0.6146 - regression_loss: 0.5013 - classification_loss: 0.1133
  189/10000 [..............................] - ETA: 1:15:00 - loss: 0.6131 - regression_loss: 0.4997 - classification_loss: 0.1134
  190/10000 [..............................] - ETA: 1:15:00 - loss: 0.6122 - regression_loss: 0.4990 - classification_loss: 0.1131
  191/10000 [..............................] - ETA: 1:14:58 - loss: 0.6181 - regression_loss: 0.5040 - classification_loss: 0.1141
  192/10000 [..............................] - ETA: 1:14:57 - loss: 0.6164 - regression_loss: 0.5025 - classification_loss: 0.1139
  193/10000 [..............................] - ETA: 1:14:58 - loss: 0.6159 - regression_loss: 0.5019 - classification_loss: 0.1140
  194/10000 [..............................] - ETA: 1:14:57 - loss: 0.6161 - regression_loss: 0.5021 - classification_loss: 0.1140
  195/10000 [..............................] - ETA: 1:14:58 - loss: 0.6164 - regression_loss: 0.5025 - classification_loss: 0.1139
  196/10000 [..............................] - ETA: 1:14:59 - loss: 0.6206 - regression_loss: 0.5061 - classification_loss: 0.1145
  197/10000 [..............................] - ETA: 1:15:02 - loss: 0.6240 - regression_loss: 0.5091 - classification_loss: 0.1148
  198/10000 [..............................] - ETA: 1:15:01 - loss: 0.6213 - regression_loss: 0.5070 - classification_loss: 0.1143
  199/10000 [..............................] - ETA: 1:15:03 - loss: 0.6229 - regression_loss: 0.5087 - classification_loss: 0.1142
  200/10000 [..............................] - ETA: 1:15:02 - loss: 0.6222 - regression_loss: 0.5083 - classification_loss: 0.1139
  201/10000 [..............................] - ETA: 1:15:01 - loss: 0.6267 - regression_loss: 0.5125 - classification_loss: 0.1142
  202/10000 [..............................] - ETA: 1:15:01 - loss: 0.6295 - regression_loss: 0.5148 - classification_loss: 0.1148
  203/10000 [..............................] - ETA: 1:15:02 - loss: 0.6341 - regression_loss: 0.5188 - classification_loss: 0.1153
  204/10000 [..............................] - ETA: 1:15:01 - loss: 0.6361 - regression_loss: 0.5201 - classification_loss: 0.1160
  205/10000 [..............................] - ETA: 1:15:00 - loss: 0.6377 - regression_loss: 0.5216 - classification_loss: 0.1160
  206/10000 [..............................] - ETA: 1:14:58 - loss: 0.6374 - regression_loss: 0.5210 - classification_loss: 0.1164
  207/10000 [..............................] - ETA: 1:14:58 - loss: 0.6376 - regression_loss: 0.5207 - classification_loss: 0.1169
  208/10000 [..............................] - ETA: 1:14:56 - loss: 0.6386 - regression_loss: 0.5209 - classification_loss: 0.1177
  209/10000 [..............................] - ETA: 1:14:57 - loss: 0.6404 - regression_loss: 0.5227 - classification_loss: 0.1178
  210/10000 [..............................] - ETA: 1:14:56 - loss: 0.6439 - regression_loss: 0.5259 - classification_loss: 0.1180
  211/10000 [..............................] - ETA: 1:14:55 - loss: 0.6428 - regression_loss: 0.5251 - classification_loss: 0.1177
  212/10000 [..............................] - ETA: 1:14:55 - loss: 0.6426 - regression_loss: 0.5252 - classification_loss: 0.1174
  213/10000 [..............................] - ETA: 1:14:56 - loss: 0.6433 - regression_loss: 0.5260 - classification_loss: 0.1172
  214/10000 [..............................] - ETA: 1:14:55 - loss: 0.6429 - regression_loss: 0.5257 - classification_loss: 0.1172
  215/10000 [..............................] - ETA: 1:14:55 - loss: 0.6424 - regression_loss: 0.5255 - classification_loss: 0.1168
  216/10000 [..............................] - ETA: 1:14:53 - loss: 0.6448 - regression_loss: 0.5275 - classification_loss: 0.1173
  217/10000 [..............................] - ETA: 1:14:52 - loss: 0.6454 - regression_loss: 0.5280 - classification_loss: 0.1174
  218/10000 [..............................] - ETA: 1:14:52 - loss: 0.6457 - regression_loss: 0.5283 - classification_loss: 0.1174
  219/10000 [..............................] - ETA: 1:14:52 - loss: 0.6452 - regression_loss: 0.5282 - classification_loss: 0.1170
  220/10000 [..............................] - ETA: 1:14:52 - loss: 0.6444 - regression_loss: 0.5274 - classification_loss: 0.1169
  221/10000 [..............................] - ETA: 1:14:52 - loss: 0.6444 - regression_loss: 0.5275 - classification_loss: 0.1168
  222/10000 [..............................] - ETA: 1:14:50 - loss: 0.6443 - regression_loss: 0.5273 - classification_loss: 0.1171
  223/10000 [..............................] - ETA: 1:14:51 - loss: 0.6469 - regression_loss: 0.5298 - classification_loss: 0.1171
  224/10000 [..............................] - ETA: 1:14:51 - loss: 0.6450 - regression_loss: 0.5278 - classification_loss: 0.1172
  225/10000 [..............................] - ETA: 1:14:51 - loss: 0.6439 - regression_loss: 0.5270 - classification_loss: 0.1170
  226/10000 [..............................] - ETA: 1:14:51 - loss: 0.6440 - regression_loss: 0.5274 - classification_loss: 0.1166
  227/10000 [..............................] - ETA: 1:14:51 - loss: 0.6427 - regression_loss: 0.5264 - classification_loss: 0.1163
  228/10000 [..............................] - ETA: 1:14:50 - loss: 0.6439 - regression_loss: 0.5275 - classification_loss: 0.1164
  229/10000 [..............................] - ETA: 1:14:51 - loss: 0.6447 - regression_loss: 0.5285 - classification_loss: 0.1162
  230/10000 [..............................] - ETA: 1:14:52 - loss: 0.6436 - regression_loss: 0.5275 - classification_loss: 0.1161
  231/10000 [..............................] - ETA: 1:14:52 - loss: 0.6418 - regression_loss: 0.5260 - classification_loss: 0.1157
  232/10000 [..............................] - ETA: 1:14:51 - loss: 0.6436 - regression_loss: 0.5274 - classification_loss: 0.1162
  233/10000 [..............................] - ETA: 1:14:51 - loss: 0.6451 - regression_loss: 0.5282 - classification_loss: 0.1169
  234/10000 [..............................] - ETA: 1:14:50 - loss: 0.6430 - regression_loss: 0.5265 - classification_loss: 0.1165
  235/10000 [..............................] - ETA: 1:14:49 - loss: 0.6442 - regression_loss: 0.5277 - classification_loss: 0.1165
  236/10000 [..............................] - ETA: 1:14:49 - loss: 0.6435 - regression_loss: 0.5270 - classification_loss: 0.1164
  237/10000 [..............................] - ETA: 1:14:50 - loss: 0.6436 - regression_loss: 0.5265 - classification_loss: 0.1171
  238/10000 [..............................] - ETA: 1:14:48 - loss: 0.6424 - regression_loss: 0.5256 - classification_loss: 0.1168
  239/10000 [..............................] - ETA: 1:14:48 - loss: 0.6403 - regression_loss: 0.5239 - classification_loss: 0.1164
  240/10000 [..............................] - ETA: 1:14:48 - loss: 0.6442 - regression_loss: 0.5268 - classification_loss: 0.1174
  241/10000 [..............................] - ETA: 1:14:48 - loss: 0.6484 - regression_loss: 0.5301 - classification_loss: 0.1183
  242/10000 [..............................] - ETA: 1:14:49 - loss: 0.6480 - regression_loss: 0.5299 - classification_loss: 0.1181
  243/10000 [..............................] - ETA: 1:14:48 - loss: 0.6474 - regression_loss: 0.5294 - classification_loss: 0.1179
  244/10000 [..............................] - ETA: 1:14:47 - loss: 0.6470 - regression_loss: 0.5292 - classification_loss: 0.1178
  245/10000 [..............................] - ETA: 1:14:46 - loss: 0.6453 - regression_loss: 0.5278 - classification_loss: 0.1175
  246/10000 [..............................] - ETA: 1:14:44 - loss: 0.6445 - regression_loss: 0.5268 - classification_loss: 0.1177
  247/10000 [..............................] - ETA: 1:14:45 - loss: 0.6433 - regression_loss: 0.5258 - classification_loss: 0.1174
  248/10000 [..............................] - ETA: 1:14:44 - loss: 0.6423 - regression_loss: 0.5249 - classification_loss: 0.1174
  249/10000 [..............................] - ETA: 1:14:43 - loss: 0.6419 - regression_loss: 0.5248 - classification_loss: 0.1171
  250/10000 [..............................] - ETA: 1:14:43 - loss: 0.6418 - regression_loss: 0.5249 - classification_loss: 0.1169
  251/10000 [..............................] - ETA: 1:14:42 - loss: 0.6415 - regression_loss: 0.5243 - classification_loss: 0.1173
  252/10000 [..............................] - ETA: 1:14:42 - loss: 0.6428 - regression_loss: 0.5252 - classification_loss: 0.1175
  253/10000 [..............................] - ETA: 1:14:42 - loss: 0.6415 - regression_loss: 0.5243 - classification_loss: 0.1172
  254/10000 [..............................] - ETA: 1:14:41 - loss: 0.6406 - regression_loss: 0.5235 - classification_loss: 0.1171
  255/10000 [..............................] - ETA: 1:14:40 - loss: 0.6388 - regression_loss: 0.5220 - classification_loss: 0.1168
  256/10000 [..............................] - ETA: 1:14:37 - loss: 0.6387 - regression_loss: 0.5218 - classification_loss: 0.1169
  257/10000 [..............................] - ETA: 1:14:37 - loss: 0.6380 - regression_loss: 0.5211 - classification_loss: 0.1169
  258/10000 [..............................] - ETA: 1:14:36 - loss: 0.6386 - regression_loss: 0.5218 - classification_loss: 0.1167
  259/10000 [..............................] - ETA: 1:14:36 - loss: 0.6401 - regression_loss: 0.5237 - classification_loss: 0.1164
  260/10000 [..............................] - ETA: 1:14:36 - loss: 0.6399 - regression_loss: 0.5235 - classification_loss: 0.1164
  261/10000 [..............................] - ETA: 1:14:36 - loss: 0.6405 - regression_loss: 0.5242 - classification_loss: 0.1163
  262/10000 [..............................] - ETA: 1:14:35 - loss: 0.6391 - regression_loss: 0.5231 - classification_loss: 0.1160
  263/10000 [..............................] - ETA: 1:14:34 - loss: 0.6413 - regression_loss: 0.5244 - classification_loss: 0.1168
  264/10000 [..............................] - ETA: 1:14:33 - loss: 0.6402 - regression_loss: 0.5235 - classification_loss: 0.1167
  265/10000 [..............................] - ETA: 1:14:33 - loss: 0.6415 - regression_loss: 0.5241 - classification_loss: 0.1175
  266/10000 [..............................] - ETA: 1:14:32 - loss: 0.6450 - regression_loss: 0.5262 - classification_loss: 0.1188
  267/10000 [..............................] - ETA: 1:14:33 - loss: 0.6444 - regression_loss: 0.5258 - classification_loss: 0.1186
  268/10000 [..............................] - ETA: 1:14:31 - loss: 0.6425 - regression_loss: 0.5242 - classification_loss: 0.1182
  269/10000 [..............................] - ETA: 1:14:29 - loss: 0.6433 - regression_loss: 0.5245 - classification_loss: 0.1188
  270/10000 [..............................] - ETA: 1:14:29 - loss: 0.6431 - regression_loss: 0.5241 - classification_loss: 0.1190
  271/10000 [..............................] - ETA: 1:14:27 - loss: 0.6419 - regression_loss: 0.5232 - classification_loss: 0.1187
  272/10000 [..............................] - ETA: 1:14:27 - loss: 0.6401 - regression_loss: 0.5216 - classification_loss: 0.1185
  273/10000 [..............................] - ETA: 1:14:26 - loss: 0.6385 - regression_loss: 0.5203 - classification_loss: 0.1182
  274/10000 [..............................] - ETA: 1:14:25 - loss: 0.6372 - regression_loss: 0.5192 - classification_loss: 0.1179
  275/10000 [..............................] - ETA: 1:14:25 - loss: 0.6368 - regression_loss: 0.5190 - classification_loss: 0.1179
  276/10000 [..............................] - ETA: 1:14:26 - loss: 0.6363 - regression_loss: 0.5187 - classification_loss: 0.1175
  277/10000 [..............................] - ETA: 1:14:25 - loss: 0.6372 - regression_loss: 0.5199 - classification_loss: 0.1173
  278/10000 [..............................] - ETA: 1:14:24 - loss: 0.6370 - regression_loss: 0.5197 - classification_loss: 0.1173
  279/10000 [..............................] - ETA: 1:14:23 - loss: 0.6363 - regression_loss: 0.5191 - classification_loss: 0.1172
  280/10000 [..............................] - ETA: 1:14:21 - loss: 0.6350 - regression_loss: 0.5180 - classification_loss: 0.1170
  281/10000 [..............................] - ETA: 1:14:20 - loss: 0.6346 - regression_loss: 0.5178 - classification_loss: 0.1169
  282/10000 [..............................] - ETA: 1:14:20 - loss: 0.6351 - regression_loss: 0.5184 - classification_loss: 0.1167
  283/10000 [..............................] - ETA: 1:14:18 - loss: 0.6350 - regression_loss: 0.5186 - classification_loss: 0.1164
  284/10000 [..............................] - ETA: 1:14:19 - loss: 0.6349 - regression_loss: 0.5187 - classification_loss: 0.1162
  285/10000 [..............................] - ETA: 1:14:17 - loss: 0.6332 - regression_loss: 0.5173 - classification_loss: 0.1159
  286/10000 [..............................] - ETA: 1:14:17 - loss: 0.6333 - regression_loss: 0.5176 - classification_loss: 0.1157
  287/10000 [..............................] - ETA: 1:14:16 - loss: 0.6367 - regression_loss: 0.5204 - classification_loss: 0.1163
  288/10000 [..............................] - ETA: 1:14:16 - loss: 0.6357 - regression_loss: 0.5196 - classification_loss: 0.1161
  289/10000 [..............................] - ETA: 1:14:15 - loss: 0.6406 - regression_loss: 0.5241 - classification_loss: 0.1165
  290/10000 [..............................] - ETA: 1:14:14 - loss: 0.6409 - regression_loss: 0.5244 - classification_loss: 0.1165
  291/10000 [..............................] - ETA: 1:14:14 - loss: 0.6446 - regression_loss: 0.5272 - classification_loss: 0.1174
  292/10000 [..............................] - ETA: 1:14:14 - loss: 0.6441 - regression_loss: 0.5262 - classification_loss: 0.1179
  293/10000 [..............................] - ETA: 1:14:13 - loss: 0.6449 - regression_loss: 0.5266 - classification_loss: 0.1183
  294/10000 [..............................] - ETA: 1:14:13 - loss: 0.6449 - regression_loss: 0.5262 - classification_loss: 0.1187
  295/10000 [..............................] - ETA: 1:14:14 - loss: 0.6470 - regression_loss: 0.5279 - classification_loss: 0.1192
  296/10000 [..............................] - ETA: 1:14:13 - loss: 0.6513 - regression_loss: 0.5314 - classification_loss: 0.1199
  297/10000 [..............................] - ETA: 1:14:12 - loss: 0.6502 - regression_loss: 0.5305 - classification_loss: 0.1197
  298/10000 [..............................] - ETA: 1:14:13 - loss: 0.6490 - regression_loss: 0.5297 - classification_loss: 0.1194
  299/10000 [..............................] - ETA: 1:14:12 - loss: 0.6498 - regression_loss: 0.5303 - classification_loss: 0.1196
  300/10000 [..............................] - ETA: 1:14:11 - loss: 0.6502 - regression_loss: 0.5304 - classification_loss: 0.1198
  301/10000 [..............................] - ETA: 1:14:11 - loss: 0.6500 - regression_loss: 0.5305 - classification_loss: 0.1196
  302/10000 [..............................] - ETA: 1:14:11 - loss: 0.6512 - regression_loss: 0.5312 - classification_loss: 0.1200
  303/10000 [..............................] - ETA: 1:14:10 - loss: 0.6529 - regression_loss: 0.5326 - classification_loss: 0.1203
  304/10000 [..............................] - ETA: 1:14:08 - loss: 0.6534 - regression_loss: 0.5330 - classification_loss: 0.1203
  305/10000 [..............................] - ETA: 1:14:08 - loss: 0.6527 - regression_loss: 0.5324 - classification_loss: 0.1203
  306/10000 [..............................] - ETA: 1:14:08 - loss: 0.6523 - regression_loss: 0.5321 - classification_loss: 0.1202
  307/10000 [..............................] - ETA: 1:14:07 - loss: 0.6538 - regression_loss: 0.5331 - classification_loss: 0.1207
  308/10000 [..............................] - ETA: 1:14:08 - loss: 0.6544 - regression_loss: 0.5335 - classification_loss: 0.1209
  309/10000 [..............................] - ETA: 1:14:08 - loss: 0.6559 - regression_loss: 0.5346 - classification_loss: 0.1213
  310/10000 [..............................] - ETA: 1:14:07 - loss: 0.6561 - regression_loss: 0.5349 - classification_loss: 0.1211
  311/10000 [..............................] - ETA: 1:14:07 - loss: 0.6577 - regression_loss: 0.5364 - classification_loss: 0.1213
  312/10000 [..............................] - ETA: 1:14:06 - loss: 0.6562 - regression_loss: 0.5352 - classification_loss: 0.1210
  313/10000 [..............................] - ETA: 1:14:04 - loss: 0.6564 - regression_loss: 0.5354 - classification_loss: 0.1210
  314/10000 [..............................] - ETA: 1:14:03 - loss: 0.6564 - regression_loss: 0.5354 - classification_loss: 0.1210
  315/10000 [..............................] - ETA: 1:14:02 - loss: 0.6580 - regression_loss: 0.5364 - classification_loss: 0.1216
  316/10000 [..............................] - ETA: 1:14:02 - loss: 0.6590 - regression_loss: 0.5372 - classification_loss: 0.1219
  317/10000 [..............................] - ETA: 1:14:02 - loss: 0.6616 - regression_loss: 0.5393 - classification_loss: 0.1222
  318/10000 [..............................] - ETA: 1:14:02 - loss: 0.6624 - regression_loss: 0.5402 - classification_loss: 0.1222
  319/10000 [..............................] - ETA: 1:14:02 - loss: 0.6643 - regression_loss: 0.5417 - classification_loss: 0.1226
  320/10000 [..............................] - ETA: 1:14:02 - loss: 0.6651 - regression_loss: 0.5422 - classification_loss: 0.1229
  321/10000 [..............................] - ETA: 1:14:01 - loss: 0.6660 - regression_loss: 0.5432 - classification_loss: 0.1229
  322/10000 [..............................] - ETA: 1:14:00 - loss: 0.6644 - regression_loss: 0.5419 - classification_loss: 0.1226
  323/10000 [..............................] - ETA: 1:13:59 - loss: 0.6651 - regression_loss: 0.5423 - classification_loss: 0.1228
  324/10000 [..............................] - ETA: 1:13:58 - loss: 0.6648 - regression_loss: 0.5422 - classification_loss: 0.1226
  325/10000 [..............................] - ETA: 1:13:57 - loss: 0.6637 - regression_loss: 0.5412 - classification_loss: 0.1225
  326/10000 [..............................] - ETA: 1:13:57 - loss: 0.6645 - regression_loss: 0.5419 - classification_loss: 0.1226
  327/10000 [..............................] - ETA: 1:13:54 - loss: 0.6637 - regression_loss: 0.5413 - classification_loss: 0.1224
  328/10000 [..............................] - ETA: 1:13:54 - loss: 0.6628 - regression_loss: 0.5405 - classification_loss: 0.1223
  329/10000 [..............................] - ETA: 1:13:53 - loss: 0.6644 - regression_loss: 0.5417 - classification_loss: 0.1227
  330/10000 [..............................] - ETA: 1:13:53 - loss: 0.6646 - regression_loss: 0.5417 - classification_loss: 0.1229
  331/10000 [..............................] - ETA: 1:13:52 - loss: 0.6646 - regression_loss: 0.5417 - classification_loss: 0.1228
  332/10000 [..............................] - ETA: 1:13:51 - loss: 0.6641 - regression_loss: 0.5412 - classification_loss: 0.1229
  333/10000 [..............................] - ETA: 1:13:50 - loss: 0.6634 - regression_loss: 0.5406 - classification_loss: 0.1228
  334/10000 [>.............................] - ETA: 1:13:51 - loss: 0.6625 - regression_loss: 0.5398 - classification_loss: 0.1226
  335/10000 [>.............................] - ETA: 1:13:51 - loss: 0.6628 - regression_loss: 0.5403 - classification_loss: 0.1225
  336/10000 [>.............................] - ETA: 1:13:50 - loss: 0.6617 - regression_loss: 0.5395 - classification_loss: 0.1223
  337/10000 [>.............................] - ETA: 1:13:50 - loss: 0.6608 - regression_loss: 0.5386 - classification_loss: 0.1221
  338/10000 [>.............................] - ETA: 1:13:49 - loss: 0.6596 - regression_loss: 0.5375 - classification_loss: 0.1220
  339/10000 [>.............................] - ETA: 1:13:47 - loss: 0.6600 - regression_loss: 0.5380 - classification_loss: 0.1220
  340/10000 [>.............................] - ETA: 1:13:48 - loss: 0.6592 - regression_loss: 0.5374 - classification_loss: 0.1218
  341/10000 [>.............................] - ETA: 1:13:47 - loss: 0.6601 - regression_loss: 0.5381 - classification_loss: 0.1220
  342/10000 [>.............................] - ETA: 1:13:46 - loss: 0.6593 - regression_loss: 0.5374 - classification_loss: 0.1219
  343/10000 [>.............................] - ETA: 1:13:46 - loss: 0.6582 - regression_loss: 0.5364 - classification_loss: 0.1219
  344/10000 [>.............................] - ETA: 1:13:45 - loss: 0.6569 - regression_loss: 0.5353 - classification_loss: 0.1217
  345/10000 [>.............................] - ETA: 1:13:45 - loss: 0.6560 - regression_loss: 0.5345 - classification_loss: 0.1215
  346/10000 [>.............................] - ETA: 1:13:45 - loss: 0.6547 - regression_loss: 0.5333 - classification_loss: 0.1214
  347/10000 [>.............................] - ETA: 1:13:44 - loss: 0.6539 - regression_loss: 0.5325 - classification_loss: 0.1214
  348/10000 [>.............................] - ETA: 1:13:43 - loss: 0.6541 - regression_loss: 0.5328 - classification_loss: 0.1213
  349/10000 [>.............................] - ETA: 1:13:43 - loss: 0.6530 - regression_loss: 0.5319 - classification_loss: 0.1211
  350/10000 [>.............................] - ETA: 1:13:43 - loss: 0.6540 - regression_loss: 0.5329 - classification_loss: 0.1211
  351/10000 [>.............................] - ETA: 1:13:42 - loss: 0.6546 - regression_loss: 0.5335 - classification_loss: 0.1211
  352/10000 [>.............................] - ETA: 1:13:41 - loss: 0.6537 - regression_loss: 0.5327 - classification_loss: 0.1210
  353/10000 [>.............................] - ETA: 1:13:40 - loss: 0.6529 - regression_loss: 0.5321 - classification_loss: 0.1208
  354/10000 [>.............................] - ETA: 1:13:41 - loss: 0.6531 - regression_loss: 0.5322 - classification_loss: 0.1209
  355/10000 [>.............................] - ETA: 1:13:40 - loss: 0.6534 - regression_loss: 0.5326 - classification_loss: 0.1208
  356/10000 [>.............................] - ETA: 1:13:41 - loss: 0.6522 - regression_loss: 0.5316 - classification_loss: 0.1206
  357/10000 [>.............................] - ETA: 1:13:40 - loss: 0.6515 - regression_loss: 0.5312 - classification_loss: 0.1203
  358/10000 [>.............................] - ETA: 1:13:39 - loss: 0.6542 - regression_loss: 0.5330 - classification_loss: 0.1212
  359/10000 [>.............................] - ETA: 1:13:38 - loss: 0.6557 - regression_loss: 0.5339 - classification_loss: 0.1218
  360/10000 [>.............................] - ETA: 1:13:38 - loss: 0.6558 - regression_loss: 0.5338 - classification_loss: 0.1220
  361/10000 [>.............................] - ETA: 1:13:38 - loss: 0.6553 - regression_loss: 0.5333 - classification_loss: 0.1219
  362/10000 [>.............................] - ETA: 1:13:37 - loss: 0.6554 - regression_loss: 0.5336 - classification_loss: 0.1218
  363/10000 [>.............................] - ETA: 1:13:36 - loss: 0.6543 - regression_loss: 0.5327 - classification_loss: 0.1217
  364/10000 [>.............................] - ETA: 1:13:36 - loss: 0.6540 - regression_loss: 0.5326 - classification_loss: 0.1214
  365/10000 [>.............................] - ETA: 1:13:37 - loss: 0.6555 - regression_loss: 0.5340 - classification_loss: 0.1215
  366/10000 [>.............................] - ETA: 1:13:36 - loss: 0.6548 - regression_loss: 0.5334 - classification_loss: 0.1214
  367/10000 [>.............................] - ETA: 1:13:36 - loss: 0.6538 - regression_loss: 0.5326 - classification_loss: 0.1212
  368/10000 [>.............................] - ETA: 1:13:35 - loss: 0.6537 - regression_loss: 0.5321 - classification_loss: 0.1217
  369/10000 [>.............................] - ETA: 1:13:35 - loss: 0.6536 - regression_loss: 0.5320 - classification_loss: 0.1216
  370/10000 [>.............................] - ETA: 1:13:34 - loss: 0.6540 - regression_loss: 0.5326 - classification_loss: 0.1214
  371/10000 [>.............................] - ETA: 1:13:34 - loss: 0.6541 - regression_loss: 0.5327 - classification_loss: 0.1214
  372/10000 [>.............................] - ETA: 1:13:34 - loss: 0.6561 - regression_loss: 0.5345 - classification_loss: 0.1216
  373/10000 [>.............................] - ETA: 1:13:33 - loss: 0.6551 - regression_loss: 0.5338 - classification_loss: 0.1213
  374/10000 [>.............................] - ETA: 1:13:33 - loss: 0.6564 - regression_loss: 0.5348 - classification_loss: 0.1217
  375/10000 [>.............................] - ETA: 1:13:32 - loss: 0.6552 - regression_loss: 0.5337 - classification_loss: 0.1215
  376/10000 [>.............................] - ETA: 1:13:31 - loss: 0.6558 - regression_loss: 0.5335 - classification_loss: 0.1223
  377/10000 [>.............................] - ETA: 1:13:31 - loss: 0.6570 - regression_loss: 0.5341 - classification_loss: 0.1229
  378/10000 [>.............................] - ETA: 1:13:32 - loss: 0.6563 - regression_loss: 0.5334 - classification_loss: 0.1228
  379/10000 [>.............................] - ETA: 1:13:32 - loss: 0.6553 - regression_loss: 0.5327 - classification_loss: 0.1226
  380/10000 [>.............................] - ETA: 1:13:31 - loss: 0.6568 - regression_loss: 0.5339 - classification_loss: 0.1229
  381/10000 [>.............................] - ETA: 1:13:31 - loss: 0.6585 - regression_loss: 0.5349 - classification_loss: 0.1235
  382/10000 [>.............................] - ETA: 1:13:30 - loss: 0.6574 - regression_loss: 0.5341 - classification_loss: 0.1233
  383/10000 [>.............................] - ETA: 1:13:29 - loss: 0.6566 - regression_loss: 0.5335 - classification_loss: 0.1231
  384/10000 [>.............................] - ETA: 1:13:29 - loss: 0.6555 - regression_loss: 0.5327 - classification_loss: 0.1228
  385/10000 [>.............................] - ETA: 1:13:29 - loss: 0.6572 - regression_loss: 0.5342 - classification_loss: 0.1230
  386/10000 [>.............................] - ETA: 1:13:29 - loss: 0.6570 - regression_loss: 0.5341 - classification_loss: 0.1229
  387/10000 [>.............................] - ETA: 1:13:28 - loss: 0.6586 - regression_loss: 0.5355 - classification_loss: 0.1231
  388/10000 [>.............................] - ETA: 1:13:27 - loss: 0.6575 - regression_loss: 0.5345 - classification_loss: 0.1230
  389/10000 [>.............................] - ETA: 1:13:27 - loss: 0.6562 - regression_loss: 0.5334 - classification_loss: 0.1228
  390/10000 [>.............................] - ETA: 1:13:26 - loss: 0.6551 - regression_loss: 0.5324 - classification_loss: 0.1226
  391/10000 [>.............................] - ETA: 1:13:25 - loss: 0.6544 - regression_loss: 0.5318 - classification_loss: 0.1226
  392/10000 [>.............................] - ETA: 1:13:23 - loss: 0.6535 - regression_loss: 0.5311 - classification_loss: 0.1224
  393/10000 [>.............................] - ETA: 1:13:23 - loss: 0.6552 - regression_loss: 0.5327 - classification_loss: 0.1225
  394/10000 [>.............................] - ETA: 1:13:23 - loss: 0.6546 - regression_loss: 0.5322 - classification_loss: 0.1224
  395/10000 [>.............................] - ETA: 1:13:23 - loss: 0.6548 - regression_loss: 0.5325 - classification_loss: 0.1223
  396/10000 [>.............................] - ETA: 1:13:22 - loss: 0.6577 - regression_loss: 0.5348 - classification_loss: 0.1229
  397/10000 [>.............................] - ETA: 1:13:21 - loss: 0.6575 - regression_loss: 0.5344 - classification_loss: 0.1232
  398/10000 [>.............................] - ETA: 1:13:20 - loss: 0.6582 - regression_loss: 0.5349 - classification_loss: 0.1233
  399/10000 [>.............................] - ETA: 1:13:20 - loss: 0.6574 - regression_loss: 0.5344 - classification_loss: 0.1230
  400/10000 [>.............................] - ETA: 1:13:20 - loss: 0.6567 - regression_loss: 0.5338 - classification_loss: 0.1229
  401/10000 [>.............................] - ETA: 1:13:20 - loss: 0.6555 - regression_loss: 0.5328 - classification_loss: 0.1226
  402/10000 [>.............................] - ETA: 1:13:20 - loss: 0.6582 - regression_loss: 0.5352 - classification_loss: 0.1230
  403/10000 [>.............................] - ETA: 1:13:20 - loss: 0.6582 - regression_loss: 0.5350 - classification_loss: 0.1232
  404/10000 [>.............................] - ETA: 1:13:18 - loss: 0.6573 - regression_loss: 0.5343 - classification_loss: 0.1230
  405/10000 [>.............................] - ETA: 1:13:17 - loss: 0.6567 - regression_loss: 0.5338 - classification_loss: 0.1229
  406/10000 [>.............................] - ETA: 1:13:16 - loss: 0.6563 - regression_loss: 0.5336 - classification_loss: 0.1227
  407/10000 [>.............................] - ETA: 1:13:15 - loss: 0.6568 - regression_loss: 0.5339 - classification_loss: 0.1228
  408/10000 [>.............................] - ETA: 1:13:15 - loss: 0.6569 - regression_loss: 0.5342 - classification_loss: 0.1227
  409/10000 [>.............................] - ETA: 1:13:14 - loss: 0.6565 - regression_loss: 0.5337 - classification_loss: 0.1228
  410/10000 [>.............................] - ETA: 1:13:13 - loss: 0.6562 - regression_loss: 0.5335 - classification_loss: 0.1227
  411/10000 [>.............................] - ETA: 1:13:13 - loss: 0.6560 - regression_loss: 0.5334 - classification_loss: 0.1226
  412/10000 [>.............................] - ETA: 1:13:13 - loss: 0.6550 - regression_loss: 0.5326 - classification_loss: 0.1224
  413/10000 [>.............................] - ETA: 1:13:12 - loss: 0.6542 - regression_loss: 0.5319 - classification_loss: 0.1222
  414/10000 [>.............................] - ETA: 1:13:12 - loss: 0.6572 - regression_loss: 0.5347 - classification_loss: 0.1226
  415/10000 [>.............................] - ETA: 1:13:12 - loss: 0.6577 - regression_loss: 0.5351 - classification_loss: 0.1225
  416/10000 [>.............................] - ETA: 1:13:11 - loss: 0.6569 - regression_loss: 0.5345 - classification_loss: 0.1223
  417/10000 [>.............................] - ETA: 1:13:10 - loss: 0.6566 - regression_loss: 0.5343 - classification_loss: 0.1223
  418/10000 [>.............................] - ETA: 1:13:10 - loss: 0.6555 - regression_loss: 0.5335 - classification_loss: 0.1220
  419/10000 [>.............................] - ETA: 1:13:10 - loss: 0.6562 - regression_loss: 0.5342 - classification_loss: 0.1220
  420/10000 [>.............................] - ETA: 1:13:08 - loss: 0.6553 - regression_loss: 0.5335 - classification_loss: 0.1219
  421/10000 [>.............................] - ETA: 1:13:07 - loss: 0.6546 - regression_loss: 0.5328 - classification_loss: 0.1218
  422/10000 [>.............................] - ETA: 1:13:06 - loss: 0.6552 - regression_loss: 0.5333 - classification_loss: 0.1219
  423/10000 [>.............................] - ETA: 1:13:04 - loss: 0.6547 - regression_loss: 0.5329 - classification_loss: 0.1217
  424/10000 [>.............................] - ETA: 1:13:02 - loss: 0.6546 - regression_loss: 0.5328 - classification_loss: 0.1218
  425/10000 [>.............................] - ETA: 1:13:03 - loss: 0.6546 - regression_loss: 0.5328 - classification_loss: 0.1218
  426/10000 [>.............................] - ETA: 1:13:02 - loss: 0.6542 - regression_loss: 0.5321 - classification_loss: 0.1221
  427/10000 [>.............................] - ETA: 1:13:01 - loss: 0.6547 - regression_loss: 0.5323 - classification_loss: 0.1223
  428/10000 [>.............................] - ETA: 1:13:01 - loss: 0.6544 - regression_loss: 0.5322 - classification_loss: 0.1222
  429/10000 [>.............................] - ETA: 1:13:01 - loss: 0.6534 - regression_loss: 0.5314 - classification_loss: 0.1220
  430/10000 [>.............................] - ETA: 1:13:00 - loss: 0.6524 - regression_loss: 0.5306 - classification_loss: 0.1218
  431/10000 [>.............................] - ETA: 1:12:59 - loss: 0.6529 - regression_loss: 0.5311 - classification_loss: 0.1219
  432/10000 [>.............................] - ETA: 1:12:59 - loss: 0.6525 - regression_loss: 0.5308 - classification_loss: 0.1217
  433/10000 [>.............................] - ETA: 1:12:59 - loss: 0.6528 - regression_loss: 0.5310 - classification_loss: 0.1218
  434/10000 [>.............................] - ETA: 1:12:59 - loss: 0.6522 - regression_loss: 0.5306 - classification_loss: 0.1216
  435/10000 [>.............................] - ETA: 1:12:58 - loss: 0.6517 - regression_loss: 0.5301 - classification_loss: 0.1216
  436/10000 [>.............................] - ETA: 1:12:58 - loss: 0.6518 - regression_loss: 0.5303 - classification_loss: 0.1215
  437/10000 [>.............................] - ETA: 1:12:58 - loss: 0.6509 - regression_loss: 0.5293 - classification_loss: 0.1215
  438/10000 [>.............................] - ETA: 1:12:57 - loss: 0.6521 - regression_loss: 0.5303 - classification_loss: 0.1217
  439/10000 [>.............................] - ETA: 1:12:57 - loss: 0.6520 - regression_loss: 0.5303 - classification_loss: 0.1217
  440/10000 [>.............................] - ETA: 1:12:56 - loss: 0.6517 - regression_loss: 0.5302 - classification_loss: 0.1215
  441/10000 [>.............................] - ETA: 1:12:56 - loss: 0.6509 - regression_loss: 0.5296 - classification_loss: 0.1213
  442/10000 [>.............................] - ETA: 1:12:55 - loss: 0.6533 - regression_loss: 0.5318 - classification_loss: 0.1216
  443/10000 [>.............................] - ETA: 1:12:56 - loss: 0.6545 - regression_loss: 0.5324 - classification_loss: 0.1221
  444/10000 [>.............................] - ETA: 1:12:55 - loss: 0.6535 - regression_loss: 0.5316 - classification_loss: 0.1219
  445/10000 [>.............................] - ETA: 1:12:55 - loss: 0.6532 - regression_loss: 0.5315 - classification_loss: 0.1217
  446/10000 [>.............................] - ETA: 1:12:55 - loss: 0.6519 - regression_loss: 0.5304 - classification_loss: 0.1215
  447/10000 [>.............................] - ETA: 1:12:55 - loss: 0.6529 - regression_loss: 0.5313 - classification_loss: 0.1216
  448/10000 [>.............................] - ETA: 1:12:55 - loss: 0.6519 - regression_loss: 0.5304 - classification_loss: 0.1214
  449/10000 [>.............................] - ETA: 1:12:55 - loss: 0.6515 - regression_loss: 0.5301 - classification_loss: 0.1215
  450/10000 [>.............................] - ETA: 1:12:54 - loss: 0.6536 - regression_loss: 0.5320 - classification_loss: 0.1217
  451/10000 [>.............................] - ETA: 1:12:53 - loss: 0.6531 - regression_loss: 0.5315 - classification_loss: 0.1216
  452/10000 [>.............................] - ETA: 1:12:53 - loss: 0.6524 - regression_loss: 0.5309 - classification_loss: 0.1215
  453/10000 [>.............................] - ETA: 1:12:53 - loss: 0.6514 - regression_loss: 0.5300 - classification_loss: 0.1214
  454/10000 [>.............................] - ETA: 1:12:50 - loss: 0.6506 - regression_loss: 0.5292 - classification_loss: 0.1214
  455/10000 [>.............................] - ETA: 1:12:50 - loss: 0.6535 - regression_loss: 0.5314 - classification_loss: 0.1220
  456/10000 [>.............................] - ETA: 1:12:52 - loss: 0.6537 - regression_loss: 0.5311 - classification_loss: 0.1226
  457/10000 [>.............................] - ETA: 1:12:52 - loss: 0.6529 - regression_loss: 0.5303 - classification_loss: 0.1226
  458/10000 [>.............................] - ETA: 1:12:51 - loss: 0.6519 - regression_loss: 0.5294 - classification_loss: 0.1225
  459/10000 [>.............................] - ETA: 1:12:51 - loss: 0.6511 - regression_loss: 0.5287 - classification_loss: 0.1224
  460/10000 [>.............................] - ETA: 1:12:50 - loss: 0.6542 - regression_loss: 0.5306 - classification_loss: 0.1236
  461/10000 [>.............................] - ETA: 1:12:49 - loss: 0.6547 - regression_loss: 0.5311 - classification_loss: 0.1236
  462/10000 [>.............................] - ETA: 1:12:50 - loss: 0.6553 - regression_loss: 0.5317 - classification_loss: 0.1236
  463/10000 [>.............................] - ETA: 1:12:49 - loss: 0.6542 - regression_loss: 0.5308 - classification_loss: 0.1234
  464/10000 [>.............................] - ETA: 1:12:48 - loss: 0.6534 - regression_loss: 0.5302 - classification_loss: 0.1232
  465/10000 [>.............................] - ETA: 1:12:48 - loss: 0.6528 - regression_loss: 0.5297 - classification_loss: 0.1231
  466/10000 [>.............................] - ETA: 1:12:48 - loss: 0.6537 - regression_loss: 0.5305 - classification_loss: 0.1232
  467/10000 [>.............................] - ETA: 1:12:48 - loss: 0.6528 - regression_loss: 0.5297 - classification_loss: 0.1231
  468/10000 [>.............................] - ETA: 1:12:47 - loss: 0.6521 - regression_loss: 0.5291 - classification_loss: 0.1230
  469/10000 [>.............................] - ETA: 1:12:47 - loss: 0.6519 - regression_loss: 0.5288 - classification_loss: 0.1231
  470/10000 [>.............................] - ETA: 1:12:46 - loss: 0.6509 - regression_loss: 0.5280 - classification_loss: 0.1230
  471/10000 [>.............................] - ETA: 1:12:47 - loss: 0.6504 - regression_loss: 0.5276 - classification_loss: 0.1228
  472/10000 [>.............................] - ETA: 1:12:46 - loss: 0.6497 - regression_loss: 0.5270 - classification_loss: 0.1227
  473/10000 [>.............................] - ETA: 1:12:46 - loss: 0.6494 - regression_loss: 0.5269 - classification_loss: 0.1225
  474/10000 [>.............................] - ETA: 1:12:45 - loss: 0.6503 - regression_loss: 0.5278 - classification_loss: 0.1225
  475/10000 [>.............................] - ETA: 1:12:45 - loss: 0.6497 - regression_loss: 0.5270 - classification_loss: 0.1227
  476/10000 [>.............................] - ETA: 1:12:45 - loss: 0.6496 - regression_loss: 0.5268 - classification_loss: 0.1227
  477/10000 [>.............................] - ETA: 1:12:45 - loss: 0.6487 - regression_loss: 0.5262 - classification_loss: 0.1225
  478/10000 [>.............................] - ETA: 1:12:45 - loss: 0.6486 - regression_loss: 0.5259 - classification_loss: 0.1228
  479/10000 [>.............................] - ETA: 1:12:44 - loss: 0.6479 - regression_loss: 0.5253 - classification_loss: 0.1226
  480/10000 [>.............................] - ETA: 1:12:44 - loss: 0.6484 - regression_loss: 0.5257 - classification_loss: 0.1228
  481/10000 [>.............................] - ETA: 1:12:43 - loss: 0.6481 - regression_loss: 0.5253 - classification_loss: 0.1229
  482/10000 [>.............................] - ETA: 1:12:44 - loss: 0.6503 - regression_loss: 0.5273 - classification_loss: 0.1231
  483/10000 [>.............................] - ETA: 1:12:43 - loss: 0.6506 - regression_loss: 0.5275 - classification_loss: 0.1231
  484/10000 [>.............................] - ETA: 1:12:43 - loss: 0.6505 - regression_loss: 0.5275 - classification_loss: 0.1230
  485/10000 [>.............................] - ETA: 1:12:42 - loss: 0.6496 - regression_loss: 0.5269 - classification_loss: 0.1228
  486/10000 [>.............................] - ETA: 1:12:41 - loss: 0.6490 - regression_loss: 0.5263 - classification_loss: 0.1227
  487/10000 [>.............................] - ETA: 1:12:40 - loss: 0.6502 - regression_loss: 0.5274 - classification_loss: 0.1228
  488/10000 [>.............................] - ETA: 1:12:41 - loss: 0.6494 - regression_loss: 0.5268 - classification_loss: 0.1226
  489/10000 [>.............................] - ETA: 1:12:40 - loss: 0.6503 - regression_loss: 0.5275 - classification_loss: 0.1228
  490/10000 [>.............................] - ETA: 1:12:39 - loss: 0.6508 - regression_loss: 0.5278 - classification_loss: 0.1229
  491/10000 [>.............................] - ETA: 1:12:38 - loss: 0.6498 - regression_loss: 0.5271 - classification_loss: 0.1228
  492/10000 [>.............................] - ETA: 1:12:38 - loss: 0.6494 - regression_loss: 0.5268 - classification_loss: 0.1225
  493/10000 [>.............................] - ETA: 1:12:37 - loss: 0.6487 - regression_loss: 0.5263 - classification_loss: 0.1224
  494/10000 [>.............................] - ETA: 1:12:36 - loss: 0.6484 - regression_loss: 0.5259 - classification_loss: 0.1224
  495/10000 [>.............................] - ETA: 1:12:35 - loss: 0.6477 - regression_loss: 0.5254 - classification_loss: 0.1223
  496/10000 [>.............................] - ETA: 1:12:35 - loss: 0.6474 - regression_loss: 0.5252 - classification_loss: 0.1222
  497/10000 [>.............................] - ETA: 1:12:34 - loss: 0.6469 - regression_loss: 0.5248 - classification_loss: 0.1221
  498/10000 [>.............................] - ETA: 1:12:34 - loss: 0.6464 - regression_loss: 0.5245 - classification_loss: 0.1220
  499/10000 [>.............................] - ETA: 1:12:33 - loss: 0.6460 - regression_loss: 0.5242 - classification_loss: 0.1218
  500/10000 [>.............................] - ETA: 1:12:33 - loss: 0.6463 - regression_loss: 0.5245 - classification_loss: 0.1218
  501/10000 [>.............................] - ETA: 1:12:33 - loss: 0.6460 - regression_loss: 0.5244 - classification_loss: 0.1216
  502/10000 [>.............................] - ETA: 1:12:33 - loss: 0.6453 - regression_loss: 0.5238 - classification_loss: 0.1214
  503/10000 [>.............................] - ETA: 1:12:33 - loss: 0.6445 - regression_loss: 0.5232 - classification_loss: 0.1213
  504/10000 [>.............................] - ETA: 1:12:32 - loss: 0.6449 - regression_loss: 0.5236 - classification_loss: 0.1212
  505/10000 [>.............................] - ETA: 1:12:32 - loss: 0.6458 - regression_loss: 0.5243 - classification_loss: 0.1216
  506/10000 [>.............................] - ETA: 1:12:32 - loss: 0.6465 - regression_loss: 0.5247 - classification_loss: 0.1218
  507/10000 [>.............................] - ETA: 1:12:31 - loss: 0.6478 - regression_loss: 0.5258 - classification_loss: 0.1219
  508/10000 [>.............................] - ETA: 1:12:31 - loss: 0.6485 - regression_loss: 0.5262 - classification_loss: 0.1223
  509/10000 [>.............................] - ETA: 1:12:30 - loss: 0.6478 - regression_loss: 0.5254 - classification_loss: 0.1224
  510/10000 [>.............................] - ETA: 1:12:30 - loss: 0.6477 - regression_loss: 0.5254 - classification_loss: 0.1223
  511/10000 [>.............................] - ETA: 1:12:29 - loss: 0.6474 - regression_loss: 0.5252 - classification_loss: 0.1222
  512/10000 [>.............................] - ETA: 1:12:28 - loss: 0.6465 - regression_loss: 0.5245 - classification_loss: 0.1220
  513/10000 [>.............................] - ETA: 1:12:27 - loss: 0.6462 - regression_loss: 0.5243 - classification_loss: 0.1219
  514/10000 [>.............................] - ETA: 1:12:27 - loss: 0.6470 - regression_loss: 0.5250 - classification_loss: 0.1220
  515/10000 [>.............................] - ETA: 1:12:25 - loss: 0.6469 - regression_loss: 0.5250 - classification_loss: 0.1219
  516/10000 [>.............................] - ETA: 1:12:24 - loss: 0.6464 - regression_loss: 0.5246 - classification_loss: 0.1218
  517/10000 [>.............................] - ETA: 1:12:24 - loss: 0.6477 - regression_loss: 0.5256 - classification_loss: 0.1221
  518/10000 [>.............................] - ETA: 1:12:23 - loss: 0.6478 - regression_loss: 0.5257 - classification_loss: 0.1221
  519/10000 [>.............................] - ETA: 1:12:22 - loss: 0.6483 - regression_loss: 0.5262 - classification_loss: 0.1221
  520/10000 [>.............................] - ETA: 1:12:20 - loss: 0.6479 - regression_loss: 0.5259 - classification_loss: 0.1220
  521/10000 [>.............................] - ETA: 1:12:20 - loss: 0.6471 - regression_loss: 0.5252 - classification_loss: 0.1219
  522/10000 [>.............................] - ETA: 1:12:19 - loss: 0.6465 - regression_loss: 0.5245 - classification_loss: 0.1220
  523/10000 [>.............................] - ETA: 1:12:19 - loss: 0.6472 - regression_loss: 0.5249 - classification_loss: 0.1223
  524/10000 [>.............................] - ETA: 1:12:18 - loss: 0.6476 - regression_loss: 0.5249 - classification_loss: 0.1227
  525/10000 [>.............................] - ETA: 1:12:18 - loss: 0.6477 - regression_loss: 0.5249 - classification_loss: 0.1227
  526/10000 [>.............................] - ETA: 1:12:17 - loss: 0.6469 - regression_loss: 0.5243 - classification_loss: 0.1226
  527/10000 [>.............................] - ETA: 1:12:15 - loss: 0.6485 - regression_loss: 0.5256 - classification_loss: 0.1229
  528/10000 [>.............................] - ETA: 1:12:14 - loss: 0.6487 - regression_loss: 0.5259 - classification_loss: 0.1228
  529/10000 [>.............................] - ETA: 1:12:14 - loss: 0.6486 - regression_loss: 0.5258 - classification_loss: 0.1228
  530/10000 [>.............................] - ETA: 1:12:14 - loss: 0.6481 - regression_loss: 0.5254 - classification_loss: 0.1227
  531/10000 [>.............................] - ETA: 1:12:14 - loss: 0.6473 - regression_loss: 0.5247 - classification_loss: 0.1226
  532/10000 [>.............................] - ETA: 1:12:13 - loss: 0.6466 - regression_loss: 0.5242 - classification_loss: 0.1224
  533/10000 [>.............................] - ETA: 1:12:13 - loss: 0.6466 - regression_loss: 0.5241 - classification_loss: 0.1225
  534/10000 [>.............................] - ETA: 1:12:12 - loss: 0.6484 - regression_loss: 0.5255 - classification_loss: 0.1228
  535/10000 [>.............................] - ETA: 1:12:12 - loss: 0.6494 - regression_loss: 0.5265 - classification_loss: 0.1229
  536/10000 [>.............................] - ETA: 1:12:11 - loss: 0.6485 - regression_loss: 0.5258 - classification_loss: 0.1227
  537/10000 [>.............................] - ETA: 1:12:11 - loss: 0.6492 - regression_loss: 0.5264 - classification_loss: 0.1228
  538/10000 [>.............................] - ETA: 1:12:10 - loss: 0.6487 - regression_loss: 0.5260 - classification_loss: 0.1227
  539/10000 [>.............................] - ETA: 1:12:10 - loss: 0.6486 - regression_loss: 0.5259 - classification_loss: 0.1226
  540/10000 [>.............................] - ETA: 1:12:10 - loss: 0.6479 - regression_loss: 0.5253 - classification_loss: 0.1227
  541/10000 [>.............................] - ETA: 1:12:10 - loss: 0.6473 - regression_loss: 0.5248 - classification_loss: 0.1225
  542/10000 [>.............................] - ETA: 1:12:10 - loss: 0.6480 - regression_loss: 0.5254 - classification_loss: 0.1226
  543/10000 [>.............................] - ETA: 1:12:08 - loss: 0.6471 - regression_loss: 0.5247 - classification_loss: 0.1224
  544/10000 [>.............................] - ETA: 1:12:07 - loss: 0.6472 - regression_loss: 0.5248 - classification_loss: 0.1224
  545/10000 [>.............................] - ETA: 1:12:07 - loss: 0.6466 - regression_loss: 0.5243 - classification_loss: 0.1223
  546/10000 [>.............................] - ETA: 1:12:07 - loss: 0.6479 - regression_loss: 0.5250 - classification_loss: 0.1229
  547/10000 [>.............................] - ETA: 1:12:06 - loss: 0.6484 - regression_loss: 0.5253 - classification_loss: 0.1232
  548/10000 [>.............................] - ETA: 1:12:06 - loss: 0.6478 - regression_loss: 0.5248 - classification_loss: 0.1231
  549/10000 [>.............................] - ETA: 1:12:06 - loss: 0.6476 - regression_loss: 0.5245 - classification_loss: 0.1231
  550/10000 [>.............................] - ETA: 1:12:06 - loss: 0.6484 - regression_loss: 0.5253 - classification_loss: 0.1231
  551/10000 [>.............................] - ETA: 1:12:05 - loss: 0.6478 - regression_loss: 0.5249 - classification_loss: 0.1230
  552/10000 [>.............................] - ETA: 1:12:04 - loss: 0.6480 - regression_loss: 0.5247 - classification_loss: 0.1233
  553/10000 [>.............................] - ETA: 1:12:04 - loss: 0.6472 - regression_loss: 0.5241 - classification_loss: 0.1231
  554/10000 [>.............................] - ETA: 1:12:03 - loss: 0.6476 - regression_loss: 0.5243 - classification_loss: 0.1233
  555/10000 [>.............................] - ETA: 1:12:03 - loss: 0.6478 - regression_loss: 0.5245 - classification_loss: 0.1233
  556/10000 [>.............................] - ETA: 1:12:03 - loss: 0.6474 - regression_loss: 0.5241 - classification_loss: 0.1232
  557/10000 [>.............................] - ETA: 1:12:02 - loss: 0.6474 - regression_loss: 0.5242 - classification_loss: 0.1232
  558/10000 [>.............................] - ETA: 1:12:01 - loss: 0.6481 - regression_loss: 0.5249 - classification_loss: 0.1233
  559/10000 [>.............................] - ETA: 1:12:01 - loss: 0.6483 - regression_loss: 0.5251 - classification_loss: 0.1232
  560/10000 [>.............................] - ETA: 1:12:00 - loss: 0.6495 - regression_loss: 0.5261 - classification_loss: 0.1234
  561/10000 [>.............................] - ETA: 1:11:59 - loss: 0.6501 - regression_loss: 0.5265 - classification_loss: 0.1236
  562/10000 [>.............................] - ETA: 1:11:59 - loss: 0.6496 - regression_loss: 0.5262 - classification_loss: 0.1235
  563/10000 [>.............................] - ETA: 1:11:58 - loss: 0.6512 - regression_loss: 0.5275 - classification_loss: 0.1237
  564/10000 [>.............................] - ETA: 1:11:58 - loss: 0.6518 - regression_loss: 0.5278 - classification_loss: 0.1240
  565/10000 [>.............................] - ETA: 1:11:57 - loss: 0.6517 - regression_loss: 0.5277 - classification_loss: 0.1241
  566/10000 [>.............................] - ETA: 1:11:56 - loss: 0.6518 - regression_loss: 0.5278 - classification_loss: 0.1240
  567/10000 [>.............................] - ETA: 1:11:55 - loss: 0.6517 - regression_loss: 0.5277 - classification_loss: 0.1240
  568/10000 [>.............................] - ETA: 1:11:55 - loss: 0.6513 - regression_loss: 0.5274 - classification_loss: 0.1239
  569/10000 [>.............................] - ETA: 1:11:55 - loss: 0.6515 - regression_loss: 0.5277 - classification_loss: 0.1238
  570/10000 [>.............................] - ETA: 1:11:53 - loss: 0.6510 - regression_loss: 0.5273 - classification_loss: 0.1236
  571/10000 [>.............................] - ETA: 1:11:53 - loss: 0.6505 - regression_loss: 0.5269 - classification_loss: 0.1236
  572/10000 [>.............................] - ETA: 1:11:52 - loss: 0.6497 - regression_loss: 0.5262 - classification_loss: 0.1235
  573/10000 [>.............................] - ETA: 1:11:51 - loss: 0.6490 - regression_loss: 0.5256 - classification_loss: 0.1233
  574/10000 [>.............................] - ETA: 1:11:51 - loss: 0.6488 - regression_loss: 0.5255 - classification_loss: 0.1233
  575/10000 [>.............................] - ETA: 1:11:50 - loss: 0.6491 - regression_loss: 0.5258 - classification_loss: 0.1232
  576/10000 [>.............................] - ETA: 1:11:48 - loss: 0.6498 - regression_loss: 0.5265 - classification_loss: 0.1233
  577/10000 [>.............................] - ETA: 1:11:48 - loss: 0.6492 - regression_loss: 0.5260 - classification_loss: 0.1232
  578/10000 [>.............................] - ETA: 1:11:48 - loss: 0.6506 - regression_loss: 0.5273 - classification_loss: 0.1233
  579/10000 [>.............................] - ETA: 1:11:47 - loss: 0.6498 - regression_loss: 0.5266 - classification_loss: 0.1232
  580/10000 [>.............................] - ETA: 1:11:46 - loss: 0.6493 - regression_loss: 0.5263 - classification_loss: 0.1230
  581/10000 [>.............................] - ETA: 1:11:46 - loss: 0.6501 - regression_loss: 0.5268 - classification_loss: 0.1232
  582/10000 [>.............................] - ETA: 1:11:45 - loss: 0.6504 - regression_loss: 0.5271 - classification_loss: 0.1232
  583/10000 [>.............................] - ETA: 1:11:45 - loss: 0.6508 - regression_loss: 0.5275 - classification_loss: 0.1233
  584/10000 [>.............................] - ETA: 1:11:45 - loss: 0.6501 - regression_loss: 0.5269 - classification_loss: 0.1232
  585/10000 [>.............................] - ETA: 1:11:45 - loss: 0.6492 - regression_loss: 0.5262 - classification_loss: 0.1230
  586/10000 [>.............................] - ETA: 1:11:44 - loss: 0.6487 - regression_loss: 0.5258 - classification_loss: 0.1229
  587/10000 [>.............................] - ETA: 1:11:44 - loss: 0.6490 - regression_loss: 0.5260 - classification_loss: 0.1230
  588/10000 [>.............................] - ETA: 1:11:44 - loss: 0.6482 - regression_loss: 0.5254 - classification_loss: 0.1228
  589/10000 [>.............................] - ETA: 1:11:44 - loss: 0.6479 - regression_loss: 0.5251 - classification_loss: 0.1228
  590/10000 [>.............................] - ETA: 1:11:44 - loss: 0.6482 - regression_loss: 0.5254 - classification_loss: 0.1227
  591/10000 [>.............................] - ETA: 1:11:43 - loss: 0.6480 - regression_loss: 0.5254 - classification_loss: 0.1226
  592/10000 [>.............................] - ETA: 1:11:43 - loss: 0.6478 - regression_loss: 0.5253 - classification_loss: 0.1225
  593/10000 [>.............................] - ETA: 1:11:43 - loss: 0.6470 - regression_loss: 0.5247 - classification_loss: 0.1223
  594/10000 [>.............................] - ETA: 1:11:42 - loss: 0.6469 - regression_loss: 0.5246 - classification_loss: 0.1223
  595/10000 [>.............................] - ETA: 1:11:42 - loss: 0.6468 - regression_loss: 0.5247 - classification_loss: 0.1222
  596/10000 [>.............................] - ETA: 1:11:42 - loss: 0.6481 - regression_loss: 0.5258 - classification_loss: 0.1223
  597/10000 [>.............................] - ETA: 1:11:41 - loss: 0.6473 - regression_loss: 0.5251 - classification_loss: 0.1222
  598/10000 [>.............................] - ETA: 1:11:41 - loss: 0.6472 - regression_loss: 0.5251 - classification_loss: 0.1221
  599/10000 [>.............................] - ETA: 1:11:41 - loss: 0.6468 - regression_loss: 0.5248 - classification_loss: 0.1220
  600/10000 [>.............................] - ETA: 1:11:41 - loss: 0.6465 - regression_loss: 0.5245 - classification_loss: 0.1220
  601/10000 [>.............................] - ETA: 1:11:40 - loss: 0.6474 - regression_loss: 0.5252 - classification_loss: 0.1222
  602/10000 [>.............................] - ETA: 1:11:39 - loss: 0.6465 - regression_loss: 0.5245 - classification_loss: 0.1220
  603/10000 [>.............................] - ETA: 1:11:39 - loss: 0.6460 - regression_loss: 0.5240 - classification_loss: 0.1219
  604/10000 [>.............................] - ETA: 1:11:38 - loss: 0.6468 - regression_loss: 0.5247 - classification_loss: 0.1221
  605/10000 [>.............................] - ETA: 1:11:37 - loss: 0.6476 - regression_loss: 0.5254 - classification_loss: 0.1222
  606/10000 [>.............................] - ETA: 1:11:37 - loss: 0.6477 - regression_loss: 0.5254 - classification_loss: 0.1223
  607/10000 [>.............................] - ETA: 1:11:36 - loss: 0.6473 - regression_loss: 0.5250 - classification_loss: 0.1222
  608/10000 [>.............................] - ETA: 1:11:35 - loss: 0.6478 - regression_loss: 0.5254 - classification_loss: 0.1224
  609/10000 [>.............................] - ETA: 1:11:35 - loss: 0.6476 - regression_loss: 0.5253 - classification_loss: 0.1222
  610/10000 [>.............................] - ETA: 1:11:35 - loss: 0.6470 - regression_loss: 0.5249 - classification_loss: 0.1221
  611/10000 [>.............................] - ETA: 1:11:34 - loss: 0.6464 - regression_loss: 0.5245 - classification_loss: 0.1220
  612/10000 [>.............................] - ETA: 1:11:34 - loss: 0.6456 - regression_loss: 0.5238 - classification_loss: 0.1218
  613/10000 [>.............................] - ETA: 1:11:34 - loss: 0.6448 - regression_loss: 0.5231 - classification_loss: 0.1217
  614/10000 [>.............................] - ETA: 1:11:34 - loss: 0.6448 - regression_loss: 0.5232 - classification_loss: 0.1216
  615/10000 [>.............................] - ETA: 1:11:33 - loss: 0.6442 - regression_loss: 0.5227 - classification_loss: 0.1215
  616/10000 [>.............................] - ETA: 1:11:32 - loss: 0.6448 - regression_loss: 0.5232 - classification_loss: 0.1216
  617/10000 [>.............................] - ETA: 1:11:31 - loss: 0.6447 - regression_loss: 0.5231 - classification_loss: 0.1215
  618/10000 [>.............................] - ETA: 1:11:30 - loss: 0.6440 - regression_loss: 0.5226 - classification_loss: 0.1214
  619/10000 [>.............................] - ETA: 1:11:30 - loss: 0.6435 - regression_loss: 0.5222 - classification_loss: 0.1214
  620/10000 [>.............................] - ETA: 1:11:29 - loss: 0.6432 - regression_loss: 0.5219 - classification_loss: 0.1213
  621/10000 [>.............................] - ETA: 1:11:28 - loss: 0.6425 - regression_loss: 0.5214 - classification_loss: 0.1211
  622/10000 [>.............................] - ETA: 1:11:28 - loss: 0.6421 - regression_loss: 0.5210 - classification_loss: 0.1211
  623/10000 [>.............................] - ETA: 1:11:28 - loss: 0.6418 - regression_loss: 0.5208 - classification_loss: 0.1210
  624/10000 [>.............................] - ETA: 1:11:27 - loss: 0.6416 - regression_loss: 0.5208 - classification_loss: 0.1209
  625/10000 [>.............................] - ETA: 1:11:27 - loss: 0.6411 - regression_loss: 0.5204 - classification_loss: 0.1207
  626/10000 [>.............................] - ETA: 1:11:27 - loss: 0.6413 - regression_loss: 0.5205 - classification_loss: 0.1208
  627/10000 [>.............................] - ETA: 1:11:27 - loss: 0.6410 - regression_loss: 0.5203 - classification_loss: 0.1207
  628/10000 [>.............................] - ETA: 1:11:27 - loss: 0.6407 - regression_loss: 0.5200 - classification_loss: 0.1207
  629/10000 [>.............................] - ETA: 1:11:27 - loss: 0.6418 - regression_loss: 0.5208 - classification_loss: 0.1210
  630/10000 [>.............................] - ETA: 1:11:26 - loss: 0.6420 - regression_loss: 0.5209 - classification_loss: 0.1210
  631/10000 [>.............................] - ETA: 1:11:26 - loss: 0.6419 - regression_loss: 0.5210 - classification_loss: 0.1209
  632/10000 [>.............................] - ETA: 1:11:25 - loss: 0.6417 - regression_loss: 0.5207 - classification_loss: 0.1209
  633/10000 [>.............................] - ETA: 1:11:25 - loss: 0.6424 - regression_loss: 0.5214 - classification_loss: 0.1209
  634/10000 [>.............................] - ETA: 1:11:25 - loss: 0.6416 - regression_loss: 0.5207 - classification_loss: 0.1208
  635/10000 [>.............................] - ETA: 1:11:24 - loss: 0.6419 - regression_loss: 0.5211 - classification_loss: 0.1208
  636/10000 [>.............................] - ETA: 1:11:24 - loss: 0.6415 - regression_loss: 0.5209 - classification_loss: 0.1206
  637/10000 [>.............................] - ETA: 1:11:24 - loss: 0.6407 - regression_loss: 0.5202 - classification_loss: 0.1205
  638/10000 [>.............................] - ETA: 1:11:23 - loss: 0.6401 - regression_loss: 0.5197 - classification_loss: 0.1203
  639/10000 [>.............................] - ETA: 1:11:22 - loss: 0.6407 - regression_loss: 0.5203 - classification_loss: 0.1204
  640/10000 [>.............................] - ETA: 1:11:22 - loss: 0.6409 - regression_loss: 0.5205 - classification_loss: 0.1204
  641/10000 [>.............................] - ETA: 1:11:22 - loss: 0.6403 - regression_loss: 0.5200 - classification_loss: 0.1203
  642/10000 [>.............................] - ETA: 1:11:22 - loss: 0.6400 - regression_loss: 0.5195 - classification_loss: 0.1204
  643/10000 [>.............................] - ETA: 1:11:21 - loss: 0.6408 - regression_loss: 0.5200 - classification_loss: 0.1208
  644/10000 [>.............................] - ETA: 1:11:20 - loss: 0.6409 - regression_loss: 0.5202 - classification_loss: 0.1208
  645/10000 [>.............................] - ETA: 1:11:20 - loss: 0.6408 - regression_loss: 0.5200 - classification_loss: 0.1208
  646/10000 [>.............................] - ETA: 1:11:20 - loss: 0.6417 - regression_loss: 0.5207 - classification_loss: 0.1210
  647/10000 [>.............................] - ETA: 1:11:19 - loss: 0.6426 - regression_loss: 0.5215 - classification_loss: 0.1211
  648/10000 [>.............................] - ETA: 1:11:18 - loss: 0.6424 - regression_loss: 0.5212 - classification_loss: 0.1212
  649/10000 [>.............................] - ETA: 1:11:18 - loss: 0.6426 - regression_loss: 0.5214 - classification_loss: 0.1212
  650/10000 [>.............................] - ETA: 1:11:18 - loss: 0.6422 - regression_loss: 0.5210 - classification_loss: 0.1212
  651/10000 [>.............................] - ETA: 1:11:18 - loss: 0.6416 - regression_loss: 0.5204 - classification_loss: 0.1212
  652/10000 [>.............................] - ETA: 1:11:18 - loss: 0.6423 - regression_loss: 0.5210 - classification_loss: 0.1213
  653/10000 [>.............................] - ETA: 1:11:17 - loss: 0.6419 - regression_loss: 0.5206 - classification_loss: 0.1214
  654/10000 [>.............................] - ETA: 1:11:18 - loss: 0.6417 - regression_loss: 0.5204 - classification_loss: 0.1213
  655/10000 [>.............................] - ETA: 1:11:18 - loss: 0.6411 - regression_loss: 0.5198 - classification_loss: 0.1213
  656/10000 [>.............................] - ETA: 1:11:18 - loss: 0.6410 - regression_loss: 0.5198 - classification_loss: 0.1212
  657/10000 [>.............................] - ETA: 1:11:18 - loss: 0.6413 - regression_loss: 0.5200 - classification_loss: 0.1212
  658/10000 [>.............................] - ETA: 1:11:18 - loss: 0.6422 - regression_loss: 0.5209 - classification_loss: 0.1213
  659/10000 [>.............................] - ETA: 1:11:18 - loss: 0.6423 - regression_loss: 0.5210 - classification_loss: 0.1213
  660/10000 [>.............................] - ETA: 1:11:16 - loss: 0.6428 - regression_loss: 0.5214 - classification_loss: 0.1214
  661/10000 [>.............................] - ETA: 1:11:15 - loss: 0.6421 - regression_loss: 0.5209 - classification_loss: 0.1212
  662/10000 [>.............................] - ETA: 1:11:15 - loss: 0.6413 - regression_loss: 0.5202 - classification_loss: 0.1211
  663/10000 [>.............................] - ETA: 1:11:15 - loss: 0.6412 - regression_loss: 0.5201 - classification_loss: 0.1211
  664/10000 [>.............................] - ETA: 1:11:13 - loss: 0.6422 - regression_loss: 0.5208 - classification_loss: 0.1214
  665/10000 [>.............................] - ETA: 1:11:13 - loss: 0.6418 - regression_loss: 0.5204 - classification_loss: 0.1213
  666/10000 [>.............................] - ETA: 1:11:13 - loss: 0.6423 - regression_loss: 0.5210 - classification_loss: 0.1213
  667/10000 [=>............................] - ETA: 1:11:13 - loss: 0.6422 - regression_loss: 0.5209 - classification_loss: 0.1213
  668/10000 [=>............................] - ETA: 1:11:13 - loss: 0.6423 - regression_loss: 0.5209 - classification_loss: 0.1213
  669/10000 [=>............................] - ETA: 1:11:13 - loss: 0.6427 - regression_loss: 0.5213 - classification_loss: 0.1215
  670/10000 [=>............................] - ETA: 1:11:12 - loss: 0.6420 - regression_loss: 0.5207 - classification_loss: 0.1213
  671/10000 [=>............................] - ETA: 1:11:12 - loss: 0.6425 - regression_loss: 0.5210 - classification_loss: 0.1215
  672/10000 [=>............................] - ETA: 1:11:11 - loss: 0.6430 - regression_loss: 0.5214 - classification_loss: 0.1216
  673/10000 [=>............................] - ETA: 1:11:11 - loss: 0.6427 - regression_loss: 0.5211 - classification_loss: 0.1215
  674/10000 [=>............................] - ETA: 1:11:10 - loss: 0.6423 - regression_loss: 0.5209 - classification_loss: 0.1214
  675/10000 [=>............................] - ETA: 1:11:10 - loss: 0.6427 - regression_loss: 0.5211 - classification_loss: 0.1216
  676/10000 [=>............................] - ETA: 1:11:09 - loss: 0.6421 - regression_loss: 0.5206 - classification_loss: 0.1214
  677/10000 [=>............................] - ETA: 1:11:09 - loss: 0.6420 - regression_loss: 0.5205 - classification_loss: 0.1215
  678/10000 [=>............................] - ETA: 1:11:09 - loss: 0.6414 - regression_loss: 0.5200 - classification_loss: 0.1214
  679/10000 [=>............................] - ETA: 1:11:08 - loss: 0.6411 - regression_loss: 0.5198 - classification_loss: 0.1213
  680/10000 [=>............................] - ETA: 1:11:08 - loss: 0.6411 - regression_loss: 0.5199 - classification_loss: 0.1212
  681/10000 [=>............................] - ETA: 1:11:07 - loss: 0.6412 - regression_loss: 0.5201 - classification_loss: 0.1211
  682/10000 [=>............................] - ETA: 1:11:07 - loss: 0.6417 - regression_loss: 0.5205 - classification_loss: 0.1212
  683/10000 [=>............................] - ETA: 1:11:07 - loss: 0.6414 - regression_loss: 0.5203 - classification_loss: 0.1211
  684/10000 [=>............................] - ETA: 1:11:06 - loss: 0.6429 - regression_loss: 0.5213 - classification_loss: 0.1216
  685/10000 [=>............................] - ETA: 1:11:06 - loss: 0.6421 - regression_loss: 0.5207 - classification_loss: 0.1214
  686/10000 [=>............................] - ETA: 1:11:05 - loss: 0.6425 - regression_loss: 0.5211 - classification_loss: 0.1214
  687/10000 [=>............................] - ETA: 1:11:05 - loss: 0.6423 - regression_loss: 0.5209 - classification_loss: 0.1214
  688/10000 [=>............................] - ETA: 1:11:03 - loss: 0.6430 - regression_loss: 0.5216 - classification_loss: 0.1214
  689/10000 [=>............................] - ETA: 1:11:03 - loss: 0.6432 - regression_loss: 0.5216 - classification_loss: 0.1215
  690/10000 [=>............................] - ETA: 1:11:03 - loss: 0.6433 - regression_loss: 0.5216 - classification_loss: 0.1217
  691/10000 [=>............................] - ETA: 1:11:02 - loss: 0.6440 - regression_loss: 0.5223 - classification_loss: 0.1217
  692/10000 [=>............................] - ETA: 1:11:01 - loss: 0.6451 - regression_loss: 0.5231 - classification_loss: 0.1220
  693/10000 [=>............................] - ETA: 1:11:01 - loss: 0.6458 - regression_loss: 0.5236 - classification_loss: 0.1222
  694/10000 [=>............................] - ETA: 1:11:00 - loss: 0.6459 - regression_loss: 0.5237 - classification_loss: 0.1222
  695/10000 [=>............................] - ETA: 1:10:59 - loss: 0.6456 - regression_loss: 0.5233 - classification_loss: 0.1223
  696/10000 [=>............................] - ETA: 1:10:59 - loss: 0.6450 - regression_loss: 0.5228 - classification_loss: 0.1221
  697/10000 [=>............................] - ETA: 1:10:59 - loss: 0.6447 - regression_loss: 0.5226 - classification_loss: 0.1221
  698/10000 [=>............................] - ETA: 1:10:58 - loss: 0.6446 - regression_loss: 0.5225 - classification_loss: 0.1220
  699/10000 [=>............................] - ETA: 1:10:58 - loss: 0.6443 - regression_loss: 0.5223 - classification_loss: 0.1220
  700/10000 [=>............................] - ETA: 1:10:57 - loss: 0.6434 - regression_loss: 0.5216 - classification_loss: 0.1218
  701/10000 [=>............................] - ETA: 1:10:56 - loss: 0.6429 - regression_loss: 0.5212 - classification_loss: 0.1217
  702/10000 [=>............................] - ETA: 1:10:56 - loss: 0.6430 - regression_loss: 0.5213 - classification_loss: 0.1217
  703/10000 [=>............................] - ETA: 1:10:55 - loss: 0.6427 - regression_loss: 0.5211 - classification_loss: 0.1216
  704/10000 [=>............................] - ETA: 1:10:55 - loss: 0.6425 - regression_loss: 0.5210 - classification_loss: 0.1215
  705/10000 [=>............................] - ETA: 1:10:54 - loss: 0.6426 - regression_loss: 0.5211 - classification_loss: 0.1215
  706/10000 [=>............................] - ETA: 1:10:54 - loss: 0.6425 - regression_loss: 0.5211 - classification_loss: 0.1214
  707/10000 [=>............................] - ETA: 1:10:53 - loss: 0.6429 - regression_loss: 0.5213 - classification_loss: 0.1216
  708/10000 [=>............................] - ETA: 1:10:53 - loss: 0.6438 - regression_loss: 0.5222 - classification_loss: 0.1216
  709/10000 [=>............................] - ETA: 1:10:53 - loss: 0.6435 - regression_loss: 0.5219 - classification_loss: 0.1216
  710/10000 [=>............................] - ETA: 1:10:52 - loss: 0.6429 - regression_loss: 0.5214 - classification_loss: 0.1215
  711/10000 [=>............................] - ETA: 1:10:52 - loss: 0.6433 - regression_loss: 0.5218 - classification_loss: 0.1214
  712/10000 [=>............................] - ETA: 1:10:51 - loss: 0.6425 - regression_loss: 0.5212 - classification_loss: 0.1213
  713/10000 [=>............................] - ETA: 1:10:51 - loss: 0.6420 - regression_loss: 0.5207 - classification_loss: 0.1213
  714/10000 [=>............................] - ETA: 1:10:50 - loss: 0.6416 - regression_loss: 0.5204 - classification_loss: 0.1212
  715/10000 [=>............................] - ETA: 1:10:50 - loss: 0.6420 - regression_loss: 0.5208 - classification_loss: 0.1212
  716/10000 [=>............................] - ETA: 1:10:49 - loss: 0.6420 - regression_loss: 0.5210 - classification_loss: 0.1211
  717/10000 [=>............................] - ETA: 1:10:49 - loss: 0.6415 - regression_loss: 0.5206 - classification_loss: 0.1209
  718/10000 [=>............................] - ETA: 1:10:48 - loss: 0.6411 - regression_loss: 0.5203 - classification_loss: 0.1209
  719/10000 [=>............................] - ETA: 1:10:48 - loss: 0.6412 - regression_loss: 0.5202 - classification_loss: 0.1210
  720/10000 [=>............................] - ETA: 1:10:48 - loss: 0.6407 - regression_loss: 0.5198 - classification_loss: 0.1210
  721/10000 [=>............................] - ETA: 1:10:48 - loss: 0.6408 - regression_loss: 0.5198 - classification_loss: 0.1210
  722/10000 [=>............................] - ETA: 1:10:47 - loss: 0.6403 - regression_loss: 0.5193 - classification_loss: 0.1210
  723/10000 [=>............................] - ETA: 1:10:47 - loss: 0.6396 - regression_loss: 0.5187 - classification_loss: 0.1209
  724/10000 [=>............................] - ETA: 1:10:47 - loss: 0.6392 - regression_loss: 0.5184 - classification_loss: 0.1208
  725/10000 [=>............................] - ETA: 1:10:46 - loss: 0.6389 - regression_loss: 0.5181 - classification_loss: 0.1208
  726/10000 [=>............................] - ETA: 1:10:46 - loss: 0.6389 - regression_loss: 0.5181 - classification_loss: 0.1208
  727/10000 [=>............................] - ETA: 1:10:45 - loss: 0.6397 - regression_loss: 0.5189 - classification_loss: 0.1208
  728/10000 [=>............................] - ETA: 1:10:45 - loss: 0.6398 - regression_loss: 0.5190 - classification_loss: 0.1208
  729/10000 [=>............................] - ETA: 1:10:45 - loss: 0.6399 - regression_loss: 0.5191 - classification_loss: 0.1208
  730/10000 [=>............................] - ETA: 1:10:44 - loss: 0.6398 - regression_loss: 0.5191 - classification_loss: 0.1207
  731/10000 [=>............................] - ETA: 1:10:44 - loss: 0.6396 - regression_loss: 0.5189 - classification_loss: 0.1207
  732/10000 [=>............................] - ETA: 1:10:43 - loss: 0.6396 - regression_loss: 0.5191 - classification_loss: 0.1206
  733/10000 [=>............................] - ETA: 1:10:43 - loss: 0.6391 - regression_loss: 0.5187 - classification_loss: 0.1204
  734/10000 [=>............................] - ETA: 1:10:42 - loss: 0.6402 - regression_loss: 0.5196 - classification_loss: 0.1206
  735/10000 [=>............................] - ETA: 1:10:42 - loss: 0.6396 - regression_loss: 0.5191 - classification_loss: 0.1205
  736/10000 [=>............................] - ETA: 1:10:41 - loss: 0.6395 - regression_loss: 0.5191 - classification_loss: 0.1204
  737/10000 [=>............................] - ETA: 1:10:40 - loss: 0.6394 - regression_loss: 0.5189 - classification_loss: 0.1204
  738/10000 [=>............................] - ETA: 1:10:40 - loss: 0.6401 - regression_loss: 0.5194 - classification_loss: 0.1206
  739/10000 [=>............................] - ETA: 1:10:40 - loss: 0.6411 - regression_loss: 0.5204 - classification_loss: 0.1207
  740/10000 [=>............................] - ETA: 1:10:39 - loss: 0.6406 - regression_loss: 0.5200 - classification_loss: 0.1206
  741/10000 [=>............................] - ETA: 1:10:39 - loss: 0.6399 - regression_loss: 0.5194 - classification_loss: 0.1205
  742/10000 [=>............................] - ETA: 1:10:39 - loss: 0.6393 - regression_loss: 0.5189 - classification_loss: 0.1203
  743/10000 [=>............................] - ETA: 1:10:39 - loss: 0.6388 - regression_loss: 0.5185 - classification_loss: 0.1202
  744/10000 [=>............................] - ETA: 1:10:38 - loss: 0.6385 - regression_loss: 0.5183 - classification_loss: 0.1202
  745/10000 [=>............................] - ETA: 1:10:38 - loss: 0.6383 - regression_loss: 0.5182 - classification_loss: 0.1201
  746/10000 [=>............................] - ETA: 1:10:37 - loss: 0.6387 - regression_loss: 0.5185 - classification_loss: 0.1202
  747/10000 [=>............................] - ETA: 1:10:37 - loss: 0.6381 - regression_loss: 0.5180 - classification_loss: 0.1201
  748/10000 [=>............................] - ETA: 1:10:37 - loss: 0.6378 - regression_loss: 0.5176 - classification_loss: 0.1202
  749/10000 [=>............................] - ETA: 1:10:36 - loss: 0.6382 - regression_loss: 0.5178 - classification_loss: 0.1203
  750/10000 [=>............................] - ETA: 1:10:35 - loss: 0.6382 - regression_loss: 0.5179 - classification_loss: 0.1203
  751/10000 [=>............................] - ETA: 1:10:35 - loss: 0.6377 - regression_loss: 0.5175 - classification_loss: 0.1202
  752/10000 [=>............................] - ETA: 1:10:35 - loss: 0.6373 - regression_loss: 0.5172 - classification_loss: 0.1201
  753/10000 [=>............................] - ETA: 1:10:33 - loss: 0.6370 - regression_loss: 0.5170 - classification_loss: 0.1200
  754/10000 [=>............................] - ETA: 1:10:32 - loss: 0.6364 - regression_loss: 0.5164 - classification_loss: 0.1199
  755/10000 [=>............................] - ETA: 1:10:32 - loss: 0.6361 - regression_loss: 0.5162 - classification_loss: 0.1199
  756/10000 [=>............................] - ETA: 1:10:32 - loss: 0.6361 - regression_loss: 0.5163 - classification_loss: 0.1198
  757/10000 [=>............................] - ETA: 1:10:31 - loss: 0.6355 - regression_loss: 0.5158 - classification_loss: 0.1197
  758/10000 [=>............................] - ETA: 1:10:31 - loss: 0.6364 - regression_loss: 0.5165 - classification_loss: 0.1198
  759/10000 [=>............................] - ETA: 1:10:30 - loss: 0.6363 - regression_loss: 0.5165 - classification_loss: 0.1197
  760/10000 [=>............................] - ETA: 1:10:30 - loss: 0.6363 - regression_loss: 0.5165 - classification_loss: 0.1198
  761/10000 [=>............................] - ETA: 1:10:30 - loss: 0.6361 - regression_loss: 0.5162 - classification_loss: 0.1199
  762/10000 [=>............................] - ETA: 1:10:29 - loss: 0.6358 - regression_loss: 0.5160 - classification_loss: 0.1198
  763/10000 [=>............................] - ETA: 1:10:29 - loss: 0.6364 - regression_loss: 0.5166 - classification_loss: 0.1198
  764/10000 [=>............................] - ETA: 1:10:28 - loss: 0.6360 - regression_loss: 0.5162 - classification_loss: 0.1197
  765/10000 [=>............................] - ETA: 1:10:27 - loss: 0.6358 - regression_loss: 0.5160 - classification_loss: 0.1198
  766/10000 [=>............................] - ETA: 1:10:26 - loss: 0.6357 - regression_loss: 0.5160 - classification_loss: 0.1197
  767/10000 [=>............................] - ETA: 1:10:25 - loss: 0.6364 - regression_loss: 0.5165 - classification_loss: 0.1199
  768/10000 [=>............................] - ETA: 1:10:25 - loss: 0.6362 - regression_loss: 0.5164 - classification_loss: 0.1199
  769/10000 [=>............................] - ETA: 1:10:24 - loss: 0.6357 - regression_loss: 0.5159 - classification_loss: 0.1199
  770/10000 [=>............................] - ETA: 1:10:24 - loss: 0.6357 - regression_loss: 0.5158 - classification_loss: 0.1199
  771/10000 [=>............................] - ETA: 1:10:23 - loss: 0.6353 - regression_loss: 0.5156 - classification_loss: 0.1198
  772/10000 [=>............................] - ETA: 1:10:23 - loss: 0.6351 - regression_loss: 0.5153 - classification_loss: 0.1198
  773/10000 [=>............................] - ETA: 1:10:23 - loss: 0.6347 - regression_loss: 0.5149 - classification_loss: 0.1198
  774/10000 [=>............................] - ETA: 1:10:23 - loss: 0.6342 - regression_loss: 0.5144 - classification_loss: 0.1198
  775/10000 [=>............................] - ETA: 1:10:23 - loss: 0.6344 - regression_loss: 0.5146 - classification_loss: 0.1198
  776/10000 [=>............................] - ETA: 1:10:23 - loss: 0.6340 - regression_loss: 0.5143 - classification_loss: 0.1197
  777/10000 [=>............................] - ETA: 1:10:22 - loss: 0.6342 - regression_loss: 0.5144 - classification_loss: 0.1198
  778/10000 [=>............................] - ETA: 1:10:22 - loss: 0.6336 - regression_loss: 0.5139 - classification_loss: 0.1197
  779/10000 [=>............................] - ETA: 1:10:21 - loss: 0.6332 - regression_loss: 0.5136 - classification_loss: 0.1196
  780/10000 [=>............................] - ETA: 1:10:21 - loss: 0.6331 - regression_loss: 0.5134 - classification_loss: 0.1196
  781/10000 [=>............................] - ETA: 1:10:20 - loss: 0.6326 - regression_loss: 0.5131 - classification_loss: 0.1195
  782/10000 [=>............................] - ETA: 1:10:20 - loss: 0.6321 - regression_loss: 0.5127 - classification_loss: 0.1194
  783/10000 [=>............................] - ETA: 1:10:19 - loss: 0.6322 - regression_loss: 0.5126 - classification_loss: 0.1196
  784/10000 [=>............................] - ETA: 1:10:19 - loss: 0.6330 - regression_loss: 0.5132 - classification_loss: 0.1199
  785/10000 [=>............................] - ETA: 1:10:19 - loss: 0.6335 - regression_loss: 0.5135 - classification_loss: 0.1200
  786/10000 [=>............................] - ETA: 1:10:19 - loss: 0.6340 - regression_loss: 0.5139 - classification_loss: 0.1201
  787/10000 [=>............................] - ETA: 1:10:18 - loss: 0.6341 - regression_loss: 0.5140 - classification_loss: 0.1202
  788/10000 [=>............................] - ETA: 1:10:18 - loss: 0.6340 - regression_loss: 0.5138 - classification_loss: 0.1201
  789/10000 [=>............................] - ETA: 1:10:17 - loss: 0.6335 - regression_loss: 0.5134 - classification_loss: 0.1201
  790/10000 [=>............................] - ETA: 1:10:17 - loss: 0.6335 - regression_loss: 0.5134 - classification_loss: 0.1201
  791/10000 [=>............................] - ETA: 1:10:16 - loss: 0.6333 - regression_loss: 0.5133 - classification_loss: 0.1201
  792/10000 [=>............................] - ETA: 1:10:15 - loss: 0.6336 - regression_loss: 0.5135 - classification_loss: 0.1200
  793/10000 [=>............................] - ETA: 1:10:14 - loss: 0.6335 - regression_loss: 0.5136 - classification_loss: 0.1200
  794/10000 [=>............................] - ETA: 1:10:14 - loss: 0.6337 - regression_loss: 0.5138 - classification_loss: 0.1200
  795/10000 [=>............................] - ETA: 1:10:13 - loss: 0.6341 - regression_loss: 0.5141 - classification_loss: 0.1200
  796/10000 [=>............................] - ETA: 1:10:12 - loss: 0.6346 - regression_loss: 0.5145 - classification_loss: 0.1201
  797/10000 [=>............................] - ETA: 1:10:12 - loss: 0.6349 - regression_loss: 0.5148 - classification_loss: 0.1201
  798/10000 [=>............................] - ETA: 1:10:11 - loss: 0.6352 - regression_loss: 0.5151 - classification_loss: 0.1201
  799/10000 [=>............................] - ETA: 1:10:11 - loss: 0.6347 - regression_loss: 0.5147 - classification_loss: 0.1200
  800/10000 [=>............................] - ETA: 1:10:10 - loss: 0.6351 - regression_loss: 0.5151 - classification_loss: 0.1200
  801/10000 [=>............................] - ETA: 1:10:09 - loss: 0.6352 - regression_loss: 0.5152 - classification_loss: 0.1200
  802/10000 [=>............................] - ETA: 1:10:09 - loss: 0.6356 - regression_loss: 0.5155 - classification_loss: 0.1201
  803/10000 [=>............................] - ETA: 1:10:08 - loss: 0.6357 - regression_loss: 0.5156 - classification_loss: 0.1201
  804/10000 [=>............................] - ETA: 1:10:08 - loss: 0.6357 - regression_loss: 0.5155 - classification_loss: 0.1202
  805/10000 [=>............................] - ETA: 1:10:07 - loss: 0.6356 - regression_loss: 0.5153 - classification_loss: 0.1203
  806/10000 [=>............................] - ETA: 1:10:06 - loss: 0.6352 - regression_loss: 0.5150 - classification_loss: 0.1202
  807/10000 [=>............................] - ETA: 1:10:05 - loss: 0.6346 - regression_loss: 0.5146 - classification_loss: 0.1200
  808/10000 [=>............................] - ETA: 1:10:05 - loss: 0.6351 - regression_loss: 0.5150 - classification_loss: 0.1202
  809/10000 [=>............................] - ETA: 1:10:04 - loss: 0.6349 - regression_loss: 0.5147 - classification_loss: 0.1202
  810/10000 [=>............................] - ETA: 1:10:04 - loss: 0.6356 - regression_loss: 0.5154 - classification_loss: 0.1202
  811/10000 [=>............................] - ETA: 1:10:04 - loss: 0.6358 - regression_loss: 0.5155 - classification_loss: 0.1203
  812/10000 [=>............................] - ETA: 1:10:04 - loss: 0.6356 - regression_loss: 0.5154 - classification_loss: 0.1202
  813/10000 [=>............................] - ETA: 1:10:02 - loss: 0.6360 - regression_loss: 0.5158 - classification_loss: 0.1202
  814/10000 [=>............................] - ETA: 1:10:02 - loss: 0.6360 - regression_loss: 0.5157 - classification_loss: 0.1202
  815/10000 [=>............................] - ETA: 1:10:02 - loss: 0.6356 - regression_loss: 0.5154 - classification_loss: 0.1202
  816/10000 [=>............................] - ETA: 1:10:02 - loss: 0.6356 - regression_loss: 0.5151 - classification_loss: 0.1205
  817/10000 [=>............................] - ETA: 1:10:01 - loss: 0.6355 - regression_loss: 0.5149 - classification_loss: 0.1206
  818/10000 [=>............................] - ETA: 1:10:01 - loss: 0.6348 - regression_loss: 0.5143 - classification_loss: 0.1205
  819/10000 [=>............................] - ETA: 1:10:01 - loss: 0.6345 - regression_loss: 0.5141 - classification_loss: 0.1204
  820/10000 [=>............................] - ETA: 1:10:00 - loss: 0.6341 - regression_loss: 0.5137 - classification_loss: 0.1204
  821/10000 [=>............................] - ETA: 1:09:59 - loss: 0.6337 - regression_loss: 0.5134 - classification_loss: 0.1203
  822/10000 [=>............................] - ETA: 1:09:59 - loss: 0.6338 - regression_loss: 0.5135 - classification_loss: 0.1204
  823/10000 [=>............................] - ETA: 1:09:59 - loss: 0.6340 - regression_loss: 0.5137 - classification_loss: 0.1203
  824/10000 [=>............................] - ETA: 1:09:58 - loss: 0.6341 - regression_loss: 0.5138 - classification_loss: 0.1203
  825/10000 [=>............................] - ETA: 1:09:58 - loss: 0.6342 - regression_loss: 0.5139 - classification_loss: 0.1203
  826/10000 [=>............................] - ETA: 1:09:57 - loss: 0.6345 - regression_loss: 0.5142 - classification_loss: 0.1203
  827/10000 [=>............................] - ETA: 1:09:57 - loss: 0.6349 - regression_loss: 0.5145 - classification_loss: 0.1203
  828/10000 [=>............................] - ETA: 1:09:56 - loss: 0.6346 - regression_loss: 0.5143 - classification_loss: 0.1203
  829/10000 [=>............................] - ETA: 1:09:56 - loss: 0.6343 - regression_loss: 0.5141 - classification_loss: 0.1202
  830/10000 [=>............................] - ETA: 1:09:55 - loss: 0.6343 - regression_loss: 0.5142 - classification_loss: 0.1201
  831/10000 [=>............................] - ETA: 1:09:54 - loss: 0.6341 - regression_loss: 0.5141 - classification_loss: 0.1200
  832/10000 [=>............................] - ETA: 1:09:53 - loss: 0.6339 - regression_loss: 0.5139 - classification_loss: 0.1200
  833/10000 [=>............................] - ETA: 1:09:53 - loss: 0.6337 - regression_loss: 0.5137 - classification_loss: 0.1200
  834/10000 [=>............................] - ETA: 1:09:53 - loss: 0.6333 - regression_loss: 0.5134 - classification_loss: 0.1199
  835/10000 [=>............................] - ETA: 1:09:53 - loss: 0.6328 - regression_loss: 0.5131 - classification_loss: 0.1198
  836/10000 [=>............................] - ETA: 1:09:52 - loss: 0.6327 - regression_loss: 0.5128 - classification_loss: 0.1198
  837/10000 [=>............................] - ETA: 1:09:52 - loss: 0.6331 - regression_loss: 0.5132 - classification_loss: 0.1200
  838/10000 [=>............................] - ETA: 1:09:51 - loss: 0.6334 - regression_loss: 0.5133 - classification_loss: 0.1201
  839/10000 [=>............................] - ETA: 1:09:51 - loss: 0.6334 - regression_loss: 0.5134 - classification_loss: 0.1200
  840/10000 [=>............................] - ETA: 1:09:51 - loss: 0.6334 - regression_loss: 0.5134 - classification_loss: 0.1199
  841/10000 [=>............................] - ETA: 1:09:50 - loss: 0.6329 - regression_loss: 0.5131 - classification_loss: 0.1198
  842/10000 [=>............................] - ETA: 1:09:50 - loss: 0.6328 - regression_loss: 0.5131 - classification_loss: 0.1197
  843/10000 [=>............................] - ETA: 1:09:50 - loss: 0.6326 - regression_loss: 0.5129 - classification_loss: 0.1197
  844/10000 [=>............................] - ETA: 1:09:49 - loss: 0.6331 - regression_loss: 0.5134 - classification_loss: 0.1197
  845/10000 [=>............................] - ETA: 1:09:49 - loss: 0.6333 - regression_loss: 0.5136 - classification_loss: 0.1197
  846/10000 [=>............................] - ETA: 1:09:48 - loss: 0.6329 - regression_loss: 0.5133 - classification_loss: 0.1196
  847/10000 [=>............................] - ETA: 1:09:48 - loss: 0.6330 - regression_loss: 0.5134 - classification_loss: 0.1196
  848/10000 [=>............................] - ETA: 1:09:48 - loss: 0.6325 - regression_loss: 0.5129 - classification_loss: 0.1195
  849/10000 [=>............................] - ETA: 1:09:47 - loss: 0.6330 - regression_loss: 0.5133 - classification_loss: 0.1196
  850/10000 [=>............................] - ETA: 1:09:47 - loss: 0.6337 - regression_loss: 0.5139 - classification_loss: 0.1197
  851/10000 [=>............................] - ETA: 1:09:46 - loss: 0.6332 - regression_loss: 0.5136 - classification_loss: 0.1196
  852/10000 [=>............................] - ETA: 1:09:45 - loss: 0.6332 - regression_loss: 0.5136 - classification_loss: 0.1197
  853/10000 [=>............................] - ETA: 1:09:45 - loss: 0.6330 - regression_loss: 0.5134 - classification_loss: 0.1196
  854/10000 [=>............................] - ETA: 1:09:44 - loss: 0.6329 - regression_loss: 0.5133 - classification_loss: 0.1196
  855/10000 [=>............................] - ETA: 1:09:44 - loss: 0.6327 - regression_loss: 0.5131 - classification_loss: 0.1195
  856/10000 [=>............................] - ETA: 1:09:43 - loss: 0.6336 - regression_loss: 0.5140 - classification_loss: 0.1196
  857/10000 [=>............................] - ETA: 1:09:43 - loss: 0.6332 - regression_loss: 0.5137 - classification_loss: 0.1195
  858/10000 [=>............................] - ETA: 1:09:42 - loss: 0.6328 - regression_loss: 0.5134 - classification_loss: 0.1195
  859/10000 [=>............................] - ETA: 1:09:42 - loss: 0.6323 - regression_loss: 0.5130 - classification_loss: 0.1194
  860/10000 [=>............................] - ETA: 1:09:41 - loss: 0.6327 - regression_loss: 0.5133 - classification_loss: 0.1193
  861/10000 [=>............................] - ETA: 1:09:41 - loss: 0.6322 - regression_loss: 0.5129 - classification_loss: 0.1193
  862/10000 [=>............................] - ETA: 1:09:41 - loss: 0.6330 - regression_loss: 0.5136 - classification_loss: 0.1194
  863/10000 [=>............................] - ETA: 1:09:40 - loss: 0.6329 - regression_loss: 0.5133 - classification_loss: 0.1195
  864/10000 [=>............................] - ETA: 1:09:39 - loss: 0.6333 - regression_loss: 0.5137 - classification_loss: 0.1196
  865/10000 [=>............................] - ETA: 1:09:38 - loss: 0.6329 - regression_loss: 0.5135 - classification_loss: 0.1195
  866/10000 [=>............................] - ETA: 1:09:38 - loss: 0.6327 - regression_loss: 0.5134 - classification_loss: 0.1194
  867/10000 [=>............................] - ETA: 1:09:37 - loss: 0.6325 - regression_loss: 0.5131 - classification_loss: 0.1193
  868/10000 [=>............................] - ETA: 1:09:37 - loss: 0.6325 - regression_loss: 0.5133 - classification_loss: 0.1193
  869/10000 [=>............................] - ETA: 1:09:36 - loss: 0.6324 - regression_loss: 0.5131 - classification_loss: 0.1193
  870/10000 [=>............................] - ETA: 1:09:36 - loss: 0.6324 - regression_loss: 0.5132 - classification_loss: 0.1192
  871/10000 [=>............................] - ETA: 1:09:35 - loss: 0.6320 - regression_loss: 0.5128 - classification_loss: 0.1192
  872/10000 [=>............................] - ETA: 1:09:34 - loss: 0.6316 - regression_loss: 0.5125 - classification_loss: 0.1191
  873/10000 [=>............................] - ETA: 1:09:34 - loss: 0.6318 - regression_loss: 0.5126 - classification_loss: 0.1192
  874/10000 [=>............................] - ETA: 1:09:33 - loss: 0.6316 - regression_loss: 0.5125 - classification_loss: 0.1191
  875/10000 [=>............................] - ETA: 1:09:32 - loss: 0.6313 - regression_loss: 0.5122 - classification_loss: 0.1191
  876/10000 [=>............................] - ETA: 1:09:32 - loss: 0.6311 - regression_loss: 0.5120 - classification_loss: 0.1190
  877/10000 [=>............................] - ETA: 1:09:31 - loss: 0.6318 - regression_loss: 0.5128 - classification_loss: 0.1191
  878/10000 [=>............................] - ETA: 1:09:30 - loss: 0.6315 - regression_loss: 0.5125 - classification_loss: 0.1190
  879/10000 [=>............................] - ETA: 1:09:30 - loss: 0.6309 - regression_loss: 0.5121 - classification_loss: 0.1188
  880/10000 [=>............................] - ETA: 1:09:29 - loss: 0.6311 - regression_loss: 0.5123 - classification_loss: 0.1188
  881/10000 [=>............................] - ETA: 1:09:29 - loss: 0.6306 - regression_loss: 0.5119 - classification_loss: 0.1187
  882/10000 [=>............................] - ETA: 1:09:28 - loss: 0.6300 - regression_loss: 0.5114 - classification_loss: 0.1186
  883/10000 [=>............................] - ETA: 1:09:28 - loss: 0.6296 - regression_loss: 0.5110 - classification_loss: 0.1186
  884/10000 [=>............................] - ETA: 1:09:28 - loss: 0.6292 - regression_loss: 0.5107 - classification_loss: 0.1185
  885/10000 [=>............................] - ETA: 1:09:27 - loss: 0.6289 - regression_loss: 0.5105 - classification_loss: 0.1184
  886/10000 [=>............................] - ETA: 1:09:27 - loss: 0.6289 - regression_loss: 0.5104 - classification_loss: 0.1185
  887/10000 [=>............................] - ETA: 1:09:25 - loss: 0.6290 - regression_loss: 0.5106 - classification_loss: 0.1185
  888/10000 [=>............................] - ETA: 1:09:25 - loss: 0.6288 - regression_loss: 0.5104 - classification_loss: 0.1184
  889/10000 [=>............................] - ETA: 1:09:25 - loss: 0.6286 - regression_loss: 0.5102 - classification_loss: 0.1184
  890/10000 [=>............................] - ETA: 1:09:25 - loss: 0.6282 - regression_loss: 0.5099 - classification_loss: 0.1183
  891/10000 [=>............................] - ETA: 1:09:24 - loss: 0.6281 - regression_loss: 0.5099 - classification_loss: 0.1183
  892/10000 [=>............................] - ETA: 1:09:24 - loss: 0.6280 - regression_loss: 0.5097 - classification_loss: 0.1182
  893/10000 [=>............................] - ETA: 1:09:23 - loss: 0.6275 - regression_loss: 0.5093 - classification_loss: 0.1182
  894/10000 [=>............................] - ETA: 1:09:23 - loss: 0.6271 - regression_loss: 0.5091 - classification_loss: 0.1180
  895/10000 [=>............................] - ETA: 1:09:22 - loss: 0.6269 - regression_loss: 0.5090 - classification_loss: 0.1179
  896/10000 [=>............................] - ETA: 1:09:22 - loss: 0.6266 - regression_loss: 0.5087 - classification_loss: 0.1179
  897/10000 [=>............................] - ETA: 1:09:21 - loss: 0.6274 - regression_loss: 0.5092 - classification_loss: 0.1182
  898/10000 [=>............................] - ETA: 1:09:21 - loss: 0.6281 - regression_loss: 0.5099 - classification_loss: 0.1182
  899/10000 [=>............................] - ETA: 1:09:20 - loss: 0.6280 - regression_loss: 0.5099 - classification_loss: 0.1181
  900/10000 [=>............................] - ETA: 1:09:20 - loss: 0.6277 - regression_loss: 0.5096 - classification_loss: 0.1181
  901/10000 [=>............................] - ETA: 1:09:20 - loss: 0.6274 - regression_loss: 0.5094 - classification_loss: 0.1180
  902/10000 [=>............................] - ETA: 1:09:19 - loss: 0.6279 - regression_loss: 0.5097 - classification_loss: 0.1182
  903/10000 [=>............................] - ETA: 1:09:19 - loss: 0.6276 - regression_loss: 0.5094 - classification_loss: 0.1182
  904/10000 [=>............................] - ETA: 1:09:19 - loss: 0.6274 - regression_loss: 0.5093 - classification_loss: 0.1182
  905/10000 [=>............................] - ETA: 1:09:18 - loss: 0.6270 - regression_loss: 0.5089 - classification_loss: 0.1181
  906/10000 [=>............................] - ETA: 1:09:18 - loss: 0.6266 - regression_loss: 0.5085 - classification_loss: 0.1181
  907/10000 [=>............................] - ETA: 1:09:17 - loss: 0.6259 - regression_loss: 0.5079 - classification_loss: 0.1180
  908/10000 [=>............................] - ETA: 1:09:17 - loss: 0.6255 - regression_loss: 0.5076 - classification_loss: 0.1179
  909/10000 [=>............................] - ETA: 1:09:17 - loss: 0.6255 - regression_loss: 0.5076 - classification_loss: 0.1179
  910/10000 [=>............................] - ETA: 1:09:16 - loss: 0.6250 - regression_loss: 0.5071 - classification_loss: 0.1178
  911/10000 [=>............................] - ETA: 1:09:15 - loss: 0.6247 - regression_loss: 0.5070 - classification_loss: 0.1177
  912/10000 [=>............................] - ETA: 1:09:15 - loss: 0.6257 - regression_loss: 0.5079 - classification_loss: 0.1178
  913/10000 [=>............................] - ETA: 1:09:15 - loss: 0.6254 - regression_loss: 0.5076 - classification_loss: 0.1178
  914/10000 [=>............................] - ETA: 1:09:14 - loss: 0.6251 - regression_loss: 0.5074 - classification_loss: 0.1177
  915/10000 [=>............................] - ETA: 1:09:13 - loss: 0.6252 - regression_loss: 0.5075 - classification_loss: 0.1177
  916/10000 [=>............................] - ETA: 1:09:13 - loss: 0.6253 - regression_loss: 0.5077 - classification_loss: 0.1177
  917/10000 [=>............................] - ETA: 1:09:13 - loss: 0.6256 - regression_loss: 0.5080 - classification_loss: 0.1176
  918/10000 [=>............................] - ETA: 1:09:12 - loss: 0.6253 - regression_loss: 0.5077 - classification_loss: 0.1175
  919/10000 [=>............................] - ETA: 1:09:12 - loss: 0.6263 - regression_loss: 0.5085 - classification_loss: 0.1177
  920/10000 [=>............................] - ETA: 1:09:11 - loss: 0.6259 - regression_loss: 0.5083 - classification_loss: 0.1176
  921/10000 [=>............................] - ETA: 1:09:11 - loss: 0.6259 - regression_loss: 0.5083 - classification_loss: 0.1177
  922/10000 [=>............................] - ETA: 1:09:11 - loss: 0.6255 - regression_loss: 0.5079 - classification_loss: 0.1176
  923/10000 [=>............................] - ETA: 1:09:10 - loss: 0.6260 - regression_loss: 0.5082 - classification_loss: 0.1179
  924/10000 [=>............................] - ETA: 1:09:10 - loss: 0.6257 - regression_loss: 0.5078 - classification_loss: 0.1179
  925/10000 [=>............................] - ETA: 1:09:10 - loss: 0.6261 - regression_loss: 0.5083 - classification_loss: 0.1178
  926/10000 [=>............................] - ETA: 1:09:09 - loss: 0.6278 - regression_loss: 0.5091 - classification_loss: 0.1187
  927/10000 [=>............................] - ETA: 1:09:09 - loss: 0.6276 - regression_loss: 0.5089 - classification_loss: 0.1187
  928/10000 [=>............................] - ETA: 1:09:08 - loss: 0.6277 - regression_loss: 0.5089 - classification_loss: 0.1187
  929/10000 [=>............................] - ETA: 1:09:08 - loss: 0.6278 - regression_loss: 0.5089 - classification_loss: 0.1189
  930/10000 [=>............................] - ETA: 1:09:07 - loss: 0.6278 - regression_loss: 0.5089 - classification_loss: 0.1189
  931/10000 [=>............................] - ETA: 1:09:05 - loss: 0.6279 - regression_loss: 0.5090 - classification_loss: 0.1189
  932/10000 [=>............................] - ETA: 1:09:05 - loss: 0.6275 - regression_loss: 0.5087 - classification_loss: 0.1188
  933/10000 [=>............................] - ETA: 1:09:05 - loss: 0.6273 - regression_loss: 0.5085 - classification_loss: 0.1188
  934/10000 [=>............................] - ETA: 1:09:04 - loss: 0.6275 - regression_loss: 0.5087 - classification_loss: 0.1188
  935/10000 [=>............................] - ETA: 1:09:03 - loss: 0.6270 - regression_loss: 0.5083 - classification_loss: 0.1187
  936/10000 [=>............................] - ETA: 1:09:03 - loss: 0.6273 - regression_loss: 0.5086 - classification_loss: 0.1187
  937/10000 [=>............................] - ETA: 1:09:03 - loss: 0.6268 - regression_loss: 0.5082 - classification_loss: 0.1186
  938/10000 [=>............................] - ETA: 1:09:02 - loss: 0.6271 - regression_loss: 0.5084 - classification_loss: 0.1187
  939/10000 [=>............................] - ETA: 1:09:02 - loss: 0.6279 - regression_loss: 0.5090 - classification_loss: 0.1189
  940/10000 [=>............................] - ETA: 1:09:00 - loss: 0.6272 - regression_loss: 0.5084 - classification_loss: 0.1188
  941/10000 [=>............................] - ETA: 1:08:59 - loss: 0.6268 - regression_loss: 0.5081 - classification_loss: 0.1187
  942/10000 [=>............................] - ETA: 1:08:58 - loss: 0.6265 - regression_loss: 0.5079 - classification_loss: 0.1186
  943/10000 [=>............................] - ETA: 1:08:58 - loss: 0.6266 - regression_loss: 0.5079 - classification_loss: 0.1186
  944/10000 [=>............................] - ETA: 1:08:57 - loss: 0.6263 - regression_loss: 0.5078 - classification_loss: 0.1185
  945/10000 [=>............................] - ETA: 1:08:57 - loss: 0.6260 - regression_loss: 0.5075 - classification_loss: 0.1184
  946/10000 [=>............................] - ETA: 1:08:56 - loss: 0.6261 - regression_loss: 0.5077 - classification_loss: 0.1184
  947/10000 [=>............................] - ETA: 1:08:56 - loss: 0.6260 - regression_loss: 0.5076 - classification_loss: 0.1184
  948/10000 [=>............................] - ETA: 1:08:55 - loss: 0.6258 - regression_loss: 0.5074 - classification_loss: 0.1184
  949/10000 [=>............................] - ETA: 1:08:55 - loss: 0.6264 - regression_loss: 0.5078 - classification_loss: 0.1186
  950/10000 [=>............................] - ETA: 1:08:55 - loss: 0.6267 - regression_loss: 0.5081 - classification_loss: 0.1186
  951/10000 [=>............................] - ETA: 1:08:54 - loss: 0.6267 - regression_loss: 0.5081 - classification_loss: 0.1186
  952/10000 [=>............................] - ETA: 1:08:54 - loss: 0.6260 - regression_loss: 0.5075 - classification_loss: 0.1185
  953/10000 [=>............................] - ETA: 1:08:53 - loss: 0.6263 - regression_loss: 0.5079 - classification_loss: 0.1184
  954/10000 [=>............................] - ETA: 1:08:53 - loss: 0.6262 - regression_loss: 0.5078 - classification_loss: 0.1185
  955/10000 [=>............................] - ETA: 1:08:53 - loss: 0.6258 - regression_loss: 0.5074 - classification_loss: 0.1184
  956/10000 [=>............................] - ETA: 1:08:53 - loss: 0.6259 - regression_loss: 0.5075 - classification_loss: 0.1184
  957/10000 [=>............................] - ETA: 1:08:52 - loss: 0.6257 - regression_loss: 0.5072 - classification_loss: 0.1185
  958/10000 [=>............................] - ETA: 1:08:52 - loss: 0.6252 - regression_loss: 0.5068 - classification_loss: 0.1184
  959/10000 [=>............................] - ETA: 1:08:52 - loss: 0.6248 - regression_loss: 0.5065 - classification_loss: 0.1183
  960/10000 [=>............................] - ETA: 1:08:51 - loss: 0.6244 - regression_loss: 0.5061 - classification_loss: 0.1183
  961/10000 [=>............................] - ETA: 1:08:51 - loss: 0.6241 - regression_loss: 0.5058 - classification_loss: 0.1182
  962/10000 [=>............................] - ETA: 1:08:51 - loss: 0.6243 - regression_loss: 0.5061 - classification_loss: 0.1182
  963/10000 [=>............................] - ETA: 1:08:50 - loss: 0.6238 - regression_loss: 0.5056 - classification_loss: 0.1182
  964/10000 [=>............................] - ETA: 1:08:49 - loss: 0.6245 - regression_loss: 0.5061 - classification_loss: 0.1183
  965/10000 [=>............................] - ETA: 1:08:49 - loss: 0.6240 - regression_loss: 0.5058 - classification_loss: 0.1182
  966/10000 [=>............................] - ETA: 1:08:48 - loss: 0.6237 - regression_loss: 0.5055 - classification_loss: 0.1182
  967/10000 [=>............................] - ETA: 1:08:48 - loss: 0.6234 - regression_loss: 0.5052 - classification_loss: 0.1182
  968/10000 [=>............................] - ETA: 1:08:47 - loss: 0.6240 - regression_loss: 0.5058 - classification_loss: 0.1182
  969/10000 [=>............................] - ETA: 1:08:47 - loss: 0.6242 - regression_loss: 0.5059 - classification_loss: 0.1183
  970/10000 [=>............................] - ETA: 1:08:46 - loss: 0.6238 - regression_loss: 0.5056 - classification_loss: 0.1183
  971/10000 [=>............................] - ETA: 1:08:46 - loss: 0.6236 - regression_loss: 0.5054 - classification_loss: 0.1182
  972/10000 [=>............................] - ETA: 1:08:45 - loss: 0.6237 - regression_loss: 0.5055 - classification_loss: 0.1182
  973/10000 [=>............................] - ETA: 1:08:44 - loss: 0.6234 - regression_loss: 0.5054 - classification_loss: 0.1181
  974/10000 [=>............................] - ETA: 1:08:44 - loss: 0.6231 - regression_loss: 0.5051 - classification_loss: 0.1180
  975/10000 [=>............................] - ETA: 1:08:44 - loss: 0.6228 - regression_loss: 0.5049 - classification_loss: 0.1180
  976/10000 [=>............................] - ETA: 1:08:43 - loss: 0.6229 - regression_loss: 0.5049 - classification_loss: 0.1180
  977/10000 [=>............................] - ETA: 1:08:43 - loss: 0.6227 - regression_loss: 0.5047 - classification_loss: 0.1180
  978/10000 [=>............................] - ETA: 1:08:42 - loss: 0.6229 - regression_loss: 0.5049 - classification_loss: 0.1180
  979/10000 [=>............................] - ETA: 1:08:41 - loss: 0.6233 - regression_loss: 0.5050 - classification_loss: 0.1183
  980/10000 [=>............................] - ETA: 1:08:41 - loss: 0.6237 - regression_loss: 0.5053 - classification_loss: 0.1184
  981/10000 [=>............................] - ETA: 1:08:40 - loss: 0.6232 - regression_loss: 0.5050 - classification_loss: 0.1183
  982/10000 [=>............................] - ETA: 1:08:40 - loss: 0.6233 - regression_loss: 0.5050 - classification_loss: 0.1183
  983/10000 [=>............................] - ETA: 1:08:39 - loss: 0.6240 - regression_loss: 0.5056 - classification_loss: 0.1184
  984/10000 [=>............................] - ETA: 1:08:39 - loss: 0.6243 - regression_loss: 0.5058 - classification_loss: 0.1186
  985/10000 [=>............................] - ETA: 1:08:39 - loss: 0.6242 - regression_loss: 0.5056 - classification_loss: 0.1186
  986/10000 [=>............................] - ETA: 1:08:38 - loss: 0.6242 - regression_loss: 0.5055 - classification_loss: 0.1187
  987/10000 [=>............................] - ETA: 1:08:38 - loss: 0.6243 - regression_loss: 0.5056 - classification_loss: 0.1188
  988/10000 [=>............................] - ETA: 1:08:37 - loss: 0.6245 - regression_loss: 0.5055 - classification_loss: 0.1189
  989/10000 [=>............................] - ETA: 1:08:37 - loss: 0.6245 - regression_loss: 0.5056 - classification_loss: 0.1190
  990/10000 [=>............................] - ETA: 1:08:36 - loss: 0.6241 - regression_loss: 0.5051 - classification_loss: 0.1189
  991/10000 [=>............................] - ETA: 1:08:35 - loss: 0.6236 - regression_loss: 0.5047 - classification_loss: 0.1189
  992/10000 [=>............................] - ETA: 1:08:35 - loss: 0.6241 - regression_loss: 0.5052 - classification_loss: 0.1189
  993/10000 [=>............................] - ETA: 1:08:34 - loss: 0.6238 - regression_loss: 0.5049 - classification_loss: 0.1189
  994/10000 [=>............................] - ETA: 1:08:34 - loss: 0.6244 - regression_loss: 0.5055 - classification_loss: 0.1190
  995/10000 [=>............................] - ETA: 1:08:34 - loss: 0.6239 - regression_loss: 0.5051 - classification_loss: 0.1189
  996/10000 [=>............................] - ETA: 1:08:33 - loss: 0.6245 - regression_loss: 0.5055 - classification_loss: 0.1189
  997/10000 [=>............................] - ETA: 1:08:33 - loss: 0.6246 - regression_loss: 0.5056 - classification_loss: 0.1190
  998/10000 [=>............................] - ETA: 1:08:32 - loss: 0.6249 - regression_loss: 0.5059 - classification_loss: 0.1190
  999/10000 [=>............................] - ETA: 1:08:32 - loss: 0.6248 - regression_loss: 0.5057 - classification_loss: 0.1190
 1000/10000 [==>...........................] - ETA: 1:08:31 - loss: 0.6244 - regression_loss: 0.5055 - classification_loss: 0.1189
 1001/10000 [==>...........................] - ETA: 1:08:31 - loss: 0.6251 - regression_loss: 0.5061 - classification_loss: 0.1190
 1002/10000 [==>...........................] - ETA: 1:08:30 - loss: 0.6252 - regression_loss: 0.5062 - classification_loss: 0.1190
 1003/10000 [==>...........................] - ETA: 1:08:30 - loss: 0.6257 - regression_loss: 0.5067 - classification_loss: 0.1190
 1004/10000 [==>...........................] - ETA: 1:08:30 - loss: 0.6257 - regression_loss: 0.5066 - classification_loss: 0.1190
 1005/10000 [==>...........................] - ETA: 1:08:29 - loss: 0.6253 - regression_loss: 0.5064 - classification_loss: 0.1190
 1006/10000 [==>...........................] - ETA: 1:08:29 - loss: 0.6252 - regression_loss: 0.5063 - classification_loss: 0.1190
 1007/10000 [==>...........................] - ETA: 1:08:28 - loss: 0.6249 - regression_loss: 0.5060 - classification_loss: 0.1189
 1008/10000 [==>...........................] - ETA: 1:08:28 - loss: 0.6250 - regression_loss: 0.5060 - classification_loss: 0.1190
 1009/10000 [==>...........................] - ETA: 1:08:27 - loss: 0.6256 - regression_loss: 0.5066 - classification_loss: 0.1190
 1010/10000 [==>...........................] - ETA: 1:08:27 - loss: 0.6267 - regression_loss: 0.5074 - classification_loss: 0.1193
 1011/10000 [==>...........................] - ETA: 1:08:27 - loss: 0.6269 - regression_loss: 0.5074 - classification_loss: 0.1195
 1012/10000 [==>...........................] - ETA: 1:08:26 - loss: 0.6267 - regression_loss: 0.5072 - classification_loss: 0.1195
 1013/10000 [==>...........................] - ETA: 1:08:26 - loss: 0.6264 - regression_loss: 0.5069 - classification_loss: 0.1195
 1014/10000 [==>...........................] - ETA: 1:08:26 - loss: 0.6267 - regression_loss: 0.5072 - classification_loss: 0.1195
 1015/10000 [==>...........................] - ETA: 1:08:25 - loss: 0.6269 - regression_loss: 0.5073 - classification_loss: 0.1196
 1016/10000 [==>...........................] - ETA: 1:08:25 - loss: 0.6270 - regression_loss: 0.5074 - classification_loss: 0.1196
 1017/10000 [==>...........................] - ETA: 1:08:24 - loss: 0.6268 - regression_loss: 0.5073 - classification_loss: 0.1195
 1018/10000 [==>...........................] - ETA: 1:08:24 - loss: 0.6266 - regression_loss: 0.5072 - classification_loss: 0.1194
 1019/10000 [==>...........................] - ETA: 1:08:24 - loss: 0.6268 - regression_loss: 0.5074 - classification_loss: 0.1194
 1020/10000 [==>...........................] - ETA: 1:08:24 - loss: 0.6271 - regression_loss: 0.5076 - classification_loss: 0.1195
 1021/10000 [==>...........................] - ETA: 1:08:23 - loss: 0.6274 - regression_loss: 0.5076 - classification_loss: 0.1198
 1022/10000 [==>...........................] - ETA: 1:08:23 - loss: 0.6276 - regression_loss: 0.5079 - classification_loss: 0.1198
 1023/10000 [==>...........................] - ETA: 1:08:22 - loss: 0.6276 - regression_loss: 0.5078 - classification_loss: 0.1198
 1024/10000 [==>...........................] - ETA: 1:08:22 - loss: 0.6279 - regression_loss: 0.5080 - classification_loss: 0.1199
 1025/10000 [==>...........................] - ETA: 1:08:20 - loss: 0.6276 - regression_loss: 0.5077 - classification_loss: 0.1198
 1026/10000 [==>...........................] - ETA: 1:08:19 - loss: 0.6277 - regression_loss: 0.5078 - classification_loss: 0.1198
 1027/10000 [==>...........................] - ETA: 1:08:19 - loss: 0.6274 - regression_loss: 0.5076 - classification_loss: 0.1198
 1028/10000 [==>...........................] - ETA: 1:08:18 - loss: 0.6274 - regression_loss: 0.5076 - classification_loss: 0.1198
 1029/10000 [==>...........................] - ETA: 1:08:18 - loss: 0.6276 - regression_loss: 0.5078 - classification_loss: 0.1198
 1030/10000 [==>...........................] - ETA: 1:08:18 - loss: 0.6275 - regression_loss: 0.5077 - classification_loss: 0.1198
 1031/10000 [==>...........................] - ETA: 1:08:17 - loss: 0.6275 - regression_loss: 0.5076 - classification_loss: 0.1198
 1032/10000 [==>...........................] - ETA: 1:08:17 - loss: 0.6279 - regression_loss: 0.5079 - classification_loss: 0.1200
 1033/10000 [==>...........................] - ETA: 1:08:16 - loss: 0.6284 - regression_loss: 0.5083 - classification_loss: 0.1201
 1034/10000 [==>...........................] - ETA: 1:08:16 - loss: 0.6282 - regression_loss: 0.5081 - classification_loss: 0.1201
 1035/10000 [==>...........................] - ETA: 1:08:15 - loss: 0.6280 - regression_loss: 0.5079 - classification_loss: 0.1201
 1036/10000 [==>...........................] - ETA: 1:08:15 - loss: 0.6278 - regression_loss: 0.5078 - classification_loss: 0.1200
 1037/10000 [==>...........................] - ETA: 1:08:15 - loss: 0.6282 - regression_loss: 0.5082 - classification_loss: 0.1200
 1038/10000 [==>...........................] - ETA: 1:08:14 - loss: 0.6280 - regression_loss: 0.5081 - classification_loss: 0.1199
 1039/10000 [==>...........................] - ETA: 1:08:14 - loss: 0.6281 - regression_loss: 0.5082 - classification_loss: 0.1199
 1040/10000 [==>...........................] - ETA: 1:08:13 - loss: 0.6279 - regression_loss: 0.5081 - classification_loss: 0.1199
 1041/10000 [==>...........................] - ETA: 1:08:13 - loss: 0.6277 - regression_loss: 0.5078 - classification_loss: 0.1199
 1042/10000 [==>...........................] - ETA: 1:08:13 - loss: 0.6275 - regression_loss: 0.5076 - classification_loss: 0.1199
 1043/10000 [==>...........................] - ETA: 1:08:12 - loss: 0.6271 - regression_loss: 0.5074 - classification_loss: 0.1198
 1044/10000 [==>...........................] - ETA: 1:08:12 - loss: 0.6269 - regression_loss: 0.5070 - classification_loss: 0.1198
 1045/10000 [==>...........................] - ETA: 1:08:11 - loss: 0.6271 - regression_loss: 0.5071 - classification_loss: 0.1199
 1046/10000 [==>...........................] - ETA: 1:08:11 - loss: 0.6268 - regression_loss: 0.5069 - classification_loss: 0.1199
 1047/10000 [==>...........................] - ETA: 1:08:11 - loss: 0.6268 - regression_loss: 0.5068 - classification_loss: 0.1201
 1048/10000 [==>...........................] - ETA: 1:08:10 - loss: 0.6266 - regression_loss: 0.5065 - classification_loss: 0.1200
 1049/10000 [==>...........................] - ETA: 1:08:10 - loss: 0.6263 - regression_loss: 0.5063 - classification_loss: 0.1199
 1050/10000 [==>...........................] - ETA: 1:08:08 - loss: 0.6264 - regression_loss: 0.5063 - classification_loss: 0.1201
 1051/10000 [==>...........................] - ETA: 1:08:08 - loss: 0.6268 - regression_loss: 0.5066 - classification_loss: 0.1202
 1052/10000 [==>...........................] - ETA: 1:08:07 - loss: 0.6268 - regression_loss: 0.5066 - classification_loss: 0.1202
 1053/10000 [==>...........................] - ETA: 1:08:07 - loss: 0.6266 - regression_loss: 0.5065 - classification_loss: 0.1202
 1054/10000 [==>...........................] - ETA: 1:08:06 - loss: 0.6263 - regression_loss: 0.5062 - classification_loss: 0.1201
 1055/10000 [==>...........................] - ETA: 1:08:06 - loss: 0.6269 - regression_loss: 0.5068 - classification_loss: 0.1201
 1056/10000 [==>...........................] - ETA: 1:08:06 - loss: 0.6269 - regression_loss: 0.5068 - classification_loss: 0.1201
 1057/10000 [==>...........................] - ETA: 1:08:06 - loss: 0.6265 - regression_loss: 0.5065 - classification_loss: 0.1200
 1058/10000 [==>...........................] - ETA: 1:08:05 - loss: 0.6263 - regression_loss: 0.5064 - classification_loss: 0.1199
 1059/10000 [==>...........................] - ETA: 1:08:05 - loss: 0.6260 - regression_loss: 0.5061 - classification_loss: 0.1199
 1060/10000 [==>...........................] - ETA: 1:08:05 - loss: 0.6258 - regression_loss: 0.5059 - classification_loss: 0.1199
 1061/10000 [==>...........................] - ETA: 1:08:04 - loss: 0.6254 - regression_loss: 0.5057 - classification_loss: 0.1198
 1062/10000 [==>...........................] - ETA: 1:08:04 - loss: 0.6256 - regression_loss: 0.5059 - classification_loss: 0.1198
 1063/10000 [==>...........................] - ETA: 1:08:03 - loss: 0.6261 - regression_loss: 0.5063 - classification_loss: 0.1198
 1064/10000 [==>...........................] - ETA: 1:08:02 - loss: 0.6259 - regression_loss: 0.5062 - classification_loss: 0.1198
 1065/10000 [==>...........................] - ETA: 1:08:02 - loss: 0.6258 - regression_loss: 0.5059 - classification_loss: 0.1199
 1066/10000 [==>...........................] - ETA: 1:08:01 - loss: 0.6257 - regression_loss: 0.5058 - classification_loss: 0.1199
 1067/10000 [==>...........................] - ETA: 1:08:01 - loss: 0.6260 - regression_loss: 0.5061 - classification_loss: 0.1199
 1068/10000 [==>...........................] - ETA: 1:08:00 - loss: 0.6267 - regression_loss: 0.5067 - classification_loss: 0.1200
 1069/10000 [==>...........................] - ETA: 1:07:59 - loss: 0.6265 - regression_loss: 0.5064 - classification_loss: 0.1201
 1070/10000 [==>...........................] - ETA: 1:07:59 - loss: 0.6263 - regression_loss: 0.5063 - classification_loss: 0.1200
 1071/10000 [==>...........................] - ETA: 1:07:58 - loss: 0.6258 - regression_loss: 0.5060 - classification_loss: 0.1199
 1072/10000 [==>...........................] - ETA: 1:07:58 - loss: 0.6254 - regression_loss: 0.5056 - classification_loss: 0.1198
 1073/10000 [==>...........................] - ETA: 1:07:57 - loss: 0.6252 - regression_loss: 0.5054 - classification_loss: 0.1199
 1074/10000 [==>...........................] - ETA: 1:07:56 - loss: 0.6252 - regression_loss: 0.5054 - classification_loss: 0.1198
 1075/10000 [==>...........................] - ETA: 1:07:55 - loss: 0.6256 - regression_loss: 0.5058 - classification_loss: 0.1198
 1076/10000 [==>...........................] - ETA: 1:07:55 - loss: 0.6254 - regression_loss: 0.5057 - classification_loss: 0.1198
 1077/10000 [==>...........................] - ETA: 1:07:54 - loss: 0.6253 - regression_loss: 0.5056 - classification_loss: 0.1197
 1078/10000 [==>...........................] - ETA: 1:07:54 - loss: 0.6255 - regression_loss: 0.5057 - classification_loss: 0.1197
 1079/10000 [==>...........................] - ETA: 1:07:54 - loss: 0.6257 - regression_loss: 0.5060 - classification_loss: 0.1197
 1080/10000 [==>...........................] - ETA: 1:07:53 - loss: 0.6268 - regression_loss: 0.5069 - classification_loss: 0.1199
 1081/10000 [==>...........................] - ETA: 1:07:53 - loss: 0.6271 - regression_loss: 0.5072 - classification_loss: 0.1199
 1082/10000 [==>...........................] - ETA: 1:07:52 - loss: 0.6270 - regression_loss: 0.5072 - classification_loss: 0.1198
 1083/10000 [==>...........................] - ETA: 1:07:51 - loss: 0.6269 - regression_loss: 0.5071 - classification_loss: 0.1198
 1084/10000 [==>...........................] - ETA: 1:07:51 - loss: 0.6267 - regression_loss: 0.5070 - classification_loss: 0.1197
 1085/10000 [==>...........................] - ETA: 1:07:51 - loss: 0.6274 - regression_loss: 0.5075 - classification_loss: 0.1198
 1086/10000 [==>...........................] - ETA: 1:07:51 - loss: 0.6282 - regression_loss: 0.5082 - classification_loss: 0.1200
 1087/10000 [==>...........................] - ETA: 1:07:50 - loss: 0.6280 - regression_loss: 0.5079 - classification_loss: 0.1200
 1088/10000 [==>...........................] - ETA: 1:07:49 - loss: 0.6280 - regression_loss: 0.5080 - classification_loss: 0.1200
 1089/10000 [==>...........................] - ETA: 1:07:49 - loss: 0.6282 - regression_loss: 0.5083 - classification_loss: 0.1200
 1090/10000 [==>...........................] - ETA: 1:07:49 - loss: 0.6282 - regression_loss: 0.5083 - classification_loss: 0.1199
 1091/10000 [==>...........................] - ETA: 1:07:48 - loss: 0.6278 - regression_loss: 0.5079 - classification_loss: 0.1198
 1092/10000 [==>...........................] - ETA: 1:07:48 - loss: 0.6274 - regression_loss: 0.5076 - classification_loss: 0.1198
 1093/10000 [==>...........................] - ETA: 1:07:47 - loss: 0.6275 - regression_loss: 0.5077 - classification_loss: 0.1198
 1094/10000 [==>...........................] - ETA: 1:07:47 - loss: 0.6272 - regression_loss: 0.5075 - classification_loss: 0.1197
 1095/10000 [==>...........................] - ETA: 1:07:47 - loss: 0.6273 - regression_loss: 0.5077 - classification_loss: 0.1197
 1096/10000 [==>...........................] - ETA: 1:07:46 - loss: 0.6272 - regression_loss: 0.5075 - classification_loss: 0.1196
 1097/10000 [==>...........................] - ETA: 1:07:46 - loss: 0.6270 - regression_loss: 0.5075 - classification_loss: 0.1195
 1098/10000 [==>...........................] - ETA: 1:07:45 - loss: 0.6266 - regression_loss: 0.5071 - classification_loss: 0.1195
 1099/10000 [==>...........................] - ETA: 1:07:45 - loss: 0.6273 - regression_loss: 0.5076 - classification_loss: 0.1197
 1100/10000 [==>...........................] - ETA: 1:07:44 - loss: 0.6270 - regression_loss: 0.5074 - classification_loss: 0.1196
 1101/10000 [==>...........................] - ETA: 1:07:44 - loss: 0.6270 - regression_loss: 0.5074 - classification_loss: 0.1196
 1102/10000 [==>...........................] - ETA: 1:07:43 - loss: 0.6266 - regression_loss: 0.5071 - classification_loss: 0.1195
 1103/10000 [==>...........................] - ETA: 1:07:43 - loss: 0.6265 - regression_loss: 0.5071 - classification_loss: 0.1195
 1104/10000 [==>...........................] - ETA: 1:07:43 - loss: 0.6267 - regression_loss: 0.5072 - classification_loss: 0.1194
 1105/10000 [==>...........................] - ETA: 1:07:42 - loss: 0.6270 - regression_loss: 0.5075 - classification_loss: 0.1194
 1106/10000 [==>...........................] - ETA: 1:07:42 - loss: 0.6268 - regression_loss: 0.5073 - classification_loss: 0.1195
 1107/10000 [==>...........................] - ETA: 1:07:41 - loss: 0.6263 - regression_loss: 0.5069 - classification_loss: 0.1194
 1108/10000 [==>...........................] - ETA: 1:07:41 - loss: 0.6265 - regression_loss: 0.5070 - classification_loss: 0.1194
 1109/10000 [==>...........................] - ETA: 1:07:40 - loss: 0.6260 - regression_loss: 0.5067 - classification_loss: 0.1193
 1110/10000 [==>...........................] - ETA: 1:07:40 - loss: 0.6256 - regression_loss: 0.5063 - classification_loss: 0.1193
 1111/10000 [==>...........................] - ETA: 1:07:39 - loss: 0.6255 - regression_loss: 0.5063 - classification_loss: 0.1192
 1112/10000 [==>...........................] - ETA: 1:07:38 - loss: 0.6252 - regression_loss: 0.5060 - classification_loss: 0.1192
 1113/10000 [==>...........................] - ETA: 1:07:38 - loss: 0.6252 - regression_loss: 0.5060 - classification_loss: 0.1192
 1114/10000 [==>...........................] - ETA: 1:07:38 - loss: 0.6248 - regression_loss: 0.5057 - classification_loss: 0.1191
 1115/10000 [==>...........................] - ETA: 1:07:37 - loss: 0.6244 - regression_loss: 0.5054 - classification_loss: 0.1191
 1116/10000 [==>...........................] - ETA: 1:07:37 - loss: 0.6242 - regression_loss: 0.5052 - classification_loss: 0.1190
 1117/10000 [==>...........................] - ETA: 1:07:36 - loss: 0.6244 - regression_loss: 0.5054 - classification_loss: 0.1190
 1118/10000 [==>...........................] - ETA: 1:07:36 - loss: 0.6248 - regression_loss: 0.5059 - classification_loss: 0.1189
 1119/10000 [==>...........................] - ETA: 1:07:35 - loss: 0.6245 - regression_loss: 0.5056 - classification_loss: 0.1189
 1120/10000 [==>...........................] - ETA: 1:07:35 - loss: 0.6244 - regression_loss: 0.5056 - classification_loss: 0.1188
 1121/10000 [==>...........................] - ETA: 1:07:35 - loss: 0.6243 - regression_loss: 0.5055 - classification_loss: 0.1188
 1122/10000 [==>...........................] - ETA: 1:07:35 - loss: 0.6245 - regression_loss: 0.5058 - classification_loss: 0.1187
 1123/10000 [==>...........................] - ETA: 1:07:34 - loss: 0.6254 - regression_loss: 0.5066 - classification_loss: 0.1188
 1124/10000 [==>...........................] - ETA: 1:07:33 - loss: 0.6253 - regression_loss: 0.5065 - classification_loss: 0.1188
 1125/10000 [==>...........................] - ETA: 1:07:33 - loss: 0.6253 - regression_loss: 0.5065 - classification_loss: 0.1188
 1126/10000 [==>...........................] - ETA: 1:07:32 - loss: 0.6258 - regression_loss: 0.5069 - classification_loss: 0.1189
 1127/10000 [==>...........................] - ETA: 1:07:32 - loss: 0.6266 - regression_loss: 0.5072 - classification_loss: 0.1194
 1128/10000 [==>...........................] - ETA: 1:07:32 - loss: 0.6266 - regression_loss: 0.5073 - classification_loss: 0.1193
 1129/10000 [==>...........................] - ETA: 1:07:31 - loss: 0.6266 - regression_loss: 0.5072 - classification_loss: 0.1194
 1130/10000 [==>...........................] - ETA: 1:07:31 - loss: 0.6271 - regression_loss: 0.5076 - classification_loss: 0.1195
 1131/10000 [==>...........................] - ETA: 1:07:30 - loss: 0.6270 - regression_loss: 0.5074 - classification_loss: 0.1196
 1132/10000 [==>...........................] - ETA: 1:07:29 - loss: 0.6274 - regression_loss: 0.5077 - classification_loss: 0.1197
 1133/10000 [==>...........................] - ETA: 1:07:29 - loss: 0.6271 - regression_loss: 0.5075 - classification_loss: 0.1196
 1134/10000 [==>...........................] - ETA: 1:07:29 - loss: 0.6273 - regression_loss: 0.5078 - classification_loss: 0.1196
 1135/10000 [==>...........................] - ETA: 1:07:28 - loss: 0.6273 - regression_loss: 0.5077 - classification_loss: 0.1196
 1136/10000 [==>...........................] - ETA: 1:07:28 - loss: 0.6273 - regression_loss: 0.5078 - classification_loss: 0.1195
 1137/10000 [==>...........................] - ETA: 1:07:27 - loss: 0.6270 - regression_loss: 0.5076 - classification_loss: 0.1195
 1138/10000 [==>...........................] - ETA: 1:07:27 - loss: 0.6268 - regression_loss: 0.5074 - classification_loss: 0.1194
 1139/10000 [==>...........................] - ETA: 1:07:26 - loss: 0.6266 - regression_loss: 0.5073 - classification_loss: 0.1193
 1140/10000 [==>...........................] - ETA: 1:07:26 - loss: 0.6263 - regression_loss: 0.5070 - classification_loss: 0.1193
 1141/10000 [==>...........................] - ETA: 1:07:26 - loss: 0.6268 - regression_loss: 0.5074 - classification_loss: 0.1194
 1142/10000 [==>...........................] - ETA: 1:07:25 - loss: 0.6266 - regression_loss: 0.5072 - classification_loss: 0.1194
 1143/10000 [==>...........................] - ETA: 1:07:25 - loss: 0.6265 - regression_loss: 0.5071 - classification_loss: 0.1194
 1144/10000 [==>...........................] - ETA: 1:07:24 - loss: 0.6264 - regression_loss: 0.5070 - classification_loss: 0.1194
 1145/10000 [==>...........................] - ETA: 1:07:24 - loss: 0.6263 - regression_loss: 0.5070 - classification_loss: 0.1193
 1146/10000 [==>...........................] - ETA: 1:07:24 - loss: 0.6264 - regression_loss: 0.5071 - classification_loss: 0.1193
 1147/10000 [==>...........................] - ETA: 1:07:23 - loss: 0.6261 - regression_loss: 0.5067 - classification_loss: 0.1194
 1148/10000 [==>...........................] - ETA: 1:07:23 - loss: 0.6265 - regression_loss: 0.5070 - classification_loss: 0.1195
 1149/10000 [==>...........................] - ETA: 1:07:22 - loss: 0.6263 - regression_loss: 0.5069 - classification_loss: 0.1194
 1150/10000 [==>...........................] - ETA: 1:07:22 - loss: 0.6264 - regression_loss: 0.5069 - classification_loss: 0.1194
 1151/10000 [==>...........................] - ETA: 1:07:21 - loss: 0.6265 - regression_loss: 0.5071 - classification_loss: 0.1194
 1152/10000 [==>...........................] - ETA: 1:07:21 - loss: 0.6262 - regression_loss: 0.5068 - classification_loss: 0.1193
 1153/10000 [==>...........................] - ETA: 1:07:21 - loss: 0.6259 - regression_loss: 0.5066 - classification_loss: 0.1193
 1154/10000 [==>...........................] - ETA: 1:07:20 - loss: 0.6254 - regression_loss: 0.5062 - classification_loss: 0.1192
 1155/10000 [==>...........................] - ETA: 1:07:20 - loss: 0.6251 - regression_loss: 0.5059 - classification_loss: 0.1192
 1156/10000 [==>...........................] - ETA: 1:07:20 - loss: 0.6255 - regression_loss: 0.5063 - classification_loss: 0.1192
 1157/10000 [==>...........................] - ETA: 1:07:19 - loss: 0.6254 - regression_loss: 0.5063 - classification_loss: 0.1191
 1158/10000 [==>...........................] - ETA: 1:07:19 - loss: 0.6259 - regression_loss: 0.5065 - classification_loss: 0.1194
 1159/10000 [==>...........................] - ETA: 1:07:18 - loss: 0.6258 - regression_loss: 0.5064 - classification_loss: 0.1194
 1160/10000 [==>...........................] - ETA: 1:07:18 - loss: 0.6267 - regression_loss: 0.5071 - classification_loss: 0.1197
 1161/10000 [==>...........................] - ETA: 1:07:17 - loss: 0.6273 - regression_loss: 0.5076 - classification_loss: 0.1197
 1162/10000 [==>...........................] - ETA: 1:07:17 - loss: 0.6270 - regression_loss: 0.5073 - classification_loss: 0.1197
 1163/10000 [==>...........................] - ETA: 1:07:16 - loss: 0.6269 - regression_loss: 0.5072 - classification_loss: 0.1197
 1164/10000 [==>...........................] - ETA: 1:07:16 - loss: 0.6265 - regression_loss: 0.5069 - classification_loss: 0.1196
 1165/10000 [==>...........................] - ETA: 1:07:15 - loss: 0.6263 - regression_loss: 0.5067 - classification_loss: 0.1196
 1166/10000 [==>...........................] - ETA: 1:07:15 - loss: 0.6262 - regression_loss: 0.5066 - classification_loss: 0.1196
 1167/10000 [==>...........................] - ETA: 1:07:15 - loss: 0.6264 - regression_loss: 0.5066 - classification_loss: 0.1198
 1168/10000 [==>...........................] - ETA: 1:07:14 - loss: 0.6263 - regression_loss: 0.5065 - classification_loss: 0.1197
 1169/10000 [==>...........................] - ETA: 1:07:14 - loss: 0.6261 - regression_loss: 0.5064 - classification_loss: 0.1197
 1170/10000 [==>...........................] - ETA: 1:07:14 - loss: 0.6261 - regression_loss: 0.5064 - classification_loss: 0.1197
 1171/10000 [==>...........................] - ETA: 1:07:13 - loss: 0.6265 - regression_loss: 0.5068 - classification_loss: 0.1197
 1172/10000 [==>...........................] - ETA: 1:07:13 - loss: 0.6264 - regression_loss: 0.5067 - classification_loss: 0.1197
 1173/10000 [==>...........................] - ETA: 1:07:12 - loss: 0.6267 - regression_loss: 0.5069 - classification_loss: 0.1198
 1174/10000 [==>...........................] - ETA: 1:07:12 - loss: 0.6264 - regression_loss: 0.5066 - classification_loss: 0.1198
 1175/10000 [==>...........................] - ETA: 1:07:11 - loss: 0.6262 - regression_loss: 0.5065 - classification_loss: 0.1198
 1176/10000 [==>...........................] - ETA: 1:07:11 - loss: 0.6262 - regression_loss: 0.5065 - classification_loss: 0.1197
 1177/10000 [==>...........................] - ETA: 1:07:11 - loss: 0.6262 - regression_loss: 0.5064 - classification_loss: 0.1197
 1178/10000 [==>...........................] - ETA: 1:07:10 - loss: 0.6266 - regression_loss: 0.5068 - classification_loss: 0.1198
 1179/10000 [==>...........................] - ETA: 1:07:10 - loss: 0.6267 - regression_loss: 0.5069 - classification_loss: 0.1197
 1180/10000 [==>...........................] - ETA: 1:07:09 - loss: 0.6265 - regression_loss: 0.5068 - classification_loss: 0.1198
 1181/10000 [==>...........................] - ETA: 1:07:09 - loss: 0.6262 - regression_loss: 0.5065 - classification_loss: 0.1197
 1182/10000 [==>...........................] - ETA: 1:07:08 - loss: 0.6258 - regression_loss: 0.5062 - classification_loss: 0.1197
 1183/10000 [==>...........................] - ETA: 1:07:08 - loss: 0.6264 - regression_loss: 0.5066 - classification_loss: 0.1198
 1184/10000 [==>...........................] - ETA: 1:07:07 - loss: 0.6262 - regression_loss: 0.5064 - classification_loss: 0.1198
 1185/10000 [==>...........................] - ETA: 1:07:07 - loss: 0.6263 - regression_loss: 0.5063 - classification_loss: 0.1200
 1186/10000 [==>...........................] - ETA: 1:07:06 - loss: 0.6260 - regression_loss: 0.5061 - classification_loss: 0.1199
 1187/10000 [==>...........................] - ETA: 1:07:06 - loss: 0.6257 - regression_loss: 0.5059 - classification_loss: 0.1198
 1188/10000 [==>...........................] - ETA: 1:07:06 - loss: 0.6256 - regression_loss: 0.5057 - classification_loss: 0.1200
 1189/10000 [==>...........................] - ETA: 1:07:04 - loss: 0.6255 - regression_loss: 0.5056 - classification_loss: 0.1199
 1190/10000 [==>...........................] - ETA: 1:07:04 - loss: 0.6257 - regression_loss: 0.5058 - classification_loss: 0.1199
 1191/10000 [==>...........................] - ETA: 1:07:04 - loss: 0.6262 - regression_loss: 0.5062 - classification_loss: 0.1200
 1192/10000 [==>...........................] - ETA: 1:07:03 - loss: 0.6264 - regression_loss: 0.5064 - classification_loss: 0.1201
 1193/10000 [==>...........................] - ETA: 1:07:03 - loss: 0.6262 - regression_loss: 0.5062 - classification_loss: 0.1201
 1194/10000 [==>...........................] - ETA: 1:07:03 - loss: 0.6259 - regression_loss: 0.5060 - classification_loss: 0.1200
 1195/10000 [==>...........................] - ETA: 1:07:02 - loss: 0.6256 - regression_loss: 0.5057 - classification_loss: 0.1199
 1196/10000 [==>...........................] - ETA: 1:07:02 - loss: 0.6252 - regression_loss: 0.5054 - classification_loss: 0.1199
 1197/10000 [==>...........................] - ETA: 1:07:01 - loss: 0.6249 - regression_loss: 0.5051 - classification_loss: 0.1198
 1198/10000 [==>...........................] - ETA: 1:07:01 - loss: 0.6249 - regression_loss: 0.5051 - classification_loss: 0.1198
 1199/10000 [==>...........................] - ETA: 1:07:00 - loss: 0.6247 - regression_loss: 0.5048 - classification_loss: 0.1198
 1200/10000 [==>...........................] - ETA: 1:07:00 - loss: 0.6267 - regression_loss: 0.5059 - classification_loss: 0.1208
 1201/10000 [==>...........................] - ETA: 1:06:59 - loss: 0.6267 - regression_loss: 0.5058 - classification_loss: 0.1209
 1202/10000 [==>...........................] - ETA: 1:06:58 - loss: 0.6265 - regression_loss: 0.5056 - classification_loss: 0.1209
 1203/10000 [==>...........................] - ETA: 1:06:58 - loss: 0.6270 - regression_loss: 0.5061 - classification_loss: 0.1209
 1204/10000 [==>...........................] - ETA: 1:06:57 - loss: 0.6270 - regression_loss: 0.5061 - classification_loss: 0.1209
 1205/10000 [==>...........................] - ETA: 1:06:57 - loss: 0.6268 - regression_loss: 0.5059 - classification_loss: 0.1209
 1206/10000 [==>...........................] - ETA: 1:06:57 - loss: 0.6268 - regression_loss: 0.5059 - classification_loss: 0.1209
 1207/10000 [==>...........................] - ETA: 1:06:56 - loss: 0.6266 - regression_loss: 0.5057 - classification_loss: 0.1209
 1208/10000 [==>...........................] - ETA: 1:06:56 - loss: 0.6264 - regression_loss: 0.5056 - classification_loss: 0.1208
 1209/10000 [==>...........................] - ETA: 1:06:56 - loss: 0.6261 - regression_loss: 0.5054 - classification_loss: 0.1207
 1210/10000 [==>...........................] - ETA: 1:06:55 - loss: 0.6267 - regression_loss: 0.5060 - classification_loss: 0.1207
 1211/10000 [==>...........................] - ETA: 1:06:55 - loss: 0.6270 - regression_loss: 0.5063 - classification_loss: 0.1207
 1212/10000 [==>...........................] - ETA: 1:06:55 - loss: 0.6269 - regression_loss: 0.5062 - classification_loss: 0.1206
 1213/10000 [==>...........................] - ETA: 1:06:54 - loss: 0.6266 - regression_loss: 0.5060 - classification_loss: 0.1206
 1214/10000 [==>...........................] - ETA: 1:06:54 - loss: 0.6263 - regression_loss: 0.5057 - classification_loss: 0.1205
 1215/10000 [==>...........................] - ETA: 1:06:54 - loss: 0.6260 - regression_loss: 0.5055 - classification_loss: 0.1205
 1216/10000 [==>...........................] - ETA: 1:06:53 - loss: 0.6257 - regression_loss: 0.5053 - classification_loss: 0.1205
 1217/10000 [==>...........................] - ETA: 1:06:53 - loss: 0.6258 - regression_loss: 0.5052 - classification_loss: 0.1205
 1218/10000 [==>...........................] - ETA: 1:06:52 - loss: 0.6254 - regression_loss: 0.5050 - classification_loss: 0.1204
 1219/10000 [==>...........................] - ETA: 1:06:52 - loss: 0.6257 - regression_loss: 0.5052 - classification_loss: 0.1205
 1220/10000 [==>...........................] - ETA: 1:06:51 - loss: 0.6259 - regression_loss: 0.5054 - classification_loss: 0.1206
 1221/10000 [==>...........................] - ETA: 1:06:51 - loss: 0.6256 - regression_loss: 0.5051 - classification_loss: 0.1205
 1222/10000 [==>...........................] - ETA: 1:06:50 - loss: 0.6255 - regression_loss: 0.5050 - classification_loss: 0.1205
 1223/10000 [==>...........................] - ETA: 1:06:50 - loss: 0.6254 - regression_loss: 0.5049 - classification_loss: 0.1204
 1224/10000 [==>...........................] - ETA: 1:06:49 - loss: 0.6250 - regression_loss: 0.5047 - classification_loss: 0.1204
 1225/10000 [==>...........................] - ETA: 1:06:49 - loss: 0.6250 - regression_loss: 0.5046 - classification_loss: 0.1203
 1226/10000 [==>...........................] - ETA: 1:06:48 - loss: 0.6245 - regression_loss: 0.5043 - classification_loss: 0.1203
 1227/10000 [==>...........................] - ETA: 1:06:48 - loss: 0.6243 - regression_loss: 0.5040 - classification_loss: 0.1203
 1228/10000 [==>...........................] - ETA: 1:06:48 - loss: 0.6251 - regression_loss: 0.5047 - classification_loss: 0.1204
 1229/10000 [==>...........................] - ETA: 1:06:47 - loss: 0.6252 - regression_loss: 0.5048 - classification_loss: 0.1204
 1230/10000 [==>...........................] - ETA: 1:06:46 - loss: 0.6254 - regression_loss: 0.5049 - classification_loss: 0.1205
 1231/10000 [==>...........................] - ETA: 1:06:46 - loss: 0.6251 - regression_loss: 0.5047 - classification_loss: 0.1204
 1232/10000 [==>...........................] - ETA: 1:06:45 - loss: 0.6250 - regression_loss: 0.5046 - classification_loss: 0.1204
 1233/10000 [==>...........................] - ETA: 1:06:45 - loss: 0.6249 - regression_loss: 0.5045 - classification_loss: 0.1203
 1234/10000 [==>...........................] - ETA: 1:06:44 - loss: 0.6255 - regression_loss: 0.5050 - classification_loss: 0.1205
 1235/10000 [==>...........................] - ETA: 1:06:44 - loss: 0.6254 - regression_loss: 0.5050 - classification_loss: 0.1204
 1236/10000 [==>...........................] - ETA: 1:06:43 - loss: 0.6258 - regression_loss: 0.5054 - classification_loss: 0.1204
 1237/10000 [==>...........................] - ETA: 1:06:43 - loss: 0.6260 - regression_loss: 0.5055 - classification_loss: 0.1205
 1238/10000 [==>...........................] - ETA: 1:06:42 - loss: 0.6259 - regression_loss: 0.5055 - classification_loss: 0.1205
 1239/10000 [==>...........................] - ETA: 1:06:42 - loss: 0.6260 - regression_loss: 0.5055 - classification_loss: 0.1205
 1240/10000 [==>...........................] - ETA: 1:06:42 - loss: 0.6258 - regression_loss: 0.5053 - classification_loss: 0.1205
 1241/10000 [==>...........................] - ETA: 1:06:41 - loss: 0.6259 - regression_loss: 0.5055 - classification_loss: 0.1205
 1242/10000 [==>...........................] - ETA: 1:06:40 - loss: 0.6257 - regression_loss: 0.5053 - classification_loss: 0.1204
 1243/10000 [==>...........................] - ETA: 1:06:39 - loss: 0.6254 - regression_loss: 0.5050 - classification_loss: 0.1204
 1244/10000 [==>...........................] - ETA: 1:06:39 - loss: 0.6256 - regression_loss: 0.5052 - classification_loss: 0.1204
 1245/10000 [==>...........................] - ETA: 1:06:38 - loss: 0.6257 - regression_loss: 0.5053 - classification_loss: 0.1204
 1246/10000 [==>...........................] - ETA: 1:06:38 - loss: 0.6257 - regression_loss: 0.5053 - classification_loss: 0.1204
 1247/10000 [==>...........................] - ETA: 1:06:38 - loss: 0.6261 - regression_loss: 0.5057 - classification_loss: 0.1204
 1248/10000 [==>...........................] - ETA: 1:06:37 - loss: 0.6264 - regression_loss: 0.5059 - classification_loss: 0.1205
 1249/10000 [==>...........................] - ETA: 1:06:37 - loss: 0.6271 - regression_loss: 0.5065 - classification_loss: 0.1206
 1250/10000 [==>...........................] - ETA: 1:06:37 - loss: 0.6269 - regression_loss: 0.5063 - classification_loss: 0.1205
 1251/10000 [==>...........................] - ETA: 1:06:36 - loss: 0.6268 - regression_loss: 0.5063 - classification_loss: 0.1205
 1252/10000 [==>...........................] - ETA: 1:06:36 - loss: 0.6271 - regression_loss: 0.5066 - classification_loss: 0.1205
 1253/10000 [==>...........................] - ETA: 1:06:36 - loss: 0.6271 - regression_loss: 0.5066 - classification_loss: 0.1205
 1254/10000 [==>...........................] - ETA: 1:06:35 - loss: 0.6272 - regression_loss: 0.5067 - classification_loss: 0.1205
 1255/10000 [==>...........................] - ETA: 1:06:35 - loss: 0.6271 - regression_loss: 0.5066 - classification_loss: 0.1205
 1256/10000 [==>...........................] - ETA: 1:06:35 - loss: 0.6271 - regression_loss: 0.5066 - classification_loss: 0.1205
 1257/10000 [==>...........................] - ETA: 1:06:34 - loss: 0.6274 - regression_loss: 0.5069 - classification_loss: 0.1205
 1258/10000 [==>...........................] - ETA: 1:06:33 - loss: 0.6272 - regression_loss: 0.5066 - classification_loss: 0.1205
 1259/10000 [==>...........................] - ETA: 1:06:32 - loss: 0.6268 - regression_loss: 0.5064 - classification_loss: 0.1205
 1260/10000 [==>...........................] - ETA: 1:06:32 - loss: 0.6270 - regression_loss: 0.5065 - classification_loss: 0.1205
 1261/10000 [==>...........................] - ETA: 1:06:31 - loss: 0.6267 - regression_loss: 0.5062 - classification_loss: 0.1205
 1262/10000 [==>...........................] - ETA: 1:06:31 - loss: 0.6263 - regression_loss: 0.5059 - classification_loss: 0.1204
 1263/10000 [==>...........................] - ETA: 1:06:30 - loss: 0.6264 - regression_loss: 0.5060 - classification_loss: 0.1204
 1264/10000 [==>...........................] - ETA: 1:06:30 - loss: 0.6266 - regression_loss: 0.5062 - classification_loss: 0.1203
 1265/10000 [==>...........................] - ETA: 1:06:30 - loss: 0.6266 - regression_loss: 0.5063 - classification_loss: 0.1203
 1266/10000 [==>...........................] - ETA: 1:06:29 - loss: 0.6265 - regression_loss: 0.5062 - classification_loss: 0.1203
 1267/10000 [==>...........................] - ETA: 1:06:29 - loss: 0.6263 - regression_loss: 0.5060 - classification_loss: 0.1203
 1268/10000 [==>...........................] - ETA: 1:06:29 - loss: 0.6272 - regression_loss: 0.5066 - classification_loss: 0.1205
 1269/10000 [==>...........................] - ETA: 1:06:28 - loss: 0.6281 - regression_loss: 0.5072 - classification_loss: 0.1209
 1270/10000 [==>...........................] - ETA: 1:06:28 - loss: 0.6280 - regression_loss: 0.5071 - classification_loss: 0.1209
 1271/10000 [==>...........................] - ETA: 1:06:27 - loss: 0.6276 - regression_loss: 0.5068 - classification_loss: 0.1208
 1272/10000 [==>...........................] - ETA: 1:06:26 - loss: 0.6279 - regression_loss: 0.5071 - classification_loss: 0.1208
 1273/10000 [==>...........................] - ETA: 1:06:26 - loss: 0.6277 - regression_loss: 0.5069 - classification_loss: 0.1208
 1274/10000 [==>...........................] - ETA: 1:06:25 - loss: 0.6279 - regression_loss: 0.5070 - classification_loss: 0.1209
 1275/10000 [==>...........................] - ETA: 1:06:25 - loss: 0.6281 - regression_loss: 0.5072 - classification_loss: 0.1209
 1276/10000 [==>...........................] - ETA: 1:06:24 - loss: 0.6280 - regression_loss: 0.5071 - classification_loss: 0.1209
 1277/10000 [==>...........................] - ETA: 1:06:24 - loss: 0.6281 - regression_loss: 0.5072 - classification_loss: 0.1209
 1278/10000 [==>...........................] - ETA: 1:06:23 - loss: 0.6287 - regression_loss: 0.5076 - classification_loss: 0.1211
 1279/10000 [==>...........................] - ETA: 1:06:23 - loss: 0.6287 - regression_loss: 0.5076 - classification_loss: 0.1210
 1280/10000 [==>...........................] - ETA: 1:06:23 - loss: 0.6285 - regression_loss: 0.5075 - classification_loss: 0.1210
 1281/10000 [==>...........................] - ETA: 1:06:22 - loss: 0.6282 - regression_loss: 0.5072 - classification_loss: 0.1209
 1282/10000 [==>...........................] - ETA: 1:06:22 - loss: 0.6282 - regression_loss: 0.5073 - classification_loss: 0.1210
 1283/10000 [==>...........................] - ETA: 1:06:22 - loss: 0.6280 - regression_loss: 0.5071 - classification_loss: 0.1209
 1284/10000 [==>...........................] - ETA: 1:06:21 - loss: 0.6279 - regression_loss: 0.5070 - classification_loss: 0.1209
 1285/10000 [==>...........................] - ETA: 1:06:21 - loss: 0.6278 - regression_loss: 0.5070 - classification_loss: 0.1209
 1286/10000 [==>...........................] - ETA: 1:06:21 - loss: 0.6277 - regression_loss: 0.5068 - classification_loss: 0.1209
 1287/10000 [==>...........................] - ETA: 1:06:20 - loss: 0.6277 - regression_loss: 0.5069 - classification_loss: 0.1209
 1288/10000 [==>...........................] - ETA: 1:06:20 - loss: 0.6275 - regression_loss: 0.5066 - classification_loss: 0.1208
 1289/10000 [==>...........................] - ETA: 1:06:19 - loss: 0.6272 - regression_loss: 0.5064 - classification_loss: 0.1208
 1290/10000 [==>...........................] - ETA: 1:06:19 - loss: 0.6272 - regression_loss: 0.5065 - classification_loss: 0.1207
 1291/10000 [==>...........................] - ETA: 1:06:18 - loss: 0.6273 - regression_loss: 0.5066 - classification_loss: 0.1208
 1292/10000 [==>...........................] - ETA: 1:06:17 - loss: 0.6273 - regression_loss: 0.5065 - classification_loss: 0.1208
 1293/10000 [==>...........................] - ETA: 1:06:17 - loss: 0.6274 - regression_loss: 0.5066 - classification_loss: 0.1208
 1294/10000 [==>...........................] - ETA: 1:06:17 - loss: 0.6270 - regression_loss: 0.5063 - classification_loss: 0.1207
 1295/10000 [==>...........................] - ETA: 1:06:16 - loss: 0.6272 - regression_loss: 0.5065 - classification_loss: 0.1207
 1296/10000 [==>...........................] - ETA: 1:06:16 - loss: 0.6270 - regression_loss: 0.5063 - classification_loss: 0.1207
 1297/10000 [==>...........................] - ETA: 1:06:16 - loss: 0.6268 - regression_loss: 0.5062 - classification_loss: 0.1207
 1298/10000 [==>...........................] - ETA: 1:06:15 - loss: 0.6272 - regression_loss: 0.5065 - classification_loss: 0.1207
 1299/10000 [==>...........................] - ETA: 1:06:15 - loss: 0.6273 - regression_loss: 0.5065 - classification_loss: 0.1208
 1300/10000 [==>...........................] - ETA: 1:06:14 - loss: 0.6275 - regression_loss: 0.5068 - classification_loss: 0.1208
 1301/10000 [==>...........................] - ETA: 1:06:14 - loss: 0.6273 - regression_loss: 0.5066 - classification_loss: 0.1207
 1302/10000 [==>...........................] - ETA: 1:06:14 - loss: 0.6280 - regression_loss: 0.5071 - classification_loss: 0.1209
 1303/10000 [==>...........................] - ETA: 1:06:13 - loss: 0.6281 - regression_loss: 0.5070 - classification_loss: 0.1211
 1304/10000 [==>...........................] - ETA: 1:06:13 - loss: 0.6278 - regression_loss: 0.5068 - classification_loss: 0.1210
 1305/10000 [==>...........................] - ETA: 1:06:13 - loss: 0.6278 - regression_loss: 0.5067 - classification_loss: 0.1211
 1306/10000 [==>...........................] - ETA: 1:06:12 - loss: 0.6281 - regression_loss: 0.5070 - classification_loss: 0.1211
 1307/10000 [==>...........................] - ETA: 1:06:11 - loss: 0.6288 - regression_loss: 0.5076 - classification_loss: 0.1212
 1308/10000 [==>...........................] - ETA: 1:06:11 - loss: 0.6291 - regression_loss: 0.5079 - classification_loss: 0.1212
 1309/10000 [==>...........................] - ETA: 1:06:10 - loss: 0.6292 - regression_loss: 0.5080 - classification_loss: 0.1212
 1310/10000 [==>...........................] - ETA: 1:06:10 - loss: 0.6289 - regression_loss: 0.5078 - classification_loss: 0.1211
 1311/10000 [==>...........................] - ETA: 1:06:09 - loss: 0.6286 - regression_loss: 0.5075 - classification_loss: 0.1211
 1312/10000 [==>...........................] - ETA: 1:06:09 - loss: 0.6285 - regression_loss: 0.5075 - classification_loss: 0.1210
 1313/10000 [==>...........................] - ETA: 1:06:08 - loss: 0.6283 - regression_loss: 0.5074 - classification_loss: 0.1210
 1314/10000 [==>...........................] - ETA: 1:06:07 - loss: 0.6281 - regression_loss: 0.5072 - classification_loss: 0.1209
 1315/10000 [==>...........................] - ETA: 1:06:07 - loss: 0.6281 - regression_loss: 0.5072 - classification_loss: 0.1209
 1316/10000 [==>...........................] - ETA: 1:06:07 - loss: 0.6281 - regression_loss: 0.5072 - classification_loss: 0.1209
 1317/10000 [==>...........................] - ETA: 1:06:06 - loss: 0.6279 - regression_loss: 0.5070 - classification_loss: 0.1209
 1318/10000 [==>...........................] - ETA: 1:06:06 - loss: 0.6278 - regression_loss: 0.5070 - classification_loss: 0.1208
 1319/10000 [==>...........................] - ETA: 1:06:05 - loss: 0.6278 - regression_loss: 0.5071 - classification_loss: 0.1208
 1320/10000 [==>...........................] - ETA: 1:06:05 - loss: 0.6279 - regression_loss: 0.5071 - classification_loss: 0.1208
 1321/10000 [==>...........................] - ETA: 1:06:04 - loss: 0.6279 - regression_loss: 0.5071 - classification_loss: 0.1208
 1322/10000 [==>...........................] - ETA: 1:06:04 - loss: 0.6278 - regression_loss: 0.5070 - classification_loss: 0.1208
 1323/10000 [==>...........................] - ETA: 1:06:04 - loss: 0.6276 - regression_loss: 0.5068 - classification_loss: 0.1207
 1324/10000 [==>...........................] - ETA: 1:06:04 - loss: 0.6278 - regression_loss: 0.5071 - classification_loss: 0.1207
 1325/10000 [==>...........................] - ETA: 1:06:04 - loss: 0.6278 - regression_loss: 0.5071 - classification_loss: 0.1207
 1326/10000 [==>...........................] - ETA: 1:06:03 - loss: 0.6274 - regression_loss: 0.5068 - classification_loss: 0.1206
 1327/10000 [==>...........................] - ETA: 1:06:02 - loss: 0.6272 - regression_loss: 0.5066 - classification_loss: 0.1206
 1328/10000 [==>...........................] - ETA: 1:06:02 - loss: 0.6274 - regression_loss: 0.5068 - classification_loss: 0.1207
 1329/10000 [==>...........................] - ETA: 1:06:01 - loss: 0.6278 - regression_loss: 0.5070 - classification_loss: 0.1208
 1330/10000 [==>...........................] - ETA: 1:06:01 - loss: 0.6278 - regression_loss: 0.5070 - classification_loss: 0.1208
 1331/10000 [==>...........................] - ETA: 1:06:00 - loss: 0.6277 - regression_loss: 0.5069 - classification_loss: 0.1208
 1332/10000 [==>...........................] - ETA: 1:06:00 - loss: 0.6275 - regression_loss: 0.5067 - classification_loss: 0.1208
 1333/10000 [==>...........................] - ETA: 1:06:00 - loss: 0.6274 - regression_loss: 0.5066 - classification_loss: 0.1208
 1334/10000 [===>..........................] - ETA: 1:05:59 - loss: 0.6277 - regression_loss: 0.5069 - classification_loss: 0.1208
 1335/10000 [===>..........................] - ETA: 1:05:59 - loss: 0.6283 - regression_loss: 0.5074 - classification_loss: 0.1209
 1336/10000 [===>..........................] - ETA: 1:05:58 - loss: 0.6285 - regression_loss: 0.5075 - classification_loss: 0.1211
 1337/10000 [===>..........................] - ETA: 1:05:58 - loss: 0.6288 - regression_loss: 0.5077 - classification_loss: 0.1212
 1338/10000 [===>..........................] - ETA: 1:05:57 - loss: 0.6296 - regression_loss: 0.5083 - classification_loss: 0.1213
 1339/10000 [===>..........................] - ETA: 1:05:57 - loss: 0.6297 - regression_loss: 0.5084 - classification_loss: 0.1213
 1340/10000 [===>..........................] - ETA: 1:05:56 - loss: 0.6298 - regression_loss: 0.5085 - classification_loss: 0.1213
 1341/10000 [===>..........................] - ETA: 1:05:56 - loss: 0.6297 - regression_loss: 0.5084 - classification_loss: 0.1212
 1342/10000 [===>..........................] - ETA: 1:05:55 - loss: 0.6294 - regression_loss: 0.5083 - classification_loss: 0.1212
 1343/10000 [===>..........................] - ETA: 1:05:55 - loss: 0.6291 - regression_loss: 0.5080 - classification_loss: 0.1211
 1344/10000 [===>..........................] - ETA: 1:05:55 - loss: 0.6288 - regression_loss: 0.5078 - classification_loss: 0.1211
 1345/10000 [===>..........................] - ETA: 1:05:54 - loss: 0.6285 - regression_loss: 0.5075 - classification_loss: 0.1210
 1346/10000 [===>..........................] - ETA: 1:05:54 - loss: 0.6282 - regression_loss: 0.5072 - classification_loss: 0.1210
 1347/10000 [===>..........................] - ETA: 1:05:53 - loss: 0.6281 - regression_loss: 0.5072 - classification_loss: 0.1209
 1348/10000 [===>..........................] - ETA: 1:05:53 - loss: 0.6279 - regression_loss: 0.5070 - classification_loss: 0.1209
 1349/10000 [===>..........................] - ETA: 1:05:52 - loss: 0.6276 - regression_loss: 0.5067 - classification_loss: 0.1208
 1350/10000 [===>..........................] - ETA: 1:05:52 - loss: 0.6279 - regression_loss: 0.5068 - classification_loss: 0.1211
 1351/10000 [===>..........................] - ETA: 1:05:52 - loss: 0.6277 - regression_loss: 0.5067 - classification_loss: 0.1210
 1352/10000 [===>..........................] - ETA: 1:05:51 - loss: 0.6276 - regression_loss: 0.5065 - classification_loss: 0.1210
 1353/10000 [===>..........................] - ETA: 1:05:51 - loss: 0.6280 - regression_loss: 0.5068 - classification_loss: 0.1212
 1354/10000 [===>..........................] - ETA: 1:05:50 - loss: 0.6280 - regression_loss: 0.5067 - classification_loss: 0.1212
 1355/10000 [===>..........................] - ETA: 1:05:50 - loss: 0.6280 - regression_loss: 0.5068 - classification_loss: 0.1212
 1356/10000 [===>..........................] - ETA: 1:05:49 - loss: 0.6281 - regression_loss: 0.5068 - classification_loss: 0.1212
 1357/10000 [===>..........................] - ETA: 1:05:49 - loss: 0.6278 - regression_loss: 0.5066 - classification_loss: 0.1212
 1358/10000 [===>..........................] - ETA: 1:05:48 - loss: 0.6284 - regression_loss: 0.5072 - classification_loss: 0.1212
 1359/10000 [===>..........................] - ETA: 1:05:48 - loss: 0.6284 - regression_loss: 0.5071 - classification_loss: 0.1213
 1360/10000 [===>..........................] - ETA: 1:05:47 - loss: 0.6285 - regression_loss: 0.5072 - classification_loss: 0.1213
 1361/10000 [===>..........................] - ETA: 1:05:47 - loss: 0.6285 - regression_loss: 0.5071 - classification_loss: 0.1214
 1362/10000 [===>..........................] - ETA: 1:05:46 - loss: 0.6282 - regression_loss: 0.5069 - classification_loss: 0.1213
 1363/10000 [===>..........................] - ETA: 1:05:45 - loss: 0.6284 - regression_loss: 0.5069 - classification_loss: 0.1215
 1364/10000 [===>..........................] - ETA: 1:05:45 - loss: 0.6289 - regression_loss: 0.5073 - classification_loss: 0.1216
 1365/10000 [===>..........................] - ETA: 1:05:45 - loss: 0.6287 - regression_loss: 0.5071 - classification_loss: 0.1215
 1366/10000 [===>..........................] - ETA: 1:05:44 - loss: 0.6284 - regression_loss: 0.5069 - classification_loss: 0.1215
 1367/10000 [===>..........................] - ETA: 1:05:44 - loss: 0.6285 - regression_loss: 0.5071 - classification_loss: 0.1214
 1368/10000 [===>..........................] - ETA: 1:05:43 - loss: 0.6285 - regression_loss: 0.5072 - classification_loss: 0.1214
 1369/10000 [===>..........................] - ETA: 1:05:42 - loss: 0.6282 - regression_loss: 0.5069 - classification_loss: 0.1213
 1370/10000 [===>..........................] - ETA: 1:05:42 - loss: 0.6280 - regression_loss: 0.5067 - classification_loss: 0.1213
 1371/10000 [===>..........................] - ETA: 1:05:42 - loss: 0.6280 - regression_loss: 0.5067 - classification_loss: 0.1213
 1372/10000 [===>..........................] - ETA: 1:05:41 - loss: 0.6279 - regression_loss: 0.5066 - classification_loss: 0.1213
 1373/10000 [===>..........................] - ETA: 1:05:41 - loss: 0.6279 - regression_loss: 0.5067 - classification_loss: 0.1213
 1374/10000 [===>..........................] - ETA: 1:05:40 - loss: 0.6277 - regression_loss: 0.5064 - classification_loss: 0.1213
 1375/10000 [===>..........................] - ETA: 1:05:40 - loss: 0.6275 - regression_loss: 0.5063 - classification_loss: 0.1213
 1376/10000 [===>..........................] - ETA: 1:05:40 - loss: 0.6278 - regression_loss: 0.5065 - classification_loss: 0.1214
 1377/10000 [===>..........................] - ETA: 1:05:39 - loss: 0.6279 - regression_loss: 0.5066 - classification_loss: 0.1213
 1378/10000 [===>..........................] - ETA: 1:05:39 - loss: 0.6286 - regression_loss: 0.5067 - classification_loss: 0.1220
 1379/10000 [===>..........................] - ETA: 1:05:38 - loss: 0.6288 - regression_loss: 0.5068 - classification_loss: 0.1220
 1380/10000 [===>..........................] - ETA: 1:05:38 - loss: 0.6292 - regression_loss: 0.5072 - classification_loss: 0.1220
 1381/10000 [===>..........................] - ETA: 1:05:38 - loss: 0.6293 - regression_loss: 0.5073 - classification_loss: 0.1220
 1382/10000 [===>..........................] - ETA: 1:05:37 - loss: 0.6291 - regression_loss: 0.5072 - classification_loss: 0.1219
 1383/10000 [===>..........................] - ETA: 1:05:37 - loss: 0.6290 - regression_loss: 0.5071 - classification_loss: 0.1219
 1384/10000 [===>..........................] - ETA: 1:05:37 - loss: 0.6288 - regression_loss: 0.5069 - classification_loss: 0.1219
 1385/10000 [===>..........................] - ETA: 1:05:36 - loss: 0.6285 - regression_loss: 0.5067 - classification_loss: 0.1218
 1386/10000 [===>..........................] - ETA: 1:05:36 - loss: 0.6284 - regression_loss: 0.5066 - classification_loss: 0.1218
 1387/10000 [===>..........................] - ETA: 1:05:35 - loss: 0.6283 - regression_loss: 0.5066 - classification_loss: 0.1218
 1388/10000 [===>..........................] - ETA: 1:05:35 - loss: 0.6284 - regression_loss: 0.5067 - classification_loss: 0.1217
 1389/10000 [===>..........................] - ETA: 1:05:35 - loss: 0.6281 - regression_loss: 0.5064 - classification_loss: 0.1217
 1390/10000 [===>..........................] - ETA: 1:05:34 - loss: 0.6288 - regression_loss: 0.5069 - classification_loss: 0.1218
 1391/10000 [===>..........................] - ETA: 1:05:34 - loss: 0.6287 - regression_loss: 0.5068 - classification_loss: 0.1219
 1392/10000 [===>..........................] - ETA: 1:05:33 - loss: 0.6284 - regression_loss: 0.5066 - classification_loss: 0.1218
 1393/10000 [===>..........................] - ETA: 1:05:33 - loss: 0.6280 - regression_loss: 0.5063 - classification_loss: 0.1217
 1394/10000 [===>..........................] - ETA: 1:05:32 - loss: 0.6279 - regression_loss: 0.5062 - classification_loss: 0.1217
 1395/10000 [===>..........................] - ETA: 1:05:32 - loss: 0.6285 - regression_loss: 0.5067 - classification_loss: 0.1218
 1396/10000 [===>..........................] - ETA: 1:05:32 - loss: 0.6286 - regression_loss: 0.5068 - classification_loss: 0.1218
 1397/10000 [===>..........................] - ETA: 1:05:31 - loss: 0.6285 - regression_loss: 0.5067 - classification_loss: 0.1218
 1398/10000 [===>..........................] - ETA: 1:05:31 - loss: 0.6289 - regression_loss: 0.5071 - classification_loss: 0.1219
 1399/10000 [===>..........................] - ETA: 1:05:30 - loss: 0.6288 - regression_loss: 0.5070 - classification_loss: 0.1219
 1400/10000 [===>..........................] - ETA: 1:05:30 - loss: 0.6287 - regression_loss: 0.5068 - classification_loss: 0.1219
 1401/10000 [===>..........................] - ETA: 1:05:29 - loss: 0.6284 - regression_loss: 0.5066 - classification_loss: 0.1218
 1402/10000 [===>..........................] - ETA: 1:05:29 - loss: 0.6287 - regression_loss: 0.5069 - classification_loss: 0.1219
 1403/10000 [===>..........................] - ETA: 1:05:28 - loss: 0.6284 - regression_loss: 0.5066 - classification_loss: 0.1218
 1404/10000 [===>..........................] - ETA: 1:05:28 - loss: 0.6283 - regression_loss: 0.5065 - classification_loss: 0.1218
 1405/10000 [===>..........................] - ETA: 1:05:28 - loss: 0.6281 - regression_loss: 0.5064 - classification_loss: 0.1217
 1406/10000 [===>..........................] - ETA: 1:05:27 - loss: 0.6279 - regression_loss: 0.5062 - classification_loss: 0.1217
 1407/10000 [===>..........................] - ETA: 1:05:27 - loss: 0.6284 - regression_loss: 0.5067 - classification_loss: 0.1218
 1408/10000 [===>..........................] - ETA: 1:05:26 - loss: 0.6282 - regression_loss: 0.5065 - classification_loss: 0.1218
 1409/10000 [===>..........................] - ETA: 1:05:26 - loss: 0.6279 - regression_loss: 0.5062 - classification_loss: 0.1217
 1410/10000 [===>..........................] - ETA: 1:05:26 - loss: 0.6279 - regression_loss: 0.5063 - classification_loss: 0.1217
 1411/10000 [===>..........................] - ETA: 1:05:25 - loss: 0.6276 - regression_loss: 0.5060 - classification_loss: 0.1216
 1412/10000 [===>..........................] - ETA: 1:05:25 - loss: 0.6273 - regression_loss: 0.5057 - classification_loss: 0.1215
 1413/10000 [===>..........................] - ETA: 1:05:25 - loss: 0.6273 - regression_loss: 0.5058 - classification_loss: 0.1215
 1414/10000 [===>..........................] - ETA: 1:05:24 - loss: 0.6279 - regression_loss: 0.5063 - classification_loss: 0.1216
 1415/10000 [===>..........................] - ETA: 1:05:23 - loss: 0.6282 - regression_loss: 0.5065 - classification_loss: 0.1217
 1416/10000 [===>..........................] - ETA: 1:05:23 - loss: 0.6282 - regression_loss: 0.5065 - classification_loss: 0.1217
 1417/10000 [===>..........................] - ETA: 1:05:22 - loss: 0.6281 - regression_loss: 0.5065 - classification_loss: 0.1216
 1418/10000 [===>..........................] - ETA: 1:05:22 - loss: 0.6281 - regression_loss: 0.5065 - classification_loss: 0.1216
 1419/10000 [===>..........................] - ETA: 1:05:22 - loss: 0.6279 - regression_loss: 0.5062 - classification_loss: 0.1216
 1420/10000 [===>..........................] - ETA: 1:05:21 - loss: 0.6279 - regression_loss: 0.5062 - classification_loss: 0.1216
 1421/10000 [===>..........................] - ETA: 1:05:21 - loss: 0.6280 - regression_loss: 0.5063 - classification_loss: 0.1217
 1422/10000 [===>..........................] - ETA: 1:05:20 - loss: 0.6283 - regression_loss: 0.5066 - classification_loss: 0.1217
 1423/10000 [===>..........................] - ETA: 1:05:20 - loss: 0.6283 - regression_loss: 0.5067 - classification_loss: 0.1217
 1424/10000 [===>..........................] - ETA: 1:05:19 - loss: 0.6288 - regression_loss: 0.5071 - classification_loss: 0.1218
 1425/10000 [===>..........................] - ETA: 1:05:19 - loss: 0.6285 - regression_loss: 0.5068 - classification_loss: 0.1217
 1426/10000 [===>..........................] - ETA: 1:05:18 - loss: 0.6283 - regression_loss: 0.5067 - classification_loss: 0.1216
 1427/10000 [===>..........................] - ETA: 1:05:18 - loss: 0.6284 - regression_loss: 0.5068 - classification_loss: 0.1216
 1428/10000 [===>..........................] - ETA: 1:05:17 - loss: 0.6286 - regression_loss: 0.5070 - classification_loss: 0.1216
 1429/10000 [===>..........................] - ETA: 1:05:17 - loss: 0.6284 - regression_loss: 0.5068 - classification_loss: 0.1216
 1430/10000 [===>..........................] - ETA: 1:05:16 - loss: 0.6288 - regression_loss: 0.5071 - classification_loss: 0.1216
 1431/10000 [===>..........................] - ETA: 1:05:16 - loss: 0.6286 - regression_loss: 0.5070 - classification_loss: 0.1216
 1432/10000 [===>..........................] - ETA: 1:05:15 - loss: 0.6283 - regression_loss: 0.5067 - classification_loss: 0.1215
 1433/10000 [===>..........................] - ETA: 1:05:15 - loss: 0.6287 - regression_loss: 0.5072 - classification_loss: 0.1215
 1434/10000 [===>..........................] - ETA: 1:05:14 - loss: 0.6285 - regression_loss: 0.5069 - classification_loss: 0.1216
 1435/10000 [===>..........................] - ETA: 1:05:14 - loss: 0.6283 - regression_loss: 0.5068 - classification_loss: 0.1215
 1436/10000 [===>..........................] - ETA: 1:05:13 - loss: 0.6283 - regression_loss: 0.5068 - classification_loss: 0.1215
 1437/10000 [===>..........................] - ETA: 1:05:13 - loss: 0.6285 - regression_loss: 0.5070 - classification_loss: 0.1215
 1438/10000 [===>..........................] - ETA: 1:05:12 - loss: 0.6283 - regression_loss: 0.5068 - classification_loss: 0.1214
 1439/10000 [===>..........................] - ETA: 1:05:12 - loss: 0.6285 - regression_loss: 0.5070 - classification_loss: 0.1215
 1440/10000 [===>..........................] - ETA: 1:05:11 - loss: 0.6286 - regression_loss: 0.5071 - classification_loss: 0.1215
 1441/10000 [===>..........................] - ETA: 1:05:11 - loss: 0.6284 - regression_loss: 0.5070 - classification_loss: 0.1215
 1442/10000 [===>..........................] - ETA: 1:05:10 - loss: 0.6285 - regression_loss: 0.5070 - classification_loss: 0.1214
 1443/10000 [===>..........................] - ETA: 1:05:10 - loss: 0.6284 - regression_loss: 0.5069 - classification_loss: 0.1214
 1444/10000 [===>..........................] - ETA: 1:05:10 - loss: 0.6287 - regression_loss: 0.5072 - classification_loss: 0.1215
 1445/10000 [===>..........................] - ETA: 1:05:09 - loss: 0.6285 - regression_loss: 0.5071 - classification_loss: 0.1214
 1446/10000 [===>..........................] - ETA: 1:05:09 - loss: 0.6282 - regression_loss: 0.5068 - classification_loss: 0.1214
 1447/10000 [===>..........................] - ETA: 1:05:08 - loss: 0.6279 - regression_loss: 0.5066 - classification_loss: 0.1213
 1448/10000 [===>..........................] - ETA: 1:05:08 - loss: 0.6280 - regression_loss: 0.5067 - classification_loss: 0.1213
 1449/10000 [===>..........................] - ETA: 1:05:07 - loss: 0.6281 - regression_loss: 0.5067 - classification_loss: 0.1213
 1450/10000 [===>..........................] - ETA: 1:05:07 - loss: 0.6278 - regression_loss: 0.5066 - classification_loss: 0.1213
 1451/10000 [===>..........................] - ETA: 1:05:07 - loss: 0.6281 - regression_loss: 0.5068 - classification_loss: 0.1213
 1452/10000 [===>..........................] - ETA: 1:05:06 - loss: 0.6283 - regression_loss: 0.5070 - classification_loss: 0.1213
 1453/10000 [===>..........................] - ETA: 1:05:06 - loss: 0.6285 - regression_loss: 0.5072 - classification_loss: 0.1213
 1454/10000 [===>..........................] - ETA: 1:05:05 - loss: 0.6286 - regression_loss: 0.5072 - classification_loss: 0.1214
 1455/10000 [===>..........................] - ETA: 1:05:05 - loss: 0.6286 - regression_loss: 0.5073 - classification_loss: 0.1213
 1456/10000 [===>..........................] - ETA: 1:05:04 - loss: 0.6288 - regression_loss: 0.5074 - classification_loss: 0.1214
 1457/10000 [===>..........................] - ETA: 1:05:04 - loss: 0.6287 - regression_loss: 0.5073 - classification_loss: 0.1213
 1458/10000 [===>..........................] - ETA: 1:05:03 - loss: 0.6284 - regression_loss: 0.5072 - classification_loss: 0.1213
 1459/10000 [===>..........................] - ETA: 1:05:03 - loss: 0.6285 - regression_loss: 0.5072 - classification_loss: 0.1213
 1460/10000 [===>..........................] - ETA: 1:05:03 - loss: 0.6287 - regression_loss: 0.5074 - classification_loss: 0.1213
 1461/10000 [===>..........................] - ETA: 1:05:02 - loss: 0.6291 - regression_loss: 0.5078 - classification_loss: 0.1213
 1462/10000 [===>..........................] - ETA: 1:05:02 - loss: 0.6295 - regression_loss: 0.5081 - classification_loss: 0.1214
 1463/10000 [===>..........................] - ETA: 1:05:02 - loss: 0.6294 - regression_loss: 0.5080 - classification_loss: 0.1214
 1464/10000 [===>..........................] - ETA: 1:05:01 - loss: 0.6291 - regression_loss: 0.5078 - classification_loss: 0.1213
 1465/10000 [===>..........................] - ETA: 1:05:00 - loss: 0.6289 - regression_loss: 0.5076 - classification_loss: 0.1214
 1466/10000 [===>..........................] - ETA: 1:05:00 - loss: 0.6287 - regression_loss: 0.5074 - classification_loss: 0.1213
 1467/10000 [===>..........................] - ETA: 1:04:59 - loss: 0.6287 - regression_loss: 0.5073 - classification_loss: 0.1214
 1468/10000 [===>..........................] - ETA: 1:04:59 - loss: 0.6287 - regression_loss: 0.5073 - classification_loss: 0.1214
 1469/10000 [===>..........................] - ETA: 1:04:58 - loss: 0.6286 - regression_loss: 0.5073 - classification_loss: 0.1214
 1470/10000 [===>..........................] - ETA: 1:04:58 - loss: 0.6285 - regression_loss: 0.5071 - classification_loss: 0.1213
 1471/10000 [===>..........................] - ETA: 1:04:57 - loss: 0.6287 - regression_loss: 0.5073 - classification_loss: 0.1214
 1472/10000 [===>..........................] - ETA: 1:04:57 - loss: 0.6285 - regression_loss: 0.5071 - classification_loss: 0.1214
 1473/10000 [===>..........................] - ETA: 1:04:56 - loss: 0.6282 - regression_loss: 0.5069 - classification_loss: 0.1214
 1474/10000 [===>..........................] - ETA: 1:04:56 - loss: 0.6285 - regression_loss: 0.5071 - classification_loss: 0.1214
 1475/10000 [===>..........................] - ETA: 1:04:55 - loss: 0.6282 - regression_loss: 0.5068 - classification_loss: 0.1214
 1476/10000 [===>..........................] - ETA: 1:04:55 - loss: 0.6282 - regression_loss: 0.5068 - classification_loss: 0.1214
 1477/10000 [===>..........................] - ETA: 1:04:55 - loss: 0.6281 - regression_loss: 0.5068 - classification_loss: 0.1213
 1478/10000 [===>..........................] - ETA: 1:04:54 - loss: 0.6281 - regression_loss: 0.5068 - classification_loss: 0.1214
 1479/10000 [===>..........................] - ETA: 1:04:54 - loss: 0.6282 - regression_loss: 0.5068 - classification_loss: 0.1213
 1480/10000 [===>..........................] - ETA: 1:04:54 - loss: 0.6283 - regression_loss: 0.5069 - classification_loss: 0.1213
 1481/10000 [===>..........................] - ETA: 1:04:53 - loss: 0.6286 - regression_loss: 0.5072 - classification_loss: 0.1214
 1482/10000 [===>..........................] - ETA: 1:04:53 - loss: 0.6289 - regression_loss: 0.5074 - classification_loss: 0.1215
 1483/10000 [===>..........................] - ETA: 1:04:52 - loss: 0.6288 - regression_loss: 0.5073 - classification_loss: 0.1215
 1484/10000 [===>..........................] - ETA: 1:04:52 - loss: 0.6289 - regression_loss: 0.5074 - classification_loss: 0.1215
 1485/10000 [===>..........................] - ETA: 1:04:51 - loss: 0.6287 - regression_loss: 0.5072 - classification_loss: 0.1214
 1486/10000 [===>..........................] - ETA: 1:04:51 - loss: 0.6288 - regression_loss: 0.5073 - classification_loss: 0.1214
 1487/10000 [===>..........................] - ETA: 1:04:50 - loss: 0.6288 - regression_loss: 0.5073 - classification_loss: 0.1214
 1488/10000 [===>..........................] - ETA: 1:04:50 - loss: 0.6286 - regression_loss: 0.5072 - classification_loss: 0.1214
 1489/10000 [===>..........................] - ETA: 1:04:49 - loss: 0.6289 - regression_loss: 0.5075 - classification_loss: 0.1214
 1490/10000 [===>..........................] - ETA: 1:04:49 - loss: 0.6287 - regression_loss: 0.5073 - classification_loss: 0.1214
 1491/10000 [===>..........................] - ETA: 1:04:49 - loss: 0.6293 - regression_loss: 0.5078 - classification_loss: 0.1214
 1492/10000 [===>..........................] - ETA: 1:04:48 - loss: 0.6291 - regression_loss: 0.5077 - classification_loss: 0.1214
 1493/10000 [===>..........................] - ETA: 1:04:48 - loss: 0.6294 - regression_loss: 0.5081 - classification_loss: 0.1214
 1494/10000 [===>..........................] - ETA: 1:04:48 - loss: 0.6295 - regression_loss: 0.5081 - classification_loss: 0.1214
 1495/10000 [===>..........................] - ETA: 1:04:47 - loss: 0.6295 - regression_loss: 0.5081 - classification_loss: 0.1214
 1496/10000 [===>..........................] - ETA: 1:04:47 - loss: 0.6299 - regression_loss: 0.5084 - classification_loss: 0.1215
 1497/10000 [===>..........................] - ETA: 1:04:47 - loss: 0.6296 - regression_loss: 0.5082 - classification_loss: 0.1214
 1498/10000 [===>..........................] - ETA: 1:04:46 - loss: 0.6294 - regression_loss: 0.5080 - classification_loss: 0.1214
 1499/10000 [===>..........................] - ETA: 1:04:46 - loss: 0.6290 - regression_loss: 0.5077 - classification_loss: 0.1213
 1500/10000 [===>..........................] - ETA: 1:04:45 - loss: 0.6292 - regression_loss: 0.5079 - classification_loss: 0.1213
 1501/10000 [===>..........................] - ETA: 1:04:45 - loss: 0.6293 - regression_loss: 0.5080 - classification_loss: 0.1213
 1502/10000 [===>..........................] - ETA: 1:04:44 - loss: 0.6290 - regression_loss: 0.5078 - classification_loss: 0.1213
 1503/10000 [===>..........................] - ETA: 1:04:44 - loss: 0.6288 - regression_loss: 0.5076 - classification_loss: 0.1212
 1504/10000 [===>..........................] - ETA: 1:04:43 - loss: 0.6288 - regression_loss: 0.5076 - classification_loss: 0.1212
 1505/10000 [===>..........................] - ETA: 1:04:43 - loss: 0.6289 - regression_loss: 0.5077 - classification_loss: 0.1212
 1506/10000 [===>..........................] - ETA: 1:04:42 - loss: 0.6288 - regression_loss: 0.5077 - classification_loss: 0.1212
 1507/10000 [===>..........................] - ETA: 1:04:42 - loss: 0.6288 - regression_loss: 0.5076 - classification_loss: 0.1211
 1508/10000 [===>..........................] - ETA: 1:04:42 - loss: 0.6284 - regression_loss: 0.5073 - classification_loss: 0.1211
 1509/10000 [===>..........................] - ETA: 1:04:41 - loss: 0.6281 - regression_loss: 0.5071 - classification_loss: 0.1210
 1510/10000 [===>..........................] - ETA: 1:04:41 - loss: 0.6283 - regression_loss: 0.5071 - classification_loss: 0.1212
 1511/10000 [===>..........................] - ETA: 1:04:41 - loss: 0.6282 - regression_loss: 0.5071 - classification_loss: 0.1212
 1512/10000 [===>..........................] - ETA: 1:04:40 - loss: 0.6280 - regression_loss: 0.5069 - classification_loss: 0.1211
 1513/10000 [===>..........................] - ETA: 1:04:40 - loss: 0.6283 - regression_loss: 0.5071 - classification_loss: 0.1211
 1514/10000 [===>..........................] - ETA: 1:04:39 - loss: 0.6281 - regression_loss: 0.5070 - classification_loss: 0.1211
 1515/10000 [===>..........................] - ETA: 1:04:39 - loss: 0.6284 - regression_loss: 0.5073 - classification_loss: 0.1211
 1516/10000 [===>..........................] - ETA: 1:04:38 - loss: 0.6283 - regression_loss: 0.5072 - classification_loss: 0.1211
 1517/10000 [===>..........................] - ETA: 1:04:37 - loss: 0.6280 - regression_loss: 0.5069 - classification_loss: 0.1210
 1518/10000 [===>..........................] - ETA: 1:04:37 - loss: 0.6278 - regression_loss: 0.5068 - classification_loss: 0.1210
 1519/10000 [===>..........................] - ETA: 1:04:37 - loss: 0.6277 - regression_loss: 0.5067 - classification_loss: 0.1209
 1520/10000 [===>..........................] - ETA: 1:04:36 - loss: 0.6282 - regression_loss: 0.5071 - classification_loss: 0.1211
 1521/10000 [===>..........................] - ETA: 1:04:36 - loss: 0.6279 - regression_loss: 0.5069 - classification_loss: 0.1210
 1522/10000 [===>..........................] - ETA: 1:04:36 - loss: 0.6276 - regression_loss: 0.5066 - classification_loss: 0.1210
 1523/10000 [===>..........................] - ETA: 1:04:35 - loss: 0.6277 - regression_loss: 0.5068 - classification_loss: 0.1209
 1524/10000 [===>..........................] - ETA: 1:04:35 - loss: 0.6275 - regression_loss: 0.5067 - classification_loss: 0.1209
 1525/10000 [===>..........................] - ETA: 1:04:35 - loss: 0.6274 - regression_loss: 0.5065 - classification_loss: 0.1209
 1526/10000 [===>..........................] - ETA: 1:04:34 - loss: 0.6275 - regression_loss: 0.5066 - classification_loss: 0.1209
 1527/10000 [===>..........................] - ETA: 1:04:33 - loss: 0.6274 - regression_loss: 0.5066 - classification_loss: 0.1208
 1528/10000 [===>..........................] - ETA: 1:04:33 - loss: 0.6279 - regression_loss: 0.5070 - classification_loss: 0.1208
 1529/10000 [===>..........................] - ETA: 1:04:33 - loss: 0.6278 - regression_loss: 0.5070 - classification_loss: 0.1209
 1530/10000 [===>..........................] - ETA: 1:04:32 - loss: 0.6278 - regression_loss: 0.5069 - classification_loss: 0.1208
 1531/10000 [===>..........................] - ETA: 1:04:32 - loss: 0.6280 - regression_loss: 0.5071 - classification_loss: 0.1209
 1532/10000 [===>..........................] - ETA: 1:04:31 - loss: 0.6284 - regression_loss: 0.5076 - classification_loss: 0.1209
 1533/10000 [===>..........................] - ETA: 1:04:31 - loss: 0.6284 - regression_loss: 0.5076 - classification_loss: 0.1208
 1534/10000 [===>..........................] - ETA: 1:04:30 - loss: 0.6283 - regression_loss: 0.5074 - classification_loss: 0.1209
 1535/10000 [===>..........................] - ETA: 1:04:30 - loss: 0.6286 - regression_loss: 0.5076 - classification_loss: 0.1210
 1536/10000 [===>..........................] - ETA: 1:04:29 - loss: 0.6284 - regression_loss: 0.5075 - classification_loss: 0.1209
 1537/10000 [===>..........................] - ETA: 1:04:28 - loss: 0.6284 - regression_loss: 0.5075 - classification_loss: 0.1210
 1538/10000 [===>..........................] - ETA: 1:04:28 - loss: 0.6284 - regression_loss: 0.5074 - classification_loss: 0.1209
 1539/10000 [===>..........................] - ETA: 1:04:27 - loss: 0.6283 - regression_loss: 0.5073 - classification_loss: 0.1210
 1540/10000 [===>..........................] - ETA: 1:04:27 - loss: 0.6282 - regression_loss: 0.5073 - classification_loss: 0.1209
 1541/10000 [===>..........................] - ETA: 1:04:26 - loss: 0.6282 - regression_loss: 0.5073 - classification_loss: 0.1210
 1542/10000 [===>..........................] - ETA: 1:04:26 - loss: 0.6281 - regression_loss: 0.5072 - classification_loss: 0.1209
 1543/10000 [===>..........................] - ETA: 1:04:25 - loss: 0.6277 - regression_loss: 0.5069 - classification_loss: 0.1208
 1544/10000 [===>..........................] - ETA: 1:04:25 - loss: 0.6275 - regression_loss: 0.5067 - classification_loss: 0.1208
 1545/10000 [===>..........................] - ETA: 1:04:24 - loss: 0.6281 - regression_loss: 0.5072 - classification_loss: 0.1208
 1546/10000 [===>..........................] - ETA: 1:04:24 - loss: 0.6281 - regression_loss: 0.5072 - classification_loss: 0.1208
 1547/10000 [===>..........................] - ETA: 1:04:24 - loss: 0.6283 - regression_loss: 0.5075 - classification_loss: 0.1208
 1548/10000 [===>..........................] - ETA: 1:04:23 - loss: 0.6280 - regression_loss: 0.5073 - classification_loss: 0.1208
 1549/10000 [===>..........................] - ETA: 1:04:23 - loss: 0.6281 - regression_loss: 0.5073 - classification_loss: 0.1208
 1550/10000 [===>..........................] - ETA: 1:04:22 - loss: 0.6284 - regression_loss: 0.5076 - classification_loss: 0.1208
 1551/10000 [===>..........................] - ETA: 1:04:22 - loss: 0.6282 - regression_loss: 0.5074 - classification_loss: 0.1208
 1552/10000 [===>..........................] - ETA: 1:04:21 - loss: 0.6281 - regression_loss: 0.5073 - classification_loss: 0.1207
 1553/10000 [===>..........................] - ETA: 1:04:21 - loss: 0.6279 - regression_loss: 0.5072 - classification_loss: 0.1207
 1554/10000 [===>..........................] - ETA: 1:04:20 - loss: 0.6277 - regression_loss: 0.5071 - classification_loss: 0.1207
 1555/10000 [===>..........................] - ETA: 1:04:20 - loss: 0.6275 - regression_loss: 0.5068 - classification_loss: 0.1206
 1556/10000 [===>..........................] - ETA: 1:04:19 - loss: 0.6273 - regression_loss: 0.5067 - classification_loss: 0.1206
 1557/10000 [===>..........................] - ETA: 1:04:19 - loss: 0.6271 - regression_loss: 0.5065 - classification_loss: 0.1207
 1558/10000 [===>..........................] - ETA: 1:04:19 - loss: 0.6273 - regression_loss: 0.5066 - classification_loss: 0.1207
 1559/10000 [===>..........................] - ETA: 1:04:18 - loss: 0.6274 - regression_loss: 0.5067 - classification_loss: 0.1207
 1560/10000 [===>..........................] - ETA: 1:04:18 - loss: 0.6273 - regression_loss: 0.5066 - classification_loss: 0.1207
 1561/10000 [===>..........................] - ETA: 1:04:17 - loss: 0.6272 - regression_loss: 0.5064 - classification_loss: 0.1208
 1562/10000 [===>..........................] - ETA: 1:04:17 - loss: 0.6271 - regression_loss: 0.5063 - classification_loss: 0.1208
 1563/10000 [===>..........................] - ETA: 1:04:17 - loss: 0.6272 - regression_loss: 0.5064 - classification_loss: 0.1208
 1564/10000 [===>..........................] - ETA: 1:04:16 - loss: 0.6271 - regression_loss: 0.5064 - classification_loss: 0.1207
 1565/10000 [===>..........................] - ETA: 1:04:15 - loss: 0.6268 - regression_loss: 0.5062 - classification_loss: 0.1207
 1566/10000 [===>..........................] - ETA: 1:04:15 - loss: 0.6267 - regression_loss: 0.5060 - classification_loss: 0.1206
 1567/10000 [===>..........................] - ETA: 1:04:15 - loss: 0.6275 - regression_loss: 0.5067 - classification_loss: 0.1208
 1568/10000 [===>..........................] - ETA: 1:04:14 - loss: 0.6275 - regression_loss: 0.5067 - classification_loss: 0.1208
 1569/10000 [===>..........................] - ETA: 1:04:13 - loss: 0.6273 - regression_loss: 0.5066 - classification_loss: 0.1207
 1570/10000 [===>..........................] - ETA: 1:04:13 - loss: 0.6274 - regression_loss: 0.5067 - classification_loss: 0.1207
 1571/10000 [===>..........................] - ETA: 1:04:13 - loss: 0.6275 - regression_loss: 0.5069 - classification_loss: 0.1207
 1572/10000 [===>..........................] - ETA: 1:04:12 - loss: 0.6273 - regression_loss: 0.5066 - classification_loss: 0.1206
 1573/10000 [===>..........................] - ETA: 1:04:12 - loss: 0.6271 - regression_loss: 0.5065 - classification_loss: 0.1206
 1574/10000 [===>..........................] - ETA: 1:04:11 - loss: 0.6268 - regression_loss: 0.5063 - classification_loss: 0.1205
 1575/10000 [===>..........................] - ETA: 1:04:11 - loss: 0.6266 - regression_loss: 0.5062 - classification_loss: 0.1205
 1576/10000 [===>..........................] - ETA: 1:04:10 - loss: 0.6263 - regression_loss: 0.5059 - classification_loss: 0.1204
 1577/10000 [===>..........................] - ETA: 1:04:10 - loss: 0.6263 - regression_loss: 0.5058 - classification_loss: 0.1204
 1578/10000 [===>..........................] - ETA: 1:04:10 - loss: 0.6260 - regression_loss: 0.5056 - classification_loss: 0.1204
 1579/10000 [===>..........................] - ETA: 1:04:09 - loss: 0.6265 - regression_loss: 0.5061 - classification_loss: 0.1205
 1580/10000 [===>..........................] - ETA: 1:04:09 - loss: 0.6264 - regression_loss: 0.5059 - classification_loss: 0.1204
 1581/10000 [===>..........................] - ETA: 1:04:09 - loss: 0.6262 - regression_loss: 0.5057 - classification_loss: 0.1204
 1582/10000 [===>..........................] - ETA: 1:04:08 - loss: 0.6264 - regression_loss: 0.5060 - classification_loss: 0.1204
 1583/10000 [===>..........................] - ETA: 1:04:08 - loss: 0.6265 - regression_loss: 0.5061 - classification_loss: 0.1204
 1584/10000 [===>..........................] - ETA: 1:04:07 - loss: 0.6267 - regression_loss: 0.5063 - classification_loss: 0.1204
 1585/10000 [===>..........................] - ETA: 1:04:07 - loss: 0.6270 - regression_loss: 0.5065 - classification_loss: 0.1204
 1586/10000 [===>..........................] - ETA: 1:04:06 - loss: 0.6268 - regression_loss: 0.5064 - classification_loss: 0.1204
 1587/10000 [===>..........................] - ETA: 1:04:06 - loss: 0.6268 - regression_loss: 0.5065 - classification_loss: 0.1204
 1588/10000 [===>..........................] - ETA: 1:04:05 - loss: 0.6270 - regression_loss: 0.5066 - classification_loss: 0.1204
 1589/10000 [===>..........................] - ETA: 1:04:05 - loss: 0.6270 - regression_loss: 0.5066 - classification_loss: 0.1204
 1590/10000 [===>..........................] - ETA: 1:04:05 - loss: 0.6269 - regression_loss: 0.5065 - classification_loss: 0.1204
 1591/10000 [===>..........................] - ETA: 1:04:04 - loss: 0.6278 - regression_loss: 0.5072 - classification_loss: 0.1207
 1592/10000 [===>..........................] - ETA: 1:04:03 - loss: 0.6278 - regression_loss: 0.5071 - classification_loss: 0.1207
 1593/10000 [===>..........................] - ETA: 1:04:02 - loss: 0.6278 - regression_loss: 0.5070 - classification_loss: 0.1207
 1594/10000 [===>..........................] - ETA: 1:04:01 - loss: 0.6275 - regression_loss: 0.5068 - classification_loss: 0.1207
 1595/10000 [===>..........................] - ETA: 1:04:01 - loss: 0.6275 - regression_loss: 0.5068 - classification_loss: 0.1207
 1596/10000 [===>..........................] - ETA: 1:04:00 - loss: 0.6276 - regression_loss: 0.5068 - classification_loss: 0.1207
 1597/10000 [===>..........................] - ETA: 1:04:00 - loss: 0.6274 - regression_loss: 0.5067 - classification_loss: 0.1207
 1598/10000 [===>..........................] - ETA: 1:04:00 - loss: 0.6279 - regression_loss: 0.5072 - classification_loss: 0.1208
 1599/10000 [===>..........................] - ETA: 1:03:59 - loss: 0.6276 - regression_loss: 0.5069 - classification_loss: 0.1207
 1600/10000 [===>..........................] - ETA: 1:03:59 - loss: 0.6275 - regression_loss: 0.5068 - classification_loss: 0.1207
 1601/10000 [===>..........................] - ETA: 1:03:59 - loss: 0.6276 - regression_loss: 0.5069 - classification_loss: 0.1206
 1602/10000 [===>..........................] - ETA: 1:03:58 - loss: 0.6273 - regression_loss: 0.5067 - classification_loss: 0.1206
 1603/10000 [===>..........................] - ETA: 1:03:58 - loss: 0.6271 - regression_loss: 0.5066 - classification_loss: 0.1205
 1604/10000 [===>..........................] - ETA: 1:03:57 - loss: 0.6270 - regression_loss: 0.5065 - classification_loss: 0.1205
 1605/10000 [===>..........................] - ETA: 1:03:57 - loss: 0.6272 - regression_loss: 0.5067 - classification_loss: 0.1205
 1606/10000 [===>..........................] - ETA: 1:03:57 - loss: 0.6269 - regression_loss: 0.5064 - classification_loss: 0.1205
 1607/10000 [===>..........................] - ETA: 1:03:56 - loss: 0.6273 - regression_loss: 0.5067 - classification_loss: 0.1207
 1608/10000 [===>..........................] - ETA: 1:03:56 - loss: 0.6273 - regression_loss: 0.5067 - classification_loss: 0.1207
 1609/10000 [===>..........................] - ETA: 1:03:56 - loss: 0.6280 - regression_loss: 0.5071 - classification_loss: 0.1209
 1610/10000 [===>..........................] - ETA: 1:03:55 - loss: 0.6278 - regression_loss: 0.5070 - classification_loss: 0.1208
 1611/10000 [===>..........................] - ETA: 1:03:55 - loss: 0.6280 - regression_loss: 0.5071 - classification_loss: 0.1209
 1612/10000 [===>..........................] - ETA: 1:03:54 - loss: 0.6280 - regression_loss: 0.5072 - classification_loss: 0.1208
 1613/10000 [===>..........................] - ETA: 1:03:54 - loss: 0.6279 - regression_loss: 0.5070 - classification_loss: 0.1209
 1614/10000 [===>..........................] - ETA: 1:03:53 - loss: 0.6278 - regression_loss: 0.5069 - classification_loss: 0.1209
 1615/10000 [===>..........................] - ETA: 1:03:53 - loss: 0.6277 - regression_loss: 0.5069 - classification_loss: 0.1209
 1616/10000 [===>..........................] - ETA: 1:03:53 - loss: 0.6276 - regression_loss: 0.5068 - classification_loss: 0.1208
 1617/10000 [===>..........................] - ETA: 1:03:52 - loss: 0.6276 - regression_loss: 0.5067 - classification_loss: 0.1208
 1618/10000 [===>..........................] - ETA: 1:03:52 - loss: 0.6272 - regression_loss: 0.5064 - classification_loss: 0.1207
 1619/10000 [===>..........................] - ETA: 1:03:52 - loss: 0.6269 - regression_loss: 0.5062 - classification_loss: 0.1207
 1620/10000 [===>..........................] - ETA: 1:03:51 - loss: 0.6268 - regression_loss: 0.5062 - classification_loss: 0.1207
 1621/10000 [===>..........................] - ETA: 1:03:51 - loss: 0.6267 - regression_loss: 0.5060 - classification_loss: 0.1206
 1622/10000 [===>..........................] - ETA: 1:03:50 - loss: 0.6264 - regression_loss: 0.5058 - classification_loss: 0.1206
 1623/10000 [===>..........................] - ETA: 1:03:50 - loss: 0.6263 - regression_loss: 0.5057 - classification_loss: 0.1205
 1624/10000 [===>..........................] - ETA: 1:03:49 - loss: 0.6266 - regression_loss: 0.5059 - classification_loss: 0.1207
 1625/10000 [===>..........................] - ETA: 1:03:49 - loss: 0.6266 - regression_loss: 0.5059 - classification_loss: 0.1207
 1626/10000 [===>..........................] - ETA: 1:03:48 - loss: 0.6270 - regression_loss: 0.5062 - classification_loss: 0.1208
 1627/10000 [===>..........................] - ETA: 1:03:48 - loss: 0.6269 - regression_loss: 0.5061 - classification_loss: 0.1208
 1628/10000 [===>..........................] - ETA: 1:03:47 - loss: 0.6268 - regression_loss: 0.5061 - classification_loss: 0.1208
 1629/10000 [===>..........................] - ETA: 1:03:47 - loss: 0.6267 - regression_loss: 0.5060 - classification_loss: 0.1207
 1630/10000 [===>..........................] - ETA: 1:03:47 - loss: 0.6264 - regression_loss: 0.5057 - classification_loss: 0.1207
 1631/10000 [===>..........................] - ETA: 1:03:46 - loss: 0.6269 - regression_loss: 0.5060 - classification_loss: 0.1208
 1632/10000 [===>..........................] - ETA: 1:03:46 - loss: 0.6271 - regression_loss: 0.5062 - classification_loss: 0.1208
 1633/10000 [===>..........................] - ETA: 1:03:45 - loss: 0.6272 - regression_loss: 0.5064 - classification_loss: 0.1208
 1634/10000 [===>..........................] - ETA: 1:03:45 - loss: 0.6270 - regression_loss: 0.5062 - classification_loss: 0.1208
 1635/10000 [===>..........................] - ETA: 1:03:44 - loss: 0.6272 - regression_loss: 0.5062 - classification_loss: 0.1211
 1636/10000 [===>..........................] - ETA: 1:03:44 - loss: 0.6274 - regression_loss: 0.5063 - classification_loss: 0.1211
 1637/10000 [===>..........................] - ETA: 1:03:43 - loss: 0.6274 - regression_loss: 0.5063 - classification_loss: 0.1211
 1638/10000 [===>..........................] - ETA: 1:03:43 - loss: 0.6271 - regression_loss: 0.5061 - classification_loss: 0.1210
 1639/10000 [===>..........................] - ETA: 1:03:42 - loss: 0.6269 - regression_loss: 0.5059 - classification_loss: 0.1210
 1640/10000 [===>..........................] - ETA: 1:03:42 - loss: 0.6270 - regression_loss: 0.5059 - classification_loss: 0.1212
 1641/10000 [===>..........................] - ETA: 1:03:41 - loss: 0.6271 - regression_loss: 0.5059 - classification_loss: 0.1212
 1642/10000 [===>..........................] - ETA: 1:03:41 - loss: 0.6273 - regression_loss: 0.5060 - classification_loss: 0.1213
 1643/10000 [===>..........................] - ETA: 1:03:40 - loss: 0.6271 - regression_loss: 0.5059 - classification_loss: 0.1212
 1644/10000 [===>..........................] - ETA: 1:03:40 - loss: 0.6272 - regression_loss: 0.5059 - classification_loss: 0.1213
 1645/10000 [===>..........................] - ETA: 1:03:39 - loss: 0.6272 - regression_loss: 0.5059 - classification_loss: 0.1213
 1646/10000 [===>..........................] - ETA: 1:03:39 - loss: 0.6273 - regression_loss: 0.5059 - classification_loss: 0.1214
 1647/10000 [===>..........................] - ETA: 1:03:39 - loss: 0.6271 - regression_loss: 0.5057 - classification_loss: 0.1214
 1648/10000 [===>..........................] - ETA: 1:03:38 - loss: 0.6269 - regression_loss: 0.5056 - classification_loss: 0.1213
 1649/10000 [===>..........................] - ETA: 1:03:38 - loss: 0.6271 - regression_loss: 0.5057 - classification_loss: 0.1214
 1650/10000 [===>..........................] - ETA: 1:03:38 - loss: 0.6270 - regression_loss: 0.5057 - classification_loss: 0.1213
 1651/10000 [===>..........................] - ETA: 1:03:37 - loss: 0.6272 - regression_loss: 0.5059 - classification_loss: 0.1213
 1652/10000 [===>..........................] - ETA: 1:03:37 - loss: 0.6273 - regression_loss: 0.5060 - classification_loss: 0.1213
 1653/10000 [===>..........................] - ETA: 1:03:37 - loss: 0.6278 - regression_loss: 0.5063 - classification_loss: 0.1214
 1654/10000 [===>..........................] - ETA: 1:03:36 - loss: 0.6276 - regression_loss: 0.5062 - classification_loss: 0.1213
 1655/10000 [===>..........................] - ETA: 1:03:35 - loss: 0.6273 - regression_loss: 0.5060 - classification_loss: 0.1213
 1656/10000 [===>..........................] - ETA: 1:03:35 - loss: 0.6272 - regression_loss: 0.5059 - classification_loss: 0.1213
 1657/10000 [===>..........................] - ETA: 1:03:34 - loss: 0.6271 - regression_loss: 0.5058 - classification_loss: 0.1213
 1658/10000 [===>..........................] - ETA: 1:03:34 - loss: 0.6274 - regression_loss: 0.5061 - classification_loss: 0.1213
 1659/10000 [===>..........................] - ETA: 1:03:34 - loss: 0.6274 - regression_loss: 0.5060 - classification_loss: 0.1213
 1660/10000 [===>..........................] - ETA: 1:03:33 - loss: 0.6272 - regression_loss: 0.5060 - classification_loss: 0.1213
 1661/10000 [===>..........................] - ETA: 1:03:32 - loss: 0.6277 - regression_loss: 0.5059 - classification_loss: 0.1218
 1662/10000 [===>..........................] - ETA: 1:03:32 - loss: 0.6282 - regression_loss: 0.5064 - classification_loss: 0.1219
 1663/10000 [===>..........................] - ETA: 1:03:32 - loss: 0.6284 - regression_loss: 0.5065 - classification_loss: 0.1219
 1664/10000 [===>..........................] - ETA: 1:03:31 - loss: 0.6284 - regression_loss: 0.5065 - classification_loss: 0.1219
 1665/10000 [===>..........................] - ETA: 1:03:31 - loss: 0.6285 - regression_loss: 0.5066 - classification_loss: 0.1219
 1666/10000 [===>..........................] - ETA: 1:03:31 - loss: 0.6287 - regression_loss: 0.5067 - classification_loss: 0.1220
 1667/10000 [====>.........................] - ETA: 1:03:30 - loss: 0.6289 - regression_loss: 0.5069 - classification_loss: 0.1220
 1668/10000 [====>.........................] - ETA: 1:03:30 - loss: 0.6288 - regression_loss: 0.5069 - classification_loss: 0.1219
 1669/10000 [====>.........................] - ETA: 1:03:29 - loss: 0.6288 - regression_loss: 0.5068 - classification_loss: 0.1219
 1670/10000 [====>.........................] - ETA: 1:03:29 - loss: 0.6285 - regression_loss: 0.5066 - classification_loss: 0.1219
 1671/10000 [====>.........................] - ETA: 1:03:28 - loss: 0.6296 - regression_loss: 0.5074 - classification_loss: 0.1222
 1672/10000 [====>.........................] - ETA: 1:03:28 - loss: 0.6294 - regression_loss: 0.5073 - classification_loss: 0.1221
 1673/10000 [====>.........................] - ETA: 1:03:28 - loss: 0.6294 - regression_loss: 0.5073 - classification_loss: 0.1221
 1674/10000 [====>.........................] - ETA: 1:03:27 - loss: 0.6297 - regression_loss: 0.5075 - classification_loss: 0.1223
 1675/10000 [====>.........................] - ETA: 1:03:27 - loss: 0.6296 - regression_loss: 0.5073 - classification_loss: 0.1223
 1676/10000 [====>.........................] - ETA: 1:03:26 - loss: 0.6296 - regression_loss: 0.5073 - classification_loss: 0.1223
 1677/10000 [====>.........................] - ETA: 1:03:26 - loss: 0.6294 - regression_loss: 0.5072 - classification_loss: 0.1222
 1678/10000 [====>.........................] - ETA: 1:03:25 - loss: 0.6293 - regression_loss: 0.5071 - classification_loss: 0.1222
 1679/10000 [====>.........................] - ETA: 1:03:25 - loss: 0.6295 - regression_loss: 0.5073 - classification_loss: 0.1222
 1680/10000 [====>.........................] - ETA: 1:03:24 - loss: 0.6297 - regression_loss: 0.5074 - classification_loss: 0.1222
 1681/10000 [====>.........................] - ETA: 1:03:24 - loss: 0.6299 - regression_loss: 0.5076 - classification_loss: 0.1223
 1682/10000 [====>.........................] - ETA: 1:03:23 - loss: 0.6302 - regression_loss: 0.5079 - classification_loss: 0.1223
 1683/10000 [====>.........................] - ETA: 1:03:23 - loss: 0.6303 - regression_loss: 0.5080 - classification_loss: 0.1223
 1684/10000 [====>.........................] - ETA: 1:03:22 - loss: 0.6303 - regression_loss: 0.5080 - classification_loss: 0.1223
 1685/10000 [====>.........................] - ETA: 1:03:22 - loss: 0.6300 - regression_loss: 0.5078 - classification_loss: 0.1222
 1686/10000 [====>.........................] - ETA: 1:03:22 - loss: 0.6301 - regression_loss: 0.5079 - classification_loss: 0.1222
 1687/10000 [====>.........................] - ETA: 1:03:21 - loss: 0.6299 - regression_loss: 0.5077 - classification_loss: 0.1222
 1688/10000 [====>.........................] - ETA: 1:03:21 - loss: 0.6297 - regression_loss: 0.5075 - classification_loss: 0.1222
 1689/10000 [====>.........................] - ETA: 1:03:20 - loss: 0.6296 - regression_loss: 0.5075 - classification_loss: 0.1221
 1690/10000 [====>.........................] - ETA: 1:03:20 - loss: 0.6294 - regression_loss: 0.5073 - classification_loss: 0.1221
 1691/10000 [====>.........................] - ETA: 1:03:19 - loss: 0.6292 - regression_loss: 0.5072 - classification_loss: 0.1220
 1692/10000 [====>.........................] - ETA: 1:03:19 - loss: 0.6290 - regression_loss: 0.5070 - classification_loss: 0.1220
 1693/10000 [====>.........................] - ETA: 1:03:18 - loss: 0.6288 - regression_loss: 0.5068 - classification_loss: 0.1219
 1694/10000 [====>.........................] - ETA: 1:03:18 - loss: 0.6285 - regression_loss: 0.5066 - classification_loss: 0.1219
 1695/10000 [====>.........................] - ETA: 1:03:17 - loss: 0.6283 - regression_loss: 0.5064 - classification_loss: 0.1219
 1696/10000 [====>.........................] - ETA: 1:03:17 - loss: 0.6281 - regression_loss: 0.5062 - classification_loss: 0.1219
 1697/10000 [====>.........................] - ETA: 1:03:16 - loss: 0.6280 - regression_loss: 0.5062 - classification_loss: 0.1218
 1698/10000 [====>.........................] - ETA: 1:03:16 - loss: 0.6284 - regression_loss: 0.5065 - classification_loss: 0.1219
 1699/10000 [====>.........................] - ETA: 1:03:15 - loss: 0.6286 - regression_loss: 0.5067 - classification_loss: 0.1219
 1700/10000 [====>.........................] - ETA: 1:03:15 - loss: 0.6283 - regression_loss: 0.5065 - classification_loss: 0.1218
 1701/10000 [====>.........................] - ETA: 1:03:15 - loss: 0.6280 - regression_loss: 0.5062 - classification_loss: 0.1218
 1702/10000 [====>.........................] - ETA: 1:03:14 - loss: 0.6279 - regression_loss: 0.5062 - classification_loss: 0.1218
 1703/10000 [====>.........................] - ETA: 1:03:14 - loss: 0.6278 - regression_loss: 0.5061 - classification_loss: 0.1217
 1704/10000 [====>.........................] - ETA: 1:03:13 - loss: 0.6280 - regression_loss: 0.5063 - classification_loss: 0.1217
 1705/10000 [====>.........................] - ETA: 1:03:12 - loss: 0.6277 - regression_loss: 0.5060 - classification_loss: 0.1217
 1706/10000 [====>.........................] - ETA: 1:03:12 - loss: 0.6279 - regression_loss: 0.5063 - classification_loss: 0.1217
 1707/10000 [====>.........................] - ETA: 1:03:11 - loss: 0.6280 - regression_loss: 0.5064 - classification_loss: 0.1216
 1708/10000 [====>.........................] - ETA: 1:03:11 - loss: 0.6284 - regression_loss: 0.5067 - classification_loss: 0.1217
 1709/10000 [====>.........................] - ETA: 1:03:10 - loss: 0.6290 - regression_loss: 0.5072 - classification_loss: 0.1218
 1710/10000 [====>.........................] - ETA: 1:03:10 - loss: 0.6292 - regression_loss: 0.5073 - classification_loss: 0.1218
 1711/10000 [====>.........................] - ETA: 1:03:10 - loss: 0.6290 - regression_loss: 0.5072 - classification_loss: 0.1218
 1712/10000 [====>.........................] - ETA: 1:03:09 - loss: 0.6289 - regression_loss: 0.5072 - classification_loss: 0.1217
 1713/10000 [====>.........................] - ETA: 1:03:09 - loss: 0.6290 - regression_loss: 0.5073 - classification_loss: 0.1217
 1714/10000 [====>.........................] - ETA: 1:03:08 - loss: 0.6292 - regression_loss: 0.5075 - classification_loss: 0.1217
 1715/10000 [====>.........................] - ETA: 1:03:08 - loss: 0.6293 - regression_loss: 0.5075 - classification_loss: 0.1218
 1716/10000 [====>.........................] - ETA: 1:03:07 - loss: 0.6290 - regression_loss: 0.5073 - classification_loss: 0.1217
 1717/10000 [====>.........................] - ETA: 1:03:07 - loss: 0.6288 - regression_loss: 0.5071 - classification_loss: 0.1217
 1718/10000 [====>.........................] - ETA: 1:03:06 - loss: 0.6286 - regression_loss: 0.5070 - classification_loss: 0.1217
 1719/10000 [====>.........................] - ETA: 1:03:06 - loss: 0.6286 - regression_loss: 0.5069 - classification_loss: 0.1217
 1720/10000 [====>.........................] - ETA: 1:03:05 - loss: 0.6284 - regression_loss: 0.5068 - classification_loss: 0.1216
 1721/10000 [====>.........................] - ETA: 1:03:05 - loss: 0.6283 - regression_loss: 0.5067 - classification_loss: 0.1216
 1722/10000 [====>.........................] - ETA: 1:03:05 - loss: 0.6285 - regression_loss: 0.5069 - classification_loss: 0.1216
 1723/10000 [====>.........................] - ETA: 1:03:04 - loss: 0.6283 - regression_loss: 0.5067 - classification_loss: 0.1216
 1724/10000 [====>.........................] - ETA: 1:03:04 - loss: 0.6280 - regression_loss: 0.5065 - classification_loss: 0.1215
 1725/10000 [====>.........................] - ETA: 1:03:03 - loss: 0.6278 - regression_loss: 0.5063 - classification_loss: 0.1215
 1726/10000 [====>.........................] - ETA: 1:03:03 - loss: 0.6276 - regression_loss: 0.5062 - classification_loss: 0.1215
 1727/10000 [====>.........................] - ETA: 1:03:02 - loss: 0.6276 - regression_loss: 0.5061 - classification_loss: 0.1215
 1728/10000 [====>.........................] - ETA: 1:03:02 - loss: 0.6279 - regression_loss: 0.5064 - classification_loss: 0.1215
 1729/10000 [====>.........................] - ETA: 1:03:01 - loss: 0.6281 - regression_loss: 0.5066 - classification_loss: 0.1216
 1730/10000 [====>.........................] - ETA: 1:03:01 - loss: 0.6279 - regression_loss: 0.5064 - classification_loss: 0.1215
 1731/10000 [====>.........................] - ETA: 1:03:00 - loss: 0.6278 - regression_loss: 0.5063 - classification_loss: 0.1215
 1732/10000 [====>.........................] - ETA: 1:03:00 - loss: 0.6280 - regression_loss: 0.5065 - classification_loss: 0.1215
 1733/10000 [====>.........................] - ETA: 1:03:00 - loss: 0.6279 - regression_loss: 0.5063 - classification_loss: 0.1216
 1734/10000 [====>.........................] - ETA: 1:02:59 - loss: 0.6278 - regression_loss: 0.5063 - classification_loss: 0.1215
 1735/10000 [====>.........................] - ETA: 1:02:59 - loss: 0.6276 - regression_loss: 0.5062 - classification_loss: 0.1215
 1736/10000 [====>.........................] - ETA: 1:02:58 - loss: 0.6276 - regression_loss: 0.5061 - classification_loss: 0.1214
 1737/10000 [====>.........................] - ETA: 1:02:58 - loss: 0.6272 - regression_loss: 0.5058 - classification_loss: 0.1214
 1738/10000 [====>.........................] - ETA: 1:02:57 - loss: 0.6273 - regression_loss: 0.5059 - classification_loss: 0.1214
 1739/10000 [====>.........................] - ETA: 1:02:57 - loss: 0.6272 - regression_loss: 0.5059 - classification_loss: 0.1214
 1740/10000 [====>.........................] - ETA: 1:02:57 - loss: 0.6272 - regression_loss: 0.5058 - classification_loss: 0.1214
 1741/10000 [====>.........................] - ETA: 1:02:56 - loss: 0.6270 - regression_loss: 0.5057 - classification_loss: 0.1213
 1742/10000 [====>.........................] - ETA: 1:02:56 - loss: 0.6267 - regression_loss: 0.5054 - classification_loss: 0.1212
 1743/10000 [====>.........................] - ETA: 1:02:55 - loss: 0.6269 - regression_loss: 0.5056 - classification_loss: 0.1213
 1744/10000 [====>.........................] - ETA: 1:02:55 - loss: 0.6266 - regression_loss: 0.5055 - classification_loss: 0.1212
 1745/10000 [====>.........................] - ETA: 1:02:55 - loss: 0.6266 - regression_loss: 0.5054 - classification_loss: 0.1212
 1746/10000 [====>.........................] - ETA: 1:02:54 - loss: 0.6264 - regression_loss: 0.5053 - classification_loss: 0.1211
 1747/10000 [====>.........................] - ETA: 1:02:54 - loss: 0.6262 - regression_loss: 0.5051 - classification_loss: 0.1211
 1748/10000 [====>.........................] - ETA: 1:02:53 - loss: 0.6259 - regression_loss: 0.5049 - classification_loss: 0.1210
 1749/10000 [====>.........................] - ETA: 1:02:53 - loss: 0.6260 - regression_loss: 0.5050 - classification_loss: 0.1210
 1750/10000 [====>.........................] - ETA: 1:02:52 - loss: 0.6260 - regression_loss: 0.5050 - classification_loss: 0.1210
 1751/10000 [====>.........................] - ETA: 1:02:52 - loss: 0.6259 - regression_loss: 0.5049 - classification_loss: 0.1210
 1752/10000 [====>.........................] - ETA: 1:02:51 - loss: 0.6257 - regression_loss: 0.5048 - classification_loss: 0.1209
 1753/10000 [====>.........................] - ETA: 1:02:51 - loss: 0.6255 - regression_loss: 0.5046 - classification_loss: 0.1209
 1754/10000 [====>.........................] - ETA: 1:02:50 - loss: 0.6254 - regression_loss: 0.5045 - classification_loss: 0.1209
 1755/10000 [====>.........................] - ETA: 1:02:50 - loss: 0.6255 - regression_loss: 0.5045 - classification_loss: 0.1210
 1756/10000 [====>.........................] - ETA: 1:02:50 - loss: 0.6253 - regression_loss: 0.5044 - classification_loss: 0.1209
 1757/10000 [====>.........................] - ETA: 1:02:49 - loss: 0.6252 - regression_loss: 0.5043 - classification_loss: 0.1209
 1758/10000 [====>.........................] - ETA: 1:02:49 - loss: 0.6252 - regression_loss: 0.5042 - classification_loss: 0.1209
 1759/10000 [====>.........................] - ETA: 1:02:48 - loss: 0.6250 - regression_loss: 0.5042 - classification_loss: 0.1209
 1760/10000 [====>.........................] - ETA: 1:02:48 - loss: 0.6248 - regression_loss: 0.5040 - classification_loss: 0.1208
 1761/10000 [====>.........................] - ETA: 1:02:47 - loss: 0.6250 - regression_loss: 0.5041 - classification_loss: 0.1210
 1762/10000 [====>.........................] - ETA: 1:02:47 - loss: 0.6251 - regression_loss: 0.5041 - classification_loss: 0.1210
 1763/10000 [====>.........................] - ETA: 1:02:46 - loss: 0.6248 - regression_loss: 0.5038 - classification_loss: 0.1210
 1764/10000 [====>.........................] - ETA: 1:02:46 - loss: 0.6246 - regression_loss: 0.5037 - classification_loss: 0.1209
 1765/10000 [====>.........................] - ETA: 1:02:45 - loss: 0.6247 - regression_loss: 0.5038 - classification_loss: 0.1209
 1766/10000 [====>.........................] - ETA: 1:02:45 - loss: 0.6251 - regression_loss: 0.5041 - classification_loss: 0.1210
 1767/10000 [====>.........................] - ETA: 1:02:44 - loss: 0.6253 - regression_loss: 0.5042 - classification_loss: 0.1211
 1768/10000 [====>.........................] - ETA: 1:02:44 - loss: 0.6252 - regression_loss: 0.5042 - classification_loss: 0.1210
 1769/10000 [====>.........................] - ETA: 1:02:44 - loss: 0.6250 - regression_loss: 0.5040 - classification_loss: 0.1210
 1770/10000 [====>.........................] - ETA: 1:02:43 - loss: 0.6249 - regression_loss: 0.5038 - classification_loss: 0.1211
 1771/10000 [====>.........................] - ETA: 1:02:43 - loss: 0.6247 - regression_loss: 0.5037 - classification_loss: 0.1210
 1772/10000 [====>.........................] - ETA: 1:02:42 - loss: 0.6246 - regression_loss: 0.5036 - classification_loss: 0.1210
 1773/10000 [====>.........................] - ETA: 1:02:42 - loss: 0.6244 - regression_loss: 0.5034 - classification_loss: 0.1210
 1774/10000 [====>.........................] - ETA: 1:02:41 - loss: 0.6243 - regression_loss: 0.5033 - classification_loss: 0.1209
 1775/10000 [====>.........................] - ETA: 1:02:41 - loss: 0.6241 - regression_loss: 0.5032 - classification_loss: 0.1209
 1776/10000 [====>.........................] - ETA: 1:02:41 - loss: 0.6245 - regression_loss: 0.5035 - classification_loss: 0.1209
 1777/10000 [====>.........................] - ETA: 1:02:40 - loss: 0.6242 - regression_loss: 0.5034 - classification_loss: 0.1209
 1778/10000 [====>.........................] - ETA: 1:02:40 - loss: 0.6248 - regression_loss: 0.5035 - classification_loss: 0.1213
 1779/10000 [====>.........................] - ETA: 1:02:39 - loss: 0.6250 - regression_loss: 0.5037 - classification_loss: 0.1213
 1780/10000 [====>.........................] - ETA: 1:02:39 - loss: 0.6251 - regression_loss: 0.5037 - classification_loss: 0.1213
 1781/10000 [====>.........................] - ETA: 1:02:39 - loss: 0.6250 - regression_loss: 0.5037 - classification_loss: 0.1213
 1782/10000 [====>.........................] - ETA: 1:02:38 - loss: 0.6253 - regression_loss: 0.5040 - classification_loss: 0.1213
 1783/10000 [====>.........................] - ETA: 1:02:38 - loss: 0.6253 - regression_loss: 0.5040 - classification_loss: 0.1213
 1784/10000 [====>.........................] - ETA: 1:02:38 - loss: 0.6254 - regression_loss: 0.5041 - classification_loss: 0.1213
 1785/10000 [====>.........................] - ETA: 1:02:37 - loss: 0.6254 - regression_loss: 0.5041 - classification_loss: 0.1213
 1786/10000 [====>.........................] - ETA: 1:02:37 - loss: 0.6253 - regression_loss: 0.5040 - classification_loss: 0.1213
 1787/10000 [====>.........................] - ETA: 1:02:36 - loss: 0.6252 - regression_loss: 0.5039 - classification_loss: 0.1213
 1788/10000 [====>.........................] - ETA: 1:02:35 - loss: 0.6251 - regression_loss: 0.5039 - classification_loss: 0.1212
 1789/10000 [====>.........................] - ETA: 1:02:35 - loss: 0.6253 - regression_loss: 0.5041 - classification_loss: 0.1212
 1790/10000 [====>.........................] - ETA: 1:02:34 - loss: 0.6254 - regression_loss: 0.5041 - classification_loss: 0.1213
 1791/10000 [====>.........................] - ETA: 1:02:34 - loss: 0.6254 - regression_loss: 0.5042 - classification_loss: 0.1213
 1792/10000 [====>.........................] - ETA: 1:02:34 - loss: 0.6255 - regression_loss: 0.5042 - classification_loss: 0.1213
 1793/10000 [====>.........................] - ETA: 1:02:33 - loss: 0.6252 - regression_loss: 0.5040 - classification_loss: 0.1212
 1794/10000 [====>.........................] - ETA: 1:02:33 - loss: 0.6254 - regression_loss: 0.5042 - classification_loss: 0.1212
 1795/10000 [====>.........................] - ETA: 1:02:33 - loss: 0.6252 - regression_loss: 0.5041 - classification_loss: 0.1212
 1796/10000 [====>.........................] - ETA: 1:02:32 - loss: 0.6250 - regression_loss: 0.5039 - classification_loss: 0.1211
 1797/10000 [====>.........................] - ETA: 1:02:32 - loss: 0.6249 - regression_loss: 0.5038 - classification_loss: 0.1211
 1798/10000 [====>.........................] - ETA: 1:02:31 - loss: 0.6250 - regression_loss: 0.5039 - classification_loss: 0.1211
 1799/10000 [====>.........................] - ETA: 1:02:31 - loss: 0.6249 - regression_loss: 0.5039 - classification_loss: 0.1211
 1800/10000 [====>.........................] - ETA: 1:02:31 - loss: 0.6247 - regression_loss: 0.5037 - classification_loss: 0.1210
 1801/10000 [====>.........................] - ETA: 1:02:30 - loss: 0.6246 - regression_loss: 0.5036 - classification_loss: 0.1210
 1802/10000 [====>.........................] - ETA: 1:02:30 - loss: 0.6248 - regression_loss: 0.5038 - classification_loss: 0.1210
 1803/10000 [====>.........................] - ETA: 1:02:29 - loss: 0.6246 - regression_loss: 0.5036 - classification_loss: 0.1210
 1804/10000 [====>.........................] - ETA: 1:02:29 - loss: 0.6244 - regression_loss: 0.5034 - classification_loss: 0.1209
 1805/10000 [====>.........................] - ETA: 1:02:28 - loss: 0.6242 - regression_loss: 0.5033 - classification_loss: 0.1210
 1806/10000 [====>.........................] - ETA: 1:02:28 - loss: 0.6242 - regression_loss: 0.5033 - classification_loss: 0.1209
 1807/10000 [====>.........................] - ETA: 1:02:28 - loss: 0.6241 - regression_loss: 0.5032 - classification_loss: 0.1209
 1808/10000 [====>.........................] - ETA: 1:02:27 - loss: 0.6242 - regression_loss: 0.5033 - classification_loss: 0.1209
 1809/10000 [====>.........................] - ETA: 1:02:26 - loss: 0.6239 - regression_loss: 0.5030 - classification_loss: 0.1208
 1810/10000 [====>.........................] - ETA: 1:02:26 - loss: 0.6237 - regression_loss: 0.5029 - classification_loss: 0.1208
 1811/10000 [====>.........................] - ETA: 1:02:25 - loss: 0.6242 - regression_loss: 0.5033 - classification_loss: 0.1209
 1812/10000 [====>.........................] - ETA: 1:02:25 - loss: 0.6245 - regression_loss: 0.5035 - classification_loss: 0.1210
 1813/10000 [====>.........................] - ETA: 1:02:25 - loss: 0.6247 - regression_loss: 0.5036 - classification_loss: 0.1210
 1814/10000 [====>.........................] - ETA: 1:02:24 - loss: 0.6246 - regression_loss: 0.5035 - classification_loss: 0.1210
 1815/10000 [====>.........................] - ETA: 1:02:24 - loss: 0.6246 - regression_loss: 0.5036 - classification_loss: 0.1210
 1816/10000 [====>.........................] - ETA: 1:02:23 - loss: 0.6250 - regression_loss: 0.5039 - classification_loss: 0.1211
 1817/10000 [====>.........................] - ETA: 1:02:23 - loss: 0.6248 - regression_loss: 0.5037 - classification_loss: 0.1211
 1818/10000 [====>.........................] - ETA: 1:02:22 - loss: 0.6249 - regression_loss: 0.5039 - classification_loss: 0.1211
 1819/10000 [====>.........................] - ETA: 1:02:22 - loss: 0.6247 - regression_loss: 0.5036 - classification_loss: 0.1210
 1820/10000 [====>.........................] - ETA: 1:02:21 - loss: 0.6250 - regression_loss: 0.5039 - classification_loss: 0.1211
 1821/10000 [====>.........................] - ETA: 1:02:21 - loss: 0.6247 - regression_loss: 0.5037 - classification_loss: 0.1210
 1822/10000 [====>.........................] - ETA: 1:02:21 - loss: 0.6245 - regression_loss: 0.5035 - classification_loss: 0.1210
 1823/10000 [====>.........................] - ETA: 1:02:20 - loss: 0.6243 - regression_loss: 0.5033 - classification_loss: 0.1209
 1824/10000 [====>.........................] - ETA: 1:02:19 - loss: 0.6242 - regression_loss: 0.5033 - classification_loss: 0.1209
 1825/10000 [====>.........................] - ETA: 1:02:19 - loss: 0.6242 - regression_loss: 0.5034 - classification_loss: 0.1209
 1826/10000 [====>.........................] - ETA: 1:02:18 - loss: 0.6241 - regression_loss: 0.5032 - classification_loss: 0.1209
 1827/10000 [====>.........................] - ETA: 1:02:18 - loss: 0.6244 - regression_loss: 0.5035 - classification_loss: 0.1209
 1828/10000 [====>.........................] - ETA: 1:02:17 - loss: 0.6243 - regression_loss: 0.5034 - classification_loss: 0.1209
 1829/10000 [====>.........................] - ETA: 1:02:17 - loss: 0.6243 - regression_loss: 0.5034 - classification_loss: 0.1209
 1830/10000 [====>.........................] - ETA: 1:02:16 - loss: 0.6245 - regression_loss: 0.5035 - classification_loss: 0.1209
 1831/10000 [====>.........................] - ETA: 1:02:16 - loss: 0.6242 - regression_loss: 0.5033 - classification_loss: 0.1209
 1832/10000 [====>.........................] - ETA: 1:02:15 - loss: 0.6241 - regression_loss: 0.5033 - classification_loss: 0.1209
 1833/10000 [====>.........................] - ETA: 1:02:15 - loss: 0.6242 - regression_loss: 0.5034 - classification_loss: 0.1208
 1834/10000 [====>.........................] - ETA: 1:02:15 - loss: 0.6240 - regression_loss: 0.5032 - classification_loss: 0.1208
 1835/10000 [====>.........................] - ETA: 1:02:14 - loss: 0.6244 - regression_loss: 0.5035 - classification_loss: 0.1209
 1836/10000 [====>.........................] - ETA: 1:02:14 - loss: 0.6247 - regression_loss: 0.5037 - classification_loss: 0.1210
 1837/10000 [====>.........................] - ETA: 1:02:14 - loss: 0.6253 - regression_loss: 0.5041 - classification_loss: 0.1211
 1838/10000 [====>.........................] - ETA: 1:02:13 - loss: 0.6251 - regression_loss: 0.5040 - classification_loss: 0.1211
 1839/10000 [====>.........................] - ETA: 1:02:13 - loss: 0.6249 - regression_loss: 0.5039 - classification_loss: 0.1210
 1840/10000 [====>.........................] - ETA: 1:02:12 - loss: 0.6248 - regression_loss: 0.5038 - classification_loss: 0.1210
 1841/10000 [====>.........................] - ETA: 1:02:12 - loss: 0.6246 - regression_loss: 0.5037 - classification_loss: 0.1209
 1842/10000 [====>.........................] - ETA: 1:02:11 - loss: 0.6247 - regression_loss: 0.5038 - classification_loss: 0.1209
 1843/10000 [====>.........................] - ETA: 1:02:11 - loss: 0.6246 - regression_loss: 0.5037 - classification_loss: 0.1209
 1844/10000 [====>.........................] - ETA: 1:02:10 - loss: 0.6243 - regression_loss: 0.5035 - classification_loss: 0.1209
 1845/10000 [====>.........................] - ETA: 1:02:10 - loss: 0.6242 - regression_loss: 0.5033 - classification_loss: 0.1208
 1846/10000 [====>.........................] - ETA: 1:02:09 - loss: 0.6244 - regression_loss: 0.5034 - classification_loss: 0.1209
 1847/10000 [====>.........................] - ETA: 1:02:09 - loss: 0.6242 - regression_loss: 0.5033 - classification_loss: 0.1209
 1848/10000 [====>.........................] - ETA: 1:02:08 - loss: 0.6243 - regression_loss: 0.5034 - classification_loss: 0.1209
 1849/10000 [====>.........................] - ETA: 1:02:08 - loss: 0.6242 - regression_loss: 0.5033 - classification_loss: 0.1209
 1850/10000 [====>.........................] - ETA: 1:02:08 - loss: 0.6242 - regression_loss: 0.5033 - classification_loss: 0.1209
 1851/10000 [====>.........................] - ETA: 1:02:07 - loss: 0.6242 - regression_loss: 0.5033 - classification_loss: 0.1208
 1852/10000 [====>.........................] - ETA: 1:02:07 - loss: 0.6242 - regression_loss: 0.5033 - classification_loss: 0.1209
 1853/10000 [====>.........................] - ETA: 1:02:06 - loss: 0.6244 - regression_loss: 0.5035 - classification_loss: 0.1208
 1854/10000 [====>.........................] - ETA: 1:02:06 - loss: 0.6243 - regression_loss: 0.5035 - classification_loss: 0.1208
 1855/10000 [====>.........................] - ETA: 1:02:05 - loss: 0.6246 - regression_loss: 0.5037 - classification_loss: 0.1209
 1856/10000 [====>.........................] - ETA: 1:02:05 - loss: 0.6245 - regression_loss: 0.5036 - classification_loss: 0.1209
 1857/10000 [====>.........................] - ETA: 1:02:05 - loss: 0.6243 - regression_loss: 0.5035 - classification_loss: 0.1209
 1858/10000 [====>.........................] - ETA: 1:02:04 - loss: 0.6240 - regression_loss: 0.5032 - classification_loss: 0.1208
 1859/10000 [====>.........................] - ETA: 1:02:04 - loss: 0.6240 - regression_loss: 0.5031 - classification_loss: 0.1208
 1860/10000 [====>.........................] - ETA: 1:02:03 - loss: 0.6239 - regression_loss: 0.5030 - classification_loss: 0.1209
 1861/10000 [====>.........................] - ETA: 1:02:03 - loss: 0.6238 - regression_loss: 0.5029 - classification_loss: 0.1208
 1862/10000 [====>.........................] - ETA: 1:02:02 - loss: 0.6238 - regression_loss: 0.5029 - classification_loss: 0.1209
 1863/10000 [====>.........................] - ETA: 1:02:02 - loss: 0.6237 - regression_loss: 0.5028 - classification_loss: 0.1209
 1864/10000 [====>.........................] - ETA: 1:02:02 - loss: 0.6238 - regression_loss: 0.5028 - classification_loss: 0.1210
 1865/10000 [====>.........................] - ETA: 1:02:01 - loss: 0.6236 - regression_loss: 0.5027 - classification_loss: 0.1209
 1866/10000 [====>.........................] - ETA: 1:02:01 - loss: 0.6233 - regression_loss: 0.5025 - classification_loss: 0.1209
 1867/10000 [====>.........................] - ETA: 1:02:00 - loss: 0.6231 - regression_loss: 0.5023 - classification_loss: 0.1208
 1868/10000 [====>.........................] - ETA: 1:02:00 - loss: 0.6230 - regression_loss: 0.5022 - classification_loss: 0.1207
 1869/10000 [====>.........................] - ETA: 1:01:59 - loss: 0.6228 - regression_loss: 0.5021 - classification_loss: 0.1207
 1870/10000 [====>.........................] - ETA: 1:01:59 - loss: 0.6226 - regression_loss: 0.5019 - classification_loss: 0.1207
 1871/10000 [====>.........................] - ETA: 1:01:59 - loss: 0.6227 - regression_loss: 0.5019 - classification_loss: 0.1207
 1872/10000 [====>.........................] - ETA: 1:01:58 - loss: 0.6229 - regression_loss: 0.5022 - classification_loss: 0.1207
 1873/10000 [====>.........................] - ETA: 1:01:58 - loss: 0.6231 - regression_loss: 0.5023 - classification_loss: 0.1208
 1874/10000 [====>.........................] - ETA: 1:01:57 - loss: 0.6229 - regression_loss: 0.5022 - classification_loss: 0.1207
 1875/10000 [====>.........................] - ETA: 1:01:57 - loss: 0.6226 - regression_loss: 0.5019 - classification_loss: 0.1207
 1876/10000 [====>.........................] - ETA: 1:01:56 - loss: 0.6225 - regression_loss: 0.5018 - classification_loss: 0.1207
 1877/10000 [====>.........................] - ETA: 1:01:56 - loss: 0.6224 - regression_loss: 0.5017 - classification_loss: 0.1207
 1878/10000 [====>.........................] - ETA: 1:01:55 - loss: 0.6227 - regression_loss: 0.5020 - classification_loss: 0.1207
 1879/10000 [====>.........................] - ETA: 1:01:55 - loss: 0.6225 - regression_loss: 0.5018 - classification_loss: 0.1207
 1880/10000 [====>.........................] - ETA: 1:01:54 - loss: 0.6223 - regression_loss: 0.5016 - classification_loss: 0.1207
 1881/10000 [====>.........................] - ETA: 1:01:54 - loss: 0.6223 - regression_loss: 0.5016 - classification_loss: 0.1207
 1882/10000 [====>.........................] - ETA: 1:01:53 - loss: 0.6224 - regression_loss: 0.5017 - classification_loss: 0.1207
 1883/10000 [====>.........................] - ETA: 1:01:53 - loss: 0.6222 - regression_loss: 0.5015 - classification_loss: 0.1207
 1884/10000 [====>.........................] - ETA: 1:01:53 - loss: 0.6223 - regression_loss: 0.5017 - classification_loss: 0.1206
 1885/10000 [====>.........................] - ETA: 1:01:52 - loss: 0.6225 - regression_loss: 0.5018 - classification_loss: 0.1207
 1886/10000 [====>.........................] - ETA: 1:01:52 - loss: 0.6223 - regression_loss: 0.5017 - classification_loss: 0.1207
 1887/10000 [====>.........................] - ETA: 1:01:52 - loss: 0.6223 - regression_loss: 0.5016 - classification_loss: 0.1206
 1888/10000 [====>.........................] - ETA: 1:01:51 - loss: 0.6224 - regression_loss: 0.5018 - classification_loss: 0.1206
 1889/10000 [====>.........................] - ETA: 1:01:51 - loss: 0.6223 - regression_loss: 0.5017 - classification_loss: 0.1206
 1890/10000 [====>.........................] - ETA: 1:01:50 - loss: 0.6225 - regression_loss: 0.5018 - classification_loss: 0.1206
 1891/10000 [====>.........................] - ETA: 1:01:50 - loss: 0.6223 - regression_loss: 0.5017 - classification_loss: 0.1206
 1892/10000 [====>.........................] - ETA: 1:01:49 - loss: 0.6222 - regression_loss: 0.5016 - classification_loss: 0.1206
 1893/10000 [====>.........................] - ETA: 1:01:49 - loss: 0.6220 - regression_loss: 0.5015 - classification_loss: 0.1206
 1894/10000 [====>.........................] - ETA: 1:01:48 - loss: 0.6222 - regression_loss: 0.5016 - classification_loss: 0.1206
 1895/10000 [====>.........................] - ETA: 1:01:48 - loss: 0.6222 - regression_loss: 0.5016 - classification_loss: 0.1206
 1896/10000 [====>.........................] - ETA: 1:01:48 - loss: 0.6222 - regression_loss: 0.5016 - classification_loss: 0.1206
 1897/10000 [====>.........................] - ETA: 1:01:47 - loss: 0.6220 - regression_loss: 0.5014 - classification_loss: 0.1206
 1898/10000 [====>.........................] - ETA: 1:01:47 - loss: 0.6218 - regression_loss: 0.5013 - classification_loss: 0.1205
 1899/10000 [====>.........................] - ETA: 1:01:46 - loss: 0.6216 - regression_loss: 0.5011 - classification_loss: 0.1205
 1900/10000 [====>.........................] - ETA: 1:01:46 - loss: 0.6217 - regression_loss: 0.5013 - classification_loss: 0.1205
 1901/10000 [====>.........................] - ETA: 1:01:45 - loss: 0.6222 - regression_loss: 0.5017 - classification_loss: 0.1205
 1902/10000 [====>.........................] - ETA: 1:01:45 - loss: 0.6221 - regression_loss: 0.5016 - classification_loss: 0.1205
 1903/10000 [====>.........................] - ETA: 1:01:45 - loss: 0.6219 - regression_loss: 0.5015 - classification_loss: 0.1205
 1904/10000 [====>.........................] - ETA: 1:01:44 - loss: 0.6219 - regression_loss: 0.5014 - classification_loss: 0.1205
 1905/10000 [====>.........................] - ETA: 1:01:44 - loss: 0.6222 - regression_loss: 0.5017 - classification_loss: 0.1205
 1906/10000 [====>.........................] - ETA: 1:01:43 - loss: 0.6220 - regression_loss: 0.5016 - classification_loss: 0.1205
 1907/10000 [====>.........................] - ETA: 1:01:43 - loss: 0.6218 - regression_loss: 0.5014 - classification_loss: 0.1204
 1908/10000 [====>.........................] - ETA: 1:01:42 - loss: 0.6220 - regression_loss: 0.5015 - classification_loss: 0.1205
 1909/10000 [====>.........................] - ETA: 1:01:42 - loss: 0.6219 - regression_loss: 0.5014 - classification_loss: 0.1204
 1910/10000 [====>.........................] - ETA: 1:01:41 - loss: 0.6220 - regression_loss: 0.5015 - classification_loss: 0.1204
 1911/10000 [====>.........................] - ETA: 1:01:41 - loss: 0.6218 - regression_loss: 0.5014 - classification_loss: 0.1204
 1912/10000 [====>.........................] - ETA: 1:01:40 - loss: 0.6216 - regression_loss: 0.5013 - classification_loss: 0.1203
 1913/10000 [====>.........................] - ETA: 1:01:40 - loss: 0.6214 - regression_loss: 0.5011 - classification_loss: 0.1203
 1914/10000 [====>.........................] - ETA: 1:01:40 - loss: 0.6213 - regression_loss: 0.5011 - classification_loss: 0.1202
 1915/10000 [====>.........................] - ETA: 1:01:39 - loss: 0.6213 - regression_loss: 0.5010 - classification_loss: 0.1202
 1916/10000 [====>.........................] - ETA: 1:01:39 - loss: 0.6214 - regression_loss: 0.5012 - classification_loss: 0.1202
 1917/10000 [====>.........................] - ETA: 1:01:38 - loss: 0.6213 - regression_loss: 0.5011 - classification_loss: 0.1202
 1918/10000 [====>.........................] - ETA: 1:01:38 - loss: 0.6211 - regression_loss: 0.5009 - classification_loss: 0.1202
 1919/10000 [====>.........................] - ETA: 1:01:37 - loss: 0.6214 - regression_loss: 0.5011 - classification_loss: 0.1203
 1920/10000 [====>.........................] - ETA: 1:01:37 - loss: 0.6212 - regression_loss: 0.5010 - classification_loss: 0.1202
 1921/10000 [====>.........................] - ETA: 1:01:36 - loss: 0.6214 - regression_loss: 0.5011 - classification_loss: 0.1203
 1922/10000 [====>.........................] - ETA: 1:01:36 - loss: 0.6214 - regression_loss: 0.5011 - classification_loss: 0.1203
 1923/10000 [====>.........................] - ETA: 1:01:35 - loss: 0.6214 - regression_loss: 0.5010 - classification_loss: 0.1203
 1924/10000 [====>.........................] - ETA: 1:01:35 - loss: 0.6215 - regression_loss: 0.5012 - classification_loss: 0.1203
 1925/10000 [====>.........................] - ETA: 1:01:34 - loss: 0.6212 - regression_loss: 0.5010 - classification_loss: 0.1202
 1926/10000 [====>.........................] - ETA: 1:01:34 - loss: 0.6218 - regression_loss: 0.5015 - classification_loss: 0.1203
 1927/10000 [====>.........................] - ETA: 1:01:33 - loss: 0.6216 - regression_loss: 0.5014 - classification_loss: 0.1203
 1928/10000 [====>.........................] - ETA: 1:01:33 - loss: 0.6218 - regression_loss: 0.5015 - classification_loss: 0.1203
 1929/10000 [====>.........................] - ETA: 1:01:32 - loss: 0.6216 - regression_loss: 0.5013 - classification_loss: 0.1202
 1930/10000 [====>.........................] - ETA: 1:01:32 - loss: 0.6216 - regression_loss: 0.5014 - classification_loss: 0.1202
 1931/10000 [====>.........................] - ETA: 1:01:31 - loss: 0.6217 - regression_loss: 0.5014 - classification_loss: 0.1202
 1932/10000 [====>.........................] - ETA: 1:01:31 - loss: 0.6217 - regression_loss: 0.5015 - classification_loss: 0.1202
 1933/10000 [====>.........................] - ETA: 1:01:30 - loss: 0.6217 - regression_loss: 0.5015 - classification_loss: 0.1202
 1934/10000 [====>.........................] - ETA: 1:01:30 - loss: 0.6218 - regression_loss: 0.5016 - classification_loss: 0.1203
 1935/10000 [====>.........................] - ETA: 1:01:29 - loss: 0.6222 - regression_loss: 0.5019 - classification_loss: 0.1203
 1936/10000 [====>.........................] - ETA: 1:01:29 - loss: 0.6224 - regression_loss: 0.5021 - classification_loss: 0.1203
 1937/10000 [====>.........................] - ETA: 1:01:29 - loss: 0.6224 - regression_loss: 0.5021 - classification_loss: 0.1203
 1938/10000 [====>.........................] - ETA: 1:01:28 - loss: 0.6225 - regression_loss: 0.5022 - classification_loss: 0.1204
 1939/10000 [====>.........................] - ETA: 1:01:28 - loss: 0.6223 - regression_loss: 0.5020 - classification_loss: 0.1203
 1940/10000 [====>.........................] - ETA: 1:01:27 - loss: 0.6225 - regression_loss: 0.5021 - classification_loss: 0.1204
 1941/10000 [====>.........................] - ETA: 1:01:27 - loss: 0.6222 - regression_loss: 0.5019 - classification_loss: 0.1204
 1942/10000 [====>.........................] - ETA: 1:01:26 - loss: 0.6226 - regression_loss: 0.5021 - classification_loss: 0.1204
 1943/10000 [====>.........................] - ETA: 1:01:26 - loss: 0.6225 - regression_loss: 0.5020 - classification_loss: 0.1204
 1944/10000 [====>.........................] - ETA: 1:01:25 - loss: 0.6225 - regression_loss: 0.5021 - classification_loss: 0.1204
 1945/10000 [====>.........................] - ETA: 1:01:25 - loss: 0.6225 - regression_loss: 0.5021 - classification_loss: 0.1204
 1946/10000 [====>.........................] - ETA: 1:01:25 - loss: 0.6226 - regression_loss: 0.5022 - classification_loss: 0.1204
 1947/10000 [====>.........................] - ETA: 1:01:24 - loss: 0.6226 - regression_loss: 0.5022 - classification_loss: 0.1204
 1948/10000 [====>.........................] - ETA: 1:01:23 - loss: 0.6227 - regression_loss: 0.5022 - classification_loss: 0.1205
 1949/10000 [====>.........................] - ETA: 1:01:23 - loss: 0.6226 - regression_loss: 0.5022 - classification_loss: 0.1204
 1950/10000 [====>.........................] - ETA: 1:01:23 - loss: 0.6224 - regression_loss: 0.5020 - classification_loss: 0.1204
 1951/10000 [====>.........................] - ETA: 1:01:22 - loss: 0.6222 - regression_loss: 0.5018 - classification_loss: 0.1204
 1952/10000 [====>.........................] - ETA: 1:01:22 - loss: 0.6221 - regression_loss: 0.5018 - classification_loss: 0.1204
 1953/10000 [====>.........................] - ETA: 1:01:21 - loss: 0.6220 - regression_loss: 0.5016 - classification_loss: 0.1203
 1954/10000 [====>.........................] - ETA: 1:01:21 - loss: 0.6222 - regression_loss: 0.5018 - classification_loss: 0.1204
 1955/10000 [====>.........................] - ETA: 1:01:20 - loss: 0.6221 - regression_loss: 0.5017 - classification_loss: 0.1203
 1956/10000 [====>.........................] - ETA: 1:01:20 - loss: 0.6219 - regression_loss: 0.5016 - classification_loss: 0.1203
 1957/10000 [====>.........................] - ETA: 1:01:19 - loss: 0.6216 - regression_loss: 0.5014 - classification_loss: 0.1203
 1958/10000 [====>.........................] - ETA: 1:01:19 - loss: 0.6216 - regression_loss: 0.5014 - classification_loss: 0.1202
 1959/10000 [====>.........................] - ETA: 1:01:18 - loss: 0.6214 - regression_loss: 0.5012 - classification_loss: 0.1202
 1960/10000 [====>.........................] - ETA: 1:01:18 - loss: 0.6215 - regression_loss: 0.5013 - classification_loss: 0.1202
 1961/10000 [====>.........................] - ETA: 1:01:17 - loss: 0.6217 - regression_loss: 0.5015 - classification_loss: 0.1203
 1962/10000 [====>.........................] - ETA: 1:01:17 - loss: 0.6216 - regression_loss: 0.5014 - classification_loss: 0.1202
 1963/10000 [====>.........................] - ETA: 1:01:16 - loss: 0.6219 - regression_loss: 0.5017 - classification_loss: 0.1202
 1964/10000 [====>.........................] - ETA: 1:01:16 - loss: 0.6216 - regression_loss: 0.5015 - classification_loss: 0.1201
 1965/10000 [====>.........................] - ETA: 1:01:15 - loss: 0.6219 - regression_loss: 0.5017 - classification_loss: 0.1202
 1966/10000 [====>.........................] - ETA: 1:01:15 - loss: 0.6217 - regression_loss: 0.5016 - classification_loss: 0.1201
 1967/10000 [====>.........................] - ETA: 1:01:15 - loss: 0.6217 - regression_loss: 0.5016 - classification_loss: 0.1201
 1968/10000 [====>.........................] - ETA: 1:01:14 - loss: 0.6217 - regression_loss: 0.5016 - classification_loss: 0.1201
 1969/10000 [====>.........................] - ETA: 1:01:14 - loss: 0.6216 - regression_loss: 0.5015 - classification_loss: 0.1201
 1970/10000 [====>.........................] - ETA: 1:01:13 - loss: 0.6219 - regression_loss: 0.5017 - classification_loss: 0.1201
 1971/10000 [====>.........................] - ETA: 1:01:13 - loss: 0.6220 - regression_loss: 0.5018 - classification_loss: 0.1202
 1972/10000 [====>.........................] - ETA: 1:01:12 - loss: 0.6218 - regression_loss: 0.5016 - classification_loss: 0.1202
 1973/10000 [====>.........................] - ETA: 1:01:12 - loss: 0.6216 - regression_loss: 0.5015 - classification_loss: 0.1201
 1974/10000 [====>.........................] - ETA: 1:01:11 - loss: 0.6217 - regression_loss: 0.5016 - classification_loss: 0.1201
 1975/10000 [====>.........................] - ETA: 1:01:11 - loss: 0.6218 - regression_loss: 0.5018 - classification_loss: 0.1201
 1976/10000 [====>.........................] - ETA: 1:01:11 - loss: 0.6217 - regression_loss: 0.5017 - classification_loss: 0.1201
 1977/10000 [====>.........................] - ETA: 1:01:10 - loss: 0.6216 - regression_loss: 0.5016 - classification_loss: 0.1200
 1978/10000 [====>.........................] - ETA: 1:01:10 - loss: 0.6215 - regression_loss: 0.5015 - classification_loss: 0.1200
 1979/10000 [====>.........................] - ETA: 1:01:10 - loss: 0.6214 - regression_loss: 0.5015 - classification_loss: 0.1200
 1980/10000 [====>.........................] - ETA: 1:01:09 - loss: 0.6213 - regression_loss: 0.5013 - classification_loss: 0.1199
 1981/10000 [====>.........................] - ETA: 1:01:09 - loss: 0.6212 - regression_loss: 0.5012 - classification_loss: 0.1199
 1982/10000 [====>.........................] - ETA: 1:01:08 - loss: 0.6212 - regression_loss: 0.5013 - classification_loss: 0.1199
 1983/10000 [====>.........................] - ETA: 1:01:08 - loss: 0.6212 - regression_loss: 0.5013 - classification_loss: 0.1199
 1984/10000 [====>.........................] - ETA: 1:01:07 - loss: 0.6213 - regression_loss: 0.5013 - classification_loss: 0.1200
 1985/10000 [====>.........................] - ETA: 1:01:07 - loss: 0.6216 - regression_loss: 0.5015 - classification_loss: 0.1201
 1986/10000 [====>.........................] - ETA: 1:01:06 - loss: 0.6215 - regression_loss: 0.5014 - classification_loss: 0.1200
 1987/10000 [====>.........................] - ETA: 1:01:06 - loss: 0.6218 - regression_loss: 0.5018 - classification_loss: 0.1200
 1988/10000 [====>.........................] - ETA: 1:01:05 - loss: 0.6217 - regression_loss: 0.5017 - classification_loss: 0.1200
 1989/10000 [====>.........................] - ETA: 1:01:05 - loss: 0.6215 - regression_loss: 0.5015 - classification_loss: 0.1200
 1990/10000 [====>.........................] - ETA: 1:01:04 - loss: 0.6214 - regression_loss: 0.5014 - classification_loss: 0.1200
 1991/10000 [====>.........................] - ETA: 1:01:03 - loss: 0.6214 - regression_loss: 0.5013 - classification_loss: 0.1200
 1992/10000 [====>.........................] - ETA: 1:01:03 - loss: 0.6216 - regression_loss: 0.5016 - classification_loss: 0.1200
 1993/10000 [====>.........................] - ETA: 1:01:02 - loss: 0.6214 - regression_loss: 0.5014 - classification_loss: 0.1200
 1994/10000 [====>.........................] - ETA: 1:01:01 - loss: 0.6214 - regression_loss: 0.5014 - classification_loss: 0.1200
 1995/10000 [====>.........................] - ETA: 1:01:01 - loss: 0.6216 - regression_loss: 0.5015 - classification_loss: 0.1201
 1996/10000 [====>.........................] - ETA: 1:01:00 - loss: 0.6215 - regression_loss: 0.5014 - classification_loss: 0.1201
 1997/10000 [====>.........................] - ETA: 1:01:00 - loss: 0.6215 - regression_loss: 0.5014 - classification_loss: 0.1201
 1998/10000 [====>.........................] - ETA: 1:00:59 - loss: 0.6213 - regression_loss: 0.5012 - classification_loss: 0.1201
 1999/10000 [====>.........................] - ETA: 1:00:59 - loss: 0.6211 - regression_loss: 0.5010 - classification_loss: 0.1201
 2000/10000 [=====>........................] - ETA: 1:00:58 - loss: 0.6213 - regression_loss: 0.5012 - classification_loss: 0.1201
 2001/10000 [=====>........................] - ETA: 1:00:58 - loss: 0.6212 - regression_loss: 0.5012 - classification_loss: 0.1201
 2002/10000 [=====>........................] - ETA: 1:00:57 - loss: 0.6211 - regression_loss: 0.5011 - classification_loss: 0.1200
 2003/10000 [=====>........................] - ETA: 1:00:57 - loss: 0.6209 - regression_loss: 0.5009 - classification_loss: 0.1200
 2004/10000 [=====>........................] - ETA: 1:00:57 - loss: 0.6208 - regression_loss: 0.5008 - classification_loss: 0.1200
 2005/10000 [=====>........................] - ETA: 1:00:56 - loss: 0.6210 - regression_loss: 0.5010 - classification_loss: 0.1200
 2006/10000 [=====>........................] - ETA: 1:00:56 - loss: 0.6211 - regression_loss: 0.5010 - classification_loss: 0.1201
 2007/10000 [=====>........................] - ETA: 1:00:55 - loss: 0.6214 - regression_loss: 0.5012 - classification_loss: 0.1202
 2008/10000 [=====>........................] - ETA: 1:00:55 - loss: 0.6219 - regression_loss: 0.5016 - classification_loss: 0.1202
 2009/10000 [=====>........................] - ETA: 1:00:54 - loss: 0.6219 - regression_loss: 0.5016 - classification_loss: 0.1203
 2010/10000 [=====>........................] - ETA: 1:00:54 - loss: 0.6219 - regression_loss: 0.5016 - classification_loss: 0.1203
 2011/10000 [=====>........................] - ETA: 1:00:53 - loss: 0.6220 - regression_loss: 0.5017 - classification_loss: 0.1204
 2012/10000 [=====>........................] - ETA: 1:00:53 - loss: 0.6220 - regression_loss: 0.5016 - classification_loss: 0.1204
 2013/10000 [=====>........................] - ETA: 1:00:52 - loss: 0.6219 - regression_loss: 0.5015 - classification_loss: 0.1204
 2014/10000 [=====>........................] - ETA: 1:00:52 - loss: 0.6223 - regression_loss: 0.5019 - classification_loss: 0.1204
 2015/10000 [=====>........................] - ETA: 1:00:51 - loss: 0.6223 - regression_loss: 0.5020 - classification_loss: 0.1204
 2016/10000 [=====>........................] - ETA: 1:00:51 - loss: 0.6224 - regression_loss: 0.5020 - classification_loss: 0.1204
 2017/10000 [=====>........................] - ETA: 1:00:50 - loss: 0.6224 - regression_loss: 0.5020 - classification_loss: 0.1204
 2018/10000 [=====>........................] - ETA: 1:00:50 - loss: 0.6221 - regression_loss: 0.5018 - classification_loss: 0.1203
 2019/10000 [=====>........................] - ETA: 1:00:49 - loss: 0.6220 - regression_loss: 0.5017 - classification_loss: 0.1203
 2020/10000 [=====>........................] - ETA: 1:00:49 - loss: 0.6218 - regression_loss: 0.5015 - classification_loss: 0.1203
 2021/10000 [=====>........................] - ETA: 1:00:49 - loss: 0.6216 - regression_loss: 0.5013 - classification_loss: 0.1203
 2022/10000 [=====>........................] - ETA: 1:00:48 - loss: 0.6216 - regression_loss: 0.5013 - classification_loss: 0.1203
 2023/10000 [=====>........................] - ETA: 1:00:48 - loss: 0.6215 - regression_loss: 0.5012 - classification_loss: 0.1203
 2024/10000 [=====>........................] - ETA: 1:00:47 - loss: 0.6216 - regression_loss: 0.5013 - classification_loss: 0.1203
 2025/10000 [=====>........................] - ETA: 1:00:46 - loss: 0.6216 - regression_loss: 0.5013 - classification_loss: 0.1203
 2026/10000 [=====>........................] - ETA: 1:00:46 - loss: 0.6215 - regression_loss: 0.5012 - classification_loss: 0.1203
 2027/10000 [=====>........................] - ETA: 1:00:46 - loss: 0.6214 - regression_loss: 0.5011 - classification_loss: 0.1203
 2028/10000 [=====>........................] - ETA: 1:00:45 - loss: 0.6212 - regression_loss: 0.5009 - classification_loss: 0.1203
 2029/10000 [=====>........................] - ETA: 1:00:45 - loss: 0.6212 - regression_loss: 0.5009 - classification_loss: 0.1203
 2030/10000 [=====>........................] - ETA: 1:00:44 - loss: 0.6217 - regression_loss: 0.5014 - classification_loss: 0.1203
 2031/10000 [=====>........................] - ETA: 1:00:44 - loss: 0.6216 - regression_loss: 0.5013 - classification_loss: 0.1203
 2032/10000 [=====>........................] - ETA: 1:00:44 - loss: 0.6218 - regression_loss: 0.5015 - classification_loss: 0.1203
 2033/10000 [=====>........................] - ETA: 1:00:43 - loss: 0.6219 - regression_loss: 0.5016 - classification_loss: 0.1203
 2034/10000 [=====>........................] - ETA: 1:00:43 - loss: 0.6220 - regression_loss: 0.5017 - classification_loss: 0.1203
 2035/10000 [=====>........................] - ETA: 1:00:42 - loss: 0.6223 - regression_loss: 0.5020 - classification_loss: 0.1203
 2036/10000 [=====>........................] - ETA: 1:00:42 - loss: 0.6222 - regression_loss: 0.5019 - classification_loss: 0.1203
 2037/10000 [=====>........................] - ETA: 1:00:41 - loss: 0.6222 - regression_loss: 0.5019 - classification_loss: 0.1203
 2038/10000 [=====>........................] - ETA: 1:00:41 - loss: 0.6222 - regression_loss: 0.5018 - classification_loss: 0.1204
 2039/10000 [=====>........................] - ETA: 1:00:41 - loss: 0.6220 - regression_loss: 0.5016 - classification_loss: 0.1204
 2040/10000 [=====>........................] - ETA: 1:00:40 - loss: 0.6222 - regression_loss: 0.5018 - classification_loss: 0.1204
 2041/10000 [=====>........................] - ETA: 1:00:40 - loss: 0.6221 - regression_loss: 0.5017 - classification_loss: 0.1204
 2042/10000 [=====>........................] - ETA: 1:00:39 - loss: 0.6219 - regression_loss: 0.5015 - classification_loss: 0.1204
 2043/10000 [=====>........................] - ETA: 1:00:39 - loss: 0.6217 - regression_loss: 0.5013 - classification_loss: 0.1204
 2044/10000 [=====>........................] - ETA: 1:00:38 - loss: 0.6215 - regression_loss: 0.5012 - classification_loss: 0.1204
 2045/10000 [=====>........................] - ETA: 1:00:38 - loss: 0.6217 - regression_loss: 0.5013 - classification_loss: 0.1204
 2046/10000 [=====>........................] - ETA: 1:00:37 - loss: 0.6219 - regression_loss: 0.5014 - classification_loss: 0.1205
 2047/10000 [=====>........................] - ETA: 1:00:37 - loss: 0.6220 - regression_loss: 0.5016 - classification_loss: 0.1205
 2048/10000 [=====>........................] - ETA: 1:00:36 - loss: 0.6222 - regression_loss: 0.5017 - classification_loss: 0.1205
 2049/10000 [=====>........................] - ETA: 1:00:36 - loss: 0.6222 - regression_loss: 0.5017 - classification_loss: 0.1205
 2050/10000 [=====>........................] - ETA: 1:00:35 - loss: 0.6223 - regression_loss: 0.5018 - classification_loss: 0.1205
 2051/10000 [=====>........................] - ETA: 1:00:35 - loss: 0.6223 - regression_loss: 0.5019 - classification_loss: 0.1205
 2052/10000 [=====>........................] - ETA: 1:00:35 - loss: 0.6223 - regression_loss: 0.5019 - classification_loss: 0.1205
 2053/10000 [=====>........................] - ETA: 1:00:34 - loss: 0.6226 - regression_loss: 0.5020 - classification_loss: 0.1205
 2054/10000 [=====>........................] - ETA: 1:00:34 - loss: 0.6224 - regression_loss: 0.5019 - classification_loss: 0.1205
 2055/10000 [=====>........................] - ETA: 1:00:34 - loss: 0.6223 - regression_loss: 0.5018 - classification_loss: 0.1205
 2056/10000 [=====>........................] - ETA: 1:00:33 - loss: 0.6226 - regression_loss: 0.5020 - classification_loss: 0.1206
 2057/10000 [=====>........................] - ETA: 1:00:33 - loss: 0.6229 - regression_loss: 0.5023 - classification_loss: 0.1206
 2058/10000 [=====>........................] - ETA: 1:00:32 - loss: 0.6228 - regression_loss: 0.5022 - classification_loss: 0.1206
 2059/10000 [=====>........................] - ETA: 1:00:32 - loss: 0.6227 - regression_loss: 0.5021 - classification_loss: 0.1205
 2060/10000 [=====>........................] - ETA: 1:00:31 - loss: 0.6228 - regression_loss: 0.5022 - classification_loss: 0.1206
 2061/10000 [=====>........................] - ETA: 1:00:31 - loss: 0.6226 - regression_loss: 0.5021 - classification_loss: 0.1205
 2062/10000 [=====>........................] - ETA: 1:00:30 - loss: 0.6228 - regression_loss: 0.5023 - classification_loss: 0.1205
 2063/10000 [=====>........................] - ETA: 1:00:30 - loss: 0.6226 - regression_loss: 0.5021 - classification_loss: 0.1205
 2064/10000 [=====>........................] - ETA: 1:00:29 - loss: 0.6229 - regression_loss: 0.5024 - classification_loss: 0.1206
 2065/10000 [=====>........................] - ETA: 1:00:29 - loss: 0.6228 - regression_loss: 0.5023 - classification_loss: 0.1206
 2066/10000 [=====>........................] - ETA: 1:00:28 - loss: 0.6228 - regression_loss: 0.5023 - classification_loss: 0.1205
 2067/10000 [=====>........................] - ETA: 1:00:28 - loss: 0.6230 - regression_loss: 0.5024 - classification_loss: 0.1205
 2068/10000 [=====>........................] - ETA: 1:00:27 - loss: 0.6228 - regression_loss: 0.5023 - classification_loss: 0.1205
 2069/10000 [=====>........................] - ETA: 1:00:27 - loss: 0.6229 - regression_loss: 0.5023 - classification_loss: 0.1205
 2070/10000 [=====>........................] - ETA: 1:00:26 - loss: 0.6229 - regression_loss: 0.5023 - classification_loss: 0.1205
 2071/10000 [=====>........................] - ETA: 1:00:26 - loss: 0.6230 - regression_loss: 0.5024 - classification_loss: 0.1205
 2072/10000 [=====>........................] - ETA: 1:00:25 - loss: 0.6228 - regression_loss: 0.5023 - classification_loss: 0.1205
 2073/10000 [=====>........................] - ETA: 1:00:25 - loss: 0.6229 - regression_loss: 0.5024 - classification_loss: 0.1205
 2074/10000 [=====>........................] - ETA: 1:00:24 - loss: 0.6229 - regression_loss: 0.5023 - classification_loss: 0.1206
 2075/10000 [=====>........................] - ETA: 1:00:23 - loss: 0.6228 - regression_loss: 0.5022 - classification_loss: 0.1205
 2076/10000 [=====>........................] - ETA: 1:00:23 - loss: 0.6234 - regression_loss: 0.5027 - classification_loss: 0.1207
 2077/10000 [=====>........................] - ETA: 1:00:22 - loss: 0.6234 - regression_loss: 0.5027 - classification_loss: 0.1207
 2078/10000 [=====>........................] - ETA: 1:00:22 - loss: 0.6235 - regression_loss: 0.5028 - classification_loss: 0.1207
 2079/10000 [=====>........................] - ETA: 1:00:22 - loss: 0.6233 - regression_loss: 0.5027 - classification_loss: 0.1206
 2080/10000 [=====>........................] - ETA: 1:00:21 - loss: 0.6233 - regression_loss: 0.5027 - classification_loss: 0.1206
 2081/10000 [=====>........................] - ETA: 1:00:21 - loss: 0.6235 - regression_loss: 0.5028 - classification_loss: 0.1206
 2082/10000 [=====>........................] - ETA: 1:00:20 - loss: 0.6234 - regression_loss: 0.5028 - classification_loss: 0.1206
 2083/10000 [=====>........................] - ETA: 1:00:20 - loss: 0.6231 - regression_loss: 0.5025 - classification_loss: 0.1206
 2084/10000 [=====>........................] - ETA: 1:00:20 - loss: 0.6231 - regression_loss: 0.5025 - classification_loss: 0.1206
 2085/10000 [=====>........................] - ETA: 1:00:19 - loss: 0.6232 - regression_loss: 0.5025 - classification_loss: 0.1207
 2086/10000 [=====>........................] - ETA: 1:00:19 - loss: 0.6230 - regression_loss: 0.5023 - classification_loss: 0.1207
 2087/10000 [=====>........................] - ETA: 1:00:18 - loss: 0.6230 - regression_loss: 0.5023 - classification_loss: 0.1207
 2088/10000 [=====>........................] - ETA: 1:00:18 - loss: 0.6232 - regression_loss: 0.5025 - classification_loss: 0.1207
 2089/10000 [=====>........................] - ETA: 1:00:17 - loss: 0.6231 - regression_loss: 0.5024 - classification_loss: 0.1207
 2090/10000 [=====>........................] - ETA: 1:00:17 - loss: 0.6229 - regression_loss: 0.5023 - classification_loss: 0.1207
 2091/10000 [=====>........................] - ETA: 1:00:16 - loss: 0.6229 - regression_loss: 0.5022 - classification_loss: 0.1207
 2092/10000 [=====>........................] - ETA: 1:00:16 - loss: 0.6228 - regression_loss: 0.5021 - classification_loss: 0.1207
 2093/10000 [=====>........................] - ETA: 1:00:15 - loss: 0.6227 - regression_loss: 0.5020 - classification_loss: 0.1207
 2094/10000 [=====>........................] - ETA: 1:00:15 - loss: 0.6227 - regression_loss: 0.5021 - classification_loss: 0.1206
 2095/10000 [=====>........................] - ETA: 1:00:15 - loss: 0.6227 - regression_loss: 0.5021 - classification_loss: 0.1206
 2096/10000 [=====>........................] - ETA: 1:00:14 - loss: 0.6225 - regression_loss: 0.5019 - classification_loss: 0.1206
 2097/10000 [=====>........................] - ETA: 1:00:13 - loss: 0.6228 - regression_loss: 0.5022 - classification_loss: 0.1206
 2098/10000 [=====>........................] - ETA: 1:00:13 - loss: 0.6226 - regression_loss: 0.5021 - classification_loss: 0.1206
 2099/10000 [=====>........................] - ETA: 1:00:13 - loss: 0.6226 - regression_loss: 0.5020 - classification_loss: 0.1206
 2100/10000 [=====>........................] - ETA: 1:00:12 - loss: 0.6224 - regression_loss: 0.5019 - classification_loss: 0.1205
 2101/10000 [=====>........................] - ETA: 1:00:12 - loss: 0.6225 - regression_loss: 0.5020 - classification_loss: 0.1205
 2102/10000 [=====>........................] - ETA: 1:00:11 - loss: 0.6223 - regression_loss: 0.5018 - classification_loss: 0.1205
 2103/10000 [=====>........................] - ETA: 1:00:11 - loss: 0.6222 - regression_loss: 0.5018 - classification_loss: 0.1204
 2104/10000 [=====>........................] - ETA: 1:00:10 - loss: 0.6221 - regression_loss: 0.5017 - classification_loss: 0.1204
 2105/10000 [=====>........................] - ETA: 1:00:10 - loss: 0.6223 - regression_loss: 0.5019 - classification_loss: 0.1204
 2106/10000 [=====>........................] - ETA: 1:00:10 - loss: 0.6221 - regression_loss: 0.5018 - classification_loss: 0.1204
 2107/10000 [=====>........................] - ETA: 1:00:09 - loss: 0.6219 - regression_loss: 0.5016 - classification_loss: 0.1203
 2108/10000 [=====>........................] - ETA: 1:00:09 - loss: 0.6219 - regression_loss: 0.5015 - classification_loss: 0.1203
 2109/10000 [=====>........................] - ETA: 1:00:08 - loss: 0.6216 - regression_loss: 0.5013 - classification_loss: 0.1203
 2110/10000 [=====>........................] - ETA: 1:00:08 - loss: 0.6217 - regression_loss: 0.5014 - classification_loss: 0.1204
 2111/10000 [=====>........................] - ETA: 1:00:07 - loss: 0.6220 - regression_loss: 0.5016 - classification_loss: 0.1204
 2112/10000 [=====>........................] - ETA: 1:00:07 - loss: 0.6221 - regression_loss: 0.5017 - classification_loss: 0.1205
 2113/10000 [=====>........................] - ETA: 1:00:06 - loss: 0.6223 - regression_loss: 0.5018 - classification_loss: 0.1205
 2114/10000 [=====>........................] - ETA: 1:00:06 - loss: 0.6221 - regression_loss: 0.5017 - classification_loss: 0.1205
 2115/10000 [=====>........................] - ETA: 1:00:05 - loss: 0.6221 - regression_loss: 0.5016 - classification_loss: 0.1205
 2116/10000 [=====>........................] - ETA: 1:00:05 - loss: 0.6221 - regression_loss: 0.5017 - classification_loss: 0.1204
 2117/10000 [=====>........................] - ETA: 1:00:04 - loss: 0.6219 - regression_loss: 0.5015 - classification_loss: 0.1204
 2118/10000 [=====>........................] - ETA: 1:00:04 - loss: 0.6218 - regression_loss: 0.5014 - classification_loss: 0.1204
 2119/10000 [=====>........................] - ETA: 1:00:03 - loss: 0.6221 - regression_loss: 0.5016 - classification_loss: 0.1204
 2120/10000 [=====>........................] - ETA: 1:00:03 - loss: 0.6219 - regression_loss: 0.5015 - classification_loss: 0.1204
 2121/10000 [=====>........................] - ETA: 1:00:02 - loss: 0.6217 - regression_loss: 0.5013 - classification_loss: 0.1204
 2122/10000 [=====>........................] - ETA: 1:00:02 - loss: 0.6219 - regression_loss: 0.5015 - classification_loss: 0.1204
 2123/10000 [=====>........................] - ETA: 1:00:02 - loss: 0.6224 - regression_loss: 0.5019 - classification_loss: 0.1205
 2124/10000 [=====>........................] - ETA: 1:00:01 - loss: 0.6223 - regression_loss: 0.5018 - classification_loss: 0.1205
 2125/10000 [=====>........................] - ETA: 1:00:01 - loss: 0.6221 - regression_loss: 0.5017 - classification_loss: 0.1205
 2126/10000 [=====>........................] - ETA: 1:00:00 - loss: 0.6222 - regression_loss: 0.5017 - classification_loss: 0.1205
 2127/10000 [=====>........................] - ETA: 1:00:00 - loss: 0.6222 - regression_loss: 0.5017 - classification_loss: 0.1205
 2128/10000 [=====>........................] - ETA: 1:00:00 - loss: 0.6220 - regression_loss: 0.5016 - classification_loss: 0.1204
 2129/10000 [=====>........................] - ETA: 59:59 - loss: 0.6222 - regression_loss: 0.5016 - classification_loss: 0.1205  
 2130/10000 [=====>........................] - ETA: 59:59 - loss: 0.6221 - regression_loss: 0.5016 - classification_loss: 0.1205
 2131/10000 [=====>........................] - ETA: 59:58 - loss: 0.6223 - regression_loss: 0.5018 - classification_loss: 0.1206
 2132/10000 [=====>........................] - ETA: 59:58 - loss: 0.6228 - regression_loss: 0.5021 - classification_loss: 0.1207
 2133/10000 [=====>........................] - ETA: 59:57 - loss: 0.6228 - regression_loss: 0.5021 - classification_loss: 0.1207
 2134/10000 [=====>........................] - ETA: 59:57 - loss: 0.6229 - regression_loss: 0.5021 - classification_loss: 0.1208
 2135/10000 [=====>........................] - ETA: 59:56 - loss: 0.6229 - regression_loss: 0.5021 - classification_loss: 0.1208
 2136/10000 [=====>........................] - ETA: 59:56 - loss: 0.6229 - regression_loss: 0.5021 - classification_loss: 0.1208
 2137/10000 [=====>........................] - ETA: 59:56 - loss: 0.6230 - regression_loss: 0.5022 - classification_loss: 0.1208
 2138/10000 [=====>........................] - ETA: 59:55 - loss: 0.6229 - regression_loss: 0.5021 - classification_loss: 0.1208
 2139/10000 [=====>........................] - ETA: 59:55 - loss: 0.6230 - regression_loss: 0.5021 - classification_loss: 0.1208
 2140/10000 [=====>........................] - ETA: 59:54 - loss: 0.6233 - regression_loss: 0.5024 - classification_loss: 0.1209
 2141/10000 [=====>........................] - ETA: 59:54 - loss: 0.6233 - regression_loss: 0.5025 - classification_loss: 0.1208
 2142/10000 [=====>........................] - ETA: 59:53 - loss: 0.6236 - regression_loss: 0.5028 - classification_loss: 0.1208
 2143/10000 [=====>........................] - ETA: 59:53 - loss: 0.6234 - regression_loss: 0.5026 - classification_loss: 0.1208
 2144/10000 [=====>........................] - ETA: 59:52 - loss: 0.6234 - regression_loss: 0.5026 - classification_loss: 0.1208
 2145/10000 [=====>........................] - ETA: 59:52 - loss: 0.6234 - regression_loss: 0.5026 - classification_loss: 0.1208
 2146/10000 [=====>........................] - ETA: 59:51 - loss: 0.6233 - regression_loss: 0.5026 - classification_loss: 0.1208
 2147/10000 [=====>........................] - ETA: 59:51 - loss: 0.6233 - regression_loss: 0.5026 - classification_loss: 0.1207
 2148/10000 [=====>........................] - ETA: 59:50 - loss: 0.6234 - regression_loss: 0.5027 - classification_loss: 0.1207
 2149/10000 [=====>........................] - ETA: 59:50 - loss: 0.6237 - regression_loss: 0.5029 - classification_loss: 0.1207
 2150/10000 [=====>........................] - ETA: 59:49 - loss: 0.6235 - regression_loss: 0.5028 - classification_loss: 0.1207
 2151/10000 [=====>........................] - ETA: 59:48 - loss: 0.6233 - regression_loss: 0.5026 - classification_loss: 0.1207
 2152/10000 [=====>........................] - ETA: 59:48 - loss: 0.6233 - regression_loss: 0.5025 - classification_loss: 0.1207
 2153/10000 [=====>........................] - ETA: 59:47 - loss: 0.6234 - regression_loss: 0.5026 - classification_loss: 0.1208
 2154/10000 [=====>........................] - ETA: 59:47 - loss: 0.6235 - regression_loss: 0.5027 - classification_loss: 0.1208
 2155/10000 [=====>........................] - ETA: 59:47 - loss: 0.6237 - regression_loss: 0.5029 - classification_loss: 0.1209
 2156/10000 [=====>........................] - ETA: 59:46 - loss: 0.6236 - regression_loss: 0.5027 - classification_loss: 0.1209
 2157/10000 [=====>........................] - ETA: 59:46 - loss: 0.6235 - regression_loss: 0.5026 - classification_loss: 0.1209
 2158/10000 [=====>........................] - ETA: 59:45 - loss: 0.6236 - regression_loss: 0.5028 - classification_loss: 0.1209
 2159/10000 [=====>........................] - ETA: 59:45 - loss: 0.6237 - regression_loss: 0.5029 - classification_loss: 0.1209
 2160/10000 [=====>........................] - ETA: 59:44 - loss: 0.6238 - regression_loss: 0.5030 - classification_loss: 0.1208
 2161/10000 [=====>........................] - ETA: 59:44 - loss: 0.6238 - regression_loss: 0.5029 - classification_loss: 0.1209
 2162/10000 [=====>........................] - ETA: 59:44 - loss: 0.6236 - regression_loss: 0.5028 - classification_loss: 0.1209
 2163/10000 [=====>........................] - ETA: 59:43 - loss: 0.6235 - regression_loss: 0.5027 - classification_loss: 0.1209
 2164/10000 [=====>........................] - ETA: 59:43 - loss: 0.6234 - regression_loss: 0.5025 - classification_loss: 0.1208
 2165/10000 [=====>........................] - ETA: 59:42 - loss: 0.6234 - regression_loss: 0.5025 - classification_loss: 0.1209
 2166/10000 [=====>........................] - ETA: 59:42 - loss: 0.6233 - regression_loss: 0.5025 - classification_loss: 0.1209
 2167/10000 [=====>........................] - ETA: 59:41 - loss: 0.6233 - regression_loss: 0.5024 - classification_loss: 0.1209
 2168/10000 [=====>........................] - ETA: 59:41 - loss: 0.6232 - regression_loss: 0.5023 - classification_loss: 0.1209
 2169/10000 [=====>........................] - ETA: 59:40 - loss: 0.6230 - regression_loss: 0.5022 - classification_loss: 0.1208
 2170/10000 [=====>........................] - ETA: 59:40 - loss: 0.6233 - regression_loss: 0.5024 - classification_loss: 0.1209
 2171/10000 [=====>........................] - ETA: 59:39 - loss: 0.6231 - regression_loss: 0.5022 - classification_loss: 0.1208
 2172/10000 [=====>........................] - ETA: 59:39 - loss: 0.6230 - regression_loss: 0.5022 - classification_loss: 0.1208
 2173/10000 [=====>........................] - ETA: 59:39 - loss: 0.6231 - regression_loss: 0.5022 - classification_loss: 0.1208
 2174/10000 [=====>........................] - ETA: 59:38 - loss: 0.6234 - regression_loss: 0.5026 - classification_loss: 0.1209
 2175/10000 [=====>........................] - ETA: 59:38 - loss: 0.6236 - regression_loss: 0.5027 - classification_loss: 0.1209
 2176/10000 [=====>........................] - ETA: 59:37 - loss: 0.6238 - regression_loss: 0.5028 - classification_loss: 0.1210
 2177/10000 [=====>........................] - ETA: 59:37 - loss: 0.6236 - regression_loss: 0.5026 - classification_loss: 0.1210
 2178/10000 [=====>........................] - ETA: 59:37 - loss: 0.6235 - regression_loss: 0.5025 - classification_loss: 0.1210
 2179/10000 [=====>........................] - ETA: 59:36 - loss: 0.6234 - regression_loss: 0.5025 - classification_loss: 0.1210
 2180/10000 [=====>........................] - ETA: 59:36 - loss: 0.6234 - regression_loss: 0.5024 - classification_loss: 0.1210
 2181/10000 [=====>........................] - ETA: 59:35 - loss: 0.6232 - regression_loss: 0.5022 - classification_loss: 0.1210
 2182/10000 [=====>........................] - ETA: 59:35 - loss: 0.6232 - regression_loss: 0.5022 - classification_loss: 0.1210
 2183/10000 [=====>........................] - ETA: 59:34 - loss: 0.6230 - regression_loss: 0.5020 - classification_loss: 0.1209
 2184/10000 [=====>........................] - ETA: 59:34 - loss: 0.6229 - regression_loss: 0.5019 - classification_loss: 0.1209
 2185/10000 [=====>........................] - ETA: 59:33 - loss: 0.6229 - regression_loss: 0.5019 - classification_loss: 0.1210
 2186/10000 [=====>........................] - ETA: 59:33 - loss: 0.6228 - regression_loss: 0.5018 - classification_loss: 0.1210
 2187/10000 [=====>........................] - ETA: 59:32 - loss: 0.6229 - regression_loss: 0.5019 - classification_loss: 0.1210
 2188/10000 [=====>........................] - ETA: 59:32 - loss: 0.6228 - regression_loss: 0.5018 - classification_loss: 0.1210
 2189/10000 [=====>........................] - ETA: 59:31 - loss: 0.6232 - regression_loss: 0.5022 - classification_loss: 0.1210
 2190/10000 [=====>........................] - ETA: 59:31 - loss: 0.6232 - regression_loss: 0.5022 - classification_loss: 0.1210
 2191/10000 [=====>........................] - ETA: 59:30 - loss: 0.6232 - regression_loss: 0.5022 - classification_loss: 0.1210
 2192/10000 [=====>........................] - ETA: 59:30 - loss: 0.6232 - regression_loss: 0.5022 - classification_loss: 0.1210
 2193/10000 [=====>........................] - ETA: 59:29 - loss: 0.6233 - regression_loss: 0.5023 - classification_loss: 0.1211
 2194/10000 [=====>........................] - ETA: 59:29 - loss: 0.6232 - regression_loss: 0.5022 - classification_loss: 0.1210
 2195/10000 [=====>........................] - ETA: 59:28 - loss: 0.6232 - regression_loss: 0.5022 - classification_loss: 0.1210
 2196/10000 [=====>........................] - ETA: 59:28 - loss: 0.6232 - regression_loss: 0.5022 - classification_loss: 0.1210
 2197/10000 [=====>........................] - ETA: 59:27 - loss: 0.6230 - regression_loss: 0.5021 - classification_loss: 0.1210
 2198/10000 [=====>........................] - ETA: 59:27 - loss: 0.6229 - regression_loss: 0.5020 - classification_loss: 0.1209
 2199/10000 [=====>........................] - ETA: 59:26 - loss: 0.6229 - regression_loss: 0.5020 - classification_loss: 0.1209
 2200/10000 [=====>........................] - ETA: 59:26 - loss: 0.6230 - regression_loss: 0.5021 - classification_loss: 0.1209
 2201/10000 [=====>........................] - ETA: 59:25 - loss: 0.6230 - regression_loss: 0.5020 - classification_loss: 0.1209
 2202/10000 [=====>........................] - ETA: 59:25 - loss: 0.6231 - regression_loss: 0.5021 - classification_loss: 0.1210
 2203/10000 [=====>........................] - ETA: 59:24 - loss: 0.6229 - regression_loss: 0.5019 - classification_loss: 0.1209
 2204/10000 [=====>........................] - ETA: 59:24 - loss: 0.6229 - regression_loss: 0.5020 - classification_loss: 0.1209
 2205/10000 [=====>........................] - ETA: 59:24 - loss: 0.6230 - regression_loss: 0.5021 - classification_loss: 0.1209
 2206/10000 [=====>........................] - ETA: 59:23 - loss: 0.6228 - regression_loss: 0.5020 - classification_loss: 0.1209
 2207/10000 [=====>........................] - ETA: 59:23 - loss: 0.6227 - regression_loss: 0.5018 - classification_loss: 0.1208
 2208/10000 [=====>........................] - ETA: 59:22 - loss: 0.6225 - regression_loss: 0.5017 - classification_loss: 0.1208
 2209/10000 [=====>........................] - ETA: 59:22 - loss: 0.6226 - regression_loss: 0.5018 - classification_loss: 0.1208
 2210/10000 [=====>........................] - ETA: 59:21 - loss: 0.6226 - regression_loss: 0.5018 - classification_loss: 0.1208
 2211/10000 [=====>........................] - ETA: 59:21 - loss: 0.6224 - regression_loss: 0.5017 - classification_loss: 0.1207
 2212/10000 [=====>........................] - ETA: 59:21 - loss: 0.6223 - regression_loss: 0.5016 - classification_loss: 0.1207
 2213/10000 [=====>........................] - ETA: 59:20 - loss: 0.6223 - regression_loss: 0.5016 - classification_loss: 0.1207
 2214/10000 [=====>........................] - ETA: 59:20 - loss: 0.6221 - regression_loss: 0.5015 - classification_loss: 0.1206
 2215/10000 [=====>........................] - ETA: 59:19 - loss: 0.6220 - regression_loss: 0.5014 - classification_loss: 0.1206
 2216/10000 [=====>........................] - ETA: 59:19 - loss: 0.6222 - regression_loss: 0.5016 - classification_loss: 0.1206
 2217/10000 [=====>........................] - ETA: 59:18 - loss: 0.6221 - regression_loss: 0.5015 - classification_loss: 0.1206
 2218/10000 [=====>........................] - ETA: 59:18 - loss: 0.6220 - regression_loss: 0.5014 - classification_loss: 0.1206
 2219/10000 [=====>........................] - ETA: 59:17 - loss: 0.6220 - regression_loss: 0.5014 - classification_loss: 0.1206
 2220/10000 [=====>........................] - ETA: 59:16 - loss: 0.6218 - regression_loss: 0.5012 - classification_loss: 0.1206
 2221/10000 [=====>........................] - ETA: 59:16 - loss: 0.6217 - regression_loss: 0.5011 - classification_loss: 0.1206
 2222/10000 [=====>........................] - ETA: 59:15 - loss: 0.6216 - regression_loss: 0.5011 - classification_loss: 0.1205
 2223/10000 [=====>........................] - ETA: 59:15 - loss: 0.6217 - regression_loss: 0.5011 - classification_loss: 0.1205
 2224/10000 [=====>........................] - ETA: 59:15 - loss: 0.6217 - regression_loss: 0.5012 - classification_loss: 0.1205
 2225/10000 [=====>........................] - ETA: 59:14 - loss: 0.6222 - regression_loss: 0.5016 - classification_loss: 0.1206
 2226/10000 [=====>........................] - ETA: 59:14 - loss: 0.6221 - regression_loss: 0.5016 - classification_loss: 0.1206
 2227/10000 [=====>........................] - ETA: 59:14 - loss: 0.6224 - regression_loss: 0.5018 - classification_loss: 0.1206
 2228/10000 [=====>........................] - ETA: 59:13 - loss: 0.6224 - regression_loss: 0.5018 - classification_loss: 0.1206
 2229/10000 [=====>........................] - ETA: 59:13 - loss: 0.6225 - regression_loss: 0.5018 - classification_loss: 0.1206
 2230/10000 [=====>........................] - ETA: 59:12 - loss: 0.6225 - regression_loss: 0.5019 - classification_loss: 0.1206
 2231/10000 [=====>........................] - ETA: 59:12 - loss: 0.6223 - regression_loss: 0.5017 - classification_loss: 0.1206
 2232/10000 [=====>........................] - ETA: 59:11 - loss: 0.6222 - regression_loss: 0.5016 - classification_loss: 0.1206
 2233/10000 [=====>........................] - ETA: 59:11 - loss: 0.6220 - regression_loss: 0.5014 - classification_loss: 0.1206
 2234/10000 [=====>........................] - ETA: 59:11 - loss: 0.6220 - regression_loss: 0.5014 - classification_loss: 0.1205
 2235/10000 [=====>........................] - ETA: 59:10 - loss: 0.6218 - regression_loss: 0.5013 - classification_loss: 0.1205
 2236/10000 [=====>........................] - ETA: 59:10 - loss: 0.6221 - regression_loss: 0.5015 - classification_loss: 0.1205
 2237/10000 [=====>........................] - ETA: 59:09 - loss: 0.6221 - regression_loss: 0.5015 - classification_loss: 0.1205
 2238/10000 [=====>........................] - ETA: 59:09 - loss: 0.6223 - regression_loss: 0.5018 - classification_loss: 0.1205
 2239/10000 [=====>........................] - ETA: 59:08 - loss: 0.6222 - regression_loss: 0.5017 - classification_loss: 0.1205
 2240/10000 [=====>........................] - ETA: 59:08 - loss: 0.6222 - regression_loss: 0.5017 - classification_loss: 0.1205
 2241/10000 [=====>........................] - ETA: 59:07 - loss: 0.6220 - regression_loss: 0.5015 - classification_loss: 0.1205
 2242/10000 [=====>........................] - ETA: 59:07 - loss: 0.6221 - regression_loss: 0.5017 - classification_loss: 0.1204
 2243/10000 [=====>........................] - ETA: 59:06 - loss: 0.6221 - regression_loss: 0.5017 - classification_loss: 0.1204
 2244/10000 [=====>........................] - ETA: 59:06 - loss: 0.6222 - regression_loss: 0.5017 - classification_loss: 0.1205
 2245/10000 [=====>........................] - ETA: 59:05 - loss: 0.6222 - regression_loss: 0.5017 - classification_loss: 0.1205
 2246/10000 [=====>........................] - ETA: 59:05 - loss: 0.6225 - regression_loss: 0.5020 - classification_loss: 0.1205
 2247/10000 [=====>........................] - ETA: 59:04 - loss: 0.6228 - regression_loss: 0.5023 - classification_loss: 0.1205
 2248/10000 [=====>........................] - ETA: 59:04 - loss: 0.6230 - regression_loss: 0.5025 - classification_loss: 0.1205
 2249/10000 [=====>........................] - ETA: 59:03 - loss: 0.6228 - regression_loss: 0.5023 - classification_loss: 0.1205
 2250/10000 [=====>........................] - ETA: 59:03 - loss: 0.6232 - regression_loss: 0.5027 - classification_loss: 0.1205
 2251/10000 [=====>........................] - ETA: 59:02 - loss: 0.6230 - regression_loss: 0.5025 - classification_loss: 0.1205
 2252/10000 [=====>........................] - ETA: 59:02 - loss: 0.6231 - regression_loss: 0.5027 - classification_loss: 0.1205
 2253/10000 [=====>........................] - ETA: 59:01 - loss: 0.6232 - regression_loss: 0.5027 - classification_loss: 0.1205
 2254/10000 [=====>........................] - ETA: 59:01 - loss: 0.6233 - regression_loss: 0.5029 - classification_loss: 0.1205
 2255/10000 [=====>........................] - ETA: 59:00 - loss: 0.6231 - regression_loss: 0.5027 - classification_loss: 0.1204
 2256/10000 [=====>........................] - ETA: 59:00 - loss: 0.6232 - regression_loss: 0.5027 - classification_loss: 0.1204
 2257/10000 [=====>........................] - ETA: 58:59 - loss: 0.6230 - regression_loss: 0.5026 - classification_loss: 0.1204
 2258/10000 [=====>........................] - ETA: 58:59 - loss: 0.6230 - regression_loss: 0.5026 - classification_loss: 0.1204
 2259/10000 [=====>........................] - ETA: 58:58 - loss: 0.6229 - regression_loss: 0.5026 - classification_loss: 0.1204
 2260/10000 [=====>........................] - ETA: 58:58 - loss: 0.6232 - regression_loss: 0.5028 - classification_loss: 0.1204
 2261/10000 [=====>........................] - ETA: 58:57 - loss: 0.6231 - regression_loss: 0.5027 - classification_loss: 0.1204
 2262/10000 [=====>........................] - ETA: 58:57 - loss: 0.6229 - regression_loss: 0.5026 - classification_loss: 0.1204
 2263/10000 [=====>........................] - ETA: 58:56 - loss: 0.6231 - regression_loss: 0.5027 - classification_loss: 0.1204
 2264/10000 [=====>........................] - ETA: 58:56 - loss: 0.6229 - regression_loss: 0.5026 - classification_loss: 0.1203
 2265/10000 [=====>........................] - ETA: 58:56 - loss: 0.6231 - regression_loss: 0.5027 - classification_loss: 0.1204
 2266/10000 [=====>........................] - ETA: 58:55 - loss: 0.6231 - regression_loss: 0.5027 - classification_loss: 0.1204
 2267/10000 [=====>........................] - ETA: 58:55 - loss: 0.6229 - regression_loss: 0.5026 - classification_loss: 0.1204
 2268/10000 [=====>........................] - ETA: 58:54 - loss: 0.6230 - regression_loss: 0.5026 - classification_loss: 0.1204
 2269/10000 [=====>........................] - ETA: 58:54 - loss: 0.6229 - regression_loss: 0.5026 - classification_loss: 0.1203
 2270/10000 [=====>........................] - ETA: 58:53 - loss: 0.6227 - regression_loss: 0.5024 - classification_loss: 0.1203
 2271/10000 [=====>........................] - ETA: 58:53 - loss: 0.6226 - regression_loss: 0.5023 - classification_loss: 0.1203
 2272/10000 [=====>........................] - ETA: 58:52 - loss: 0.6227 - regression_loss: 0.5024 - classification_loss: 0.1203
 2273/10000 [=====>........................] - ETA: 58:52 - loss: 0.6225 - regression_loss: 0.5022 - classification_loss: 0.1202
 2274/10000 [=====>........................] - ETA: 58:51 - loss: 0.6227 - regression_loss: 0.5024 - classification_loss: 0.1203
 2275/10000 [=====>........................] - ETA: 58:51 - loss: 0.6227 - regression_loss: 0.5024 - classification_loss: 0.1203
 2276/10000 [=====>........................] - ETA: 58:50 - loss: 0.6229 - regression_loss: 0.5026 - classification_loss: 0.1203
 2277/10000 [=====>........................] - ETA: 58:50 - loss: 0.6234 - regression_loss: 0.5031 - classification_loss: 0.1203
 2278/10000 [=====>........................] - ETA: 58:49 - loss: 0.6234 - regression_loss: 0.5031 - classification_loss: 0.1203
 2279/10000 [=====>........................] - ETA: 58:49 - loss: 0.6234 - regression_loss: 0.5031 - classification_loss: 0.1203
 2280/10000 [=====>........................] - ETA: 58:48 - loss: 0.6236 - regression_loss: 0.5033 - classification_loss: 0.1204
 2281/10000 [=====>........................] - ETA: 58:48 - loss: 0.6236 - regression_loss: 0.5033 - classification_loss: 0.1203
 2282/10000 [=====>........................] - ETA: 58:47 - loss: 0.6238 - regression_loss: 0.5035 - classification_loss: 0.1203
 2283/10000 [=====>........................] - ETA: 58:47 - loss: 0.6239 - regression_loss: 0.5036 - classification_loss: 0.1203
 2284/10000 [=====>........................] - ETA: 58:46 - loss: 0.6239 - regression_loss: 0.5036 - classification_loss: 0.1203
 2285/10000 [=====>........................] - ETA: 58:46 - loss: 0.6238 - regression_loss: 0.5035 - classification_loss: 0.1203
 2286/10000 [=====>........................] - ETA: 58:45 - loss: 0.6238 - regression_loss: 0.5035 - classification_loss: 0.1203
 2287/10000 [=====>........................] - ETA: 58:45 - loss: 0.6240 - regression_loss: 0.5037 - classification_loss: 0.1203
 2288/10000 [=====>........................] - ETA: 58:45 - loss: 0.6238 - regression_loss: 0.5035 - classification_loss: 0.1203
 2289/10000 [=====>........................] - ETA: 58:44 - loss: 0.6238 - regression_loss: 0.5035 - classification_loss: 0.1203
 2290/10000 [=====>........................] - ETA: 58:44 - loss: 0.6239 - regression_loss: 0.5036 - classification_loss: 0.1203
 2291/10000 [=====>........................] - ETA: 58:43 - loss: 0.6238 - regression_loss: 0.5036 - classification_loss: 0.1202
 2292/10000 [=====>........................] - ETA: 58:43 - loss: 0.6237 - regression_loss: 0.5035 - classification_loss: 0.1202
 2293/10000 [=====>........................] - ETA: 58:43 - loss: 0.6237 - regression_loss: 0.5035 - classification_loss: 0.1202
 2294/10000 [=====>........................] - ETA: 58:42 - loss: 0.6239 - regression_loss: 0.5036 - classification_loss: 0.1203
 2295/10000 [=====>........................] - ETA: 58:42 - loss: 0.6238 - regression_loss: 0.5035 - classification_loss: 0.1202
 2296/10000 [=====>........................] - ETA: 58:41 - loss: 0.6237 - regression_loss: 0.5035 - classification_loss: 0.1202
 2297/10000 [=====>........................] - ETA: 58:41 - loss: 0.6237 - regression_loss: 0.5035 - classification_loss: 0.1202
 2298/10000 [=====>........................] - ETA: 58:40 - loss: 0.6242 - regression_loss: 0.5038 - classification_loss: 0.1204
 2299/10000 [=====>........................] - ETA: 58:40 - loss: 0.6241 - regression_loss: 0.5037 - classification_loss: 0.1204
 2300/10000 [=====>........................] - ETA: 58:40 - loss: 0.6240 - regression_loss: 0.5036 - classification_loss: 0.1204
 2301/10000 [=====>........................] - ETA: 58:39 - loss: 0.6238 - regression_loss: 0.5035 - classification_loss: 0.1204
 2302/10000 [=====>........................] - ETA: 58:39 - loss: 0.6238 - regression_loss: 0.5034 - classification_loss: 0.1204
 2303/10000 [=====>........................] - ETA: 58:38 - loss: 0.6237 - regression_loss: 0.5033 - classification_loss: 0.1203
 2304/10000 [=====>........................] - ETA: 58:38 - loss: 0.6235 - regression_loss: 0.5032 - classification_loss: 0.1203
 2305/10000 [=====>........................] - ETA: 58:37 - loss: 0.6237 - regression_loss: 0.5033 - classification_loss: 0.1204
 2306/10000 [=====>........................] - ETA: 58:37 - loss: 0.6237 - regression_loss: 0.5034 - classification_loss: 0.1204
 2307/10000 [=====>........................] - ETA: 58:36 - loss: 0.6238 - regression_loss: 0.5034 - classification_loss: 0.1203
 2308/10000 [=====>........................] - ETA: 58:36 - loss: 0.6238 - regression_loss: 0.5034 - classification_loss: 0.1203
 2309/10000 [=====>........................] - ETA: 58:35 - loss: 0.6236 - regression_loss: 0.5033 - classification_loss: 0.1203
 2310/10000 [=====>........................] - ETA: 58:35 - loss: 0.6234 - regression_loss: 0.5032 - classification_loss: 0.1202
 2311/10000 [=====>........................] - ETA: 58:34 - loss: 0.6237 - regression_loss: 0.5035 - classification_loss: 0.1202
 2312/10000 [=====>........................] - ETA: 58:34 - loss: 0.6235 - regression_loss: 0.5033 - classification_loss: 0.1202
 2313/10000 [=====>........................] - ETA: 58:33 - loss: 0.6233 - regression_loss: 0.5032 - classification_loss: 0.1202
 2314/10000 [=====>........................] - ETA: 58:33 - loss: 0.6233 - regression_loss: 0.5031 - classification_loss: 0.1202
 2315/10000 [=====>........................] - ETA: 58:33 - loss: 0.6233 - regression_loss: 0.5029 - classification_loss: 0.1204
 2316/10000 [=====>........................] - ETA: 58:32 - loss: 0.6235 - regression_loss: 0.5031 - classification_loss: 0.1204
 2317/10000 [=====>........................] - ETA: 58:32 - loss: 0.6235 - regression_loss: 0.5030 - classification_loss: 0.1204
 2318/10000 [=====>........................] - ETA: 58:31 - loss: 0.6235 - regression_loss: 0.5030 - classification_loss: 0.1204
 2319/10000 [=====>........................] - ETA: 58:31 - loss: 0.6233 - regression_loss: 0.5029 - classification_loss: 0.1204
 2320/10000 [=====>........................] - ETA: 58:30 - loss: 0.6233 - regression_loss: 0.5029 - classification_loss: 0.1204
 2321/10000 [=====>........................] - ETA: 58:30 - loss: 0.6232 - regression_loss: 0.5028 - classification_loss: 0.1204
 2322/10000 [=====>........................] - ETA: 58:29 - loss: 0.6236 - regression_loss: 0.5032 - classification_loss: 0.1204
 2323/10000 [=====>........................] - ETA: 58:29 - loss: 0.6238 - regression_loss: 0.5034 - classification_loss: 0.1204
 2324/10000 [=====>........................] - ETA: 58:28 - loss: 0.6238 - regression_loss: 0.5033 - classification_loss: 0.1205
 2325/10000 [=====>........................] - ETA: 58:28 - loss: 0.6238 - regression_loss: 0.5033 - classification_loss: 0.1205
 2326/10000 [=====>........................] - ETA: 58:27 - loss: 0.6236 - regression_loss: 0.5032 - classification_loss: 0.1204
 2327/10000 [=====>........................] - ETA: 58:27 - loss: 0.6236 - regression_loss: 0.5031 - classification_loss: 0.1204
 2328/10000 [=====>........................] - ETA: 58:27 - loss: 0.6236 - regression_loss: 0.5032 - classification_loss: 0.1204
 2329/10000 [=====>........................] - ETA: 58:26 - loss: 0.6235 - regression_loss: 0.5031 - classification_loss: 0.1204
 2330/10000 [=====>........................] - ETA: 58:26 - loss: 0.6233 - regression_loss: 0.5030 - classification_loss: 0.1203
 2331/10000 [=====>........................] - ETA: 58:25 - loss: 0.6237 - regression_loss: 0.5033 - classification_loss: 0.1204
 2332/10000 [=====>........................] - ETA: 58:25 - loss: 0.6236 - regression_loss: 0.5032 - classification_loss: 0.1203
 2333/10000 [=====>........................] - ETA: 58:24 - loss: 0.6236 - regression_loss: 0.5031 - classification_loss: 0.1205
 2334/10000 [======>.......................] - ETA: 58:24 - loss: 0.6235 - regression_loss: 0.5031 - classification_loss: 0.1204
 2335/10000 [======>.......................] - ETA: 58:24 - loss: 0.6235 - regression_loss: 0.5031 - classification_loss: 0.1204
 2336/10000 [======>.......................] - ETA: 58:23 - loss: 0.6233 - regression_loss: 0.5030 - classification_loss: 0.1204
 2337/10000 [======>.......................] - ETA: 58:23 - loss: 0.6234 - regression_loss: 0.5031 - classification_loss: 0.1204
 2338/10000 [======>.......................] - ETA: 58:22 - loss: 0.6234 - regression_loss: 0.5031 - classification_loss: 0.1204
 2339/10000 [======>.......................] - ETA: 58:22 - loss: 0.6233 - regression_loss: 0.5030 - classification_loss: 0.1203
 2340/10000 [======>.......................] - ETA: 58:22 - loss: 0.6231 - regression_loss: 0.5028 - classification_loss: 0.1203
 2341/10000 [======>.......................] - ETA: 58:21 - loss: 0.6229 - regression_loss: 0.5026 - classification_loss: 0.1203
 2342/10000 [======>.......................] - ETA: 58:21 - loss: 0.6229 - regression_loss: 0.5026 - classification_loss: 0.1203
 2343/10000 [======>.......................] - ETA: 58:20 - loss: 0.6227 - regression_loss: 0.5025 - classification_loss: 0.1203
 2344/10000 [======>.......................] - ETA: 58:20 - loss: 0.6228 - regression_loss: 0.5025 - classification_loss: 0.1202
 2345/10000 [======>.......................] - ETA: 58:19 - loss: 0.6231 - regression_loss: 0.5028 - classification_loss: 0.1203
 2346/10000 [======>.......................] - ETA: 58:19 - loss: 0.6234 - regression_loss: 0.5030 - classification_loss: 0.1204
 2347/10000 [======>.......................] - ETA: 58:18 - loss: 0.6233 - regression_loss: 0.5030 - classification_loss: 0.1203
 2348/10000 [======>.......................] - ETA: 58:17 - loss: 0.6235 - regression_loss: 0.5031 - classification_loss: 0.1204
 2349/10000 [======>.......................] - ETA: 58:17 - loss: 0.6233 - regression_loss: 0.5030 - classification_loss: 0.1203
 2350/10000 [======>.......................] - ETA: 58:17 - loss: 0.6236 - regression_loss: 0.5032 - classification_loss: 0.1204
 2351/10000 [======>.......................] - ETA: 58:16 - loss: 0.6237 - regression_loss: 0.5033 - classification_loss: 0.1204
 2352/10000 [======>.......................] - ETA: 58:16 - loss: 0.6237 - regression_loss: 0.5033 - classification_loss: 0.1204
 2353/10000 [======>.......................] - ETA: 58:15 - loss: 0.6235 - regression_loss: 0.5031 - classification_loss: 0.1204
 2354/10000 [======>.......................] - ETA: 58:15 - loss: 0.6235 - regression_loss: 0.5032 - classification_loss: 0.1204
 2355/10000 [======>.......................] - ETA: 58:14 - loss: 0.6236 - regression_loss: 0.5031 - classification_loss: 0.1204
 2356/10000 [======>.......................] - ETA: 58:14 - loss: 0.6236 - regression_loss: 0.5031 - classification_loss: 0.1204
 2357/10000 [======>.......................] - ETA: 58:13 - loss: 0.6236 - regression_loss: 0.5031 - classification_loss: 0.1204
 2358/10000 [======>.......................] - ETA: 58:13 - loss: 0.6238 - regression_loss: 0.5033 - classification_loss: 0.1205
 2359/10000 [======>.......................] - ETA: 58:13 - loss: 0.6238 - regression_loss: 0.5033 - classification_loss: 0.1204
 2360/10000 [======>.......................] - ETA: 58:12 - loss: 0.6236 - regression_loss: 0.5032 - classification_loss: 0.1204
 2361/10000 [======>.......................] - ETA: 58:12 - loss: 0.6239 - regression_loss: 0.5034 - classification_loss: 0.1205
 2362/10000 [======>.......................] - ETA: 58:11 - loss: 0.6237 - regression_loss: 0.5032 - classification_loss: 0.1205
 2363/10000 [======>.......................] - ETA: 58:11 - loss: 0.6238 - regression_loss: 0.5033 - classification_loss: 0.1205
 2364/10000 [======>.......................] - ETA: 58:10 - loss: 0.6239 - regression_loss: 0.5033 - classification_loss: 0.1206
 2365/10000 [======>.......................] - ETA: 58:10 - loss: 0.6241 - regression_loss: 0.5035 - classification_loss: 0.1206
 2366/10000 [======>.......................] - ETA: 58:09 - loss: 0.6241 - regression_loss: 0.5036 - classification_loss: 0.1206
 2367/10000 [======>.......................] - ETA: 58:09 - loss: 0.6241 - regression_loss: 0.5035 - classification_loss: 0.1206
 2368/10000 [======>.......................] - ETA: 58:08 - loss: 0.6247 - regression_loss: 0.5040 - classification_loss: 0.1206
 2369/10000 [======>.......................] - ETA: 58:08 - loss: 0.6248 - regression_loss: 0.5041 - classification_loss: 0.1206
 2370/10000 [======>.......................] - ETA: 58:07 - loss: 0.6250 - regression_loss: 0.5043 - classification_loss: 0.1207
 2371/10000 [======>.......................] - ETA: 58:07 - loss: 0.6251 - regression_loss: 0.5044 - classification_loss: 0.1207
 2372/10000 [======>.......................] - ETA: 58:07 - loss: 0.6253 - regression_loss: 0.5046 - classification_loss: 0.1207
 2373/10000 [======>.......................] - ETA: 58:06 - loss: 0.6253 - regression_loss: 0.5045 - classification_loss: 0.1208
 2374/10000 [======>.......................] - ETA: 58:06 - loss: 0.6253 - regression_loss: 0.5045 - classification_loss: 0.1208
 2375/10000 [======>.......................] - ETA: 58:05 - loss: 0.6253 - regression_loss: 0.5046 - classification_loss: 0.1208
 2376/10000 [======>.......................] - ETA: 58:05 - loss: 0.6252 - regression_loss: 0.5045 - classification_loss: 0.1208
 2377/10000 [======>.......................] - ETA: 58:04 - loss: 0.6251 - regression_loss: 0.5044 - classification_loss: 0.1207
 2378/10000 [======>.......................] - ETA: 58:04 - loss: 0.6250 - regression_loss: 0.5043 - classification_loss: 0.1207
 2379/10000 [======>.......................] - ETA: 58:03 - loss: 0.6251 - regression_loss: 0.5044 - classification_loss: 0.1208
 2380/10000 [======>.......................] - ETA: 58:03 - loss: 0.6250 - regression_loss: 0.5043 - classification_loss: 0.1207
 2381/10000 [======>.......................] - ETA: 58:02 - loss: 0.6249 - regression_loss: 0.5042 - classification_loss: 0.1207
 2382/10000 [======>.......................] - ETA: 58:01 - loss: 0.6254 - regression_loss: 0.5046 - classification_loss: 0.1208
 2383/10000 [======>.......................] - ETA: 58:01 - loss: 0.6254 - regression_loss: 0.5044 - classification_loss: 0.1210
 2384/10000 [======>.......................] - ETA: 58:01 - loss: 0.6257 - regression_loss: 0.5047 - classification_loss: 0.1210
 2385/10000 [======>.......................] - ETA: 58:00 - loss: 0.6260 - regression_loss: 0.5050 - classification_loss: 0.1210
 2386/10000 [======>.......................] - ETA: 58:00 - loss: 0.6262 - regression_loss: 0.5051 - classification_loss: 0.1211
 2387/10000 [======>.......................] - ETA: 57:59 - loss: 0.6260 - regression_loss: 0.5050 - classification_loss: 0.1210
 2388/10000 [======>.......................] - ETA: 57:59 - loss: 0.6260 - regression_loss: 0.5050 - classification_loss: 0.1210
 2389/10000 [======>.......................] - ETA: 57:58 - loss: 0.6260 - regression_loss: 0.5050 - classification_loss: 0.1210
 2390/10000 [======>.......................] - ETA: 57:58 - loss: 0.6262 - regression_loss: 0.5052 - classification_loss: 0.1210
 2391/10000 [======>.......................] - ETA: 57:57 - loss: 0.6262 - regression_loss: 0.5052 - classification_loss: 0.1210
 2392/10000 [======>.......................] - ETA: 57:57 - loss: 0.6264 - regression_loss: 0.5053 - classification_loss: 0.1211
 2393/10000 [======>.......................] - ETA: 57:56 - loss: 0.6265 - regression_loss: 0.5054 - classification_loss: 0.1211
 2394/10000 [======>.......................] - ETA: 57:56 - loss: 0.6274 - regression_loss: 0.5059 - classification_loss: 0.1216
 2395/10000 [======>.......................] - ETA: 57:55 - loss: 0.6275 - regression_loss: 0.5060 - classification_loss: 0.1216
 2396/10000 [======>.......................] - ETA: 57:55 - loss: 0.6275 - regression_loss: 0.5059 - classification_loss: 0.1215
 2397/10000 [======>.......................] - ETA: 57:55 - loss: 0.6274 - regression_loss: 0.5059 - classification_loss: 0.1215
 2398/10000 [======>.......................] - ETA: 57:54 - loss: 0.6274 - regression_loss: 0.5058 - classification_loss: 0.1215
 2399/10000 [======>.......................] - ETA: 57:54 - loss: 0.6272 - regression_loss: 0.5057 - classification_loss: 0.1215
 2400/10000 [======>.......................] - ETA: 57:53 - loss: 0.6273 - regression_loss: 0.5058 - classification_loss: 0.1215
 2401/10000 [======>.......................] - ETA: 57:53 - loss: 0.6276 - regression_loss: 0.5060 - classification_loss: 0.1216
 2402/10000 [======>.......................] - ETA: 57:52 - loss: 0.6274 - regression_loss: 0.5059 - classification_loss: 0.1215
 2403/10000 [======>.......................] - ETA: 57:52 - loss: 0.6276 - regression_loss: 0.5060 - classification_loss: 0.1216
 2404/10000 [======>.......................] - ETA: 57:51 - loss: 0.6275 - regression_loss: 0.5060 - classification_loss: 0.1216
 2405/10000 [======>.......................] - ETA: 57:51 - loss: 0.6274 - regression_loss: 0.5058 - classification_loss: 0.1216
 2406/10000 [======>.......................] - ETA: 57:50 - loss: 0.6273 - regression_loss: 0.5057 - classification_loss: 0.1216
 2407/10000 [======>.......................] - ETA: 57:50 - loss: 0.6271 - regression_loss: 0.5056 - classification_loss: 0.1216
 2408/10000 [======>.......................] - ETA: 57:50 - loss: 0.6271 - regression_loss: 0.5055 - classification_loss: 0.1216
 2409/10000 [======>.......................] - ETA: 57:49 - loss: 0.6270 - regression_loss: 0.5055 - classification_loss: 0.1215
 2410/10000 [======>.......................] - ETA: 57:48 - loss: 0.6269 - regression_loss: 0.5054 - classification_loss: 0.1215
 2411/10000 [======>.......................] - ETA: 57:48 - loss: 0.6268 - regression_loss: 0.5053 - classification_loss: 0.1215
 2412/10000 [======>.......................] - ETA: 57:47 - loss: 0.6266 - regression_loss: 0.5052 - classification_loss: 0.1215
 2413/10000 [======>.......................] - ETA: 57:47 - loss: 0.6268 - regression_loss: 0.5053 - classification_loss: 0.1215
 2414/10000 [======>.......................] - ETA: 57:46 - loss: 0.6268 - regression_loss: 0.5053 - classification_loss: 0.1215
 2415/10000 [======>.......................] - ETA: 57:46 - loss: 0.6271 - regression_loss: 0.5056 - classification_loss: 0.1215
 2416/10000 [======>.......................] - ETA: 57:46 - loss: 0.6269 - regression_loss: 0.5054 - classification_loss: 0.1215
 2417/10000 [======>.......................] - ETA: 57:45 - loss: 0.6268 - regression_loss: 0.5053 - classification_loss: 0.1215
 2418/10000 [======>.......................] - ETA: 57:45 - loss: 0.6270 - regression_loss: 0.5056 - classification_loss: 0.1215
 2419/10000 [======>.......................] - ETA: 57:44 - loss: 0.6269 - regression_loss: 0.5054 - classification_loss: 0.1215
 2420/10000 [======>.......................] - ETA: 57:44 - loss: 0.6268 - regression_loss: 0.5053 - classification_loss: 0.1214
 2421/10000 [======>.......................] - ETA: 57:43 - loss: 0.6268 - regression_loss: 0.5054 - classification_loss: 0.1214
 2422/10000 [======>.......................] - ETA: 57:43 - loss: 0.6267 - regression_loss: 0.5053 - classification_loss: 0.1214
 2423/10000 [======>.......................] - ETA: 57:42 - loss: 0.6269 - regression_loss: 0.5055 - classification_loss: 0.1215
 2424/10000 [======>.......................] - ETA: 57:42 - loss: 0.6268 - regression_loss: 0.5054 - classification_loss: 0.1214
 2425/10000 [======>.......................] - ETA: 57:41 - loss: 0.6267 - regression_loss: 0.5053 - classification_loss: 0.1214
 2426/10000 [======>.......................] - ETA: 57:41 - loss: 0.6270 - regression_loss: 0.5055 - classification_loss: 0.1215
 2427/10000 [======>.......................] - ETA: 57:40 - loss: 0.6271 - regression_loss: 0.5056 - classification_loss: 0.1215
 2428/10000 [======>.......................] - ETA: 57:40 - loss: 0.6269 - regression_loss: 0.5054 - classification_loss: 0.1215
 2429/10000 [======>.......................] - ETA: 57:39 - loss: 0.6269 - regression_loss: 0.5055 - classification_loss: 0.1215
 2430/10000 [======>.......................] - ETA: 57:39 - loss: 0.6270 - regression_loss: 0.5056 - classification_loss: 0.1215
 2431/10000 [======>.......................] - ETA: 57:39 - loss: 0.6268 - regression_loss: 0.5054 - classification_loss: 0.1214
 2432/10000 [======>.......................] - ETA: 57:38 - loss: 0.6267 - regression_loss: 0.5053 - classification_loss: 0.1214
 2433/10000 [======>.......................] - ETA: 57:38 - loss: 0.6269 - regression_loss: 0.5055 - classification_loss: 0.1214
 2434/10000 [======>.......................] - ETA: 57:37 - loss: 0.6267 - regression_loss: 0.5053 - classification_loss: 0.1214
 2435/10000 [======>.......................] - ETA: 57:37 - loss: 0.6266 - regression_loss: 0.5053 - classification_loss: 0.1214
 2436/10000 [======>.......................] - ETA: 57:36 - loss: 0.6267 - regression_loss: 0.5053 - classification_loss: 0.1213
 2437/10000 [======>.......................] - ETA: 57:36 - loss: 0.6268 - regression_loss: 0.5055 - classification_loss: 0.1214
 2438/10000 [======>.......................] - ETA: 57:35 - loss: 0.6270 - regression_loss: 0.5056 - classification_loss: 0.1214
 2439/10000 [======>.......................] - ETA: 57:35 - loss: 0.6271 - regression_loss: 0.5057 - classification_loss: 0.1215
 2440/10000 [======>.......................] - ETA: 57:34 - loss: 0.6272 - regression_loss: 0.5058 - classification_loss: 0.1215
 2441/10000 [======>.......................] - ETA: 57:34 - loss: 0.6273 - regression_loss: 0.5058 - classification_loss: 0.1215
 2442/10000 [======>.......................] - ETA: 57:34 - loss: 0.6275 - regression_loss: 0.5060 - classification_loss: 0.1215
 2443/10000 [======>.......................] - ETA: 57:33 - loss: 0.6277 - regression_loss: 0.5062 - classification_loss: 0.1215
 2444/10000 [======>.......................] - ETA: 57:33 - loss: 0.6277 - regression_loss: 0.5062 - classification_loss: 0.1215
 2445/10000 [======>.......................] - ETA: 57:32 - loss: 0.6277 - regression_loss: 0.5062 - classification_loss: 0.1215
 2446/10000 [======>.......................] - ETA: 57:32 - loss: 0.6278 - regression_loss: 0.5064 - classification_loss: 0.1215
 2447/10000 [======>.......................] - ETA: 57:31 - loss: 0.6277 - regression_loss: 0.5063 - classification_loss: 0.1214
 2448/10000 [======>.......................] - ETA: 57:30 - loss: 0.6277 - regression_loss: 0.5062 - classification_loss: 0.1214
 2449/10000 [======>.......................] - ETA: 57:30 - loss: 0.6275 - regression_loss: 0.5061 - classification_loss: 0.1214
 2450/10000 [======>.......................] - ETA: 57:29 - loss: 0.6275 - regression_loss: 0.5060 - classification_loss: 0.1214
 2451/10000 [======>.......................] - ETA: 57:29 - loss: 0.6276 - regression_loss: 0.5061 - classification_loss: 0.1215
 2452/10000 [======>.......................] - ETA: 57:28 - loss: 0.6276 - regression_loss: 0.5061 - classification_loss: 0.1215
 2453/10000 [======>.......................] - ETA: 57:28 - loss: 0.6275 - regression_loss: 0.5060 - classification_loss: 0.1215
 2454/10000 [======>.......................] - ETA: 57:28 - loss: 0.6274 - regression_loss: 0.5059 - classification_loss: 0.1215
 2455/10000 [======>.......................] - ETA: 57:27 - loss: 0.6276 - regression_loss: 0.5061 - classification_loss: 0.1215
 2456/10000 [======>.......................] - ETA: 57:27 - loss: 0.6276 - regression_loss: 0.5061 - classification_loss: 0.1215
 2457/10000 [======>.......................] - ETA: 57:26 - loss: 0.6274 - regression_loss: 0.5059 - classification_loss: 0.1215
 2458/10000 [======>.......................] - ETA: 57:26 - loss: 0.6275 - regression_loss: 0.5060 - classification_loss: 0.1215
 2459/10000 [======>.......................] - ETA: 57:25 - loss: 0.6273 - regression_loss: 0.5059 - classification_loss: 0.1215
 2460/10000 [======>.......................] - ETA: 57:25 - loss: 0.6274 - regression_loss: 0.5059 - classification_loss: 0.1215
 2461/10000 [======>.......................] - ETA: 57:24 - loss: 0.6274 - regression_loss: 0.5059 - classification_loss: 0.1215
 2462/10000 [======>.......................] - ETA: 57:24 - loss: 0.6273 - regression_loss: 0.5058 - classification_loss: 0.1215
 2463/10000 [======>.......................] - ETA: 57:23 - loss: 0.6272 - regression_loss: 0.5057 - classification_loss: 0.1214
 2464/10000 [======>.......................] - ETA: 57:23 - loss: 0.6272 - regression_loss: 0.5058 - classification_loss: 0.1214
 2465/10000 [======>.......................] - ETA: 57:22 - loss: 0.6272 - regression_loss: 0.5058 - classification_loss: 0.1214
 2466/10000 [======>.......................] - ETA: 57:22 - loss: 0.6275 - regression_loss: 0.5061 - classification_loss: 0.1214
 2467/10000 [======>.......................] - ETA: 57:21 - loss: 0.6276 - regression_loss: 0.5062 - classification_loss: 0.1214
 2468/10000 [======>.......................] - ETA: 57:21 - loss: 0.6276 - regression_loss: 0.5062 - classification_loss: 0.1214
 2469/10000 [======>.......................] - ETA: 57:21 - loss: 0.6274 - regression_loss: 0.5060 - classification_loss: 0.1214
 2470/10000 [======>.......................] - ETA: 57:20 - loss: 0.6273 - regression_loss: 0.5059 - classification_loss: 0.1214
 2471/10000 [======>.......................] - ETA: 57:20 - loss: 0.6271 - regression_loss: 0.5057 - classification_loss: 0.1214
 2472/10000 [======>.......................] - ETA: 57:19 - loss: 0.6271 - regression_loss: 0.5057 - classification_loss: 0.1214
 2473/10000 [======>.......................] - ETA: 57:19 - loss: 0.6272 - regression_loss: 0.5058 - classification_loss: 0.1214
 2474/10000 [======>.......................] - ETA: 57:18 - loss: 0.6272 - regression_loss: 0.5059 - classification_loss: 0.1214
 2475/10000 [======>.......................] - ETA: 57:18 - loss: 0.6273 - regression_loss: 0.5060 - classification_loss: 0.1214
 2476/10000 [======>.......................] - ETA: 57:17 - loss: 0.6274 - regression_loss: 0.5060 - classification_loss: 0.1213
 2477/10000 [======>.......................] - ETA: 57:17 - loss: 0.6273 - regression_loss: 0.5059 - classification_loss: 0.1213
 2478/10000 [======>.......................] - ETA: 57:16 - loss: 0.6272 - regression_loss: 0.5059 - classification_loss: 0.1214
 2479/10000 [======>.......................] - ETA: 57:16 - loss: 0.6271 - regression_loss: 0.5058 - classification_loss: 0.1213
 2480/10000 [======>.......................] - ETA: 57:16 - loss: 0.6273 - regression_loss: 0.5059 - classification_loss: 0.1213
 2481/10000 [======>.......................] - ETA: 57:15 - loss: 0.6277 - regression_loss: 0.5063 - classification_loss: 0.1214
 2482/10000 [======>.......................] - ETA: 57:15 - loss: 0.6281 - regression_loss: 0.5066 - classification_loss: 0.1214
 2483/10000 [======>.......................] - ETA: 57:14 - loss: 0.6279 - regression_loss: 0.5065 - classification_loss: 0.1214
 2484/10000 [======>.......................] - ETA: 57:14 - loss: 0.6281 - regression_loss: 0.5066 - classification_loss: 0.1215
 2485/10000 [======>.......................] - ETA: 57:13 - loss: 0.6281 - regression_loss: 0.5066 - classification_loss: 0.1214
 2486/10000 [======>.......................] - ETA: 57:13 - loss: 0.6283 - regression_loss: 0.5068 - classification_loss: 0.1215
 2487/10000 [======>.......................] - ETA: 57:12 - loss: 0.6283 - regression_loss: 0.5068 - classification_loss: 0.1215
 2488/10000 [======>.......................] - ETA: 57:12 - loss: 0.6282 - regression_loss: 0.5067 - classification_loss: 0.1215
 2489/10000 [======>.......................] - ETA: 57:11 - loss: 0.6283 - regression_loss: 0.5068 - classification_loss: 0.1215
 2490/10000 [======>.......................] - ETA: 57:11 - loss: 0.6283 - regression_loss: 0.5069 - classification_loss: 0.1215
 2491/10000 [======>.......................] - ETA: 57:11 - loss: 0.6282 - regression_loss: 0.5067 - classification_loss: 0.1215
 2492/10000 [======>.......................] - ETA: 57:10 - loss: 0.6281 - regression_loss: 0.5067 - classification_loss: 0.1214
 2493/10000 [======>.......................] - ETA: 57:10 - loss: 0.6280 - regression_loss: 0.5066 - classification_loss: 0.1214
 2494/10000 [======>.......................] - ETA: 57:09 - loss: 0.6279 - regression_loss: 0.5064 - classification_loss: 0.1215
 2495/10000 [======>.......................] - ETA: 57:09 - loss: 0.6278 - regression_loss: 0.5064 - classification_loss: 0.1215
 2496/10000 [======>.......................] - ETA: 57:08 - loss: 0.6279 - regression_loss: 0.5065 - classification_loss: 0.1214
 2497/10000 [======>.......................] - ETA: 57:08 - loss: 0.6279 - regression_loss: 0.5065 - classification_loss: 0.1214
 2498/10000 [======>.......................] - ETA: 57:07 - loss: 0.6278 - regression_loss: 0.5064 - classification_loss: 0.1214
 2499/10000 [======>.......................] - ETA: 57:07 - loss: 0.6277 - regression_loss: 0.5063 - classification_loss: 0.1214
 2500/10000 [======>.......................] - ETA: 57:06 - loss: 0.6275 - regression_loss: 0.5062 - classification_loss: 0.1213
 2501/10000 [======>.......................] - ETA: 57:06 - loss: 0.6275 - regression_loss: 0.5061 - classification_loss: 0.1213
 2502/10000 [======>.......................] - ETA: 57:05 - loss: 0.6274 - regression_loss: 0.5061 - classification_loss: 0.1213
 2503/10000 [======>.......................] - ETA: 57:05 - loss: 0.6273 - regression_loss: 0.5060 - classification_loss: 0.1213
 2504/10000 [======>.......................] - ETA: 57:04 - loss: 0.6273 - regression_loss: 0.5060 - classification_loss: 0.1213
 2505/10000 [======>.......................] - ETA: 57:04 - loss: 0.6273 - regression_loss: 0.5060 - classification_loss: 0.1213
 2506/10000 [======>.......................] - ETA: 57:03 - loss: 0.6272 - regression_loss: 0.5059 - classification_loss: 0.1213
 2507/10000 [======>.......................] - ETA: 57:03 - loss: 0.6272 - regression_loss: 0.5059 - classification_loss: 0.1213
 2508/10000 [======>.......................] - ETA: 57:02 - loss: 0.6270 - regression_loss: 0.5057 - classification_loss: 0.1213
 2509/10000 [======>.......................] - ETA: 57:02 - loss: 0.6268 - regression_loss: 0.5056 - classification_loss: 0.1212
 2510/10000 [======>.......................] - ETA: 57:02 - loss: 0.6267 - regression_loss: 0.5055 - classification_loss: 0.1212
 2511/10000 [======>.......................] - ETA: 57:01 - loss: 0.6266 - regression_loss: 0.5054 - classification_loss: 0.1212
 2512/10000 [======>.......................] - ETA: 57:01 - loss: 0.6265 - regression_loss: 0.5054 - classification_loss: 0.1212
 2513/10000 [======>.......................] - ETA: 57:00 - loss: 0.6263 - regression_loss: 0.5052 - classification_loss: 0.1211
 2514/10000 [======>.......................] - ETA: 57:00 - loss: 0.6262 - regression_loss: 0.5050 - classification_loss: 0.1211
 2515/10000 [======>.......................] - ETA: 56:59 - loss: 0.6261 - regression_loss: 0.5049 - classification_loss: 0.1211
 2516/10000 [======>.......................] - ETA: 56:59 - loss: 0.6260 - regression_loss: 0.5049 - classification_loss: 0.1211
 2517/10000 [======>.......................] - ETA: 56:58 - loss: 0.6261 - regression_loss: 0.5049 - classification_loss: 0.1211
 2518/10000 [======>.......................] - ETA: 56:58 - loss: 0.6262 - regression_loss: 0.5051 - classification_loss: 0.1211
 2519/10000 [======>.......................] - ETA: 56:57 - loss: 0.6261 - regression_loss: 0.5050 - classification_loss: 0.1211
 2520/10000 [======>.......................] - ETA: 56:57 - loss: 0.6260 - regression_loss: 0.5049 - classification_loss: 0.1211
 2521/10000 [======>.......................] - ETA: 56:57 - loss: 0.6262 - regression_loss: 0.5051 - classification_loss: 0.1211
 2522/10000 [======>.......................] - ETA: 56:56 - loss: 0.6261 - regression_loss: 0.5050 - classification_loss: 0.1211
 2523/10000 [======>.......................] - ETA: 56:56 - loss: 0.6261 - regression_loss: 0.5051 - classification_loss: 0.1211
 2524/10000 [======>.......................] - ETA: 56:55 - loss: 0.6260 - regression_loss: 0.5050 - classification_loss: 0.1211
 2525/10000 [======>.......................] - ETA: 56:55 - loss: 0.6260 - regression_loss: 0.5050 - classification_loss: 0.1210
 2526/10000 [======>.......................] - ETA: 56:54 - loss: 0.6262 - regression_loss: 0.5051 - classification_loss: 0.1211
 2527/10000 [======>.......................] - ETA: 56:54 - loss: 0.6264 - regression_loss: 0.5053 - classification_loss: 0.1211
 2528/10000 [======>.......................] - ETA: 56:54 - loss: 0.6266 - regression_loss: 0.5055 - classification_loss: 0.1211
 2529/10000 [======>.......................] - ETA: 56:53 - loss: 0.6267 - regression_loss: 0.5056 - classification_loss: 0.1211
 2530/10000 [======>.......................] - ETA: 56:53 - loss: 0.6268 - regression_loss: 0.5056 - classification_loss: 0.1211
 2531/10000 [======>.......................] - ETA: 56:52 - loss: 0.6267 - regression_loss: 0.5056 - classification_loss: 0.1211
 2532/10000 [======>.......................] - ETA: 56:52 - loss: 0.6269 - regression_loss: 0.5058 - classification_loss: 0.1212
 2533/10000 [======>.......................] - ETA: 56:51 - loss: 0.6272 - regression_loss: 0.5060 - classification_loss: 0.1212
 2534/10000 [======>.......................] - ETA: 56:51 - loss: 0.6270 - regression_loss: 0.5059 - classification_loss: 0.1211
 2535/10000 [======>.......................] - ETA: 56:50 - loss: 0.6270 - regression_loss: 0.5059 - classification_loss: 0.1211
 2536/10000 [======>.......................] - ETA: 56:50 - loss: 0.6269 - regression_loss: 0.5058 - classification_loss: 0.1211
 2537/10000 [======>.......................] - ETA: 56:50 - loss: 0.6272 - regression_loss: 0.5060 - classification_loss: 0.1212
 2538/10000 [======>.......................] - ETA: 56:49 - loss: 0.6274 - regression_loss: 0.5062 - classification_loss: 0.1212
 2539/10000 [======>.......................] - ETA: 56:49 - loss: 0.6275 - regression_loss: 0.5063 - classification_loss: 0.1212
 2540/10000 [======>.......................] - ETA: 56:48 - loss: 0.6276 - regression_loss: 0.5064 - classification_loss: 0.1212
 2541/10000 [======>.......................] - ETA: 56:48 - loss: 0.6274 - regression_loss: 0.5062 - classification_loss: 0.1212
 2542/10000 [======>.......................] - ETA: 56:47 - loss: 0.6276 - regression_loss: 0.5064 - classification_loss: 0.1212
 2543/10000 [======>.......................] - ETA: 56:47 - loss: 0.6276 - regression_loss: 0.5064 - classification_loss: 0.1212
 2544/10000 [======>.......................] - ETA: 56:46 - loss: 0.6275 - regression_loss: 0.5063 - classification_loss: 0.1212
 2545/10000 [======>.......................] - ETA: 56:46 - loss: 0.6274 - regression_loss: 0.5062 - classification_loss: 0.1211
 2546/10000 [======>.......................] - ETA: 56:45 - loss: 0.6277 - regression_loss: 0.5065 - classification_loss: 0.1211
 2547/10000 [======>.......................] - ETA: 56:45 - loss: 0.6278 - regression_loss: 0.5066 - classification_loss: 0.1212
 2548/10000 [======>.......................] - ETA: 56:44 - loss: 0.6280 - regression_loss: 0.5068 - classification_loss: 0.1212
 2549/10000 [======>.......................] - ETA: 56:44 - loss: 0.6281 - regression_loss: 0.5068 - classification_loss: 0.1213
 2550/10000 [======>.......................] - ETA: 56:43 - loss: 0.6282 - regression_loss: 0.5069 - classification_loss: 0.1213
 2551/10000 [======>.......................] - ETA: 56:43 - loss: 0.6281 - regression_loss: 0.5068 - classification_loss: 0.1213
 2552/10000 [======>.......................] - ETA: 56:42 - loss: 0.6280 - regression_loss: 0.5068 - classification_loss: 0.1213
 2553/10000 [======>.......................] - ETA: 56:42 - loss: 0.6280 - regression_loss: 0.5067 - classification_loss: 0.1213
 2554/10000 [======>.......................] - ETA: 56:41 - loss: 0.6281 - regression_loss: 0.5068 - classification_loss: 0.1214
 2555/10000 [======>.......................] - ETA: 56:41 - loss: 0.6282 - regression_loss: 0.5069 - classification_loss: 0.1214
 2556/10000 [======>.......................] - ETA: 56:40 - loss: 0.6281 - regression_loss: 0.5067 - classification_loss: 0.1214
 2557/10000 [======>.......................] - ETA: 56:40 - loss: 0.6279 - regression_loss: 0.5066 - classification_loss: 0.1213
 2558/10000 [======>.......................] - ETA: 56:40 - loss: 0.6277 - regression_loss: 0.5064 - classification_loss: 0.1213
 2559/10000 [======>.......................] - ETA: 56:39 - loss: 0.6275 - regression_loss: 0.5062 - classification_loss: 0.1212
 2560/10000 [======>.......................] - ETA: 56:39 - loss: 0.6273 - regression_loss: 0.5061 - classification_loss: 0.1212
 2561/10000 [======>.......................] - ETA: 56:38 - loss: 0.6273 - regression_loss: 0.5061 - classification_loss: 0.1212
 2562/10000 [======>.......................] - ETA: 56:38 - loss: 0.6275 - regression_loss: 0.5062 - classification_loss: 0.1213
 2563/10000 [======>.......................] - ETA: 56:37 - loss: 0.6276 - regression_loss: 0.5063 - classification_loss: 0.1213
 2564/10000 [======>.......................] - ETA: 56:37 - loss: 0.6276 - regression_loss: 0.5063 - classification_loss: 0.1213
 2565/10000 [======>.......................] - ETA: 56:36 - loss: 0.6275 - regression_loss: 0.5063 - classification_loss: 0.1212
 2566/10000 [======>.......................] - ETA: 56:36 - loss: 0.6275 - regression_loss: 0.5063 - classification_loss: 0.1212
 2567/10000 [======>.......................] - ETA: 56:35 - loss: 0.6278 - regression_loss: 0.5065 - classification_loss: 0.1213
 2568/10000 [======>.......................] - ETA: 56:35 - loss: 0.6280 - regression_loss: 0.5067 - classification_loss: 0.1213
 2569/10000 [======>.......................] - ETA: 56:35 - loss: 0.6279 - regression_loss: 0.5067 - classification_loss: 0.1212
 2570/10000 [======>.......................] - ETA: 56:34 - loss: 0.6279 - regression_loss: 0.5067 - classification_loss: 0.1212
 2571/10000 [======>.......................] - ETA: 56:34 - loss: 0.6280 - regression_loss: 0.5068 - classification_loss: 0.1212
 2572/10000 [======>.......................] - ETA: 56:33 - loss: 0.6282 - regression_loss: 0.5069 - classification_loss: 0.1212
 2573/10000 [======>.......................] - ETA: 56:33 - loss: 0.6282 - regression_loss: 0.5069 - classification_loss: 0.1212
 2574/10000 [======>.......................] - ETA: 56:32 - loss: 0.6281 - regression_loss: 0.5069 - classification_loss: 0.1212
 2575/10000 [======>.......................] - ETA: 56:32 - loss: 0.6281 - regression_loss: 0.5069 - classification_loss: 0.1212
 2576/10000 [======>.......................] - ETA: 56:32 - loss: 0.6283 - regression_loss: 0.5070 - classification_loss: 0.1212
 2577/10000 [======>.......................] - ETA: 56:31 - loss: 0.6284 - regression_loss: 0.5071 - classification_loss: 0.1212
 2578/10000 [======>.......................] - ETA: 56:31 - loss: 0.6285 - regression_loss: 0.5072 - classification_loss: 0.1212
 2579/10000 [======>.......................] - ETA: 56:30 - loss: 0.6283 - regression_loss: 0.5071 - classification_loss: 0.1212
 2580/10000 [======>.......................] - ETA: 56:30 - loss: 0.6282 - regression_loss: 0.5071 - classification_loss: 0.1212
 2581/10000 [======>.......................] - ETA: 56:30 - loss: 0.6283 - regression_loss: 0.5071 - classification_loss: 0.1212
 2582/10000 [======>.......................] - ETA: 56:29 - loss: 0.6283 - regression_loss: 0.5071 - classification_loss: 0.1212
 2583/10000 [======>.......................] - ETA: 56:28 - loss: 0.6283 - regression_loss: 0.5070 - classification_loss: 0.1212
 2584/10000 [======>.......................] - ETA: 56:28 - loss: 0.6281 - regression_loss: 0.5069 - classification_loss: 0.1212
 2585/10000 [======>.......................] - ETA: 56:28 - loss: 0.6283 - regression_loss: 0.5071 - classification_loss: 0.1212
 2586/10000 [======>.......................] - ETA: 56:27 - loss: 0.6283 - regression_loss: 0.5070 - classification_loss: 0.1213
 2587/10000 [======>.......................] - ETA: 56:27 - loss: 0.6284 - regression_loss: 0.5072 - classification_loss: 0.1213
 2588/10000 [======>.......................] - ETA: 56:26 - loss: 0.6283 - regression_loss: 0.5070 - classification_loss: 0.1213
 2589/10000 [======>.......................] - ETA: 56:26 - loss: 0.6281 - regression_loss: 0.5069 - classification_loss: 0.1212
 2590/10000 [======>.......................] - ETA: 56:25 - loss: 0.6283 - regression_loss: 0.5070 - classification_loss: 0.1213
 2591/10000 [======>.......................] - ETA: 56:25 - loss: 0.6283 - regression_loss: 0.5070 - classification_loss: 0.1213
 2592/10000 [======>.......................] - ETA: 56:24 - loss: 0.6283 - regression_loss: 0.5070 - classification_loss: 0.1213
 2593/10000 [======>.......................] - ETA: 56:24 - loss: 0.6282 - regression_loss: 0.5069 - classification_loss: 0.1213
 2594/10000 [======>.......................] - ETA: 56:23 - loss: 0.6280 - regression_loss: 0.5067 - classification_loss: 0.1213
 2595/10000 [======>.......................] - ETA: 56:23 - loss: 0.6278 - regression_loss: 0.5065 - classification_loss: 0.1212
 2596/10000 [======>.......................] - ETA: 56:23 - loss: 0.6277 - regression_loss: 0.5065 - classification_loss: 0.1212
 2597/10000 [======>.......................] - ETA: 56:22 - loss: 0.6276 - regression_loss: 0.5064 - classification_loss: 0.1212
 2598/10000 [======>.......................] - ETA: 56:22 - loss: 0.6275 - regression_loss: 0.5063 - classification_loss: 0.1212
 2599/10000 [======>.......................] - ETA: 56:21 - loss: 0.6274 - regression_loss: 0.5062 - classification_loss: 0.1212
 2600/10000 [======>.......................] - ETA: 56:21 - loss: 0.6274 - regression_loss: 0.5062 - classification_loss: 0.1213
 2601/10000 [======>.......................] - ETA: 56:20 - loss: 0.6273 - regression_loss: 0.5061 - classification_loss: 0.1212
 2602/10000 [======>.......................] - ETA: 56:20 - loss: 0.6275 - regression_loss: 0.5063 - classification_loss: 0.1213
 2603/10000 [======>.......................] - ETA: 56:19 - loss: 0.6274 - regression_loss: 0.5062 - classification_loss: 0.1212
 2604/10000 [======>.......................] - ETA: 56:19 - loss: 0.6274 - regression_loss: 0.5061 - classification_loss: 0.1213
 2605/10000 [======>.......................] - ETA: 56:18 - loss: 0.6273 - regression_loss: 0.5060 - classification_loss: 0.1212
 2606/10000 [======>.......................] - ETA: 56:18 - loss: 0.6272 - regression_loss: 0.5060 - classification_loss: 0.1212
 2607/10000 [======>.......................] - ETA: 56:17 - loss: 0.6274 - regression_loss: 0.5062 - classification_loss: 0.1213
 2608/10000 [======>.......................] - ETA: 56:17 - loss: 0.6277 - regression_loss: 0.5064 - classification_loss: 0.1213
 2609/10000 [======>.......................] - ETA: 56:17 - loss: 0.6276 - regression_loss: 0.5064 - classification_loss: 0.1213
 2610/10000 [======>.......................] - ETA: 56:16 - loss: 0.6275 - regression_loss: 0.5063 - classification_loss: 0.1213
 2611/10000 [======>.......................] - ETA: 56:15 - loss: 0.6274 - regression_loss: 0.5062 - classification_loss: 0.1212
 2612/10000 [======>.......................] - ETA: 56:15 - loss: 0.6272 - regression_loss: 0.5060 - classification_loss: 0.1212
 2613/10000 [======>.......................] - ETA: 56:14 - loss: 0.6272 - regression_loss: 0.5060 - classification_loss: 0.1212
 2614/10000 [======>.......................] - ETA: 56:14 - loss: 0.6272 - regression_loss: 0.5060 - classification_loss: 0.1212
 2615/10000 [======>.......................] - ETA: 56:14 - loss: 0.6271 - regression_loss: 0.5059 - classification_loss: 0.1212
 2616/10000 [======>.......................] - ETA: 56:13 - loss: 0.6269 - regression_loss: 0.5058 - classification_loss: 0.1211
 2617/10000 [======>.......................] - ETA: 56:13 - loss: 0.6271 - regression_loss: 0.5059 - classification_loss: 0.1212
 2618/10000 [======>.......................] - ETA: 56:12 - loss: 0.6270 - regression_loss: 0.5058 - classification_loss: 0.1212
 2619/10000 [======>.......................] - ETA: 56:12 - loss: 0.6270 - regression_loss: 0.5057 - classification_loss: 0.1212
 2620/10000 [======>.......................] - ETA: 56:11 - loss: 0.6269 - regression_loss: 0.5057 - classification_loss: 0.1212
 2621/10000 [======>.......................] - ETA: 56:11 - loss: 0.6270 - regression_loss: 0.5057 - classification_loss: 0.1212
 2622/10000 [======>.......................] - ETA: 56:10 - loss: 0.6271 - regression_loss: 0.5059 - classification_loss: 0.1212
 2623/10000 [======>.......................] - ETA: 56:10 - loss: 0.6270 - regression_loss: 0.5058 - classification_loss: 0.1212
 2624/10000 [======>.......................] - ETA: 56:09 - loss: 0.6270 - regression_loss: 0.5058 - classification_loss: 0.1212
 2625/10000 [======>.......................] - ETA: 56:09 - loss: 0.6270 - regression_loss: 0.5058 - classification_loss: 0.1212
 2626/10000 [======>.......................] - ETA: 56:09 - loss: 0.6269 - regression_loss: 0.5057 - classification_loss: 0.1212
 2627/10000 [======>.......................] - ETA: 56:08 - loss: 0.6269 - regression_loss: 0.5057 - classification_loss: 0.1212
 2628/10000 [======>.......................] - ETA: 56:07 - loss: 0.6267 - regression_loss: 0.5056 - classification_loss: 0.1211
 2629/10000 [======>.......................] - ETA: 56:07 - loss: 0.6267 - regression_loss: 0.5055 - classification_loss: 0.1211
 2630/10000 [======>.......................] - ETA: 56:07 - loss: 0.6266 - regression_loss: 0.5055 - classification_loss: 0.1211
 2631/10000 [======>.......................] - ETA: 56:06 - loss: 0.6264 - regression_loss: 0.5053 - classification_loss: 0.1211
 2632/10000 [======>.......................] - ETA: 56:06 - loss: 0.6264 - regression_loss: 0.5053 - classification_loss: 0.1210
 2633/10000 [======>.......................] - ETA: 56:05 - loss: 0.6268 - regression_loss: 0.5057 - classification_loss: 0.1211
 2634/10000 [======>.......................] - ETA: 56:05 - loss: 0.6268 - regression_loss: 0.5057 - classification_loss: 0.1211
 2635/10000 [======>.......................] - ETA: 56:04 - loss: 0.6268 - regression_loss: 0.5057 - classification_loss: 0.1211
 2636/10000 [======>.......................] - ETA: 56:04 - loss: 0.6266 - regression_loss: 0.5056 - classification_loss: 0.1210
 2637/10000 [======>.......................] - ETA: 56:03 - loss: 0.6269 - regression_loss: 0.5059 - classification_loss: 0.1210
 2638/10000 [======>.......................] - ETA: 56:03 - loss: 0.6271 - regression_loss: 0.5061 - classification_loss: 0.1211
 2639/10000 [======>.......................] - ETA: 56:03 - loss: 0.6272 - regression_loss: 0.5062 - classification_loss: 0.1211
 2640/10000 [======>.......................] - ETA: 56:02 - loss: 0.6272 - regression_loss: 0.5061 - classification_loss: 0.1211
 2641/10000 [======>.......................] - ETA: 56:02 - loss: 0.6273 - regression_loss: 0.5062 - classification_loss: 0.1211
 2642/10000 [======>.......................] - ETA: 56:01 - loss: 0.6275 - regression_loss: 0.5063 - classification_loss: 0.1211
 2643/10000 [======>.......................] - ETA: 56:01 - loss: 0.6273 - regression_loss: 0.5061 - classification_loss: 0.1211
 2644/10000 [======>.......................] - ETA: 56:00 - loss: 0.6274 - regression_loss: 0.5063 - classification_loss: 0.1211
 2645/10000 [======>.......................] - ETA: 56:00 - loss: 0.6273 - regression_loss: 0.5062 - classification_loss: 0.1211
 2646/10000 [======>.......................] - ETA: 55:59 - loss: 0.6273 - regression_loss: 0.5062 - classification_loss: 0.1211
 2647/10000 [======>.......................] - ETA: 55:59 - loss: 0.6273 - regression_loss: 0.5062 - classification_loss: 0.1211
 2648/10000 [======>.......................] - ETA: 55:58 - loss: 0.6272 - regression_loss: 0.5062 - classification_loss: 0.1211
 2649/10000 [======>.......................] - ETA: 55:58 - loss: 0.6271 - regression_loss: 0.5061 - classification_loss: 0.1210
 2650/10000 [======>.......................] - ETA: 55:57 - loss: 0.6271 - regression_loss: 0.5060 - classification_loss: 0.1210
 2651/10000 [======>.......................] - ETA: 55:57 - loss: 0.6270 - regression_loss: 0.5060 - classification_loss: 0.1210
 2652/10000 [======>.......................] - ETA: 55:57 - loss: 0.6269 - regression_loss: 0.5059 - classification_loss: 0.1210
 2653/10000 [======>.......................] - ETA: 55:56 - loss: 0.6271 - regression_loss: 0.5060 - classification_loss: 0.1210
 2654/10000 [======>.......................] - ETA: 55:56 - loss: 0.6271 - regression_loss: 0.5061 - classification_loss: 0.1210
 2655/10000 [======>.......................] - ETA: 55:55 - loss: 0.6272 - regression_loss: 0.5062 - classification_loss: 0.1210
 2656/10000 [======>.......................] - ETA: 55:55 - loss: 0.6273 - regression_loss: 0.5063 - classification_loss: 0.1210
 2657/10000 [======>.......................] - ETA: 55:54 - loss: 0.6274 - regression_loss: 0.5064 - classification_loss: 0.1210
 2658/10000 [======>.......................] - ETA: 55:54 - loss: 0.6272 - regression_loss: 0.5062 - classification_loss: 0.1210
 2659/10000 [======>.......................] - ETA: 55:53 - loss: 0.6272 - regression_loss: 0.5062 - classification_loss: 0.1210
 2660/10000 [======>.......................] - ETA: 55:53 - loss: 0.6273 - regression_loss: 0.5063 - classification_loss: 0.1210
 2661/10000 [======>.......................] - ETA: 55:52 - loss: 0.6276 - regression_loss: 0.5066 - classification_loss: 0.1210
 2662/10000 [======>.......................] - ETA: 55:52 - loss: 0.6275 - regression_loss: 0.5065 - classification_loss: 0.1210
 2663/10000 [======>.......................] - ETA: 55:51 - loss: 0.6278 - regression_loss: 0.5067 - classification_loss: 0.1210
 2664/10000 [======>.......................] - ETA: 55:51 - loss: 0.6278 - regression_loss: 0.5067 - classification_loss: 0.1210
 2665/10000 [======>.......................] - ETA: 55:50 - loss: 0.6277 - regression_loss: 0.5067 - classification_loss: 0.1210
 2666/10000 [======>.......................] - ETA: 55:50 - loss: 0.6276 - regression_loss: 0.5066 - classification_loss: 0.1210
 2667/10000 [=======>......................] - ETA: 55:49 - loss: 0.6277 - regression_loss: 0.5067 - classification_loss: 0.1210
 2668/10000 [=======>......................] - ETA: 55:49 - loss: 0.6275 - regression_loss: 0.5065 - classification_loss: 0.1210
 2669/10000 [=======>......................] - ETA: 55:49 - loss: 0.6277 - regression_loss: 0.5067 - classification_loss: 0.1210
 2670/10000 [=======>......................] - ETA: 55:48 - loss: 0.6279 - regression_loss: 0.5068 - classification_loss: 0.1211
 2671/10000 [=======>......................] - ETA: 55:48 - loss: 0.6279 - regression_loss: 0.5068 - classification_loss: 0.1211
 2672/10000 [=======>......................] - ETA: 55:47 - loss: 0.6280 - regression_loss: 0.5069 - classification_loss: 0.1211
 2673/10000 [=======>......................] - ETA: 55:47 - loss: 0.6285 - regression_loss: 0.5073 - classification_loss: 0.1212
 2674/10000 [=======>......................] - ETA: 55:46 - loss: 0.6284 - regression_loss: 0.5072 - classification_loss: 0.1212
 2675/10000 [=======>......................] - ETA: 55:46 - loss: 0.6284 - regression_loss: 0.5072 - classification_loss: 0.1212
 2676/10000 [=======>......................] - ETA: 55:45 - loss: 0.6283 - regression_loss: 0.5072 - classification_loss: 0.1211
 2677/10000 [=======>......................] - ETA: 55:45 - loss: 0.6281 - regression_loss: 0.5070 - classification_loss: 0.1211
 2678/10000 [=======>......................] - ETA: 55:45 - loss: 0.6281 - regression_loss: 0.5070 - classification_loss: 0.1211
 2679/10000 [=======>......................] - ETA: 55:44 - loss: 0.6280 - regression_loss: 0.5069 - classification_loss: 0.1211
 2680/10000 [=======>......................] - ETA: 55:44 - loss: 0.6281 - regression_loss: 0.5070 - classification_loss: 0.1211
 2681/10000 [=======>......................] - ETA: 55:43 - loss: 0.6280 - regression_loss: 0.5069 - classification_loss: 0.1211
 2682/10000 [=======>......................] - ETA: 55:43 - loss: 0.6279 - regression_loss: 0.5068 - classification_loss: 0.1211
 2683/10000 [=======>......................] - ETA: 55:43 - loss: 0.6278 - regression_loss: 0.5067 - classification_loss: 0.1210
 2684/10000 [=======>......................] - ETA: 55:42 - loss: 0.6278 - regression_loss: 0.5068 - classification_loss: 0.1210
 2685/10000 [=======>......................] - ETA: 55:42 - loss: 0.6277 - regression_loss: 0.5067 - classification_loss: 0.1210
 2686/10000 [=======>......................] - ETA: 55:41 - loss: 0.6279 - regression_loss: 0.5068 - classification_loss: 0.1210
 2687/10000 [=======>......................] - ETA: 55:41 - loss: 0.6277 - regression_loss: 0.5067 - classification_loss: 0.1210
 2688/10000 [=======>......................] - ETA: 55:40 - loss: 0.6276 - regression_loss: 0.5066 - classification_loss: 0.1210
 2689/10000 [=======>......................] - ETA: 55:40 - loss: 0.6276 - regression_loss: 0.5066 - classification_loss: 0.1210
 2690/10000 [=======>......................] - ETA: 55:39 - loss: 0.6277 - regression_loss: 0.5067 - classification_loss: 0.1210
 2691/10000 [=======>......................] - ETA: 55:39 - loss: 0.6275 - regression_loss: 0.5066 - classification_loss: 0.1209
 2692/10000 [=======>......................] - ETA: 55:38 - loss: 0.6276 - regression_loss: 0.5067 - classification_loss: 0.1210
 2693/10000 [=======>......................] - ETA: 55:38 - loss: 0.6276 - regression_loss: 0.5066 - classification_loss: 0.1209
 2694/10000 [=======>......................] - ETA: 55:38 - loss: 0.6280 - regression_loss: 0.5070 - classification_loss: 0.1210
 2695/10000 [=======>......................] - ETA: 55:37 - loss: 0.6281 - regression_loss: 0.5070 - classification_loss: 0.1211
 2696/10000 [=======>......................] - ETA: 55:37 - loss: 0.6281 - regression_loss: 0.5070 - classification_loss: 0.1211
 2697/10000 [=======>......................] - ETA: 55:36 - loss: 0.6280 - regression_loss: 0.5070 - classification_loss: 0.1211
 2698/10000 [=======>......................] - ETA: 55:36 - loss: 0.6282 - regression_loss: 0.5071 - classification_loss: 0.1211
 2699/10000 [=======>......................] - ETA: 55:35 - loss: 0.6281 - regression_loss: 0.5070 - classification_loss: 0.1210
 2700/10000 [=======>......................] - ETA: 55:35 - loss: 0.6281 - regression_loss: 0.5070 - classification_loss: 0.1211
 2701/10000 [=======>......................] - ETA: 55:34 - loss: 0.6283 - regression_loss: 0.5072 - classification_loss: 0.1210
 2702/10000 [=======>......................] - ETA: 55:34 - loss: 0.6281 - regression_loss: 0.5071 - classification_loss: 0.1210
 2703/10000 [=======>......................] - ETA: 55:34 - loss: 0.6281 - regression_loss: 0.5071 - classification_loss: 0.1210
 2704/10000 [=======>......................] - ETA: 55:33 - loss: 0.6284 - regression_loss: 0.5074 - classification_loss: 0.1210
 2705/10000 [=======>......................] - ETA: 55:33 - loss: 0.6285 - regression_loss: 0.5074 - classification_loss: 0.1211
 2706/10000 [=======>......................] - ETA: 55:32 - loss: 0.6284 - regression_loss: 0.5074 - classification_loss: 0.1210
 2707/10000 [=======>......................] - ETA: 55:32 - loss: 0.6283 - regression_loss: 0.5073 - classification_loss: 0.1210
 2708/10000 [=======>......................] - ETA: 55:31 - loss: 0.6283 - regression_loss: 0.5073 - classification_loss: 0.1210
 2709/10000 [=======>......................] - ETA: 55:31 - loss: 0.6282 - regression_loss: 0.5072 - classification_loss: 0.1210
 2710/10000 [=======>......................] - ETA: 55:30 - loss: 0.6281 - regression_loss: 0.5071 - classification_loss: 0.1210
 2711/10000 [=======>......................] - ETA: 55:30 - loss: 0.6285 - regression_loss: 0.5074 - classification_loss: 0.1211
 2712/10000 [=======>......................] - ETA: 55:29 - loss: 0.6285 - regression_loss: 0.5075 - classification_loss: 0.1210
 2713/10000 [=======>......................] - ETA: 55:29 - loss: 0.6284 - regression_loss: 0.5074 - classification_loss: 0.1210
 2714/10000 [=======>......................] - ETA: 55:28 - loss: 0.6284 - regression_loss: 0.5074 - classification_loss: 0.1210
 2715/10000 [=======>......................] - ETA: 55:28 - loss: 0.6283 - regression_loss: 0.5073 - classification_loss: 0.1210
 2716/10000 [=======>......................] - ETA: 55:27 - loss: 0.6281 - regression_loss: 0.5071 - classification_loss: 0.1210
 2717/10000 [=======>......................] - ETA: 55:27 - loss: 0.6282 - regression_loss: 0.5072 - classification_loss: 0.1210
 2718/10000 [=======>......................] - ETA: 55:26 - loss: 0.6281 - regression_loss: 0.5071 - classification_loss: 0.1210
 2719/10000 [=======>......................] - ETA: 55:26 - loss: 0.6280 - regression_loss: 0.5070 - classification_loss: 0.1210
 2720/10000 [=======>......................] - ETA: 55:25 - loss: 0.6279 - regression_loss: 0.5070 - classification_loss: 0.1209
 2721/10000 [=======>......................] - ETA: 55:25 - loss: 0.6279 - regression_loss: 0.5069 - classification_loss: 0.1210
 2722/10000 [=======>......................] - ETA: 55:25 - loss: 0.6279 - regression_loss: 0.5069 - classification_loss: 0.1210
 2723/10000 [=======>......................] - ETA: 55:24 - loss: 0.6277 - regression_loss: 0.5068 - classification_loss: 0.1210
 2724/10000 [=======>......................] - ETA: 55:24 - loss: 0.6278 - regression_loss: 0.5068 - classification_loss: 0.1210
 2725/10000 [=======>......................] - ETA: 55:23 - loss: 0.6280 - regression_loss: 0.5071 - classification_loss: 0.1210
 2726/10000 [=======>......................] - ETA: 55:23 - loss: 0.6282 - regression_loss: 0.5072 - classification_loss: 0.1210
 2727/10000 [=======>......................] - ETA: 55:22 - loss: 0.6282 - regression_loss: 0.5072 - classification_loss: 0.1210
 2728/10000 [=======>......................] - ETA: 55:21 - loss: 0.6282 - regression_loss: 0.5072 - classification_loss: 0.1210
 2729/10000 [=======>......................] - ETA: 55:21 - loss: 0.6280 - regression_loss: 0.5070 - classification_loss: 0.1210
 2730/10000 [=======>......................] - ETA: 55:21 - loss: 0.6279 - regression_loss: 0.5070 - classification_loss: 0.1210
 2731/10000 [=======>......................] - ETA: 55:20 - loss: 0.6280 - regression_loss: 0.5070 - classification_loss: 0.1209
 2732/10000 [=======>......................] - ETA: 55:20 - loss: 0.6279 - regression_loss: 0.5070 - classification_loss: 0.1209
 2733/10000 [=======>......................] - ETA: 55:19 - loss: 0.6278 - regression_loss: 0.5069 - classification_loss: 0.1209
 2734/10000 [=======>......................] - ETA: 55:19 - loss: 0.6280 - regression_loss: 0.5071 - classification_loss: 0.1209
 2735/10000 [=======>......................] - ETA: 55:18 - loss: 0.6281 - regression_loss: 0.5072 - classification_loss: 0.1209
 2736/10000 [=======>......................] - ETA: 55:18 - loss: 0.6279 - regression_loss: 0.5070 - classification_loss: 0.1209
 2737/10000 [=======>......................] - ETA: 55:17 - loss: 0.6281 - regression_loss: 0.5072 - classification_loss: 0.1209
 2738/10000 [=======>......................] - ETA: 55:17 - loss: 0.6279 - regression_loss: 0.5070 - classification_loss: 0.1209
 2739/10000 [=======>......................] - ETA: 55:17 - loss: 0.6279 - regression_loss: 0.5070 - classification_loss: 0.1209
 2740/10000 [=======>......................] - ETA: 55:16 - loss: 0.6278 - regression_loss: 0.5069 - classification_loss: 0.1209
 2741/10000 [=======>......................] - ETA: 55:16 - loss: 0.6280 - regression_loss: 0.5071 - classification_loss: 0.1209
 2742/10000 [=======>......................] - ETA: 55:15 - loss: 0.6281 - regression_loss: 0.5072 - classification_loss: 0.1209
 2743/10000 [=======>......................] - ETA: 55:15 - loss: 0.6282 - regression_loss: 0.5073 - classification_loss: 0.1209
 2744/10000 [=======>......................] - ETA: 55:15 - loss: 0.6282 - regression_loss: 0.5073 - classification_loss: 0.1209
 2745/10000 [=======>......................] - ETA: 55:14 - loss: 0.6283 - regression_loss: 0.5074 - classification_loss: 0.1209
 2746/10000 [=======>......................] - ETA: 55:14 - loss: 0.6281 - regression_loss: 0.5073 - classification_loss: 0.1209
 2747/10000 [=======>......................] - ETA: 55:13 - loss: 0.6283 - regression_loss: 0.5074 - classification_loss: 0.1209
 2748/10000 [=======>......................] - ETA: 55:13 - loss: 0.6283 - regression_loss: 0.5073 - classification_loss: 0.1209
 2749/10000 [=======>......................] - ETA: 55:12 - loss: 0.6281 - regression_loss: 0.5072 - classification_loss: 0.1209
 2750/10000 [=======>......................] - ETA: 55:12 - loss: 0.6280 - regression_loss: 0.5071 - classification_loss: 0.1209
 2751/10000 [=======>......................] - ETA: 55:11 - loss: 0.6278 - regression_loss: 0.5070 - classification_loss: 0.1209
 2752/10000 [=======>......................] - ETA: 55:11 - loss: 0.6278 - regression_loss: 0.5069 - classification_loss: 0.1209
 2753/10000 [=======>......................] - ETA: 55:11 - loss: 0.6278 - regression_loss: 0.5070 - classification_loss: 0.1209
 2754/10000 [=======>......................] - ETA: 55:10 - loss: 0.6278 - regression_loss: 0.5069 - classification_loss: 0.1209
 2755/10000 [=======>......................] - ETA: 55:10 - loss: 0.6277 - regression_loss: 0.5068 - classification_loss: 0.1208
 2756/10000 [=======>......................] - ETA: 55:09 - loss: 0.6276 - regression_loss: 0.5067 - classification_loss: 0.1209
 2757/10000 [=======>......................] - ETA: 55:09 - loss: 0.6279 - regression_loss: 0.5070 - classification_loss: 0.1209
 2758/10000 [=======>......................] - ETA: 55:08 - loss: 0.6282 - regression_loss: 0.5073 - classification_loss: 0.1209
 2759/10000 [=======>......................] - ETA: 55:08 - loss: 0.6281 - regression_loss: 0.5072 - classification_loss: 0.1209
 2760/10000 [=======>......................] - ETA: 55:07 - loss: 0.6282 - regression_loss: 0.5073 - classification_loss: 0.1209
 2761/10000 [=======>......................] - ETA: 55:07 - loss: 0.6282 - regression_loss: 0.5073 - classification_loss: 0.1209
 2762/10000 [=======>......................] - ETA: 55:07 - loss: 0.6281 - regression_loss: 0.5072 - classification_loss: 0.1209
 2763/10000 [=======>......................] - ETA: 55:06 - loss: 0.6283 - regression_loss: 0.5074 - classification_loss: 0.1209
 2764/10000 [=======>......................] - ETA: 55:06 - loss: 0.6285 - regression_loss: 0.5076 - classification_loss: 0.1209
 2765/10000 [=======>......................] - ETA: 55:05 - loss: 0.6286 - regression_loss: 0.5077 - classification_loss: 0.1210
 2766/10000 [=======>......................] - ETA: 55:05 - loss: 0.6286 - regression_loss: 0.5077 - classification_loss: 0.1210
 2767/10000 [=======>......................] - ETA: 55:04 - loss: 0.6285 - regression_loss: 0.5076 - classification_loss: 0.1209
 2768/10000 [=======>......................] - ETA: 55:04 - loss: 0.6285 - regression_loss: 0.5076 - classification_loss: 0.1209
 2769/10000 [=======>......................] - ETA: 55:03 - loss: 0.6285 - regression_loss: 0.5075 - classification_loss: 0.1210
 2770/10000 [=======>......................] - ETA: 55:03 - loss: 0.6284 - regression_loss: 0.5074 - classification_loss: 0.1210
 2771/10000 [=======>......................] - ETA: 55:02 - loss: 0.6283 - regression_loss: 0.5073 - classification_loss: 0.1210
 2772/10000 [=======>......................] - ETA: 55:02 - loss: 0.6282 - regression_loss: 0.5072 - classification_loss: 0.1209
 2773/10000 [=======>......................] - ETA: 55:01 - loss: 0.6281 - regression_loss: 0.5072 - classification_loss: 0.1210
 2774/10000 [=======>......................] - ETA: 55:01 - loss: 0.6282 - regression_loss: 0.5072 - classification_loss: 0.1210
 2775/10000 [=======>......................] - ETA: 55:00 - loss: 0.6281 - regression_loss: 0.5071 - classification_loss: 0.1210
 2776/10000 [=======>......................] - ETA: 55:00 - loss: 0.6280 - regression_loss: 0.5071 - classification_loss: 0.1209
 2777/10000 [=======>......................] - ETA: 54:59 - loss: 0.6279 - regression_loss: 0.5070 - classification_loss: 0.1209
 2778/10000 [=======>......................] - ETA: 54:59 - loss: 0.6279 - regression_loss: 0.5071 - classification_loss: 0.1209
 2779/10000 [=======>......................] - ETA: 54:58 - loss: 0.6280 - regression_loss: 0.5071 - classification_loss: 0.1209
 2780/10000 [=======>......................] - ETA: 54:58 - loss: 0.6281 - regression_loss: 0.5071 - classification_loss: 0.1210
 2781/10000 [=======>......................] - ETA: 54:58 - loss: 0.6283 - regression_loss: 0.5074 - classification_loss: 0.1210
 2782/10000 [=======>......................] - ETA: 54:57 - loss: 0.6284 - regression_loss: 0.5075 - classification_loss: 0.1210
 2783/10000 [=======>......................] - ETA: 54:56 - loss: 0.6286 - regression_loss: 0.5076 - classification_loss: 0.1210
 2784/10000 [=======>......................] - ETA: 54:56 - loss: 0.6286 - regression_loss: 0.5076 - classification_loss: 0.1210
 2785/10000 [=======>......................] - ETA: 54:55 - loss: 0.6285 - regression_loss: 0.5075 - classification_loss: 0.1209
 2786/10000 [=======>......................] - ETA: 54:55 - loss: 0.6287 - regression_loss: 0.5077 - classification_loss: 0.1210
 2787/10000 [=======>......................] - ETA: 54:55 - loss: 0.6285 - regression_loss: 0.5076 - classification_loss: 0.1209
 2788/10000 [=======>......................] - ETA: 54:54 - loss: 0.6285 - regression_loss: 0.5076 - classification_loss: 0.1209
 2789/10000 [=======>......................] - ETA: 54:54 - loss: 0.6285 - regression_loss: 0.5076 - classification_loss: 0.1209
 2790/10000 [=======>......................] - ETA: 54:53 - loss: 0.6283 - regression_loss: 0.5075 - classification_loss: 0.1209
 2791/10000 [=======>......................] - ETA: 54:53 - loss: 0.6281 - regression_loss: 0.5073 - classification_loss: 0.1208
 2792/10000 [=======>......................] - ETA: 54:52 - loss: 0.6282 - regression_loss: 0.5074 - classification_loss: 0.1208
 2793/10000 [=======>......................] - ETA: 54:52 - loss: 0.6280 - regression_loss: 0.5073 - classification_loss: 0.1208
 2794/10000 [=======>......................] - ETA: 54:52 - loss: 0.6279 - regression_loss: 0.5071 - classification_loss: 0.1208
 2795/10000 [=======>......................] - ETA: 54:51 - loss: 0.6279 - regression_loss: 0.5071 - classification_loss: 0.1207
 2796/10000 [=======>......................] - ETA: 54:51 - loss: 0.6278 - regression_loss: 0.5070 - classification_loss: 0.1207
 2797/10000 [=======>......................] - ETA: 54:50 - loss: 0.6277 - regression_loss: 0.5070 - classification_loss: 0.1207
 2798/10000 [=======>......................] - ETA: 54:50 - loss: 0.6278 - regression_loss: 0.5071 - classification_loss: 0.1208
 2799/10000 [=======>......................] - ETA: 54:49 - loss: 0.6277 - regression_loss: 0.5070 - classification_loss: 0.1207
 2800/10000 [=======>......................] - ETA: 54:49 - loss: 0.6276 - regression_loss: 0.5069 - classification_loss: 0.1207
 2801/10000 [=======>......................] - ETA: 54:48 - loss: 0.6275 - regression_loss: 0.5068 - classification_loss: 0.1207
 2802/10000 [=======>......................] - ETA: 54:48 - loss: 0.6277 - regression_loss: 0.5070 - classification_loss: 0.1207
 2803/10000 [=======>......................] - ETA: 54:48 - loss: 0.6275 - regression_loss: 0.5068 - classification_loss: 0.1206
 2804/10000 [=======>......................] - ETA: 54:47 - loss: 0.6275 - regression_loss: 0.5069 - classification_loss: 0.1206
 2805/10000 [=======>......................] - ETA: 54:47 - loss: 0.6275 - regression_loss: 0.5068 - classification_loss: 0.1206
 2806/10000 [=======>......................] - ETA: 54:46 - loss: 0.6274 - regression_loss: 0.5068 - classification_loss: 0.1206
 2807/10000 [=======>......................] - ETA: 54:46 - loss: 0.6274 - regression_loss: 0.5068 - classification_loss: 0.1206
 2808/10000 [=======>......................] - ETA: 54:45 - loss: 0.6273 - regression_loss: 0.5067 - classification_loss: 0.1206
 2809/10000 [=======>......................] - ETA: 54:45 - loss: 0.6273 - regression_loss: 0.5067 - classification_loss: 0.1206
 2810/10000 [=======>......................] - ETA: 54:44 - loss: 0.6273 - regression_loss: 0.5067 - classification_loss: 0.1206
 2811/10000 [=======>......................] - ETA: 54:44 - loss: 0.6274 - regression_loss: 0.5068 - classification_loss: 0.1206
 2812/10000 [=======>......................] - ETA: 54:43 - loss: 0.6273 - regression_loss: 0.5067 - classification_loss: 0.1206
 2813/10000 [=======>......................] - ETA: 54:43 - loss: 0.6275 - regression_loss: 0.5068 - classification_loss: 0.1206
 2814/10000 [=======>......................] - ETA: 54:42 - loss: 0.6275 - regression_loss: 0.5069 - classification_loss: 0.1206
 2815/10000 [=======>......................] - ETA: 54:42 - loss: 0.6277 - regression_loss: 0.5070 - classification_loss: 0.1206
 2816/10000 [=======>......................] - ETA: 54:42 - loss: 0.6275 - regression_loss: 0.5069 - classification_loss: 0.1206
 2817/10000 [=======>......................] - ETA: 54:41 - loss: 0.6275 - regression_loss: 0.5069 - classification_loss: 0.1206
 2818/10000 [=======>......................] - ETA: 54:41 - loss: 0.6274 - regression_loss: 0.5068 - classification_loss: 0.1206
 2819/10000 [=======>......................] - ETA: 54:40 - loss: 0.6273 - regression_loss: 0.5067 - classification_loss: 0.1206
 2820/10000 [=======>......................] - ETA: 54:40 - loss: 0.6273 - regression_loss: 0.5067 - classification_loss: 0.1206
 2821/10000 [=======>......................] - ETA: 54:39 - loss: 0.6272 - regression_loss: 0.5066 - classification_loss: 0.1205
 2822/10000 [=======>......................] - ETA: 54:39 - loss: 0.6271 - regression_loss: 0.5066 - classification_loss: 0.1205
 2823/10000 [=======>......................] - ETA: 54:38 - loss: 0.6272 - regression_loss: 0.5066 - classification_loss: 0.1206
 2824/10000 [=======>......................] - ETA: 54:38 - loss: 0.6271 - regression_loss: 0.5065 - classification_loss: 0.1206
 2825/10000 [=======>......................] - ETA: 54:37 - loss: 0.6271 - regression_loss: 0.5065 - classification_loss: 0.1207
 2826/10000 [=======>......................] - ETA: 54:37 - loss: 0.6271 - regression_loss: 0.5064 - classification_loss: 0.1206
 2827/10000 [=======>......................] - ETA: 54:36 - loss: 0.6270 - regression_loss: 0.5063 - classification_loss: 0.1206
 2828/10000 [=======>......................] - ETA: 54:36 - loss: 0.6269 - regression_loss: 0.5063 - classification_loss: 0.1206
 2829/10000 [=======>......................] - ETA: 54:36 - loss: 0.6268 - regression_loss: 0.5062 - classification_loss: 0.1206
 2830/10000 [=======>......................] - ETA: 54:35 - loss: 0.6266 - regression_loss: 0.5060 - classification_loss: 0.1206
 2831/10000 [=======>......................] - ETA: 54:35 - loss: 0.6269 - regression_loss: 0.5062 - classification_loss: 0.1206
 2832/10000 [=======>......................] - ETA: 54:34 - loss: 0.6268 - regression_loss: 0.5062 - classification_loss: 0.1206
 2833/10000 [=======>......................] - ETA: 54:34 - loss: 0.6268 - regression_loss: 0.5062 - classification_loss: 0.1206
 2834/10000 [=======>......................] - ETA: 54:33 - loss: 0.6267 - regression_loss: 0.5061 - classification_loss: 0.1206
 2835/10000 [=======>......................] - ETA: 54:33 - loss: 0.6266 - regression_loss: 0.5060 - classification_loss: 0.1206
 2836/10000 [=======>......................] - ETA: 54:32 - loss: 0.6269 - regression_loss: 0.5063 - classification_loss: 0.1206
 2837/10000 [=======>......................] - ETA: 54:32 - loss: 0.6268 - regression_loss: 0.5062 - classification_loss: 0.1206
 2838/10000 [=======>......................] - ETA: 54:31 - loss: 0.6272 - regression_loss: 0.5064 - classification_loss: 0.1207
 2839/10000 [=======>......................] - ETA: 54:31 - loss: 0.6271 - regression_loss: 0.5063 - classification_loss: 0.1208
 2840/10000 [=======>......................] - ETA: 54:30 - loss: 0.6272 - regression_loss: 0.5064 - classification_loss: 0.1208
 2841/10000 [=======>......................] - ETA: 54:30 - loss: 0.6271 - regression_loss: 0.5063 - classification_loss: 0.1208
 2842/10000 [=======>......................] - ETA: 54:30 - loss: 0.6269 - regression_loss: 0.5061 - classification_loss: 0.1208
 2843/10000 [=======>......................] - ETA: 54:29 - loss: 0.6268 - regression_loss: 0.5061 - classification_loss: 0.1208
 2844/10000 [=======>......................] - ETA: 54:29 - loss: 0.6267 - regression_loss: 0.5059 - classification_loss: 0.1207
 2845/10000 [=======>......................] - ETA: 54:28 - loss: 0.6268 - regression_loss: 0.5060 - classification_loss: 0.1208
 2846/10000 [=======>......................] - ETA: 54:28 - loss: 0.6267 - regression_loss: 0.5059 - classification_loss: 0.1208
 2847/10000 [=======>......................] - ETA: 54:27 - loss: 0.6266 - regression_loss: 0.5058 - classification_loss: 0.1208
 2848/10000 [=======>......................] - ETA: 54:27 - loss: 0.6266 - regression_loss: 0.5059 - classification_loss: 0.1208
 2849/10000 [=======>......................] - ETA: 54:26 - loss: 0.6265 - regression_loss: 0.5058 - classification_loss: 0.1208
 2850/10000 [=======>......................] - ETA: 54:26 - loss: 0.6264 - regression_loss: 0.5057 - classification_loss: 0.1207
 2851/10000 [=======>......................] - ETA: 54:25 - loss: 0.6263 - regression_loss: 0.5056 - classification_loss: 0.1207
 2852/10000 [=======>......................] - ETA: 54:25 - loss: 0.6264 - regression_loss: 0.5056 - classification_loss: 0.1208
 2853/10000 [=======>......................] - ETA: 54:24 - loss: 0.6266 - regression_loss: 0.5058 - classification_loss: 0.1208
 2854/10000 [=======>......................] - ETA: 54:24 - loss: 0.6266 - regression_loss: 0.5058 - classification_loss: 0.1208
 2855/10000 [=======>......................] - ETA: 54:23 - loss: 0.6268 - regression_loss: 0.5059 - classification_loss: 0.1208
 2856/10000 [=======>......................] - ETA: 54:23 - loss: 0.6266 - regression_loss: 0.5058 - classification_loss: 0.1208
 2857/10000 [=======>......................] - ETA: 54:23 - loss: 0.6265 - regression_loss: 0.5058 - classification_loss: 0.1208
 2858/10000 [=======>......................] - ETA: 54:22 - loss: 0.6265 - regression_loss: 0.5057 - classification_loss: 0.1208
 2859/10000 [=======>......................] - ETA: 54:22 - loss: 0.6264 - regression_loss: 0.5056 - classification_loss: 0.1208
 2860/10000 [=======>......................] - ETA: 54:21 - loss: 0.6264 - regression_loss: 0.5056 - classification_loss: 0.1208
 2861/10000 [=======>......................] - ETA: 54:21 - loss: 0.6264 - regression_loss: 0.5056 - classification_loss: 0.1208
 2862/10000 [=======>......................] - ETA: 54:20 - loss: 0.6265 - regression_loss: 0.5057 - classification_loss: 0.1208
 2863/10000 [=======>......................] - ETA: 54:20 - loss: 0.6264 - regression_loss: 0.5056 - classification_loss: 0.1208
 2864/10000 [=======>......................] - ETA: 54:19 - loss: 0.6264 - regression_loss: 0.5056 - classification_loss: 0.1208
 2865/10000 [=======>......................] - ETA: 54:19 - loss: 0.6262 - regression_loss: 0.5055 - classification_loss: 0.1207
 2866/10000 [=======>......................] - ETA: 54:19 - loss: 0.6263 - regression_loss: 0.5056 - classification_loss: 0.1208
 2867/10000 [=======>......................] - ETA: 54:18 - loss: 0.6264 - regression_loss: 0.5057 - classification_loss: 0.1207
 2868/10000 [=======>......................] - ETA: 54:18 - loss: 0.6263 - regression_loss: 0.5056 - classification_loss: 0.1207
 2869/10000 [=======>......................] - ETA: 54:17 - loss: 0.6262 - regression_loss: 0.5055 - classification_loss: 0.1207
 2870/10000 [=======>......................] - ETA: 54:17 - loss: 0.6261 - regression_loss: 0.5054 - classification_loss: 0.1207
 2871/10000 [=======>......................] - ETA: 54:16 - loss: 0.6260 - regression_loss: 0.5053 - classification_loss: 0.1207
 2872/10000 [=======>......................] - ETA: 54:16 - loss: 0.6260 - regression_loss: 0.5053 - classification_loss: 0.1206
 2873/10000 [=======>......................] - ETA: 54:15 - loss: 0.6259 - regression_loss: 0.5053 - classification_loss: 0.1206
 2874/10000 [=======>......................] - ETA: 54:15 - loss: 0.6259 - regression_loss: 0.5052 - classification_loss: 0.1207
 2875/10000 [=======>......................] - ETA: 54:15 - loss: 0.6257 - regression_loss: 0.5051 - classification_loss: 0.1206
 2876/10000 [=======>......................] - ETA: 54:14 - loss: 0.6259 - regression_loss: 0.5053 - classification_loss: 0.1207
 2877/10000 [=======>......................] - ETA: 54:14 - loss: 0.6260 - regression_loss: 0.5053 - classification_loss: 0.1207
 2878/10000 [=======>......................] - ETA: 54:13 - loss: 0.6261 - regression_loss: 0.5053 - classification_loss: 0.1207
 2879/10000 [=======>......................] - ETA: 54:13 - loss: 0.6262 - regression_loss: 0.5054 - classification_loss: 0.1208
 2880/10000 [=======>......................] - ETA: 54:12 - loss: 0.6263 - regression_loss: 0.5055 - classification_loss: 0.1208
 2881/10000 [=======>......................] - ETA: 54:12 - loss: 0.6263 - regression_loss: 0.5055 - classification_loss: 0.1208
 2882/10000 [=======>......................] - ETA: 54:11 - loss: 0.6261 - regression_loss: 0.5054 - classification_loss: 0.1207
 2883/10000 [=======>......................] - ETA: 54:11 - loss: 0.6263 - regression_loss: 0.5055 - classification_loss: 0.1207
 2884/10000 [=======>......................] - ETA: 54:10 - loss: 0.6263 - regression_loss: 0.5056 - classification_loss: 0.1207
 2885/10000 [=======>......................] - ETA: 54:10 - loss: 0.6262 - regression_loss: 0.5055 - classification_loss: 0.1207
 2886/10000 [=======>......................] - ETA: 54:09 - loss: 0.6263 - regression_loss: 0.5056 - classification_loss: 0.1207
 2887/10000 [=======>......................] - ETA: 54:09 - loss: 0.6263 - regression_loss: 0.5056 - classification_loss: 0.1207
 2888/10000 [=======>......................] - ETA: 54:09 - loss: 0.6264 - regression_loss: 0.5057 - classification_loss: 0.1207
 2889/10000 [=======>......................] - ETA: 54:08 - loss: 0.6264 - regression_loss: 0.5057 - classification_loss: 0.1207
 2890/10000 [=======>......................] - ETA: 54:08 - loss: 0.6265 - regression_loss: 0.5058 - classification_loss: 0.1207
 2891/10000 [=======>......................] - ETA: 54:07 - loss: 0.6266 - regression_loss: 0.5058 - classification_loss: 0.1207
 2892/10000 [=======>......................] - ETA: 54:07 - loss: 0.6265 - regression_loss: 0.5058 - classification_loss: 0.1207
 2893/10000 [=======>......................] - ETA: 54:06 - loss: 0.6264 - regression_loss: 0.5057 - classification_loss: 0.1207
 2894/10000 [=======>......................] - ETA: 54:06 - loss: 0.6263 - regression_loss: 0.5057 - classification_loss: 0.1207
 2895/10000 [=======>......................] - ETA: 54:05 - loss: 0.6262 - regression_loss: 0.5055 - classification_loss: 0.1207
 2896/10000 [=======>......................] - ETA: 54:05 - loss: 0.6264 - regression_loss: 0.5057 - classification_loss: 0.1207
 2897/10000 [=======>......................] - ETA: 54:04 - loss: 0.6266 - regression_loss: 0.5059 - classification_loss: 0.1207
 2898/10000 [=======>......................] - ETA: 54:04 - loss: 0.6264 - regression_loss: 0.5057 - classification_loss: 0.1207
 2899/10000 [=======>......................] - ETA: 54:04 - loss: 0.6264 - regression_loss: 0.5058 - classification_loss: 0.1207
 2900/10000 [=======>......................] - ETA: 54:03 - loss: 0.6265 - regression_loss: 0.5059 - classification_loss: 0.1207
 2901/10000 [=======>......................] - ETA: 54:03 - loss: 0.6267 - regression_loss: 0.5060 - classification_loss: 0.1207
 2902/10000 [=======>......................] - ETA: 54:02 - loss: 0.6266 - regression_loss: 0.5060 - classification_loss: 0.1206
 2903/10000 [=======>......................] - ETA: 54:02 - loss: 0.6266 - regression_loss: 0.5060 - classification_loss: 0.1207
 2904/10000 [=======>......................] - ETA: 54:01 - loss: 0.6265 - regression_loss: 0.5059 - classification_loss: 0.1206
 2905/10000 [=======>......................] - ETA: 54:01 - loss: 0.6265 - regression_loss: 0.5059 - classification_loss: 0.1206
 2906/10000 [=======>......................] - ETA: 54:00 - loss: 0.6264 - regression_loss: 0.5057 - classification_loss: 0.1206
 2907/10000 [=======>......................] - ETA: 54:00 - loss: 0.6265 - regression_loss: 0.5058 - classification_loss: 0.1206
 2908/10000 [=======>......................] - ETA: 53:59 - loss: 0.6265 - regression_loss: 0.5059 - classification_loss: 0.1206
 2909/10000 [=======>......................] - ETA: 53:59 - loss: 0.6265 - regression_loss: 0.5059 - classification_loss: 0.1206
 2910/10000 [=======>......................] - ETA: 53:59 - loss: 0.6265 - regression_loss: 0.5059 - classification_loss: 0.1206
 2911/10000 [=======>......................] - ETA: 53:58 - loss: 0.6264 - regression_loss: 0.5058 - classification_loss: 0.1206
 2912/10000 [=======>......................] - ETA: 53:58 - loss: 0.6269 - regression_loss: 0.5063 - classification_loss: 0.1206
 2913/10000 [=======>......................] - ETA: 53:57 - loss: 0.6270 - regression_loss: 0.5063 - classification_loss: 0.1206
 2914/10000 [=======>......................] - ETA: 53:57 - loss: 0.6271 - regression_loss: 0.5064 - classification_loss: 0.1207
 2915/10000 [=======>......................] - ETA: 53:56 - loss: 0.6271 - regression_loss: 0.5064 - classification_loss: 0.1207
 2916/10000 [=======>......................] - ETA: 53:56 - loss: 0.6270 - regression_loss: 0.5063 - classification_loss: 0.1207
 2917/10000 [=======>......................] - ETA: 53:56 - loss: 0.6270 - regression_loss: 0.5063 - classification_loss: 0.1208
 2918/10000 [=======>......................] - ETA: 53:55 - loss: 0.6273 - regression_loss: 0.5065 - classification_loss: 0.1208
 2919/10000 [=======>......................] - ETA: 53:55 - loss: 0.6273 - regression_loss: 0.5066 - classification_loss: 0.1208
 2920/10000 [=======>......................] - ETA: 53:54 - loss: 0.6275 - regression_loss: 0.5066 - classification_loss: 0.1208
 2921/10000 [=======>......................] - ETA: 53:54 - loss: 0.6274 - regression_loss: 0.5066 - classification_loss: 0.1208
 2922/10000 [=======>......................] - ETA: 53:53 - loss: 0.6276 - regression_loss: 0.5067 - classification_loss: 0.1208
 2923/10000 [=======>......................] - ETA: 53:53 - loss: 0.6274 - regression_loss: 0.5066 - classification_loss: 0.1208
 2924/10000 [=======>......................] - ETA: 53:52 - loss: 0.6273 - regression_loss: 0.5065 - classification_loss: 0.1208
 2925/10000 [=======>......................] - ETA: 53:52 - loss: 0.6274 - regression_loss: 0.5066 - classification_loss: 0.1208
 2926/10000 [=======>......................] - ETA: 53:51 - loss: 0.6275 - regression_loss: 0.5067 - classification_loss: 0.1208
 2927/10000 [=======>......................] - ETA: 53:51 - loss: 0.6273 - regression_loss: 0.5065 - classification_loss: 0.1208
 2928/10000 [=======>......................] - ETA: 53:51 - loss: 0.6273 - regression_loss: 0.5065 - classification_loss: 0.1208
 2929/10000 [=======>......................] - ETA: 53:50 - loss: 0.6272 - regression_loss: 0.5064 - classification_loss: 0.1208
 2930/10000 [=======>......................] - ETA: 53:50 - loss: 0.6274 - regression_loss: 0.5066 - classification_loss: 0.1208
 2931/10000 [=======>......................] - ETA: 53:49 - loss: 0.6273 - regression_loss: 0.5065 - classification_loss: 0.1208
 2932/10000 [=======>......................] - ETA: 53:49 - loss: 0.6276 - regression_loss: 0.5068 - classification_loss: 0.1208
 2933/10000 [=======>......................] - ETA: 53:48 - loss: 0.6275 - regression_loss: 0.5067 - classification_loss: 0.1208
 2934/10000 [=======>......................] - ETA: 53:48 - loss: 0.6275 - regression_loss: 0.5066 - classification_loss: 0.1209
 2935/10000 [=======>......................] - ETA: 53:47 - loss: 0.6276 - regression_loss: 0.5066 - classification_loss: 0.1209
 2936/10000 [=======>......................] - ETA: 53:47 - loss: 0.6275 - regression_loss: 0.5066 - classification_loss: 0.1209
 2937/10000 [=======>......................] - ETA: 53:46 - loss: 0.6276 - regression_loss: 0.5067 - classification_loss: 0.1209
 2938/10000 [=======>......................] - ETA: 53:46 - loss: 0.6277 - regression_loss: 0.5067 - classification_loss: 0.1209
 2939/10000 [=======>......................] - ETA: 53:45 - loss: 0.6276 - regression_loss: 0.5067 - classification_loss: 0.1209
 2940/10000 [=======>......................] - ETA: 53:45 - loss: 0.6274 - regression_loss: 0.5065 - classification_loss: 0.1209
 2941/10000 [=======>......................] - ETA: 53:44 - loss: 0.6274 - regression_loss: 0.5065 - classification_loss: 0.1209
 2942/10000 [=======>......................] - ETA: 53:44 - loss: 0.6276 - regression_loss: 0.5066 - classification_loss: 0.1209
 2943/10000 [=======>......................] - ETA: 53:43 - loss: 0.6275 - regression_loss: 0.5066 - classification_loss: 0.1209
 2944/10000 [=======>......................] - ETA: 53:43 - loss: 0.6273 - regression_loss: 0.5065 - classification_loss: 0.1209
 2945/10000 [=======>......................] - ETA: 53:42 - loss: 0.6272 - regression_loss: 0.5064 - classification_loss: 0.1208
 2946/10000 [=======>......................] - ETA: 53:42 - loss: 0.6277 - regression_loss: 0.5068 - classification_loss: 0.1209
 2947/10000 [=======>......................] - ETA: 53:41 - loss: 0.6275 - regression_loss: 0.5067 - classification_loss: 0.1208
 2948/10000 [=======>......................] - ETA: 53:41 - loss: 0.6276 - regression_loss: 0.5068 - classification_loss: 0.1208
 2949/10000 [=======>......................] - ETA: 53:40 - loss: 0.6275 - regression_loss: 0.5066 - classification_loss: 0.1208
 2950/10000 [=======>......................] - ETA: 53:40 - loss: 0.6274 - regression_loss: 0.5066 - classification_loss: 0.1208
 2951/10000 [=======>......................] - ETA: 53:39 - loss: 0.6272 - regression_loss: 0.5064 - classification_loss: 0.1208
 2952/10000 [=======>......................] - ETA: 53:39 - loss: 0.6271 - regression_loss: 0.5063 - classification_loss: 0.1208
 2953/10000 [=======>......................] - ETA: 53:38 - loss: 0.6271 - regression_loss: 0.5063 - classification_loss: 0.1208
 2954/10000 [=======>......................] - ETA: 53:38 - loss: 0.6273 - regression_loss: 0.5064 - classification_loss: 0.1208
 2955/10000 [=======>......................] - ETA: 53:37 - loss: 0.6273 - regression_loss: 0.5064 - classification_loss: 0.1208
 2956/10000 [=======>......................] - ETA: 53:37 - loss: 0.6274 - regression_loss: 0.5065 - classification_loss: 0.1209
 2957/10000 [=======>......................] - ETA: 53:37 - loss: 0.6273 - regression_loss: 0.5064 - classification_loss: 0.1208
 2958/10000 [=======>......................] - ETA: 53:36 - loss: 0.6273 - regression_loss: 0.5064 - classification_loss: 0.1209
 2959/10000 [=======>......................] - ETA: 53:36 - loss: 0.6272 - regression_loss: 0.5064 - classification_loss: 0.1208
 2960/10000 [=======>......................] - ETA: 53:35 - loss: 0.6273 - regression_loss: 0.5065 - classification_loss: 0.1208
 2961/10000 [=======>......................] - ETA: 53:35 - loss: 0.6272 - regression_loss: 0.5063 - classification_loss: 0.1209
 2962/10000 [=======>......................] - ETA: 53:34 - loss: 0.6271 - regression_loss: 0.5062 - classification_loss: 0.1208
 2963/10000 [=======>......................] - ETA: 53:34 - loss: 0.6269 - regression_loss: 0.5061 - classification_loss: 0.1208
 2964/10000 [=======>......................] - ETA: 53:33 - loss: 0.6271 - regression_loss: 0.5062 - classification_loss: 0.1208
 2965/10000 [=======>......................] - ETA: 53:33 - loss: 0.6269 - regression_loss: 0.5061 - classification_loss: 0.1208
 2966/10000 [=======>......................] - ETA: 53:32 - loss: 0.6268 - regression_loss: 0.5060 - classification_loss: 0.1208
 2967/10000 [=======>......................] - ETA: 53:32 - loss: 0.6271 - regression_loss: 0.5063 - classification_loss: 0.1208
 2968/10000 [=======>......................] - ETA: 53:31 - loss: 0.6270 - regression_loss: 0.5062 - classification_loss: 0.1208
 2969/10000 [=======>......................] - ETA: 53:31 - loss: 0.6269 - regression_loss: 0.5062 - classification_loss: 0.1208
 2970/10000 [=======>......................] - ETA: 53:31 - loss: 0.6271 - regression_loss: 0.5063 - classification_loss: 0.1208
 2971/10000 [=======>......................] - ETA: 53:30 - loss: 0.6270 - regression_loss: 0.5062 - classification_loss: 0.1207
 2972/10000 [=======>......................] - ETA: 53:30 - loss: 0.6269 - regression_loss: 0.5062 - classification_loss: 0.1207
 2973/10000 [=======>......................] - ETA: 53:29 - loss: 0.6271 - regression_loss: 0.5063 - classification_loss: 0.1208
 2974/10000 [=======>......................] - ETA: 53:29 - loss: 0.6271 - regression_loss: 0.5063 - classification_loss: 0.1208
 2975/10000 [=======>......................] - ETA: 53:28 - loss: 0.6271 - regression_loss: 0.5063 - classification_loss: 0.1208
 2976/10000 [=======>......................] - ETA: 53:28 - loss: 0.6270 - regression_loss: 0.5063 - classification_loss: 0.1207
 2977/10000 [=======>......................] - ETA: 53:27 - loss: 0.6268 - regression_loss: 0.5061 - classification_loss: 0.1207
 2978/10000 [=======>......................] - ETA: 53:27 - loss: 0.6270 - regression_loss: 0.5063 - classification_loss: 0.1208
 2979/10000 [=======>......................] - ETA: 53:27 - loss: 0.6271 - regression_loss: 0.5064 - classification_loss: 0.1208
 2980/10000 [=======>......................] - ETA: 53:26 - loss: 0.6272 - regression_loss: 0.5064 - classification_loss: 0.1208
 2981/10000 [=======>......................] - ETA: 53:26 - loss: 0.6272 - regression_loss: 0.5064 - classification_loss: 0.1208
 2982/10000 [=======>......................] - ETA: 53:25 - loss: 0.6276 - regression_loss: 0.5067 - classification_loss: 0.1209
 2983/10000 [=======>......................] - ETA: 53:25 - loss: 0.6276 - regression_loss: 0.5067 - classification_loss: 0.1209
 2984/10000 [=======>......................] - ETA: 53:25 - loss: 0.6276 - regression_loss: 0.5067 - classification_loss: 0.1209
 2985/10000 [=======>......................] - ETA: 53:24 - loss: 0.6275 - regression_loss: 0.5066 - classification_loss: 0.1209
 2986/10000 [=======>......................] - ETA: 53:24 - loss: 0.6274 - regression_loss: 0.5065 - classification_loss: 0.1209
 2987/10000 [=======>......................] - ETA: 53:23 - loss: 0.6276 - regression_loss: 0.5068 - classification_loss: 0.1209
 2988/10000 [=======>......................] - ETA: 53:23 - loss: 0.6275 - regression_loss: 0.5066 - classification_loss: 0.1208
 2989/10000 [=======>......................] - ETA: 53:22 - loss: 0.6275 - regression_loss: 0.5066 - classification_loss: 0.1208
 2990/10000 [=======>......................] - ETA: 53:22 - loss: 0.6274 - regression_loss: 0.5065 - classification_loss: 0.1208
 2991/10000 [=======>......................] - ETA: 53:21 - loss: 0.6272 - regression_loss: 0.5064 - classification_loss: 0.1208
 2992/10000 [=======>......................] - ETA: 53:21 - loss: 0.6275 - regression_loss: 0.5067 - classification_loss: 0.1209
 2993/10000 [=======>......................] - ETA: 53:20 - loss: 0.6275 - regression_loss: 0.5067 - classification_loss: 0.1209
 2994/10000 [=======>......................] - ETA: 53:20 - loss: 0.6274 - regression_loss: 0.5066 - classification_loss: 0.1208
 2995/10000 [=======>......................] - ETA: 53:20 - loss: 0.6273 - regression_loss: 0.5065 - classification_loss: 0.1208
 2996/10000 [=======>......................] - ETA: 53:19 - loss: 0.6273 - regression_loss: 0.5065 - classification_loss: 0.1208
 2997/10000 [=======>......................] - ETA: 53:19 - loss: 0.6272 - regression_loss: 0.5063 - classification_loss: 0.1208
 2998/10000 [=======>......................] - ETA: 53:18 - loss: 0.6274 - regression_loss: 0.5065 - classification_loss: 0.1208
 2999/10000 [=======>......................] - ETA: 53:18 - loss: 0.6273 - regression_loss: 0.5064 - classification_loss: 0.1208
 3000/10000 [========>.....................] - ETA: 53:17 - loss: 0.6272 - regression_loss: 0.5063 - classification_loss: 0.1209
 3001/10000 [========>.....................] - ETA: 53:17 - loss: 0.6271 - regression_loss: 0.5063 - classification_loss: 0.1209
 3002/10000 [========>.....................] - ETA: 53:16 - loss: 0.6271 - regression_loss: 0.5062 - classification_loss: 0.1209
 3003/10000 [========>.....................] - ETA: 53:16 - loss: 0.6272 - regression_loss: 0.5064 - classification_loss: 0.1209
 3004/10000 [========>.....................] - ETA: 53:15 - loss: 0.6272 - regression_loss: 0.5064 - classification_loss: 0.1209
 3005/10000 [========>.....................] - ETA: 53:15 - loss: 0.6272 - regression_loss: 0.5063 - classification_loss: 0.1209
 3006/10000 [========>.....................] - ETA: 53:14 - loss: 0.6273 - regression_loss: 0.5064 - classification_loss: 0.1209
 3007/10000 [========>.....................] - ETA: 53:14 - loss: 0.6272 - regression_loss: 0.5064 - classification_loss: 0.1208
 3008/10000 [========>.....................] - ETA: 53:14 - loss: 0.6275 - regression_loss: 0.5066 - classification_loss: 0.1209
 3009/10000 [========>.....................] - ETA: 53:13 - loss: 0.6274 - regression_loss: 0.5065 - classification_loss: 0.1209
 3010/10000 [========>.....................] - ETA: 53:13 - loss: 0.6274 - regression_loss: 0.5066 - classification_loss: 0.1209
 3011/10000 [========>.....................] - ETA: 53:12 - loss: 0.6273 - regression_loss: 0.5064 - classification_loss: 0.1208
 3012/10000 [========>.....................] - ETA: 53:12 - loss: 0.6272 - regression_loss: 0.5064 - classification_loss: 0.1208
 3013/10000 [========>.....................] - ETA: 53:11 - loss: 0.6271 - regression_loss: 0.5063 - classification_loss: 0.1208
 3014/10000 [========>.....................] - ETA: 53:11 - loss: 0.6271 - regression_loss: 0.5063 - classification_loss: 0.1208
 3015/10000 [========>.....................] - ETA: 53:10 - loss: 0.6271 - regression_loss: 0.5062 - classification_loss: 0.1209
 3016/10000 [========>.....................] - ETA: 53:10 - loss: 0.6270 - regression_loss: 0.5061 - classification_loss: 0.1209
 3017/10000 [========>.....................] - ETA: 53:09 - loss: 0.6270 - regression_loss: 0.5061 - classification_loss: 0.1209
 3018/10000 [========>.....................] - ETA: 53:09 - loss: 0.6270 - regression_loss: 0.5061 - classification_loss: 0.1209
 3019/10000 [========>.....................] - ETA: 53:08 - loss: 0.6271 - regression_loss: 0.5062 - classification_loss: 0.1209
 3020/10000 [========>.....................] - ETA: 53:08 - loss: 0.6271 - regression_loss: 0.5062 - classification_loss: 0.1209
 3021/10000 [========>.....................] - ETA: 53:07 - loss: 0.6271 - regression_loss: 0.5062 - classification_loss: 0.1209
 3022/10000 [========>.....................] - ETA: 53:07 - loss: 0.6269 - regression_loss: 0.5060 - classification_loss: 0.1209
 3023/10000 [========>.....................] - ETA: 53:07 - loss: 0.6268 - regression_loss: 0.5059 - classification_loss: 0.1208
 3024/10000 [========>.....................] - ETA: 53:06 - loss: 0.6267 - regression_loss: 0.5059 - classification_loss: 0.1208
 3025/10000 [========>.....................] - ETA: 53:06 - loss: 0.6266 - regression_loss: 0.5058 - classification_loss: 0.1208
 3026/10000 [========>.....................] - ETA: 53:05 - loss: 0.6266 - regression_loss: 0.5058 - classification_loss: 0.1208
 3027/10000 [========>.....................] - ETA: 53:05 - loss: 0.6265 - regression_loss: 0.5057 - classification_loss: 0.1208
 3028/10000 [========>.....................] - ETA: 53:04 - loss: 0.6264 - regression_loss: 0.5056 - classification_loss: 0.1208
 3029/10000 [========>.....................] - ETA: 53:04 - loss: 0.6263 - regression_loss: 0.5055 - classification_loss: 0.1208
 3030/10000 [========>.....................] - ETA: 53:04 - loss: 0.6264 - regression_loss: 0.5056 - classification_loss: 0.1208
 3031/10000 [========>.....................] - ETA: 53:03 - loss: 0.6264 - regression_loss: 0.5056 - classification_loss: 0.1208
 3032/10000 [========>.....................] - ETA: 53:03 - loss: 0.6263 - regression_loss: 0.5055 - classification_loss: 0.1207
 3033/10000 [========>.....................] - ETA: 53:02 - loss: 0.6262 - regression_loss: 0.5055 - classification_loss: 0.1207
 3034/10000 [========>.....................] - ETA: 53:02 - loss: 0.6262 - regression_loss: 0.5055 - classification_loss: 0.1207
 3035/10000 [========>.....................] - ETA: 53:01 - loss: 0.6261 - regression_loss: 0.5054 - classification_loss: 0.1207
 3036/10000 [========>.....................] - ETA: 53:01 - loss: 0.6264 - regression_loss: 0.5056 - classification_loss: 0.1207
 3037/10000 [========>.....................] - ETA: 53:00 - loss: 0.6265 - regression_loss: 0.5058 - classification_loss: 0.1207
 3038/10000 [========>.....................] - ETA: 53:00 - loss: 0.6266 - regression_loss: 0.5059 - classification_loss: 0.1207
 3039/10000 [========>.....................] - ETA: 52:59 - loss: 0.6267 - regression_loss: 0.5060 - classification_loss: 0.1208
 3040/10000 [========>.....................] - ETA: 52:59 - loss: 0.6270 - regression_loss: 0.5062 - classification_loss: 0.1208
 3041/10000 [========>.....................] - ETA: 52:58 - loss: 0.6269 - regression_loss: 0.5061 - classification_loss: 0.1208
 3042/10000 [========>.....................] - ETA: 52:58 - loss: 0.6271 - regression_loss: 0.5063 - classification_loss: 0.1208
 3043/10000 [========>.....................] - ETA: 52:57 - loss: 0.6272 - regression_loss: 0.5064 - classification_loss: 0.1208
 3044/10000 [========>.....................] - ETA: 52:57 - loss: 0.6271 - regression_loss: 0.5063 - classification_loss: 0.1208
 3045/10000 [========>.....................] - ETA: 52:56 - loss: 0.6275 - regression_loss: 0.5066 - classification_loss: 0.1209
 3046/10000 [========>.....................] - ETA: 52:56 - loss: 0.6274 - regression_loss: 0.5065 - classification_loss: 0.1209
 3047/10000 [========>.....................] - ETA: 52:55 - loss: 0.6273 - regression_loss: 0.5065 - classification_loss: 0.1209
 3048/10000 [========>.....................] - ETA: 52:55 - loss: 0.6271 - regression_loss: 0.5063 - classification_loss: 0.1208
 3049/10000 [========>.....................] - ETA: 52:55 - loss: 0.6270 - regression_loss: 0.5063 - classification_loss: 0.1208
 3050/10000 [========>.....................] - ETA: 52:54 - loss: 0.6273 - regression_loss: 0.5065 - classification_loss: 0.1208
 3051/10000 [========>.....................] - ETA: 52:54 - loss: 0.6277 - regression_loss: 0.5067 - classification_loss: 0.1209
 3052/10000 [========>.....................] - ETA: 52:53 - loss: 0.6276 - regression_loss: 0.5067 - classification_loss: 0.1209
 3053/10000 [========>.....................] - ETA: 52:53 - loss: 0.6278 - regression_loss: 0.5068 - classification_loss: 0.1210
 3054/10000 [========>.....................] - ETA: 52:52 - loss: 0.6276 - regression_loss: 0.5067 - classification_loss: 0.1209
 3055/10000 [========>.....................] - ETA: 52:52 - loss: 0.6278 - regression_loss: 0.5069 - classification_loss: 0.1209
 3056/10000 [========>.....................] - ETA: 52:51 - loss: 0.6278 - regression_loss: 0.5069 - classification_loss: 0.1209
 3057/10000 [========>.....................] - ETA: 52:51 - loss: 0.6277 - regression_loss: 0.5068 - classification_loss: 0.1209
 3058/10000 [========>.....................] - ETA: 52:50 - loss: 0.6277 - regression_loss: 0.5068 - classification_loss: 0.1209
 3059/10000 [========>.....................] - ETA: 52:50 - loss: 0.6277 - regression_loss: 0.5068 - classification_loss: 0.1209
 3060/10000 [========>.....................] - ETA: 52:50 - loss: 0.6276 - regression_loss: 0.5067 - classification_loss: 0.1209
 3061/10000 [========>.....................] - ETA: 52:49 - loss: 0.6277 - regression_loss: 0.5067 - classification_loss: 0.1210
 3062/10000 [========>.....................] - ETA: 52:48 - loss: 0.6278 - regression_loss: 0.5068 - classification_loss: 0.1210
 3063/10000 [========>.....................] - ETA: 52:48 - loss: 0.6278 - regression_loss: 0.5068 - classification_loss: 0.1210
 3064/10000 [========>.....................] - ETA: 52:47 - loss: 0.6279 - regression_loss: 0.5069 - classification_loss: 0.1210
 3065/10000 [========>.....................] - ETA: 52:47 - loss: 0.6278 - regression_loss: 0.5068 - classification_loss: 0.1209
 3066/10000 [========>.....................] - ETA: 52:47 - loss: 0.6278 - regression_loss: 0.5069 - classification_loss: 0.1210
 3067/10000 [========>.....................] - ETA: 52:46 - loss: 0.6278 - regression_loss: 0.5068 - classification_loss: 0.1210
 3068/10000 [========>.....................] - ETA: 52:46 - loss: 0.6278 - regression_loss: 0.5068 - classification_loss: 0.1210
 3069/10000 [========>.....................] - ETA: 52:45 - loss: 0.6280 - regression_loss: 0.5070 - classification_loss: 0.1210
 3070/10000 [========>.....................] - ETA: 52:45 - loss: 0.6279 - regression_loss: 0.5070 - classification_loss: 0.1210
 3071/10000 [========>.....................] - ETA: 52:45 - loss: 0.6280 - regression_loss: 0.5070 - classification_loss: 0.1210
 3072/10000 [========>.....................] - ETA: 52:44 - loss: 0.6279 - regression_loss: 0.5070 - classification_loss: 0.1210
 3073/10000 [========>.....................] - ETA: 52:44 - loss: 0.6278 - regression_loss: 0.5068 - classification_loss: 0.1210
 3074/10000 [========>.....................] - ETA: 52:43 - loss: 0.6277 - regression_loss: 0.5068 - classification_loss: 0.1210
 3075/10000 [========>.....................] - ETA: 52:43 - loss: 0.6278 - regression_loss: 0.5068 - classification_loss: 0.1210
 3076/10000 [========>.....................] - ETA: 52:42 - loss: 0.6276 - regression_loss: 0.5067 - classification_loss: 0.1210
 3077/10000 [========>.....................] - ETA: 52:42 - loss: 0.6276 - regression_loss: 0.5066 - classification_loss: 0.1210
 3078/10000 [========>.....................] - ETA: 52:41 - loss: 0.6276 - regression_loss: 0.5066 - classification_loss: 0.1210
 3079/10000 [========>.....................] - ETA: 52:41 - loss: 0.6276 - regression_loss: 0.5066 - classification_loss: 0.1210
 3080/10000 [========>.....................] - ETA: 52:40 - loss: 0.6275 - regression_loss: 0.5065 - classification_loss: 0.1210
 3081/10000 [========>.....................] - ETA: 52:40 - loss: 0.6273 - regression_loss: 0.5064 - classification_loss: 0.1209
 3082/10000 [========>.....................] - ETA: 52:39 - loss: 0.6273 - regression_loss: 0.5063 - classification_loss: 0.1210
 3083/10000 [========>.....................] - ETA: 52:39 - loss: 0.6274 - regression_loss: 0.5064 - classification_loss: 0.1210
 3084/10000 [========>.....................] - ETA: 52:38 - loss: 0.6276 - regression_loss: 0.5065 - classification_loss: 0.1211
 3085/10000 [========>.....................] - ETA: 52:38 - loss: 0.6276 - regression_loss: 0.5066 - classification_loss: 0.1211
 3086/10000 [========>.....................] - ETA: 52:38 - loss: 0.6276 - regression_loss: 0.5065 - classification_loss: 0.1211
 3087/10000 [========>.....................] - ETA: 52:37 - loss: 0.6275 - regression_loss: 0.5064 - classification_loss: 0.1210
 3088/10000 [========>.....................] - ETA: 52:37 - loss: 0.6276 - regression_loss: 0.5066 - classification_loss: 0.1210
 3089/10000 [========>.....................] - ETA: 52:36 - loss: 0.6279 - regression_loss: 0.5068 - classification_loss: 0.1211
 3090/10000 [========>.....................] - ETA: 52:35 - loss: 0.6280 - regression_loss: 0.5069 - classification_loss: 0.1211
 3091/10000 [========>.....................] - ETA: 52:35 - loss: 0.6281 - regression_loss: 0.5070 - classification_loss: 0.1211
 3092/10000 [========>.....................] - ETA: 52:35 - loss: 0.6283 - regression_loss: 0.5072 - classification_loss: 0.1211
 3093/10000 [========>.....................] - ETA: 52:34 - loss: 0.6282 - regression_loss: 0.5071 - classification_loss: 0.1211
 3094/10000 [========>.....................] - ETA: 52:34 - loss: 0.6282 - regression_loss: 0.5072 - classification_loss: 0.1211
 3095/10000 [========>.....................] - ETA: 52:33 - loss: 0.6281 - regression_loss: 0.5070 - classification_loss: 0.1211
 3096/10000 [========>.....................] - ETA: 52:33 - loss: 0.6282 - regression_loss: 0.5070 - classification_loss: 0.1212
 3097/10000 [========>.....................] - ETA: 52:32 - loss: 0.6280 - regression_loss: 0.5069 - classification_loss: 0.1212
 3098/10000 [========>.....................] - ETA: 52:32 - loss: 0.6281 - regression_loss: 0.5069 - classification_loss: 0.1212
 3099/10000 [========>.....................] - ETA: 52:31 - loss: 0.6280 - regression_loss: 0.5068 - classification_loss: 0.1211
 3100/10000 [========>.....................] - ETA: 52:31 - loss: 0.6279 - regression_loss: 0.5068 - classification_loss: 0.1211
 3101/10000 [========>.....................] - ETA: 52:30 - loss: 0.6279 - regression_loss: 0.5068 - classification_loss: 0.1211
 3102/10000 [========>.....................] - ETA: 52:30 - loss: 0.6281 - regression_loss: 0.5070 - classification_loss: 0.1212
 3103/10000 [========>.....................] - ETA: 52:29 - loss: 0.6281 - regression_loss: 0.5069 - classification_loss: 0.1211
 3104/10000 [========>.....................] - ETA: 52:29 - loss: 0.6280 - regression_loss: 0.5069 - classification_loss: 0.1211
 3105/10000 [========>.....................] - ETA: 52:28 - loss: 0.6280 - regression_loss: 0.5068 - classification_loss: 0.1212
 3106/10000 [========>.....................] - ETA: 52:28 - loss: 0.6280 - regression_loss: 0.5068 - classification_loss: 0.1212
 3107/10000 [========>.....................] - ETA: 52:28 - loss: 0.6280 - regression_loss: 0.5068 - classification_loss: 0.1212
 3108/10000 [========>.....................] - ETA: 52:27 - loss: 0.6285 - regression_loss: 0.5072 - classification_loss: 0.1213
 3109/10000 [========>.....................] - ETA: 52:27 - loss: 0.6284 - regression_loss: 0.5071 - classification_loss: 0.1213
 3110/10000 [========>.....................] - ETA: 52:26 - loss: 0.6284 - regression_loss: 0.5072 - classification_loss: 0.1213
 3111/10000 [========>.....................] - ETA: 52:26 - loss: 0.6284 - regression_loss: 0.5071 - classification_loss: 0.1213
 3112/10000 [========>.....................] - ETA: 52:26 - loss: 0.6288 - regression_loss: 0.5074 - classification_loss: 0.1214
 3113/10000 [========>.....................] - ETA: 52:25 - loss: 0.6288 - regression_loss: 0.5074 - classification_loss: 0.1214
 3114/10000 [========>.....................] - ETA: 52:25 - loss: 0.6289 - regression_loss: 0.5075 - classification_loss: 0.1214
 3115/10000 [========>.....................] - ETA: 52:24 - loss: 0.6288 - regression_loss: 0.5074 - classification_loss: 0.1214
 3116/10000 [========>.....................] - ETA: 52:24 - loss: 0.6288 - regression_loss: 0.5074 - classification_loss: 0.1214
 3117/10000 [========>.....................] - ETA: 52:23 - loss: 0.6289 - regression_loss: 0.5075 - classification_loss: 0.1214
 3118/10000 [========>.....................] - ETA: 52:23 - loss: 0.6290 - regression_loss: 0.5077 - classification_loss: 0.1214
 3119/10000 [========>.....................] - ETA: 52:22 - loss: 0.6291 - regression_loss: 0.5077 - classification_loss: 0.1214
 3120/10000 [========>.....................] - ETA: 52:22 - loss: 0.6291 - regression_loss: 0.5077 - classification_loss: 0.1214
 3121/10000 [========>.....................] - ETA: 52:21 - loss: 0.6291 - regression_loss: 0.5077 - classification_loss: 0.1213
 3122/10000 [========>.....................] - ETA: 52:21 - loss: 0.6291 - regression_loss: 0.5077 - classification_loss: 0.1213
 3123/10000 [========>.....................] - ETA: 52:21 - loss: 0.6290 - regression_loss: 0.5076 - classification_loss: 0.1214
 3124/10000 [========>.....................] - ETA: 52:20 - loss: 0.6293 - regression_loss: 0.5078 - classification_loss: 0.1215
 3125/10000 [========>.....................] - ETA: 52:20 - loss: 0.6293 - regression_loss: 0.5078 - classification_loss: 0.1215
 3126/10000 [========>.....................] - ETA: 52:19 - loss: 0.6291 - regression_loss: 0.5077 - classification_loss: 0.1214
 3127/10000 [========>.....................] - ETA: 52:19 - loss: 0.6291 - regression_loss: 0.5076 - classification_loss: 0.1215
 3128/10000 [========>.....................] - ETA: 52:18 - loss: 0.6291 - regression_loss: 0.5076 - classification_loss: 0.1215
 3129/10000 [========>.....................] - ETA: 52:18 - loss: 0.6290 - regression_loss: 0.5075 - classification_loss: 0.1215
 3130/10000 [========>.....................] - ETA: 52:17 - loss: 0.6290 - regression_loss: 0.5075 - classification_loss: 0.1216
 3131/10000 [========>.....................] - ETA: 52:17 - loss: 0.6292 - regression_loss: 0.5076 - classification_loss: 0.1216
 3132/10000 [========>.....................] - ETA: 52:16 - loss: 0.6292 - regression_loss: 0.5076 - classification_loss: 0.1216
 3133/10000 [========>.....................] - ETA: 52:16 - loss: 0.6291 - regression_loss: 0.5075 - classification_loss: 0.1216
 3134/10000 [========>.....................] - ETA: 52:15 - loss: 0.6291 - regression_loss: 0.5076 - classification_loss: 0.1216
 3135/10000 [========>.....................] - ETA: 52:15 - loss: 0.6294 - regression_loss: 0.5078 - classification_loss: 0.1216
 3136/10000 [========>.....................] - ETA: 52:15 - loss: 0.6296 - regression_loss: 0.5080 - classification_loss: 0.1216
 3137/10000 [========>.....................] - ETA: 52:14 - loss: 0.6298 - regression_loss: 0.5082 - classification_loss: 0.1216
 3138/10000 [========>.....................] - ETA: 52:13 - loss: 0.6299 - regression_loss: 0.5083 - classification_loss: 0.1216
 3139/10000 [========>.....................] - ETA: 52:13 - loss: 0.6300 - regression_loss: 0.5084 - classification_loss: 0.1216
 3140/10000 [========>.....................] - ETA: 52:13 - loss: 0.6301 - regression_loss: 0.5084 - classification_loss: 0.1217
 3141/10000 [========>.....................] - ETA: 52:12 - loss: 0.6300 - regression_loss: 0.5084 - classification_loss: 0.1217
 3142/10000 [========>.....................] - ETA: 52:12 - loss: 0.6300 - regression_loss: 0.5083 - classification_loss: 0.1217
 3143/10000 [========>.....................] - ETA: 52:11 - loss: 0.6299 - regression_loss: 0.5082 - classification_loss: 0.1217
 3144/10000 [========>.....................] - ETA: 52:11 - loss: 0.6298 - regression_loss: 0.5082 - classification_loss: 0.1217
 3145/10000 [========>.....................] - ETA: 52:10 - loss: 0.6299 - regression_loss: 0.5082 - classification_loss: 0.1217
 3146/10000 [========>.....................] - ETA: 52:10 - loss: 0.6298 - regression_loss: 0.5082 - classification_loss: 0.1216
 3147/10000 [========>.....................] - ETA: 52:09 - loss: 0.6298 - regression_loss: 0.5082 - classification_loss: 0.1216
 3148/10000 [========>.....................] - ETA: 52:09 - loss: 0.6298 - regression_loss: 0.5081 - classification_loss: 0.1216
 3149/10000 [========>.....................] - ETA: 52:08 - loss: 0.6297 - regression_loss: 0.5081 - classification_loss: 0.1216
 3150/10000 [========>.....................] - ETA: 52:08 - loss: 0.6296 - regression_loss: 0.5080 - classification_loss: 0.1216
 3151/10000 [========>.....................] - ETA: 52:07 - loss: 0.6295 - regression_loss: 0.5079 - classification_loss: 0.1216
 3152/10000 [========>.....................] - ETA: 52:07 - loss: 0.6296 - regression_loss: 0.5080 - classification_loss: 0.1216
 3153/10000 [========>.....................] - ETA: 52:06 - loss: 0.6297 - regression_loss: 0.5081 - classification_loss: 0.1216
 3154/10000 [========>.....................] - ETA: 52:06 - loss: 0.6297 - regression_loss: 0.5080 - classification_loss: 0.1216
 3155/10000 [========>.....................] - ETA: 52:06 - loss: 0.6295 - regression_loss: 0.5079 - classification_loss: 0.1216
 3156/10000 [========>.....................] - ETA: 52:05 - loss: 0.6294 - regression_loss: 0.5078 - classification_loss: 0.1216
 3157/10000 [========>.....................] - ETA: 52:05 - loss: 0.6293 - regression_loss: 0.5077 - classification_loss: 0.1215
 3158/10000 [========>.....................] - ETA: 52:04 - loss: 0.6292 - regression_loss: 0.5077 - classification_loss: 0.1215
 3159/10000 [========>.....................] - ETA: 52:04 - loss: 0.6292 - regression_loss: 0.5077 - classification_loss: 0.1215
 3160/10000 [========>.....................] - ETA: 52:03 - loss: 0.6293 - regression_loss: 0.5077 - classification_loss: 0.1215
 3161/10000 [========>.....................] - ETA: 52:03 - loss: 0.6293 - regression_loss: 0.5077 - classification_loss: 0.1215
 3162/10000 [========>.....................] - ETA: 52:02 - loss: 0.6293 - regression_loss: 0.5078 - classification_loss: 0.1215
 3163/10000 [========>.....................] - ETA: 52:02 - loss: 0.6295 - regression_loss: 0.5080 - classification_loss: 0.1216
 3164/10000 [========>.....................] - ETA: 52:01 - loss: 0.6294 - regression_loss: 0.5079 - classification_loss: 0.1215
 3165/10000 [========>.....................] - ETA: 52:01 - loss: 0.6293 - regression_loss: 0.5078 - classification_loss: 0.1215
 3166/10000 [========>.....................] - ETA: 52:01 - loss: 0.6295 - regression_loss: 0.5080 - classification_loss: 0.1215
 3167/10000 [========>.....................] - ETA: 52:00 - loss: 0.6297 - regression_loss: 0.5082 - classification_loss: 0.1216
 3168/10000 [========>.....................] - ETA: 52:00 - loss: 0.6297 - regression_loss: 0.5082 - classification_loss: 0.1215
 3169/10000 [========>.....................] - ETA: 51:59 - loss: 0.6296 - regression_loss: 0.5081 - classification_loss: 0.1215
 3170/10000 [========>.....................] - ETA: 51:59 - loss: 0.6297 - regression_loss: 0.5082 - classification_loss: 0.1216
 3171/10000 [========>.....................] - ETA: 51:58 - loss: 0.6296 - regression_loss: 0.5081 - classification_loss: 0.1215
 3172/10000 [========>.....................] - ETA: 51:58 - loss: 0.6298 - regression_loss: 0.5082 - classification_loss: 0.1216
 3173/10000 [========>.....................] - ETA: 51:57 - loss: 0.6299 - regression_loss: 0.5083 - classification_loss: 0.1215
 3174/10000 [========>.....................] - ETA: 51:57 - loss: 0.6299 - regression_loss: 0.5083 - classification_loss: 0.1216
 3175/10000 [========>.....................] - ETA: 51:57 - loss: 0.6301 - regression_loss: 0.5085 - classification_loss: 0.1216
 3176/10000 [========>.....................] - ETA: 51:56 - loss: 0.6300 - regression_loss: 0.5084 - classification_loss: 0.1216
 3177/10000 [========>.....................] - ETA: 51:56 - loss: 0.6299 - regression_loss: 0.5083 - classification_loss: 0.1216
 3178/10000 [========>.....................] - ETA: 51:55 - loss: 0.6298 - regression_loss: 0.5082 - classification_loss: 0.1216
 3179/10000 [========>.....................] - ETA: 51:55 - loss: 0.6298 - regression_loss: 0.5083 - classification_loss: 0.1216
 3180/10000 [========>.....................] - ETA: 51:54 - loss: 0.6297 - regression_loss: 0.5082 - classification_loss: 0.1216
 3181/10000 [========>.....................] - ETA: 51:54 - loss: 0.6298 - regression_loss: 0.5082 - classification_loss: 0.1216
 3182/10000 [========>.....................] - ETA: 51:53 - loss: 0.6299 - regression_loss: 0.5083 - classification_loss: 0.1216
 3183/10000 [========>.....................] - ETA: 51:53 - loss: 0.6298 - regression_loss: 0.5082 - classification_loss: 0.1216
 3184/10000 [========>.....................] - ETA: 51:53 - loss: 0.6297 - regression_loss: 0.5081 - classification_loss: 0.1216
 3185/10000 [========>.....................] - ETA: 51:52 - loss: 0.6297 - regression_loss: 0.5081 - classification_loss: 0.1216
 3186/10000 [========>.....................] - ETA: 51:52 - loss: 0.6296 - regression_loss: 0.5080 - classification_loss: 0.1216
 3187/10000 [========>.....................] - ETA: 51:51 - loss: 0.6296 - regression_loss: 0.5081 - classification_loss: 0.1216
 3188/10000 [========>.....................] - ETA: 51:51 - loss: 0.6296 - regression_loss: 0.5080 - classification_loss: 0.1216
 3189/10000 [========>.....................] - ETA: 51:50 - loss: 0.6295 - regression_loss: 0.5079 - classification_loss: 0.1215
 3190/10000 [========>.....................] - ETA: 51:50 - loss: 0.6294 - regression_loss: 0.5079 - classification_loss: 0.1215
 3191/10000 [========>.....................] - ETA: 51:50 - loss: 0.6295 - regression_loss: 0.5080 - classification_loss: 0.1215
 3192/10000 [========>.....................] - ETA: 51:49 - loss: 0.6298 - regression_loss: 0.5081 - classification_loss: 0.1217
 3193/10000 [========>.....................] - ETA: 51:48 - loss: 0.6296 - regression_loss: 0.5080 - classification_loss: 0.1216
 3194/10000 [========>.....................] - ETA: 51:48 - loss: 0.6297 - regression_loss: 0.5080 - classification_loss: 0.1217
 3195/10000 [========>.....................] - ETA: 51:47 - loss: 0.6298 - regression_loss: 0.5081 - classification_loss: 0.1217
 3196/10000 [========>.....................] - ETA: 51:47 - loss: 0.6298 - regression_loss: 0.5081 - classification_loss: 0.1217
 3197/10000 [========>.....................] - ETA: 51:47 - loss: 0.6296 - regression_loss: 0.5080 - classification_loss: 0.1216
 3198/10000 [========>.....................] - ETA: 51:46 - loss: 0.6295 - regression_loss: 0.5079 - classification_loss: 0.1216
 3199/10000 [========>.....................] - ETA: 51:46 - loss: 0.6295 - regression_loss: 0.5079 - classification_loss: 0.1216
 3200/10000 [========>.....................] - ETA: 51:45 - loss: 0.6294 - regression_loss: 0.5078 - classification_loss: 0.1216
 3201/10000 [========>.....................] - ETA: 51:45 - loss: 0.6294 - regression_loss: 0.5078 - classification_loss: 0.1216
 3202/10000 [========>.....................] - ETA: 51:44 - loss: 0.6296 - regression_loss: 0.5080 - classification_loss: 0.1216
 3203/10000 [========>.....................] - ETA: 51:44 - loss: 0.6295 - regression_loss: 0.5079 - classification_loss: 0.1216
 3204/10000 [========>.....................] - ETA: 51:43 - loss: 0.6295 - regression_loss: 0.5079 - classification_loss: 0.1216
 3205/10000 [========>.....................] - ETA: 51:43 - loss: 0.6295 - regression_loss: 0.5079 - classification_loss: 0.1216
 3206/10000 [========>.....................] - ETA: 51:42 - loss: 0.6294 - regression_loss: 0.5078 - classification_loss: 0.1216
 3207/10000 [========>.....................] - ETA: 51:42 - loss: 0.6294 - regression_loss: 0.5078 - classification_loss: 0.1216
 3208/10000 [========>.....................] - ETA: 51:41 - loss: 0.6295 - regression_loss: 0.5079 - classification_loss: 0.1216
 3209/10000 [========>.....................] - ETA: 51:41 - loss: 0.6294 - regression_loss: 0.5078 - classification_loss: 0.1216
 3210/10000 [========>.....................] - ETA: 51:41 - loss: 0.6295 - regression_loss: 0.5079 - classification_loss: 0.1216
 3211/10000 [========>.....................] - ETA: 51:40 - loss: 0.6294 - regression_loss: 0.5078 - classification_loss: 0.1216
 3212/10000 [========>.....................] - ETA: 51:40 - loss: 0.6294 - regression_loss: 0.5078 - classification_loss: 0.1216
 3213/10000 [========>.....................] - ETA: 51:39 - loss: 0.6295 - regression_loss: 0.5079 - classification_loss: 0.1216
 3214/10000 [========>.....................] - ETA: 51:39 - loss: 0.6296 - regression_loss: 0.5080 - classification_loss: 0.1216
 3215/10000 [========>.....................] - ETA: 51:38 - loss: 0.6298 - regression_loss: 0.5082 - classification_loss: 0.1216
 3216/10000 [========>.....................] - ETA: 51:38 - loss: 0.6297 - regression_loss: 0.5081 - classification_loss: 0.1216
 3217/10000 [========>.....................] - ETA: 51:37 - loss: 0.6297 - regression_loss: 0.5081 - classification_loss: 0.1216
 3218/10000 [========>.....................] - ETA: 51:37 - loss: 0.6296 - regression_loss: 0.5081 - classification_loss: 0.1216
 3219/10000 [========>.....................] - ETA: 51:36 - loss: 0.6295 - regression_loss: 0.5080 - classification_loss: 0.1216
 3220/10000 [========>.....................] - ETA: 51:36 - loss: 0.6297 - regression_loss: 0.5081 - classification_loss: 0.1216
 3221/10000 [========>.....................] - ETA: 51:35 - loss: 0.6298 - regression_loss: 0.5083 - classification_loss: 0.1216
 3222/10000 [========>.....................] - ETA: 51:35 - loss: 0.6299 - regression_loss: 0.5083 - classification_loss: 0.1216
 3223/10000 [========>.....................] - ETA: 51:35 - loss: 0.6297 - regression_loss: 0.5082 - classification_loss: 0.1216
 3224/10000 [========>.....................] - ETA: 51:34 - loss: 0.6297 - regression_loss: 0.5081 - classification_loss: 0.1216
 3225/10000 [========>.....................] - ETA: 51:34 - loss: 0.6301 - regression_loss: 0.5085 - classification_loss: 0.1216
 3226/10000 [========>.....................] - ETA: 51:33 - loss: 0.6300 - regression_loss: 0.5084 - classification_loss: 0.1216
 3227/10000 [========>.....................] - ETA: 51:33 - loss: 0.6299 - regression_loss: 0.5083 - classification_loss: 0.1216
 3228/10000 [========>.....................] - ETA: 51:32 - loss: 0.6298 - regression_loss: 0.5082 - classification_loss: 0.1215
 3229/10000 [========>.....................] - ETA: 51:32 - loss: 0.6297 - regression_loss: 0.5082 - classification_loss: 0.1216
 3230/10000 [========>.....................] - ETA: 51:31 - loss: 0.6300 - regression_loss: 0.5084 - classification_loss: 0.1216
 3231/10000 [========>.....................] - ETA: 51:31 - loss: 0.6299 - regression_loss: 0.5083 - classification_loss: 0.1215
 3232/10000 [========>.....................] - ETA: 51:30 - loss: 0.6299 - regression_loss: 0.5083 - classification_loss: 0.1215
 3233/10000 [========>.....................] - ETA: 51:30 - loss: 0.6299 - regression_loss: 0.5084 - classification_loss: 0.1215
 3234/10000 [========>.....................] - ETA: 51:30 - loss: 0.6299 - regression_loss: 0.5083 - classification_loss: 0.1216
 3235/10000 [========>.....................] - ETA: 51:29 - loss: 0.6297 - regression_loss: 0.5082 - classification_loss: 0.1215
 3236/10000 [========>.....................] - ETA: 51:29 - loss: 0.6297 - regression_loss: 0.5082 - classification_loss: 0.1215
 3237/10000 [========>.....................] - ETA: 51:28 - loss: 0.6295 - regression_loss: 0.5081 - classification_loss: 0.1215
 3238/10000 [========>.....................] - ETA: 51:28 - loss: 0.6295 - regression_loss: 0.5080 - classification_loss: 0.1215
 3239/10000 [========>.....................] - ETA: 51:27 - loss: 0.6294 - regression_loss: 0.5079 - classification_loss: 0.1215
 3240/10000 [========>.....................] - ETA: 51:27 - loss: 0.6295 - regression_loss: 0.5079 - classification_loss: 0.1216
 3241/10000 [========>.....................] - ETA: 51:26 - loss: 0.6295 - regression_loss: 0.5079 - classification_loss: 0.1216
 3242/10000 [========>.....................] - ETA: 51:26 - loss: 0.6294 - regression_loss: 0.5078 - classification_loss: 0.1216
 3243/10000 [========>.....................] - ETA: 51:26 - loss: 0.6295 - regression_loss: 0.5079 - classification_loss: 0.1216
 3244/10000 [========>.....................] - ETA: 51:25 - loss: 0.6294 - regression_loss: 0.5078 - classification_loss: 0.1216
 3245/10000 [========>.....................] - ETA: 51:25 - loss: 0.6298 - regression_loss: 0.5081 - classification_loss: 0.1217
 3246/10000 [========>.....................] - ETA: 51:24 - loss: 0.6301 - regression_loss: 0.5084 - classification_loss: 0.1217
 3247/10000 [========>.....................] - ETA: 51:24 - loss: 0.6300 - regression_loss: 0.5083 - classification_loss: 0.1217
 3248/10000 [========>.....................] - ETA: 51:23 - loss: 0.6301 - regression_loss: 0.5084 - classification_loss: 0.1217
 3249/10000 [========>.....................] - ETA: 51:23 - loss: 0.6303 - regression_loss: 0.5086 - classification_loss: 0.1217
 3250/10000 [========>.....................] - ETA: 51:22 - loss: 0.6302 - regression_loss: 0.5085 - classification_loss: 0.1217
 3251/10000 [========>.....................] - ETA: 51:22 - loss: 0.6303 - regression_loss: 0.5085 - classification_loss: 0.1217
 3252/10000 [========>.....................] - ETA: 51:21 - loss: 0.6302 - regression_loss: 0.5085 - classification_loss: 0.1217
 3253/10000 [========>.....................] - ETA: 51:21 - loss: 0.6301 - regression_loss: 0.5084 - classification_loss: 0.1217
 3254/10000 [========>.....................] - ETA: 51:20 - loss: 0.6300 - regression_loss: 0.5083 - classification_loss: 0.1217
 3255/10000 [========>.....................] - ETA: 51:20 - loss: 0.6299 - regression_loss: 0.5082 - classification_loss: 0.1217
 3256/10000 [========>.....................] - ETA: 51:20 - loss: 0.6299 - regression_loss: 0.5082 - classification_loss: 0.1216
 3257/10000 [========>.....................] - ETA: 51:19 - loss: 0.6300 - regression_loss: 0.5083 - classification_loss: 0.1217
 3258/10000 [========>.....................] - ETA: 51:19 - loss: 0.6302 - regression_loss: 0.5085 - classification_loss: 0.1217
 3259/10000 [========>.....................] - ETA: 51:18 - loss: 0.6302 - regression_loss: 0.5085 - classification_loss: 0.1217
 3260/10000 [========>.....................] - ETA: 51:18 - loss: 0.6304 - regression_loss: 0.5087 - classification_loss: 0.1217
 3261/10000 [========>.....................] - ETA: 51:17 - loss: 0.6305 - regression_loss: 0.5088 - classification_loss: 0.1217
 3262/10000 [========>.....................] - ETA: 51:17 - loss: 0.6306 - regression_loss: 0.5089 - classification_loss: 0.1218
 3263/10000 [========>.....................] - ETA: 51:16 - loss: 0.6307 - regression_loss: 0.5089 - classification_loss: 0.1218
 3264/10000 [========>.....................] - ETA: 51:16 - loss: 0.6307 - regression_loss: 0.5089 - classification_loss: 0.1218
 3265/10000 [========>.....................] - ETA: 51:15 - loss: 0.6306 - regression_loss: 0.5089 - classification_loss: 0.1218
 3266/10000 [========>.....................] - ETA: 51:15 - loss: 0.6306 - regression_loss: 0.5089 - classification_loss: 0.1218
 3267/10000 [========>.....................] - ETA: 51:15 - loss: 0.6306 - regression_loss: 0.5088 - classification_loss: 0.1218
 3268/10000 [========>.....................] - ETA: 51:14 - loss: 0.6308 - regression_loss: 0.5090 - classification_loss: 0.1218
 3269/10000 [========>.....................] - ETA: 51:14 - loss: 0.6307 - regression_loss: 0.5089 - classification_loss: 0.1218
 3270/10000 [========>.....................] - ETA: 51:13 - loss: 0.6307 - regression_loss: 0.5089 - classification_loss: 0.1218
 3271/10000 [========>.....................] - ETA: 51:13 - loss: 0.6307 - regression_loss: 0.5089 - classification_loss: 0.1218
 3272/10000 [========>.....................] - ETA: 51:12 - loss: 0.6307 - regression_loss: 0.5089 - classification_loss: 0.1218
 3273/10000 [========>.....................] - ETA: 51:12 - loss: 0.6306 - regression_loss: 0.5089 - classification_loss: 0.1217
 3274/10000 [========>.....................] - ETA: 51:11 - loss: 0.6305 - regression_loss: 0.5088 - classification_loss: 0.1217
 3275/10000 [========>.....................] - ETA: 51:10 - loss: 0.6308 - regression_loss: 0.5090 - classification_loss: 0.1218
 3276/10000 [========>.....................] - ETA: 51:10 - loss: 0.6307 - regression_loss: 0.5089 - classification_loss: 0.1218
 3277/10000 [========>.....................] - ETA: 51:10 - loss: 0.6310 - regression_loss: 0.5092 - classification_loss: 0.1218
 3278/10000 [========>.....................] - ETA: 51:09 - loss: 0.6313 - regression_loss: 0.5094 - classification_loss: 0.1220
 3279/10000 [========>.....................] - ETA: 51:09 - loss: 0.6313 - regression_loss: 0.5093 - classification_loss: 0.1219
 3280/10000 [========>.....................] - ETA: 51:08 - loss: 0.6315 - regression_loss: 0.5095 - classification_loss: 0.1220
 3281/10000 [========>.....................] - ETA: 51:08 - loss: 0.6315 - regression_loss: 0.5096 - classification_loss: 0.1220
 3282/10000 [========>.....................] - ETA: 51:08 - loss: 0.6316 - regression_loss: 0.5096 - classification_loss: 0.1220
 3283/10000 [========>.....................] - ETA: 51:07 - loss: 0.6315 - regression_loss: 0.5095 - classification_loss: 0.1219
 3284/10000 [========>.....................] - ETA: 51:07 - loss: 0.6314 - regression_loss: 0.5095 - classification_loss: 0.1219
 3285/10000 [========>.....................] - ETA: 51:06 - loss: 0.6313 - regression_loss: 0.5094 - classification_loss: 0.1219
 3286/10000 [========>.....................] - ETA: 51:06 - loss: 0.6316 - regression_loss: 0.5096 - classification_loss: 0.1220
 3287/10000 [========>.....................] - ETA: 51:05 - loss: 0.6316 - regression_loss: 0.5096 - classification_loss: 0.1220
 3288/10000 [========>.....................] - ETA: 51:04 - loss: 0.6317 - regression_loss: 0.5097 - classification_loss: 0.1220
 3289/10000 [========>.....................] - ETA: 51:04 - loss: 0.6318 - regression_loss: 0.5098 - classification_loss: 0.1220
 3290/10000 [========>.....................] - ETA: 51:03 - loss: 0.6317 - regression_loss: 0.5097 - classification_loss: 0.1220
 3291/10000 [========>.....................] - ETA: 51:03 - loss: 0.6316 - regression_loss: 0.5096 - classification_loss: 0.1220
 3292/10000 [========>.....................] - ETA: 51:03 - loss: 0.6316 - regression_loss: 0.5096 - classification_loss: 0.1220
 3293/10000 [========>.....................] - ETA: 51:02 - loss: 0.6317 - regression_loss: 0.5097 - classification_loss: 0.1220
 3294/10000 [========>.....................] - ETA: 51:02 - loss: 0.6317 - regression_loss: 0.5096 - classification_loss: 0.1220
 3295/10000 [========>.....................] - ETA: 51:01 - loss: 0.6315 - regression_loss: 0.5095 - classification_loss: 0.1220
 3296/10000 [========>.....................] - ETA: 51:01 - loss: 0.6315 - regression_loss: 0.5096 - classification_loss: 0.1220
 3297/10000 [========>.....................] - ETA: 51:00 - loss: 0.6314 - regression_loss: 0.5095 - classification_loss: 0.1219
 3298/10000 [========>.....................] - ETA: 51:00 - loss: 0.6315 - regression_loss: 0.5096 - classification_loss: 0.1220
 3299/10000 [========>.....................] - ETA: 50:59 - loss: 0.6316 - regression_loss: 0.5096 - classification_loss: 0.1220
 3300/10000 [========>.....................] - ETA: 50:59 - loss: 0.6315 - regression_loss: 0.5096 - classification_loss: 0.1219
 3301/10000 [========>.....................] - ETA: 50:58 - loss: 0.6314 - regression_loss: 0.5095 - classification_loss: 0.1219
 3302/10000 [========>.....................] - ETA: 50:58 - loss: 0.6313 - regression_loss: 0.5094 - classification_loss: 0.1219
 3303/10000 [========>.....................] - ETA: 50:58 - loss: 0.6314 - regression_loss: 0.5095 - classification_loss: 0.1219
 3304/10000 [========>.....................] - ETA: 50:57 - loss: 0.6313 - regression_loss: 0.5094 - classification_loss: 0.1219
 3305/10000 [========>.....................] - ETA: 50:57 - loss: 0.6312 - regression_loss: 0.5094 - classification_loss: 0.1219
 3306/10000 [========>.....................] - ETA: 50:56 - loss: 0.6312 - regression_loss: 0.5093 - classification_loss: 0.1219
 3307/10000 [========>.....................] - ETA: 50:56 - loss: 0.6313 - regression_loss: 0.5094 - classification_loss: 0.1219
 3308/10000 [========>.....................] - ETA: 50:55 - loss: 0.6313 - regression_loss: 0.5095 - classification_loss: 0.1218
 3309/10000 [========>.....................] - ETA: 50:55 - loss: 0.6312 - regression_loss: 0.5094 - classification_loss: 0.1218
 3310/10000 [========>.....................] - ETA: 50:54 - loss: 0.6311 - regression_loss: 0.5093 - classification_loss: 0.1218
 3311/10000 [========>.....................] - ETA: 50:54 - loss: 0.6311 - regression_loss: 0.5093 - classification_loss: 0.1218
 3312/10000 [========>.....................] - ETA: 50:54 - loss: 0.6311 - regression_loss: 0.5093 - classification_loss: 0.1218
 3313/10000 [========>.....................] - ETA: 50:53 - loss: 0.6313 - regression_loss: 0.5095 - classification_loss: 0.1218
 3314/10000 [========>.....................] - ETA: 50:53 - loss: 0.6312 - regression_loss: 0.5094 - classification_loss: 0.1218
 3315/10000 [========>.....................] - ETA: 50:52 - loss: 0.6312 - regression_loss: 0.5094 - classification_loss: 0.1218
 3316/10000 [========>.....................] - ETA: 50:52 - loss: 0.6312 - regression_loss: 0.5094 - classification_loss: 0.1218
 3317/10000 [========>.....................] - ETA: 50:51 - loss: 0.6312 - regression_loss: 0.5094 - classification_loss: 0.1218
 3318/10000 [========>.....................] - ETA: 50:51 - loss: 0.6313 - regression_loss: 0.5095 - classification_loss: 0.1218
 3319/10000 [========>.....................] - ETA: 50:51 - loss: 0.6312 - regression_loss: 0.5094 - classification_loss: 0.1218
 3320/10000 [========>.....................] - ETA: 50:50 - loss: 0.6310 - regression_loss: 0.5093 - classification_loss: 0.1218
 3321/10000 [========>.....................] - ETA: 50:50 - loss: 0.6313 - regression_loss: 0.5094 - classification_loss: 0.1218
 3322/10000 [========>.....................] - ETA: 50:49 - loss: 0.6314 - regression_loss: 0.5095 - classification_loss: 0.1218
 3323/10000 [========>.....................] - ETA: 50:49 - loss: 0.6313 - regression_loss: 0.5094 - classification_loss: 0.1218
 3324/10000 [========>.....................] - ETA: 50:48 - loss: 0.6313 - regression_loss: 0.5095 - classification_loss: 0.1218
 3325/10000 [========>.....................] - ETA: 50:48 - loss: 0.6318 - regression_loss: 0.5098 - classification_loss: 0.1220
 3326/10000 [========>.....................] - ETA: 50:47 - loss: 0.6318 - regression_loss: 0.5098 - classification_loss: 0.1220
 3327/10000 [========>.....................] - ETA: 50:47 - loss: 0.6319 - regression_loss: 0.5100 - classification_loss: 0.1220
 3328/10000 [========>.....................] - ETA: 50:46 - loss: 0.6319 - regression_loss: 0.5100 - classification_loss: 0.1220
 3329/10000 [========>.....................] - ETA: 50:46 - loss: 0.6318 - regression_loss: 0.5099 - classification_loss: 0.1219
 3330/10000 [========>.....................] - ETA: 50:45 - loss: 0.6318 - regression_loss: 0.5098 - classification_loss: 0.1219
 3331/10000 [========>.....................] - ETA: 50:45 - loss: 0.6320 - regression_loss: 0.5100 - classification_loss: 0.1220
 3332/10000 [========>.....................] - ETA: 50:44 - loss: 0.6319 - regression_loss: 0.5099 - classification_loss: 0.1219
 3333/10000 [========>.....................] - ETA: 50:44 - loss: 0.6322 - regression_loss: 0.5102 - classification_loss: 0.1220
 3334/10000 [=========>....................] - ETA: 50:44 - loss: 0.6321 - regression_loss: 0.5101 - classification_loss: 0.1220
 3335/10000 [=========>....................] - ETA: 50:43 - loss: 0.6322 - regression_loss: 0.5102 - classification_loss: 0.1220
 3336/10000 [=========>....................] - ETA: 50:43 - loss: 0.6323 - regression_loss: 0.5103 - classification_loss: 0.1220
 3337/10000 [=========>....................] - ETA: 50:42 - loss: 0.6324 - regression_loss: 0.5104 - classification_loss: 0.1220
 3338/10000 [=========>....................] - ETA: 50:42 - loss: 0.6324 - regression_loss: 0.5104 - classification_loss: 0.1220
 3339/10000 [=========>....................] - ETA: 50:42 - loss: 0.6325 - regression_loss: 0.5104 - classification_loss: 0.1221
 3340/10000 [=========>....................] - ETA: 50:41 - loss: 0.6324 - regression_loss: 0.5103 - classification_loss: 0.1221
 3341/10000 [=========>....................] - ETA: 50:41 - loss: 0.6323 - regression_loss: 0.5103 - classification_loss: 0.1220
 3342/10000 [=========>....................] - ETA: 50:40 - loss: 0.6322 - regression_loss: 0.5102 - classification_loss: 0.1220
 3343/10000 [=========>....................] - ETA: 50:40 - loss: 0.6321 - regression_loss: 0.5101 - classification_loss: 0.1220
 3344/10000 [=========>....................] - ETA: 50:39 - loss: 0.6323 - regression_loss: 0.5103 - classification_loss: 0.1220
 3345/10000 [=========>....................] - ETA: 50:39 - loss: 0.6324 - regression_loss: 0.5104 - classification_loss: 0.1220
 3346/10000 [=========>....................] - ETA: 50:39 - loss: 0.6323 - regression_loss: 0.5103 - classification_loss: 0.1220
 3347/10000 [=========>....................] - ETA: 50:38 - loss: 0.6323 - regression_loss: 0.5103 - classification_loss: 0.1220
 3348/10000 [=========>....................] - ETA: 50:37 - loss: 0.6322 - regression_loss: 0.5103 - classification_loss: 0.1220
 3349/10000 [=========>....................] - ETA: 50:37 - loss: 0.6322 - regression_loss: 0.5103 - classification_loss: 0.1220
 3350/10000 [=========>....................] - ETA: 50:36 - loss: 0.6322 - regression_loss: 0.5102 - classification_loss: 0.1220
 3351/10000 [=========>....................] - ETA: 50:36 - loss: 0.6322 - regression_loss: 0.5102 - classification_loss: 0.1220
 3352/10000 [=========>....................] - ETA: 50:36 - loss: 0.6321 - regression_loss: 0.5101 - classification_loss: 0.1220
 3353/10000 [=========>....................] - ETA: 50:35 - loss: 0.6323 - regression_loss: 0.5103 - classification_loss: 0.1220
 3354/10000 [=========>....................] - ETA: 50:35 - loss: 0.6323 - regression_loss: 0.5103 - classification_loss: 0.1220
 3355/10000 [=========>....................] - ETA: 50:34 - loss: 0.6322 - regression_loss: 0.5102 - classification_loss: 0.1220
 3356/10000 [=========>....................] - ETA: 50:34 - loss: 0.6324 - regression_loss: 0.5104 - classification_loss: 0.1220
 3357/10000 [=========>....................] - ETA: 50:33 - loss: 0.6326 - regression_loss: 0.5106 - classification_loss: 0.1221
 3358/10000 [=========>....................] - ETA: 50:33 - loss: 0.6325 - regression_loss: 0.5104 - classification_loss: 0.1220
 3359/10000 [=========>....................] - ETA: 50:32 - loss: 0.6324 - regression_loss: 0.5104 - classification_loss: 0.1220
 3360/10000 [=========>....................] - ETA: 50:32 - loss: 0.6322 - regression_loss: 0.5102 - classification_loss: 0.1220
 3361/10000 [=========>....................] - ETA: 50:31 - loss: 0.6322 - regression_loss: 0.5102 - classification_loss: 0.1220
 3362/10000 [=========>....................] - ETA: 50:31 - loss: 0.6322 - regression_loss: 0.5102 - classification_loss: 0.1220
 3363/10000 [=========>....................] - ETA: 50:31 - loss: 0.6321 - regression_loss: 0.5102 - classification_loss: 0.1220
 3364/10000 [=========>....................] - ETA: 50:30 - loss: 0.6322 - regression_loss: 0.5102 - classification_loss: 0.1220
 3365/10000 [=========>....................] - ETA: 50:30 - loss: 0.6324 - regression_loss: 0.5103 - classification_loss: 0.1220
 3366/10000 [=========>....................] - ETA: 50:29 - loss: 0.6324 - regression_loss: 0.5103 - classification_loss: 0.1220
 3367/10000 [=========>....................] - ETA: 50:29 - loss: 0.6323 - regression_loss: 0.5102 - classification_loss: 0.1220
 3368/10000 [=========>....................] - ETA: 50:28 - loss: 0.6323 - regression_loss: 0.5103 - classification_loss: 0.1220
 3369/10000 [=========>....................] - ETA: 50:28 - loss: 0.6324 - regression_loss: 0.5103 - classification_loss: 0.1221
 3370/10000 [=========>....................] - ETA: 50:27 - loss: 0.6324 - regression_loss: 0.5103 - classification_loss: 0.1220
 3371/10000 [=========>....................] - ETA: 50:27 - loss: 0.6323 - regression_loss: 0.5103 - classification_loss: 0.1220
 3372/10000 [=========>....................] - ETA: 50:26 - loss: 0.6323 - regression_loss: 0.5103 - classification_loss: 0.1220
 3373/10000 [=========>....................] - ETA: 50:26 - loss: 0.6322 - regression_loss: 0.5102 - classification_loss: 0.1220
 3374/10000 [=========>....................] - ETA: 50:26 - loss: 0.6321 - regression_loss: 0.5101 - classification_loss: 0.1220
 3375/10000 [=========>....................] - ETA: 50:25 - loss: 0.6321 - regression_loss: 0.5101 - classification_loss: 0.1220
 3376/10000 [=========>....................] - ETA: 50:25 - loss: 0.6321 - regression_loss: 0.5101 - classification_loss: 0.1220
 3377/10000 [=========>....................] - ETA: 50:24 - loss: 0.6320 - regression_loss: 0.5100 - classification_loss: 0.1220
 3378/10000 [=========>....................] - ETA: 50:24 - loss: 0.6319 - regression_loss: 0.5099 - classification_loss: 0.1219
 3379/10000 [=========>....................] - ETA: 50:23 - loss: 0.6320 - regression_loss: 0.5100 - classification_loss: 0.1220
 3380/10000 [=========>....................] - ETA: 50:23 - loss: 0.6320 - regression_loss: 0.5100 - classification_loss: 0.1219
 3381/10000 [=========>....................] - ETA: 50:23 - loss: 0.6320 - regression_loss: 0.5100 - classification_loss: 0.1220
 3382/10000 [=========>....................] - ETA: 50:22 - loss: 0.6319 - regression_loss: 0.5099 - classification_loss: 0.1220
 3383/10000 [=========>....................] - ETA: 50:22 - loss: 0.6318 - regression_loss: 0.5098 - classification_loss: 0.1219
 3384/10000 [=========>....................] - ETA: 50:21 - loss: 0.6318 - regression_loss: 0.5098 - classification_loss: 0.1219
 3385/10000 [=========>....................] - ETA: 50:21 - loss: 0.6317 - regression_loss: 0.5098 - classification_loss: 0.1219
 3386/10000 [=========>....................] - ETA: 50:20 - loss: 0.6317 - regression_loss: 0.5098 - classification_loss: 0.1219
 3387/10000 [=========>....................] - ETA: 50:20 - loss: 0.6317 - regression_loss: 0.5098 - classification_loss: 0.1219
 3388/10000 [=========>....................] - ETA: 50:19 - loss: 0.6315 - regression_loss: 0.5096 - classification_loss: 0.1219
 3389/10000 [=========>....................] - ETA: 50:19 - loss: 0.6316 - regression_loss: 0.5097 - classification_loss: 0.1219
 3390/10000 [=========>....................] - ETA: 50:18 - loss: 0.6318 - regression_loss: 0.5099 - classification_loss: 0.1219
 3391/10000 [=========>....................] - ETA: 50:18 - loss: 0.6318 - regression_loss: 0.5099 - classification_loss: 0.1219
 3392/10000 [=========>....................] - ETA: 50:18 - loss: 0.6318 - regression_loss: 0.5099 - classification_loss: 0.1219
 3393/10000 [=========>....................] - ETA: 50:17 - loss: 0.6318 - regression_loss: 0.5099 - classification_loss: 0.1219
 3394/10000 [=========>....................] - ETA: 50:17 - loss: 0.6318 - regression_loss: 0.5099 - classification_loss: 0.1219
 3395/10000 [=========>....................] - ETA: 50:16 - loss: 0.6317 - regression_loss: 0.5098 - classification_loss: 0.1219
 3396/10000 [=========>....................] - ETA: 50:16 - loss: 0.6320 - regression_loss: 0.5101 - classification_loss: 0.1219
 3397/10000 [=========>....................] - ETA: 50:16 - loss: 0.6321 - regression_loss: 0.5102 - classification_loss: 0.1219
 3398/10000 [=========>....................] - ETA: 50:15 - loss: 0.6321 - regression_loss: 0.5102 - classification_loss: 0.1219
 3399/10000 [=========>....................] - ETA: 50:15 - loss: 0.6321 - regression_loss: 0.5102 - classification_loss: 0.1219
 3400/10000 [=========>....................] - ETA: 50:14 - loss: 0.6320 - regression_loss: 0.5101 - classification_loss: 0.1219
 3401/10000 [=========>....................] - ETA: 50:14 - loss: 0.6319 - regression_loss: 0.5101 - classification_loss: 0.1219
 3402/10000 [=========>....................] - ETA: 50:13 - loss: 0.6322 - regression_loss: 0.5102 - classification_loss: 0.1219
 3403/10000 [=========>....................] - ETA: 50:13 - loss: 0.6321 - regression_loss: 0.5102 - classification_loss: 0.1219
 3404/10000 [=========>....................] - ETA: 50:12 - loss: 0.6321 - regression_loss: 0.5102 - classification_loss: 0.1219
 3405/10000 [=========>....................] - ETA: 50:12 - loss: 0.6321 - regression_loss: 0.5102 - classification_loss: 0.1219
 3406/10000 [=========>....................] - ETA: 50:11 - loss: 0.6322 - regression_loss: 0.5103 - classification_loss: 0.1219
 3407/10000 [=========>....................] - ETA: 50:11 - loss: 0.6324 - regression_loss: 0.5105 - classification_loss: 0.1219
 3408/10000 [=========>....................] - ETA: 50:11 - loss: 0.6324 - regression_loss: 0.5105 - classification_loss: 0.1219
 3409/10000 [=========>....................] - ETA: 50:10 - loss: 0.6323 - regression_loss: 0.5105 - classification_loss: 0.1219
 3410/10000 [=========>....................] - ETA: 50:10 - loss: 0.6326 - regression_loss: 0.5107 - classification_loss: 0.1219
 3411/10000 [=========>....................] - ETA: 50:09 - loss: 0.6327 - regression_loss: 0.5108 - classification_loss: 0.1219
 3412/10000 [=========>....................] - ETA: 50:09 - loss: 0.6326 - regression_loss: 0.5107 - classification_loss: 0.1219
 3413/10000 [=========>....................] - ETA: 50:08 - loss: 0.6327 - regression_loss: 0.5108 - classification_loss: 0.1219
 3414/10000 [=========>....................] - ETA: 50:08 - loss: 0.6328 - regression_loss: 0.5109 - classification_loss: 0.1219
 3415/10000 [=========>....................] - ETA: 50:07 - loss: 0.6328 - regression_loss: 0.5109 - classification_loss: 0.1219
 3416/10000 [=========>....................] - ETA: 50:07 - loss: 0.6328 - regression_loss: 0.5109 - classification_loss: 0.1219
 3417/10000 [=========>....................] - ETA: 50:06 - loss: 0.6328 - regression_loss: 0.5109 - classification_loss: 0.1219
 3418/10000 [=========>....................] - ETA: 50:06 - loss: 0.6329 - regression_loss: 0.5110 - classification_loss: 0.1219
 3419/10000 [=========>....................] - ETA: 50:05 - loss: 0.6330 - regression_loss: 0.5111 - classification_loss: 0.1219
 3420/10000 [=========>....................] - ETA: 50:05 - loss: 0.6330 - regression_loss: 0.5111 - classification_loss: 0.1219
 3421/10000 [=========>....................] - ETA: 50:05 - loss: 0.6329 - regression_loss: 0.5110 - classification_loss: 0.1219
 3422/10000 [=========>....................] - ETA: 50:04 - loss: 0.6330 - regression_loss: 0.5111 - classification_loss: 0.1219
 3423/10000 [=========>....................] - ETA: 50:04 - loss: 0.6329 - regression_loss: 0.5110 - classification_loss: 0.1219
 3424/10000 [=========>....................] - ETA: 50:03 - loss: 0.6330 - regression_loss: 0.5111 - classification_loss: 0.1219
 3425/10000 [=========>....................] - ETA: 50:03 - loss: 0.6331 - regression_loss: 0.5112 - classification_loss: 0.1220
 3426/10000 [=========>....................] - ETA: 50:02 - loss: 0.6333 - regression_loss: 0.5113 - classification_loss: 0.1220
 3427/10000 [=========>....................] - ETA: 50:02 - loss: 0.6331 - regression_loss: 0.5111 - classification_loss: 0.1220
 3428/10000 [=========>....................] - ETA: 50:01 - loss: 0.6331 - regression_loss: 0.5112 - classification_loss: 0.1220
 3429/10000 [=========>....................] - ETA: 50:01 - loss: 0.6332 - regression_loss: 0.5113 - classification_loss: 0.1220
 3430/10000 [=========>....................] - ETA: 50:01 - loss: 0.6332 - regression_loss: 0.5112 - classification_loss: 0.1220
 3431/10000 [=========>....................] - ETA: 50:00 - loss: 0.6331 - regression_loss: 0.5111 - classification_loss: 0.1220
 3432/10000 [=========>....................] - ETA: 50:00 - loss: 0.6330 - regression_loss: 0.5111 - classification_loss: 0.1220
 3433/10000 [=========>....................] - ETA: 49:59 - loss: 0.6329 - regression_loss: 0.5109 - classification_loss: 0.1220
 3434/10000 [=========>....................] - ETA: 49:59 - loss: 0.6329 - regression_loss: 0.5110 - classification_loss: 0.1220
 3435/10000 [=========>....................] - ETA: 49:58 - loss: 0.6328 - regression_loss: 0.5109 - classification_loss: 0.1219
 3436/10000 [=========>....................] - ETA: 49:58 - loss: 0.6327 - regression_loss: 0.5107 - classification_loss: 0.1219
 3437/10000 [=========>....................] - ETA: 49:58 - loss: 0.6326 - regression_loss: 0.5107 - classification_loss: 0.1219
 3438/10000 [=========>....................] - ETA: 49:57 - loss: 0.6325 - regression_loss: 0.5106 - classification_loss: 0.1219
 3439/10000 [=========>....................] - ETA: 49:57 - loss: 0.6324 - regression_loss: 0.5105 - classification_loss: 0.1219
 3440/10000 [=========>....................] - ETA: 49:56 - loss: 0.6323 - regression_loss: 0.5104 - classification_loss: 0.1219
 3441/10000 [=========>....................] - ETA: 49:56 - loss: 0.6322 - regression_loss: 0.5103 - classification_loss: 0.1219
 3442/10000 [=========>....................] - ETA: 49:55 - loss: 0.6321 - regression_loss: 0.5103 - classification_loss: 0.1219
 3443/10000 [=========>....................] - ETA: 49:55 - loss: 0.6320 - regression_loss: 0.5102 - classification_loss: 0.1218
 3444/10000 [=========>....................] - ETA: 49:54 - loss: 0.6319 - regression_loss: 0.5101 - classification_loss: 0.1218
 3445/10000 [=========>....................] - ETA: 49:54 - loss: 0.6318 - regression_loss: 0.5100 - classification_loss: 0.1218
 3446/10000 [=========>....................] - ETA: 49:53 - loss: 0.6318 - regression_loss: 0.5100 - classification_loss: 0.1218
 3447/10000 [=========>....................] - ETA: 49:53 - loss: 0.6319 - regression_loss: 0.5101 - classification_loss: 0.1218
 3448/10000 [=========>....................] - ETA: 49:53 - loss: 0.6319 - regression_loss: 0.5100 - classification_loss: 0.1218
 3449/10000 [=========>....................] - ETA: 49:52 - loss: 0.6318 - regression_loss: 0.5100 - classification_loss: 0.1218
 3450/10000 [=========>....................] - ETA: 49:52 - loss: 0.6317 - regression_loss: 0.5099 - classification_loss: 0.1218
 3451/10000 [=========>....................] - ETA: 49:51 - loss: 0.6316 - regression_loss: 0.5098 - classification_loss: 0.1218
 3452/10000 [=========>....................] - ETA: 49:51 - loss: 0.6315 - regression_loss: 0.5098 - classification_loss: 0.1218
 3453/10000 [=========>....................] - ETA: 49:50 - loss: 0.6317 - regression_loss: 0.5099 - classification_loss: 0.1218
 3454/10000 [=========>....................] - ETA: 49:50 - loss: 0.6320 - regression_loss: 0.5100 - classification_loss: 0.1220
 3455/10000 [=========>....................] - ETA: 49:49 - loss: 0.6320 - regression_loss: 0.5100 - classification_loss: 0.1220
 3456/10000 [=========>....................] - ETA: 49:49 - loss: 0.6320 - regression_loss: 0.5100 - classification_loss: 0.1220
 3457/10000 [=========>....................] - ETA: 49:48 - loss: 0.6318 - regression_loss: 0.5099 - classification_loss: 0.1219
 3458/10000 [=========>....................] - ETA: 49:48 - loss: 0.6318 - regression_loss: 0.5099 - classification_loss: 0.1219
 3459/10000 [=========>....................] - ETA: 49:48 - loss: 0.6318 - regression_loss: 0.5098 - classification_loss: 0.1220
 3460/10000 [=========>....................] - ETA: 49:47 - loss: 0.6317 - regression_loss: 0.5098 - classification_loss: 0.1219
 3461/10000 [=========>....................] - ETA: 49:47 - loss: 0.6317 - regression_loss: 0.5098 - classification_loss: 0.1219
 3462/10000 [=========>....................] - ETA: 49:46 - loss: 0.6316 - regression_loss: 0.5097 - classification_loss: 0.1219
 3463/10000 [=========>....................] - ETA: 49:46 - loss: 0.6316 - regression_loss: 0.5098 - classification_loss: 0.1219
 3464/10000 [=========>....................] - ETA: 49:45 - loss: 0.6316 - regression_loss: 0.5097 - classification_loss: 0.1219
 3465/10000 [=========>....................] - ETA: 49:45 - loss: 0.6315 - regression_loss: 0.5096 - classification_loss: 0.1219
 3466/10000 [=========>....................] - ETA: 49:44 - loss: 0.6314 - regression_loss: 0.5096 - classification_loss: 0.1219
 3467/10000 [=========>....................] - ETA: 49:44 - loss: 0.6315 - regression_loss: 0.5096 - classification_loss: 0.1219
 3468/10000 [=========>....................] - ETA: 49:43 - loss: 0.6315 - regression_loss: 0.5096 - classification_loss: 0.1219
 3469/10000 [=========>....................] - ETA: 49:43 - loss: 0.6315 - regression_loss: 0.5096 - classification_loss: 0.1219
 3470/10000 [=========>....................] - ETA: 49:42 - loss: 0.6316 - regression_loss: 0.5097 - classification_loss: 0.1219
 3471/10000 [=========>....................] - ETA: 49:42 - loss: 0.6315 - regression_loss: 0.5096 - classification_loss: 0.1219
 3472/10000 [=========>....................] - ETA: 49:41 - loss: 0.6316 - regression_loss: 0.5097 - classification_loss: 0.1219
 3473/10000 [=========>....................] - ETA: 49:41 - loss: 0.6317 - regression_loss: 0.5098 - classification_loss: 0.1219
 3474/10000 [=========>....................] - ETA: 49:40 - loss: 0.6320 - regression_loss: 0.5100 - classification_loss: 0.1220
 3475/10000 [=========>....................] - ETA: 49:40 - loss: 0.6320 - regression_loss: 0.5100 - classification_loss: 0.1219
 3476/10000 [=========>....................] - ETA: 49:40 - loss: 0.6319 - regression_loss: 0.5100 - classification_loss: 0.1219
 3477/10000 [=========>....................] - ETA: 49:39 - loss: 0.6319 - regression_loss: 0.5100 - classification_loss: 0.1219
 3478/10000 [=========>....................] - ETA: 49:39 - loss: 0.6320 - regression_loss: 0.5100 - classification_loss: 0.1219
 3479/10000 [=========>....................] - ETA: 49:38 - loss: 0.6320 - regression_loss: 0.5101 - classification_loss: 0.1219
 3480/10000 [=========>....................] - ETA: 49:38 - loss: 0.6321 - regression_loss: 0.5101 - classification_loss: 0.1220
 3481/10000 [=========>....................] - ETA: 49:37 - loss: 0.6320 - regression_loss: 0.5100 - classification_loss: 0.1220
 3482/10000 [=========>....................] - ETA: 49:37 - loss: 0.6319 - regression_loss: 0.5099 - classification_loss: 0.1220
 3483/10000 [=========>....................] - ETA: 49:36 - loss: 0.6319 - regression_loss: 0.5099 - classification_loss: 0.1220
 3484/10000 [=========>....................] - ETA: 49:36 - loss: 0.6320 - regression_loss: 0.5100 - classification_loss: 0.1220
 3485/10000 [=========>....................] - ETA: 49:35 - loss: 0.6322 - regression_loss: 0.5101 - classification_loss: 0.1220
 3486/10000 [=========>....................] - ETA: 49:35 - loss: 0.6321 - regression_loss: 0.5100 - classification_loss: 0.1220
 3487/10000 [=========>....................] - ETA: 49:34 - loss: 0.6321 - regression_loss: 0.5100 - classification_loss: 0.1220
 3488/10000 [=========>....................] - ETA: 49:34 - loss: 0.6320 - regression_loss: 0.5100 - classification_loss: 0.1220
 3489/10000 [=========>....................] - ETA: 49:34 - loss: 0.6320 - regression_loss: 0.5100 - classification_loss: 0.1220
 3490/10000 [=========>....................] - ETA: 49:33 - loss: 0.6321 - regression_loss: 0.5101 - classification_loss: 0.1220
 3491/10000 [=========>....................] - ETA: 49:33 - loss: 0.6320 - regression_loss: 0.5100 - classification_loss: 0.1220
 3492/10000 [=========>....................] - ETA: 49:32 - loss: 0.6320 - regression_loss: 0.5100 - classification_loss: 0.1220
 3493/10000 [=========>....................] - ETA: 49:32 - loss: 0.6320 - regression_loss: 0.5100 - classification_loss: 0.1220
 3494/10000 [=========>....................] - ETA: 49:31 - loss: 0.6320 - regression_loss: 0.5100 - classification_loss: 0.1220
 3495/10000 [=========>....................] - ETA: 49:31 - loss: 0.6319 - regression_loss: 0.5099 - classification_loss: 0.1220
 3496/10000 [=========>....................] - ETA: 49:30 - loss: 0.6320 - regression_loss: 0.5100 - classification_loss: 0.1220
 3497/10000 [=========>....................] - ETA: 49:30 - loss: 0.6321 - regression_loss: 0.5101 - classification_loss: 0.1220
 3498/10000 [=========>....................] - ETA: 49:29 - loss: 0.6321 - regression_loss: 0.5100 - classification_loss: 0.1221
 3499/10000 [=========>....................] - ETA: 49:29 - loss: 0.6319 - regression_loss: 0.5099 - classification_loss: 0.1220
 3500/10000 [=========>....................] - ETA: 49:28 - loss: 0.6318 - regression_loss: 0.5098 - classification_loss: 0.1220
 3501/10000 [=========>....................] - ETA: 49:28 - loss: 0.6317 - regression_loss: 0.5097 - classification_loss: 0.1220
 3502/10000 [=========>....................] - ETA: 49:28 - loss: 0.6318 - regression_loss: 0.5098 - classification_loss: 0.1220
 3503/10000 [=========>....................] - ETA: 49:27 - loss: 0.6318 - regression_loss: 0.5098 - classification_loss: 0.1220
 3504/10000 [=========>....................] - ETA: 49:27 - loss: 0.6318 - regression_loss: 0.5098 - classification_loss: 0.1220
 3505/10000 [=========>....................] - ETA: 49:26 - loss: 0.6317 - regression_loss: 0.5097 - classification_loss: 0.1219
 3506/10000 [=========>....................] - ETA: 49:26 - loss: 0.6316 - regression_loss: 0.5096 - classification_loss: 0.1219
 3507/10000 [=========>....................] - ETA: 49:26 - loss: 0.6316 - regression_loss: 0.5097 - classification_loss: 0.1219
 3508/10000 [=========>....................] - ETA: 49:25 - loss: 0.6316 - regression_loss: 0.5097 - classification_loss: 0.1220
 3509/10000 [=========>....................] - ETA: 49:25 - loss: 0.6315 - regression_loss: 0.5096 - classification_loss: 0.1219
 3510/10000 [=========>....................] - ETA: 49:24 - loss: 0.6314 - regression_loss: 0.5095 - classification_loss: 0.1219
 3511/10000 [=========>....................] - ETA: 49:24 - loss: 0.6316 - regression_loss: 0.5097 - classification_loss: 0.1219
 3512/10000 [=========>....................] - ETA: 49:23 - loss: 0.6314 - regression_loss: 0.5095 - classification_loss: 0.1219
 3513/10000 [=========>....................] - ETA: 49:23 - loss: 0.6314 - regression_loss: 0.5095 - classification_loss: 0.1219
 3514/10000 [=========>....................] - ETA: 49:22 - loss: 0.6312 - regression_loss: 0.5094 - classification_loss: 0.1219
 3515/10000 [=========>....................] - ETA: 49:22 - loss: 0.6314 - regression_loss: 0.5095 - classification_loss: 0.1219
 3516/10000 [=========>....................] - ETA: 49:22 - loss: 0.6312 - regression_loss: 0.5094 - classification_loss: 0.1218
 3517/10000 [=========>....................] - ETA: 49:21 - loss: 0.6315 - regression_loss: 0.5096 - classification_loss: 0.1219
 3518/10000 [=========>....................] - ETA: 49:21 - loss: 0.6315 - regression_loss: 0.5096 - classification_loss: 0.1219
 3519/10000 [=========>....................] - ETA: 49:20 - loss: 0.6315 - regression_loss: 0.5096 - classification_loss: 0.1219
 3520/10000 [=========>....................] - ETA: 49:20 - loss: 0.6316 - regression_loss: 0.5097 - classification_loss: 0.1219
 3521/10000 [=========>....................] - ETA: 49:19 - loss: 0.6315 - regression_loss: 0.5096 - classification_loss: 0.1219
 3522/10000 [=========>....................] - ETA: 49:19 - loss: 0.6315 - regression_loss: 0.5096 - classification_loss: 0.1219
 3523/10000 [=========>....................] - ETA: 49:18 - loss: 0.6315 - regression_loss: 0.5096 - classification_loss: 0.1219
 3524/10000 [=========>....................] - ETA: 49:18 - loss: 0.6314 - regression_loss: 0.5095 - classification_loss: 0.1219
 3525/10000 [=========>....................] - ETA: 49:17 - loss: 0.6316 - regression_loss: 0.5097 - classification_loss: 0.1219
 3526/10000 [=========>....................] - ETA: 49:17 - loss: 0.6315 - regression_loss: 0.5096 - classification_loss: 0.1219
 3527/10000 [=========>....................] - ETA: 49:17 - loss: 0.6314 - regression_loss: 0.5095 - classification_loss: 0.1219
 3528/10000 [=========>....................] - ETA: 49:16 - loss: 0.6314 - regression_loss: 0.5095 - classification_loss: 0.1219
 3529/10000 [=========>....................] - ETA: 49:16 - loss: 0.6314 - regression_loss: 0.5095 - classification_loss: 0.1219
 3530/10000 [=========>....................] - ETA: 49:15 - loss: 0.6314 - regression_loss: 0.5095 - classification_loss: 0.1219
 3531/10000 [=========>....................] - ETA: 49:15 - loss: 0.6313 - regression_loss: 0.5094 - classification_loss: 0.1219
 3532/10000 [=========>....................] - ETA: 49:14 - loss: 0.6312 - regression_loss: 0.5094 - classification_loss: 0.1219
 3533/10000 [=========>....................] - ETA: 49:14 - loss: 0.6312 - regression_loss: 0.5093 - classification_loss: 0.1219
 3534/10000 [=========>....................] - ETA: 49:13 - loss: 0.6311 - regression_loss: 0.5092 - classification_loss: 0.1219
 3535/10000 [=========>....................] - ETA: 49:13 - loss: 0.6311 - regression_loss: 0.5093 - classification_loss: 0.1219
 3536/10000 [=========>....................] - ETA: 49:13 - loss: 0.6313 - regression_loss: 0.5094 - classification_loss: 0.1219
 3537/10000 [=========>....................] - ETA: 49:12 - loss: 0.6314 - regression_loss: 0.5095 - classification_loss: 0.1219
 3538/10000 [=========>....................] - ETA: 49:12 - loss: 0.6313 - regression_loss: 0.5094 - classification_loss: 0.1219
 3539/10000 [=========>....................] - ETA: 49:11 - loss: 0.6314 - regression_loss: 0.5095 - classification_loss: 0.1219
 3540/10000 [=========>....................] - ETA: 49:11 - loss: 0.6315 - regression_loss: 0.5096 - classification_loss: 0.1219
 3541/10000 [=========>....................] - ETA: 49:10 - loss: 0.6315 - regression_loss: 0.5096 - classification_loss: 0.1219
 3542/10000 [=========>....................] - ETA: 49:10 - loss: 0.6314 - regression_loss: 0.5095 - classification_loss: 0.1219
 3543/10000 [=========>....................] - ETA: 49:09 - loss: 0.6313 - regression_loss: 0.5095 - classification_loss: 0.1219
 3544/10000 [=========>....................] - ETA: 49:09 - loss: 0.6312 - regression_loss: 0.5093 - classification_loss: 0.1218
 3545/10000 [=========>....................] - ETA: 49:08 - loss: 0.6313 - regression_loss: 0.5094 - classification_loss: 0.1219
 3546/10000 [=========>....................] - ETA: 49:08 - loss: 0.6312 - regression_loss: 0.5093 - classification_loss: 0.1219
 3547/10000 [=========>....................] - ETA: 49:08 - loss: 0.6312 - regression_loss: 0.5094 - classification_loss: 0.1218
 3548/10000 [=========>....................] - ETA: 49:07 - loss: 0.6312 - regression_loss: 0.5094 - classification_loss: 0.1218
 3549/10000 [=========>....................] - ETA: 49:07 - loss: 0.6313 - regression_loss: 0.5094 - classification_loss: 0.1218
 3550/10000 [=========>....................] - ETA: 49:06 - loss: 0.6313 - regression_loss: 0.5094 - classification_loss: 0.1218
 3551/10000 [=========>....................] - ETA: 49:06 - loss: 0.6314 - regression_loss: 0.5096 - classification_loss: 0.1218
 3552/10000 [=========>....................] - ETA: 49:05 - loss: 0.6315 - regression_loss: 0.5096 - classification_loss: 0.1219
 3553/10000 [=========>....................] - ETA: 49:05 - loss: 0.6316 - regression_loss: 0.5096 - classification_loss: 0.1220
 3554/10000 [=========>....................] - ETA: 49:05 - loss: 0.6317 - regression_loss: 0.5097 - classification_loss: 0.1220
 3555/10000 [=========>....................] - ETA: 49:04 - loss: 0.6317 - regression_loss: 0.5098 - classification_loss: 0.1220
 3556/10000 [=========>....................] - ETA: 49:04 - loss: 0.6318 - regression_loss: 0.5097 - classification_loss: 0.1220
 3557/10000 [=========>....................] - ETA: 49:03 - loss: 0.6318 - regression_loss: 0.5098 - classification_loss: 0.1220
 3558/10000 [=========>....................] - ETA: 49:03 - loss: 0.6318 - regression_loss: 0.5097 - classification_loss: 0.1220
 3559/10000 [=========>....................] - ETA: 49:02 - loss: 0.6316 - regression_loss: 0.5096 - classification_loss: 0.1220
 3560/10000 [=========>....................] - ETA: 49:02 - loss: 0.6315 - regression_loss: 0.5095 - classification_loss: 0.1220
 3561/10000 [=========>....................] - ETA: 49:01 - loss: 0.6317 - regression_loss: 0.5097 - classification_loss: 0.1220
 3562/10000 [=========>....................] - ETA: 49:01 - loss: 0.6317 - regression_loss: 0.5097 - classification_loss: 0.1220
 3563/10000 [=========>....................] - ETA: 49:01 - loss: 0.6316 - regression_loss: 0.5096 - classification_loss: 0.1220
 3564/10000 [=========>....................] - ETA: 49:00 - loss: 0.6315 - regression_loss: 0.5095 - classification_loss: 0.1220
 3565/10000 [=========>....................] - ETA: 49:00 - loss: 0.6313 - regression_loss: 0.5094 - classification_loss: 0.1219
 3566/10000 [=========>....................] - ETA: 48:59 - loss: 0.6312 - regression_loss: 0.5093 - classification_loss: 0.1219
 3567/10000 [=========>....................] - ETA: 48:59 - loss: 0.6311 - regression_loss: 0.5092 - classification_loss: 0.1219
 3568/10000 [=========>....................] - ETA: 48:58 - loss: 0.6313 - regression_loss: 0.5093 - classification_loss: 0.1219
 3569/10000 [=========>....................] - ETA: 48:58 - loss: 0.6312 - regression_loss: 0.5092 - classification_loss: 0.1219
 3570/10000 [=========>....................] - ETA: 48:57 - loss: 0.6311 - regression_loss: 0.5092 - classification_loss: 0.1219
 3571/10000 [=========>....................] - ETA: 48:57 - loss: 0.6310 - regression_loss: 0.5091 - classification_loss: 0.1219
 3572/10000 [=========>....................] - ETA: 48:57 - loss: 0.6309 - regression_loss: 0.5091 - classification_loss: 0.1219
 3573/10000 [=========>....................] - ETA: 48:56 - loss: 0.6309 - regression_loss: 0.5090 - classification_loss: 0.1219
 3574/10000 [=========>....................] - ETA: 48:56 - loss: 0.6308 - regression_loss: 0.5089 - classification_loss: 0.1218
 3575/10000 [=========>....................] - ETA: 48:55 - loss: 0.6309 - regression_loss: 0.5090 - classification_loss: 0.1219
 3576/10000 [=========>....................] - ETA: 48:55 - loss: 0.6309 - regression_loss: 0.5090 - classification_loss: 0.1219
 3577/10000 [=========>....................] - ETA: 48:54 - loss: 0.6309 - regression_loss: 0.5090 - classification_loss: 0.1219
 3578/10000 [=========>....................] - ETA: 48:54 - loss: 0.6307 - regression_loss: 0.5089 - classification_loss: 0.1218
 3579/10000 [=========>....................] - ETA: 48:53 - loss: 0.6306 - regression_loss: 0.5088 - classification_loss: 0.1218
 3580/10000 [=========>....................] - ETA: 48:53 - loss: 0.6306 - regression_loss: 0.5088 - classification_loss: 0.1218
 3581/10000 [=========>....................] - ETA: 48:53 - loss: 0.6306 - regression_loss: 0.5087 - classification_loss: 0.1218
 3582/10000 [=========>....................] - ETA: 48:52 - loss: 0.6306 - regression_loss: 0.5088 - classification_loss: 0.1218
 3583/10000 [=========>....................] - ETA: 48:52 - loss: 0.6306 - regression_loss: 0.5088 - classification_loss: 0.1218
 3584/10000 [=========>....................] - ETA: 48:51 - loss: 0.6305 - regression_loss: 0.5088 - classification_loss: 0.1218
 3585/10000 [=========>....................] - ETA: 48:51 - loss: 0.6304 - regression_loss: 0.5087 - classification_loss: 0.1218
 3586/10000 [=========>....................] - ETA: 48:50 - loss: 0.6305 - regression_loss: 0.5087 - classification_loss: 0.1217
 3587/10000 [=========>....................] - ETA: 48:50 - loss: 0.6304 - regression_loss: 0.5087 - classification_loss: 0.1217
 3588/10000 [=========>....................] - ETA: 48:49 - loss: 0.6305 - regression_loss: 0.5087 - classification_loss: 0.1218
 3589/10000 [=========>....................] - ETA: 48:49 - loss: 0.6305 - regression_loss: 0.5088 - classification_loss: 0.1217
 3590/10000 [=========>....................] - ETA: 48:49 - loss: 0.6306 - regression_loss: 0.5089 - classification_loss: 0.1218
 3591/10000 [=========>....................] - ETA: 48:48 - loss: 0.6306 - regression_loss: 0.5088 - classification_loss: 0.1218
 3592/10000 [=========>....................] - ETA: 48:48 - loss: 0.6308 - regression_loss: 0.5089 - classification_loss: 0.1218
 3593/10000 [=========>....................] - ETA: 48:47 - loss: 0.6307 - regression_loss: 0.5089 - classification_loss: 0.1218
 3594/10000 [=========>....................] - ETA: 48:47 - loss: 0.6307 - regression_loss: 0.5089 - classification_loss: 0.1218
 3595/10000 [=========>....................] - ETA: 48:46 - loss: 0.6305 - regression_loss: 0.5088 - classification_loss: 0.1218
 3596/10000 [=========>....................] - ETA: 48:46 - loss: 0.6305 - regression_loss: 0.5087 - classification_loss: 0.1218
 3597/10000 [=========>....................] - ETA: 48:46 - loss: 0.6303 - regression_loss: 0.5086 - classification_loss: 0.1217
 3598/10000 [=========>....................] - ETA: 48:45 - loss: 0.6303 - regression_loss: 0.5086 - classification_loss: 0.1217
 3599/10000 [=========>....................] - ETA: 48:45 - loss: 0.6303 - regression_loss: 0.5086 - classification_loss: 0.1217
 3600/10000 [=========>....................] - ETA: 48:44 - loss: 0.6306 - regression_loss: 0.5088 - classification_loss: 0.1218
 3601/10000 [=========>....................] - ETA: 48:44 - loss: 0.6305 - regression_loss: 0.5087 - classification_loss: 0.1217
 3602/10000 [=========>....................] - ETA: 48:43 - loss: 0.6305 - regression_loss: 0.5087 - classification_loss: 0.1217
 3603/10000 [=========>....................] - ETA: 48:43 - loss: 0.6304 - regression_loss: 0.5087 - classification_loss: 0.1217
 3604/10000 [=========>....................] - ETA: 48:42 - loss: 0.6304 - regression_loss: 0.5087 - classification_loss: 0.1217
 3605/10000 [=========>....................] - ETA: 48:42 - loss: 0.6303 - regression_loss: 0.5086 - classification_loss: 0.1217
 3606/10000 [=========>....................] - ETA: 48:41 - loss: 0.6302 - regression_loss: 0.5086 - classification_loss: 0.1217
 3607/10000 [=========>....................] - ETA: 48:41 - loss: 0.6302 - regression_loss: 0.5085 - classification_loss: 0.1217
 3608/10000 [=========>....................] - ETA: 48:40 - loss: 0.6301 - regression_loss: 0.5085 - classification_loss: 0.1217
 3609/10000 [=========>....................] - ETA: 48:40 - loss: 0.6301 - regression_loss: 0.5084 - classification_loss: 0.1217
 3610/10000 [=========>....................] - ETA: 48:39 - loss: 0.6300 - regression_loss: 0.5083 - classification_loss: 0.1216
 3611/10000 [=========>....................] - ETA: 48:39 - loss: 0.6302 - regression_loss: 0.5085 - classification_loss: 0.1217
 3612/10000 [=========>....................] - ETA: 48:39 - loss: 0.6300 - regression_loss: 0.5084 - classification_loss: 0.1216
 3613/10000 [=========>....................] - ETA: 48:38 - loss: 0.6300 - regression_loss: 0.5083 - classification_loss: 0.1216
 3614/10000 [=========>....................] - ETA: 48:38 - loss: 0.6299 - regression_loss: 0.5083 - classification_loss: 0.1216
 3615/10000 [=========>....................] - ETA: 48:37 - loss: 0.6298 - regression_loss: 0.5082 - classification_loss: 0.1216
 3616/10000 [=========>....................] - ETA: 48:37 - loss: 0.6299 - regression_loss: 0.5083 - classification_loss: 0.1216
 3617/10000 [=========>....................] - ETA: 48:36 - loss: 0.6300 - regression_loss: 0.5084 - classification_loss: 0.1216
 3618/10000 [=========>....................] - ETA: 48:36 - loss: 0.6301 - regression_loss: 0.5085 - classification_loss: 0.1216
 3619/10000 [=========>....................] - ETA: 48:35 - loss: 0.6301 - regression_loss: 0.5085 - classification_loss: 0.1216
 3620/10000 [=========>....................] - ETA: 48:35 - loss: 0.6300 - regression_loss: 0.5085 - classification_loss: 0.1216
 3621/10000 [=========>....................] - ETA: 48:35 - loss: 0.6302 - regression_loss: 0.5086 - classification_loss: 0.1216
 3622/10000 [=========>....................] - ETA: 48:34 - loss: 0.6301 - regression_loss: 0.5085 - classification_loss: 0.1216
 3623/10000 [=========>....................] - ETA: 48:34 - loss: 0.6301 - regression_loss: 0.5085 - classification_loss: 0.1216
 3624/10000 [=========>....................] - ETA: 48:33 - loss: 0.6300 - regression_loss: 0.5085 - classification_loss: 0.1216
 3625/10000 [=========>....................] - ETA: 48:33 - loss: 0.6299 - regression_loss: 0.5084 - classification_loss: 0.1215
 3626/10000 [=========>....................] - ETA: 48:32 - loss: 0.6299 - regression_loss: 0.5083 - classification_loss: 0.1215
 3627/10000 [=========>....................] - ETA: 48:32 - loss: 0.6301 - regression_loss: 0.5085 - classification_loss: 0.1216
 3628/10000 [=========>....................] - ETA: 48:31 - loss: 0.6303 - regression_loss: 0.5087 - classification_loss: 0.1216
 3629/10000 [=========>....................] - ETA: 48:31 - loss: 0.6304 - regression_loss: 0.5088 - classification_loss: 0.1216
 3630/10000 [=========>....................] - ETA: 48:31 - loss: 0.6303 - regression_loss: 0.5088 - classification_loss: 0.1216
 3631/10000 [=========>....................] - ETA: 48:30 - loss: 0.6304 - regression_loss: 0.5089 - classification_loss: 0.1215
 3632/10000 [=========>....................] - ETA: 48:30 - loss: 0.6304 - regression_loss: 0.5089 - classification_loss: 0.1215
 3633/10000 [=========>....................] - ETA: 48:29 - loss: 0.6304 - regression_loss: 0.5088 - classification_loss: 0.1215
 3634/10000 [=========>....................] - ETA: 48:29 - loss: 0.6304 - regression_loss: 0.5088 - classification_loss: 0.1215
 3635/10000 [=========>....................] - ETA: 48:28 - loss: 0.6306 - regression_loss: 0.5090 - classification_loss: 0.1216
 3636/10000 [=========>....................] - ETA: 48:28 - loss: 0.6306 - regression_loss: 0.5090 - classification_loss: 0.1216
 3637/10000 [=========>....................] - ETA: 48:28 - loss: 0.6305 - regression_loss: 0.5090 - classification_loss: 0.1215
 3638/10000 [=========>....................] - ETA: 48:27 - loss: 0.6305 - regression_loss: 0.5089 - classification_loss: 0.1216
 3639/10000 [=========>....................] - ETA: 48:27 - loss: 0.6305 - regression_loss: 0.5089 - classification_loss: 0.1215
 3640/10000 [=========>....................] - ETA: 48:26 - loss: 0.6304 - regression_loss: 0.5089 - classification_loss: 0.1215
 3641/10000 [=========>....................] - ETA: 48:26 - loss: 0.6305 - regression_loss: 0.5089 - classification_loss: 0.1216
 3642/10000 [=========>....................] - ETA: 48:25 - loss: 0.6305 - regression_loss: 0.5090 - classification_loss: 0.1216
 3643/10000 [=========>....................] - ETA: 48:25 - loss: 0.6305 - regression_loss: 0.5089 - classification_loss: 0.1216
 3644/10000 [=========>....................] - ETA: 48:25 - loss: 0.6304 - regression_loss: 0.5088 - classification_loss: 0.1216
 3645/10000 [=========>....................] - ETA: 48:24 - loss: 0.6303 - regression_loss: 0.5088 - classification_loss: 0.1215
 3646/10000 [=========>....................] - ETA: 48:24 - loss: 0.6304 - regression_loss: 0.5088 - classification_loss: 0.1215
 3647/10000 [=========>....................] - ETA: 48:23 - loss: 0.6303 - regression_loss: 0.5088 - classification_loss: 0.1215
 3648/10000 [=========>....................] - ETA: 48:23 - loss: 0.6304 - regression_loss: 0.5089 - classification_loss: 0.1215
 3649/10000 [=========>....................] - ETA: 48:22 - loss: 0.6304 - regression_loss: 0.5089 - classification_loss: 0.1215
 3650/10000 [=========>....................] - ETA: 48:22 - loss: 0.6305 - regression_loss: 0.5090 - classification_loss: 0.1215
 3651/10000 [=========>....................] - ETA: 48:21 - loss: 0.6304 - regression_loss: 0.5089 - classification_loss: 0.1215
 3652/10000 [=========>....................] - ETA: 48:21 - loss: 0.6305 - regression_loss: 0.5090 - classification_loss: 0.1215
 3653/10000 [=========>....................] - ETA: 48:20 - loss: 0.6304 - regression_loss: 0.5089 - classification_loss: 0.1215
 3654/10000 [=========>....................] - ETA: 48:20 - loss: 0.6305 - regression_loss: 0.5090 - classification_loss: 0.1215
 3655/10000 [=========>....................] - ETA: 48:19 - loss: 0.6305 - regression_loss: 0.5090 - classification_loss: 0.1215
 3656/10000 [=========>....................] - ETA: 48:19 - loss: 0.6304 - regression_loss: 0.5089 - classification_loss: 0.1215
 3657/10000 [=========>....................] - ETA: 48:19 - loss: 0.6304 - regression_loss: 0.5089 - classification_loss: 0.1214
 3658/10000 [=========>....................] - ETA: 48:18 - loss: 0.6302 - regression_loss: 0.5088 - classification_loss: 0.1214
 3659/10000 [=========>....................] - ETA: 48:18 - loss: 0.6301 - regression_loss: 0.5087 - classification_loss: 0.1214
 3660/10000 [=========>....................] - ETA: 48:17 - loss: 0.6301 - regression_loss: 0.5087 - classification_loss: 0.1214
 3661/10000 [=========>....................] - ETA: 48:17 - loss: 0.6301 - regression_loss: 0.5087 - classification_loss: 0.1214
 3662/10000 [=========>....................] - ETA: 48:16 - loss: 0.6301 - regression_loss: 0.5087 - classification_loss: 0.1214
 3663/10000 [=========>....................] - ETA: 48:16 - loss: 0.6301 - regression_loss: 0.5088 - classification_loss: 0.1214
 3664/10000 [=========>....................] - ETA: 48:15 - loss: 0.6303 - regression_loss: 0.5089 - classification_loss: 0.1214
 3665/10000 [=========>....................] - ETA: 48:15 - loss: 0.6302 - regression_loss: 0.5088 - classification_loss: 0.1214
 3666/10000 [=========>....................] - ETA: 48:15 - loss: 0.6303 - regression_loss: 0.5089 - classification_loss: 0.1214
 3667/10000 [==========>...................] - ETA: 48:14 - loss: 0.6304 - regression_loss: 0.5090 - classification_loss: 0.1214
 3668/10000 [==========>...................] - ETA: 48:14 - loss: 0.6303 - regression_loss: 0.5089 - classification_loss: 0.1213
 3669/10000 [==========>...................] - ETA: 48:13 - loss: 0.6305 - regression_loss: 0.5091 - classification_loss: 0.1213
 3670/10000 [==========>...................] - ETA: 48:13 - loss: 0.6305 - regression_loss: 0.5091 - classification_loss: 0.1213
 3671/10000 [==========>...................] - ETA: 48:12 - loss: 0.6304 - regression_loss: 0.5091 - classification_loss: 0.1213
 3672/10000 [==========>...................] - ETA: 48:12 - loss: 0.6304 - regression_loss: 0.5091 - classification_loss: 0.1213
 3673/10000 [==========>...................] - ETA: 48:11 - loss: 0.6304 - regression_loss: 0.5090 - classification_loss: 0.1214
 3674/10000 [==========>...................] - ETA: 48:11 - loss: 0.6303 - regression_loss: 0.5089 - classification_loss: 0.1214
 3675/10000 [==========>...................] - ETA: 48:10 - loss: 0.6302 - regression_loss: 0.5088 - classification_loss: 0.1214
 3676/10000 [==========>...................] - ETA: 48:10 - loss: 0.6301 - regression_loss: 0.5087 - classification_loss: 0.1214
 3677/10000 [==========>...................] - ETA: 48:09 - loss: 0.6301 - regression_loss: 0.5087 - classification_loss: 0.1214
 3678/10000 [==========>...................] - ETA: 48:09 - loss: 0.6300 - regression_loss: 0.5086 - classification_loss: 0.1214
 3679/10000 [==========>...................] - ETA: 48:08 - loss: 0.6303 - regression_loss: 0.5089 - classification_loss: 0.1214
 3680/10000 [==========>...................] - ETA: 48:08 - loss: 0.6302 - regression_loss: 0.5088 - classification_loss: 0.1214
 3681/10000 [==========>...................] - ETA: 48:07 - loss: 0.6303 - regression_loss: 0.5089 - classification_loss: 0.1215
 3682/10000 [==========>...................] - ETA: 48:07 - loss: 0.6303 - regression_loss: 0.5088 - classification_loss: 0.1215
 3683/10000 [==========>...................] - ETA: 48:07 - loss: 0.6302 - regression_loss: 0.5087 - classification_loss: 0.1215
 3684/10000 [==========>...................] - ETA: 48:06 - loss: 0.6301 - regression_loss: 0.5086 - classification_loss: 0.1215
 3685/10000 [==========>...................] - ETA: 48:06 - loss: 0.6302 - regression_loss: 0.5088 - classification_loss: 0.1215
 3686/10000 [==========>...................] - ETA: 48:05 - loss: 0.6304 - regression_loss: 0.5088 - classification_loss: 0.1216
 3687/10000 [==========>...................] - ETA: 48:05 - loss: 0.6303 - regression_loss: 0.5087 - classification_loss: 0.1215
 3688/10000 [==========>...................] - ETA: 48:04 - loss: 0.6304 - regression_loss: 0.5088 - classification_loss: 0.1216
 3689/10000 [==========>...................] - ETA: 48:04 - loss: 0.6303 - regression_loss: 0.5088 - classification_loss: 0.1215
 3690/10000 [==========>...................] - ETA: 48:03 - loss: 0.6302 - regression_loss: 0.5087 - classification_loss: 0.1215
 3691/10000 [==========>...................] - ETA: 48:03 - loss: 0.6303 - regression_loss: 0.5088 - classification_loss: 0.1215
 3692/10000 [==========>...................] - ETA: 48:03 - loss: 0.6303 - regression_loss: 0.5088 - classification_loss: 0.1215
 3693/10000 [==========>...................] - ETA: 48:02 - loss: 0.6302 - regression_loss: 0.5087 - classification_loss: 0.1215
 3694/10000 [==========>...................] - ETA: 48:02 - loss: 0.6303 - regression_loss: 0.5088 - classification_loss: 0.1215
 3695/10000 [==========>...................] - ETA: 48:01 - loss: 0.6303 - regression_loss: 0.5088 - classification_loss: 0.1215
 3696/10000 [==========>...................] - ETA: 48:01 - loss: 0.6304 - regression_loss: 0.5089 - classification_loss: 0.1215
 3697/10000 [==========>...................] - ETA: 48:00 - loss: 0.6307 - regression_loss: 0.5091 - classification_loss: 0.1215
 3698/10000 [==========>...................] - ETA: 48:00 - loss: 0.6306 - regression_loss: 0.5091 - classification_loss: 0.1215
 3699/10000 [==========>...................] - ETA: 48:00 - loss: 0.6306 - regression_loss: 0.5091 - classification_loss: 0.1215
 3700/10000 [==========>...................] - ETA: 47:59 - loss: 0.6305 - regression_loss: 0.5090 - classification_loss: 0.1215
 3701/10000 [==========>...................] - ETA: 47:59 - loss: 0.6304 - regression_loss: 0.5089 - classification_loss: 0.1215
 3702/10000 [==========>...................] - ETA: 47:58 - loss: 0.6306 - regression_loss: 0.5090 - classification_loss: 0.1215
 3703/10000 [==========>...................] - ETA: 47:58 - loss: 0.6305 - regression_loss: 0.5090 - classification_loss: 0.1215
 3704/10000 [==========>...................] - ETA: 47:57 - loss: 0.6306 - regression_loss: 0.5090 - classification_loss: 0.1215
 3705/10000 [==========>...................] - ETA: 47:57 - loss: 0.6306 - regression_loss: 0.5090 - classification_loss: 0.1215
 3706/10000 [==========>...................] - ETA: 47:56 - loss: 0.6306 - regression_loss: 0.5091 - classification_loss: 0.1215
 3707/10000 [==========>...................] - ETA: 47:56 - loss: 0.6306 - regression_loss: 0.5091 - classification_loss: 0.1215
 3708/10000 [==========>...................] - ETA: 47:56 - loss: 0.6310 - regression_loss: 0.5094 - classification_loss: 0.1216
 3709/10000 [==========>...................] - ETA: 47:55 - loss: 0.6310 - regression_loss: 0.5094 - classification_loss: 0.1216
 3710/10000 [==========>...................] - ETA: 47:55 - loss: 0.6310 - regression_loss: 0.5094 - classification_loss: 0.1216
 3711/10000 [==========>...................] - ETA: 47:54 - loss: 0.6309 - regression_loss: 0.5093 - classification_loss: 0.1216
 3712/10000 [==========>...................] - ETA: 47:54 - loss: 0.6312 - regression_loss: 0.5096 - classification_loss: 0.1216
 3713/10000 [==========>...................] - ETA: 47:53 - loss: 0.6312 - regression_loss: 0.5096 - classification_loss: 0.1216
 3714/10000 [==========>...................] - ETA: 47:53 - loss: 0.6311 - regression_loss: 0.5095 - classification_loss: 0.1216
 3715/10000 [==========>...................] - ETA: 47:52 - loss: 0.6313 - regression_loss: 0.5097 - classification_loss: 0.1216
 3716/10000 [==========>...................] - ETA: 47:52 - loss: 0.6313 - regression_loss: 0.5097 - classification_loss: 0.1216
 3717/10000 [==========>...................] - ETA: 47:51 - loss: 0.6315 - regression_loss: 0.5099 - classification_loss: 0.1217
 3718/10000 [==========>...................] - ETA: 47:51 - loss: 0.6314 - regression_loss: 0.5098 - classification_loss: 0.1216
 3719/10000 [==========>...................] - ETA: 47:50 - loss: 0.6313 - regression_loss: 0.5097 - classification_loss: 0.1216
 3720/10000 [==========>...................] - ETA: 47:50 - loss: 0.6313 - regression_loss: 0.5096 - classification_loss: 0.1216
 3721/10000 [==========>...................] - ETA: 47:50 - loss: 0.6312 - regression_loss: 0.5095 - classification_loss: 0.1216
 3722/10000 [==========>...................] - ETA: 47:49 - loss: 0.6311 - regression_loss: 0.5095 - classification_loss: 0.1216
 3723/10000 [==========>...................] - ETA: 47:49 - loss: 0.6313 - regression_loss: 0.5097 - classification_loss: 0.1216
 3724/10000 [==========>...................] - ETA: 47:48 - loss: 0.6316 - regression_loss: 0.5099 - classification_loss: 0.1217
 3725/10000 [==========>...................] - ETA: 47:48 - loss: 0.6315 - regression_loss: 0.5098 - classification_loss: 0.1217
 3726/10000 [==========>...................] - ETA: 47:47 - loss: 0.6315 - regression_loss: 0.5099 - classification_loss: 0.1217
 3727/10000 [==========>...................] - ETA: 47:47 - loss: 0.6314 - regression_loss: 0.5098 - classification_loss: 0.1216
 3728/10000 [==========>...................] - ETA: 47:46 - loss: 0.6314 - regression_loss: 0.5097 - classification_loss: 0.1216
 3729/10000 [==========>...................] - ETA: 47:46 - loss: 0.6315 - regression_loss: 0.5098 - classification_loss: 0.1217
 3730/10000 [==========>...................] - ETA: 47:45 - loss: 0.6316 - regression_loss: 0.5098 - classification_loss: 0.1218
 3731/10000 [==========>...................] - ETA: 47:45 - loss: 0.6315 - regression_loss: 0.5098 - classification_loss: 0.1217
 3732/10000 [==========>...................] - ETA: 47:44 - loss: 0.6315 - regression_loss: 0.5098 - classification_loss: 0.1217
 3733/10000 [==========>...................] - ETA: 47:44 - loss: 0.6315 - regression_loss: 0.5098 - classification_loss: 0.1217
 3734/10000 [==========>...................] - ETA: 47:43 - loss: 0.6316 - regression_loss: 0.5099 - classification_loss: 0.1217
 3735/10000 [==========>...................] - ETA: 47:43 - loss: 0.6315 - regression_loss: 0.5098 - classification_loss: 0.1217
 3736/10000 [==========>...................] - ETA: 47:42 - loss: 0.6315 - regression_loss: 0.5098 - classification_loss: 0.1217
 3737/10000 [==========>...................] - ETA: 47:42 - loss: 0.6315 - regression_loss: 0.5098 - classification_loss: 0.1217
 3738/10000 [==========>...................] - ETA: 47:41 - loss: 0.6315 - regression_loss: 0.5098 - classification_loss: 0.1217
 3739/10000 [==========>...................] - ETA: 47:41 - loss: 0.6314 - regression_loss: 0.5097 - classification_loss: 0.1217
 3740/10000 [==========>...................] - ETA: 47:41 - loss: 0.6313 - regression_loss: 0.5097 - classification_loss: 0.1217
 3741/10000 [==========>...................] - ETA: 47:40 - loss: 0.6313 - regression_loss: 0.5096 - classification_loss: 0.1217
 3742/10000 [==========>...................] - ETA: 47:40 - loss: 0.6312 - regression_loss: 0.5096 - classification_loss: 0.1217
 3743/10000 [==========>...................] - ETA: 47:39 - loss: 0.6311 - regression_loss: 0.5095 - classification_loss: 0.1216
 3744/10000 [==========>...................] - ETA: 47:39 - loss: 0.6311 - regression_loss: 0.5095 - classification_loss: 0.1216
 3745/10000 [==========>...................] - ETA: 47:39 - loss: 0.6310 - regression_loss: 0.5094 - classification_loss: 0.1216
 3746/10000 [==========>...................] - ETA: 47:38 - loss: 0.6310 - regression_loss: 0.5094 - classification_loss: 0.1216
 3747/10000 [==========>...................] - ETA: 47:38 - loss: 0.6312 - regression_loss: 0.5096 - classification_loss: 0.1216
 3748/10000 [==========>...................] - ETA: 47:37 - loss: 0.6311 - regression_loss: 0.5095 - classification_loss: 0.1216
 3749/10000 [==========>...................] - ETA: 47:37 - loss: 0.6309 - regression_loss: 0.5094 - classification_loss: 0.1215
 3750/10000 [==========>...................] - ETA: 47:36 - loss: 0.6309 - regression_loss: 0.5094 - classification_loss: 0.1215
 3751/10000 [==========>...................] - ETA: 47:36 - loss: 0.6309 - regression_loss: 0.5094 - classification_loss: 0.1215
 3752/10000 [==========>...................] - ETA: 47:35 - loss: 0.6309 - regression_loss: 0.5094 - classification_loss: 0.1215
 3753/10000 [==========>...................] - ETA: 47:35 - loss: 0.6308 - regression_loss: 0.5093 - classification_loss: 0.1215
 3754/10000 [==========>...................] - ETA: 47:34 - loss: 0.6308 - regression_loss: 0.5093 - classification_loss: 0.1215
 3755/10000 [==========>...................] - ETA: 47:34 - loss: 0.6309 - regression_loss: 0.5093 - classification_loss: 0.1215
 3756/10000 [==========>...................] - ETA: 47:33 - loss: 0.6308 - regression_loss: 0.5093 - classification_loss: 0.1215
 3757/10000 [==========>...................] - ETA: 47:33 - loss: 0.6307 - regression_loss: 0.5092 - classification_loss: 0.1215
 3758/10000 [==========>...................] - ETA: 47:32 - loss: 0.6310 - regression_loss: 0.5095 - classification_loss: 0.1216
 3759/10000 [==========>...................] - ETA: 47:32 - loss: 0.6310 - regression_loss: 0.5095 - classification_loss: 0.1216
 3760/10000 [==========>...................] - ETA: 47:32 - loss: 0.6311 - regression_loss: 0.5095 - classification_loss: 0.1215
 3761/10000 [==========>...................] - ETA: 47:31 - loss: 0.6310 - regression_loss: 0.5094 - classification_loss: 0.1215
 3762/10000 [==========>...................] - ETA: 47:31 - loss: 0.6309 - regression_loss: 0.5094 - classification_loss: 0.1215
 3763/10000 [==========>...................] - ETA: 47:30 - loss: 0.6310 - regression_loss: 0.5094 - classification_loss: 0.1215
 3764/10000 [==========>...................] - ETA: 47:30 - loss: 0.6309 - regression_loss: 0.5094 - classification_loss: 0.1215
 3765/10000 [==========>...................] - ETA: 47:29 - loss: 0.6309 - regression_loss: 0.5094 - classification_loss: 0.1215
 3766/10000 [==========>...................] - ETA: 47:29 - loss: 0.6308 - regression_loss: 0.5093 - classification_loss: 0.1215
 3767/10000 [==========>...................] - ETA: 47:28 - loss: 0.6308 - regression_loss: 0.5093 - classification_loss: 0.1215
 3768/10000 [==========>...................] - ETA: 47:28 - loss: 0.6308 - regression_loss: 0.5093 - classification_loss: 0.1214
 3769/10000 [==========>...................] - ETA: 47:28 - loss: 0.6308 - regression_loss: 0.5094 - classification_loss: 0.1215
 3770/10000 [==========>...................] - ETA: 47:27 - loss: 0.6308 - regression_loss: 0.5093 - classification_loss: 0.1215
 3771/10000 [==========>...................] - ETA: 47:27 - loss: 0.6308 - regression_loss: 0.5093 - classification_loss: 0.1215
 3772/10000 [==========>...................] - ETA: 47:26 - loss: 0.6307 - regression_loss: 0.5092 - classification_loss: 0.1215
 3773/10000 [==========>...................] - ETA: 47:26 - loss: 0.6307 - regression_loss: 0.5092 - classification_loss: 0.1215
 3774/10000 [==========>...................] - ETA: 47:25 - loss: 0.6306 - regression_loss: 0.5092 - classification_loss: 0.1214
 3775/10000 [==========>...................] - ETA: 47:25 - loss: 0.6308 - regression_loss: 0.5094 - classification_loss: 0.1214
 3776/10000 [==========>...................] - ETA: 47:24 - loss: 0.6307 - regression_loss: 0.5093 - classification_loss: 0.1214
 3777/10000 [==========>...................] - ETA: 47:24 - loss: 0.6307 - regression_loss: 0.5092 - classification_loss: 0.1214
 3778/10000 [==========>...................] - ETA: 47:23 - loss: 0.6306 - regression_loss: 0.5091 - classification_loss: 0.1214
 3779/10000 [==========>...................] - ETA: 47:23 - loss: 0.6305 - regression_loss: 0.5091 - classification_loss: 0.1214
 3780/10000 [==========>...................] - ETA: 47:22 - loss: 0.6304 - regression_loss: 0.5090 - classification_loss: 0.1214
 3781/10000 [==========>...................] - ETA: 47:22 - loss: 0.6303 - regression_loss: 0.5089 - classification_loss: 0.1214
 3782/10000 [==========>...................] - ETA: 47:22 - loss: 0.6302 - regression_loss: 0.5088 - classification_loss: 0.1214
 3783/10000 [==========>...................] - ETA: 47:21 - loss: 0.6302 - regression_loss: 0.5088 - classification_loss: 0.1213
 3784/10000 [==========>...................] - ETA: 47:21 - loss: 0.6302 - regression_loss: 0.5089 - classification_loss: 0.1213
 3785/10000 [==========>...................] - ETA: 47:20 - loss: 0.6302 - regression_loss: 0.5089 - classification_loss: 0.1213
 3786/10000 [==========>...................] - ETA: 47:20 - loss: 0.6303 - regression_loss: 0.5090 - classification_loss: 0.1213
 3787/10000 [==========>...................] - ETA: 47:19 - loss: 0.6303 - regression_loss: 0.5090 - classification_loss: 0.1213
 3788/10000 [==========>...................] - ETA: 47:19 - loss: 0.6303 - regression_loss: 0.5089 - classification_loss: 0.1213
 3789/10000 [==========>...................] - ETA: 47:18 - loss: 0.6302 - regression_loss: 0.5089 - classification_loss: 0.1213
 3790/10000 [==========>...................] - ETA: 47:18 - loss: 0.6303 - regression_loss: 0.5089 - classification_loss: 0.1213
 3791/10000 [==========>...................] - ETA: 47:17 - loss: 0.6302 - regression_loss: 0.5089 - classification_loss: 0.1213
 3792/10000 [==========>...................] - ETA: 47:17 - loss: 0.6302 - regression_loss: 0.5089 - classification_loss: 0.1213
 3793/10000 [==========>...................] - ETA: 47:16 - loss: 0.6301 - regression_loss: 0.5088 - classification_loss: 0.1213
 3794/10000 [==========>...................] - ETA: 47:16 - loss: 0.6301 - regression_loss: 0.5088 - classification_loss: 0.1213
 3795/10000 [==========>...................] - ETA: 47:16 - loss: 0.6300 - regression_loss: 0.5087 - classification_loss: 0.1213
 3796/10000 [==========>...................] - ETA: 47:15 - loss: 0.6300 - regression_loss: 0.5087 - classification_loss: 0.1213
 3797/10000 [==========>...................] - ETA: 47:15 - loss: 0.6300 - regression_loss: 0.5086 - classification_loss: 0.1213
 3798/10000 [==========>...................] - ETA: 47:14 - loss: 0.6299 - regression_loss: 0.5086 - classification_loss: 0.1213
 3799/10000 [==========>...................] - ETA: 47:14 - loss: 0.6298 - regression_loss: 0.5085 - classification_loss: 0.1213
 3800/10000 [==========>...................] - ETA: 47:13 - loss: 0.6299 - regression_loss: 0.5085 - classification_loss: 0.1214
 3801/10000 [==========>...................] - ETA: 47:13 - loss: 0.6300 - regression_loss: 0.5087 - classification_loss: 0.1214
 3802/10000 [==========>...................] - ETA: 47:12 - loss: 0.6299 - regression_loss: 0.5086 - classification_loss: 0.1213
 3803/10000 [==========>...................] - ETA: 47:12 - loss: 0.6299 - regression_loss: 0.5086 - classification_loss: 0.1213
 3804/10000 [==========>...................] - ETA: 47:12 - loss: 0.6298 - regression_loss: 0.5085 - classification_loss: 0.1213
 3805/10000 [==========>...................] - ETA: 47:11 - loss: 0.6302 - regression_loss: 0.5088 - classification_loss: 0.1214
 3806/10000 [==========>...................] - ETA: 47:11 - loss: 0.6301 - regression_loss: 0.5087 - classification_loss: 0.1214
 3807/10000 [==========>...................] - ETA: 47:10 - loss: 0.6301 - regression_loss: 0.5087 - classification_loss: 0.1214
 3808/10000 [==========>...................] - ETA: 47:10 - loss: 0.6301 - regression_loss: 0.5087 - classification_loss: 0.1214
 3809/10000 [==========>...................] - ETA: 47:09 - loss: 0.6300 - regression_loss: 0.5087 - classification_loss: 0.1213
 3810/10000 [==========>...................] - ETA: 47:09 - loss: 0.6301 - regression_loss: 0.5087 - classification_loss: 0.1213
 3811/10000 [==========>...................] - ETA: 47:08 - loss: 0.6301 - regression_loss: 0.5087 - classification_loss: 0.1213
 3812/10000 [==========>...................] - ETA: 47:08 - loss: 0.6300 - regression_loss: 0.5086 - classification_loss: 0.1213
 3813/10000 [==========>...................] - ETA: 47:07 - loss: 0.6300 - regression_loss: 0.5087 - classification_loss: 0.1213
 3814/10000 [==========>...................] - ETA: 47:07 - loss: 0.6301 - regression_loss: 0.5088 - classification_loss: 0.1214
 3815/10000 [==========>...................] - ETA: 47:07 - loss: 0.6300 - regression_loss: 0.5087 - classification_loss: 0.1213
 3816/10000 [==========>...................] - ETA: 47:06 - loss: 0.6302 - regression_loss: 0.5088 - classification_loss: 0.1214
 3817/10000 [==========>...................] - ETA: 47:06 - loss: 0.6302 - regression_loss: 0.5088 - classification_loss: 0.1214
 3818/10000 [==========>...................] - ETA: 47:05 - loss: 0.6301 - regression_loss: 0.5088 - classification_loss: 0.1214
 3819/10000 [==========>...................] - ETA: 47:05 - loss: 0.6301 - regression_loss: 0.5088 - classification_loss: 0.1213
 3820/10000 [==========>...................] - ETA: 47:04 - loss: 0.6300 - regression_loss: 0.5087 - classification_loss: 0.1213
 3821/10000 [==========>...................] - ETA: 47:04 - loss: 0.6300 - regression_loss: 0.5087 - classification_loss: 0.1213
 3822/10000 [==========>...................] - ETA: 47:03 - loss: 0.6299 - regression_loss: 0.5086 - classification_loss: 0.1213
 3823/10000 [==========>...................] - ETA: 47:03 - loss: 0.6298 - regression_loss: 0.5085 - classification_loss: 0.1213
 3824/10000 [==========>...................] - ETA: 47:02 - loss: 0.6300 - regression_loss: 0.5087 - classification_loss: 0.1213
 3825/10000 [==========>...................] - ETA: 47:02 - loss: 0.6299 - regression_loss: 0.5086 - classification_loss: 0.1213
 3826/10000 [==========>...................] - ETA: 47:01 - loss: 0.6300 - regression_loss: 0.5087 - classification_loss: 0.1213
 3827/10000 [==========>...................] - ETA: 47:01 - loss: 0.6300 - regression_loss: 0.5087 - classification_loss: 0.1213
 3828/10000 [==========>...................] - ETA: 47:00 - loss: 0.6299 - regression_loss: 0.5087 - classification_loss: 0.1213
 3829/10000 [==========>...................] - ETA: 47:00 - loss: 0.6301 - regression_loss: 0.5087 - classification_loss: 0.1213
 3830/10000 [==========>...................] - ETA: 46:59 - loss: 0.6300 - regression_loss: 0.5087 - classification_loss: 0.1213
 3831/10000 [==========>...................] - ETA: 46:59 - loss: 0.6302 - regression_loss: 0.5089 - classification_loss: 0.1213
 3832/10000 [==========>...................] - ETA: 46:58 - loss: 0.6305 - regression_loss: 0.5092 - classification_loss: 0.1214
 3833/10000 [==========>...................] - ETA: 46:58 - loss: 0.6308 - regression_loss: 0.5094 - classification_loss: 0.1214
 3834/10000 [==========>...................] - ETA: 46:57 - loss: 0.6307 - regression_loss: 0.5093 - classification_loss: 0.1214
 3835/10000 [==========>...................] - ETA: 46:57 - loss: 0.6308 - regression_loss: 0.5093 - classification_loss: 0.1215
 3836/10000 [==========>...................] - ETA: 46:56 - loss: 0.6307 - regression_loss: 0.5092 - classification_loss: 0.1215
 3837/10000 [==========>...................] - ETA: 46:56 - loss: 0.6307 - regression_loss: 0.5093 - classification_loss: 0.1214
 3838/10000 [==========>...................] - ETA: 46:56 - loss: 0.6307 - regression_loss: 0.5092 - classification_loss: 0.1214
 3839/10000 [==========>...................] - ETA: 46:55 - loss: 0.6306 - regression_loss: 0.5092 - classification_loss: 0.1214
 3840/10000 [==========>...................] - ETA: 46:55 - loss: 0.6306 - regression_loss: 0.5091 - classification_loss: 0.1214
 3841/10000 [==========>...................] - ETA: 46:54 - loss: 0.6307 - regression_loss: 0.5092 - classification_loss: 0.1214
 3842/10000 [==========>...................] - ETA: 46:54 - loss: 0.6307 - regression_loss: 0.5093 - classification_loss: 0.1214
 3843/10000 [==========>...................] - ETA: 46:53 - loss: 0.6307 - regression_loss: 0.5093 - classification_loss: 0.1214
 3844/10000 [==========>...................] - ETA: 46:53 - loss: 0.6306 - regression_loss: 0.5092 - classification_loss: 0.1214
 3845/10000 [==========>...................] - ETA: 46:52 - loss: 0.6306 - regression_loss: 0.5092 - classification_loss: 0.1214
 3846/10000 [==========>...................] - ETA: 46:52 - loss: 0.6305 - regression_loss: 0.5091 - classification_loss: 0.1214
 3847/10000 [==========>...................] - ETA: 46:51 - loss: 0.6304 - regression_loss: 0.5091 - classification_loss: 0.1214
 3848/10000 [==========>...................] - ETA: 46:51 - loss: 0.6304 - regression_loss: 0.5091 - classification_loss: 0.1214
 3849/10000 [==========>...................] - ETA: 46:51 - loss: 0.6304 - regression_loss: 0.5090 - classification_loss: 0.1214
 3850/10000 [==========>...................] - ETA: 46:50 - loss: 0.6303 - regression_loss: 0.5089 - classification_loss: 0.1213
 3851/10000 [==========>...................] - ETA: 46:50 - loss: 0.6301 - regression_loss: 0.5088 - classification_loss: 0.1213
 3852/10000 [==========>...................] - ETA: 46:49 - loss: 0.6302 - regression_loss: 0.5089 - classification_loss: 0.1213
 3853/10000 [==========>...................] - ETA: 46:49 - loss: 0.6305 - regression_loss: 0.5091 - classification_loss: 0.1214
 3854/10000 [==========>...................] - ETA: 46:48 - loss: 0.6304 - regression_loss: 0.5091 - classification_loss: 0.1214
 3855/10000 [==========>...................] - ETA: 46:48 - loss: 0.6304 - regression_loss: 0.5090 - classification_loss: 0.1214
 3856/10000 [==========>...................] - ETA: 46:47 - loss: 0.6303 - regression_loss: 0.5090 - classification_loss: 0.1214
 3857/10000 [==========>...................] - ETA: 46:47 - loss: 0.6304 - regression_loss: 0.5090 - classification_loss: 0.1214
 3858/10000 [==========>...................] - ETA: 46:46 - loss: 0.6304 - regression_loss: 0.5091 - classification_loss: 0.1214
 3859/10000 [==========>...................] - ETA: 46:46 - loss: 0.6304 - regression_loss: 0.5090 - classification_loss: 0.1214
 3860/10000 [==========>...................] - ETA: 46:46 - loss: 0.6305 - regression_loss: 0.5091 - classification_loss: 0.1214
 3861/10000 [==========>...................] - ETA: 46:45 - loss: 0.6307 - regression_loss: 0.5092 - classification_loss: 0.1214
 3862/10000 [==========>...................] - ETA: 46:45 - loss: 0.6307 - regression_loss: 0.5092 - classification_loss: 0.1214
 3863/10000 [==========>...................] - ETA: 46:44 - loss: 0.6306 - regression_loss: 0.5091 - classification_loss: 0.1214
 3864/10000 [==========>...................] - ETA: 46:44 - loss: 0.6306 - regression_loss: 0.5091 - classification_loss: 0.1214
 3865/10000 [==========>...................] - ETA: 46:43 - loss: 0.6305 - regression_loss: 0.5091 - classification_loss: 0.1214
 3866/10000 [==========>...................] - ETA: 46:43 - loss: 0.6304 - regression_loss: 0.5090 - classification_loss: 0.1214
 3867/10000 [==========>...................] - ETA: 46:42 - loss: 0.6303 - regression_loss: 0.5089 - classification_loss: 0.1214
 3868/10000 [==========>...................] - ETA: 46:42 - loss: 0.6302 - regression_loss: 0.5088 - classification_loss: 0.1214
 3869/10000 [==========>...................] - ETA: 46:41 - loss: 0.6303 - regression_loss: 0.5089 - classification_loss: 0.1214
 3870/10000 [==========>...................] - ETA: 46:41 - loss: 0.6302 - regression_loss: 0.5088 - classification_loss: 0.1214
 3871/10000 [==========>...................] - ETA: 46:41 - loss: 0.6302 - regression_loss: 0.5089 - classification_loss: 0.1213
 3872/10000 [==========>...................] - ETA: 46:40 - loss: 0.6303 - regression_loss: 0.5089 - classification_loss: 0.1213
 3873/10000 [==========>...................] - ETA: 46:40 - loss: 0.6302 - regression_loss: 0.5088 - classification_loss: 0.1213
 3874/10000 [==========>...................] - ETA: 46:39 - loss: 0.6301 - regression_loss: 0.5088 - classification_loss: 0.1213
 3875/10000 [==========>...................] - ETA: 46:39 - loss: 0.6300 - regression_loss: 0.5087 - classification_loss: 0.1213
 3876/10000 [==========>...................] - ETA: 46:39 - loss: 0.6300 - regression_loss: 0.5087 - classification_loss: 0.1213
 3877/10000 [==========>...................] - ETA: 46:38 - loss: 0.6298 - regression_loss: 0.5086 - classification_loss: 0.1213
 3878/10000 [==========>...................] - ETA: 46:38 - loss: 0.6298 - regression_loss: 0.5086 - classification_loss: 0.1213
 3879/10000 [==========>...................] - ETA: 46:37 - loss: 0.6298 - regression_loss: 0.5085 - classification_loss: 0.1213
 3880/10000 [==========>...................] - ETA: 46:37 - loss: 0.6297 - regression_loss: 0.5085 - classification_loss: 0.1213
 3881/10000 [==========>...................] - ETA: 46:36 - loss: 0.6297 - regression_loss: 0.5085 - classification_loss: 0.1212
 3882/10000 [==========>...................] - ETA: 46:36 - loss: 0.6299 - regression_loss: 0.5086 - classification_loss: 0.1213
 3883/10000 [==========>...................] - ETA: 46:36 - loss: 0.6299 - regression_loss: 0.5086 - classification_loss: 0.1213
 3884/10000 [==========>...................] - ETA: 46:35 - loss: 0.6301 - regression_loss: 0.5088 - classification_loss: 0.1213
 3885/10000 [==========>...................] - ETA: 46:35 - loss: 0.6302 - regression_loss: 0.5089 - classification_loss: 0.1214
 3886/10000 [==========>...................] - ETA: 46:34 - loss: 0.6302 - regression_loss: 0.5089 - classification_loss: 0.1214
 3887/10000 [==========>...................] - ETA: 46:34 - loss: 0.6303 - regression_loss: 0.5090 - classification_loss: 0.1214
 3888/10000 [==========>...................] - ETA: 46:33 - loss: 0.6304 - regression_loss: 0.5090 - classification_loss: 0.1214
 3889/10000 [==========>...................] - ETA: 46:33 - loss: 0.6304 - regression_loss: 0.5090 - classification_loss: 0.1214
 3890/10000 [==========>...................] - ETA: 46:32 - loss: 0.6303 - regression_loss: 0.5090 - classification_loss: 0.1214
 3891/10000 [==========>...................] - ETA: 46:32 - loss: 0.6302 - regression_loss: 0.5089 - classification_loss: 0.1214
 3892/10000 [==========>...................] - ETA: 46:31 - loss: 0.6302 - regression_loss: 0.5088 - classification_loss: 0.1213
 3893/10000 [==========>...................] - ETA: 46:31 - loss: 0.6302 - regression_loss: 0.5088 - classification_loss: 0.1213
 3894/10000 [==========>...................] - ETA: 46:31 - loss: 0.6300 - regression_loss: 0.5087 - classification_loss: 0.1213
 3895/10000 [==========>...................] - ETA: 46:30 - loss: 0.6302 - regression_loss: 0.5089 - classification_loss: 0.1213
 3896/10000 [==========>...................] - ETA: 46:30 - loss: 0.6301 - regression_loss: 0.5088 - classification_loss: 0.1213
 3897/10000 [==========>...................] - ETA: 46:29 - loss: 0.6301 - regression_loss: 0.5088 - classification_loss: 0.1213
 3898/10000 [==========>...................] - ETA: 46:29 - loss: 0.6300 - regression_loss: 0.5088 - classification_loss: 0.1213
 3899/10000 [==========>...................] - ETA: 46:28 - loss: 0.6300 - regression_loss: 0.5087 - classification_loss: 0.1213
 3900/10000 [==========>...................] - ETA: 46:28 - loss: 0.6300 - regression_loss: 0.5087 - classification_loss: 0.1213
 3901/10000 [==========>...................] - ETA: 46:27 - loss: 0.6300 - regression_loss: 0.5087 - classification_loss: 0.1213
 3902/10000 [==========>...................] - ETA: 46:27 - loss: 0.6299 - regression_loss: 0.5086 - classification_loss: 0.1213
 3903/10000 [==========>...................] - ETA: 46:26 - loss: 0.6298 - regression_loss: 0.5086 - classification_loss: 0.1212
 3904/10000 [==========>...................] - ETA: 46:26 - loss: 0.6298 - regression_loss: 0.5085 - classification_loss: 0.1212
 3905/10000 [==========>...................] - ETA: 46:25 - loss: 0.6297 - regression_loss: 0.5085 - classification_loss: 0.1212
 3906/10000 [==========>...................] - ETA: 46:25 - loss: 0.6297 - regression_loss: 0.5085 - classification_loss: 0.1212
 3907/10000 [==========>...................] - ETA: 46:25 - loss: 0.6297 - regression_loss: 0.5085 - classification_loss: 0.1212
 3908/10000 [==========>...................] - ETA: 46:24 - loss: 0.6296 - regression_loss: 0.5084 - classification_loss: 0.1212
 3909/10000 [==========>...................] - ETA: 46:24 - loss: 0.6296 - regression_loss: 0.5084 - classification_loss: 0.1212
 3910/10000 [==========>...................] - ETA: 46:23 - loss: 0.6295 - regression_loss: 0.5084 - classification_loss: 0.1212
 3911/10000 [==========>...................] - ETA: 46:23 - loss: 0.6294 - regression_loss: 0.5083 - classification_loss: 0.1211
 3912/10000 [==========>...................] - ETA: 46:22 - loss: 0.6295 - regression_loss: 0.5083 - classification_loss: 0.1211
 3913/10000 [==========>...................] - ETA: 46:22 - loss: 0.6295 - regression_loss: 0.5084 - classification_loss: 0.1211
 3914/10000 [==========>...................] - ETA: 46:21 - loss: 0.6296 - regression_loss: 0.5084 - classification_loss: 0.1211
 3915/10000 [==========>...................] - ETA: 46:21 - loss: 0.6296 - regression_loss: 0.5084 - classification_loss: 0.1211
 3916/10000 [==========>...................] - ETA: 46:20 - loss: 0.6296 - regression_loss: 0.5084 - classification_loss: 0.1211
 3917/10000 [==========>...................] - ETA: 46:20 - loss: 0.6295 - regression_loss: 0.5084 - classification_loss: 0.1211
 3918/10000 [==========>...................] - ETA: 46:19 - loss: 0.6297 - regression_loss: 0.5086 - classification_loss: 0.1211
 3919/10000 [==========>...................] - ETA: 46:19 - loss: 0.6297 - regression_loss: 0.5085 - classification_loss: 0.1211
 3920/10000 [==========>...................] - ETA: 46:18 - loss: 0.6296 - regression_loss: 0.5085 - classification_loss: 0.1211
 3921/10000 [==========>...................] - ETA: 46:18 - loss: 0.6296 - regression_loss: 0.5084 - classification_loss: 0.1211
 3922/10000 [==========>...................] - ETA: 46:18 - loss: 0.6296 - regression_loss: 0.5085 - classification_loss: 0.1211
 3923/10000 [==========>...................] - ETA: 46:17 - loss: 0.6294 - regression_loss: 0.5084 - classification_loss: 0.1211
 3924/10000 [==========>...................] - ETA: 46:17 - loss: 0.6294 - regression_loss: 0.5083 - classification_loss: 0.1211
 3925/10000 [==========>...................] - ETA: 46:16 - loss: 0.6294 - regression_loss: 0.5083 - classification_loss: 0.1211
 3926/10000 [==========>...................] - ETA: 46:16 - loss: 0.6294 - regression_loss: 0.5083 - classification_loss: 0.1211
 3927/10000 [==========>...................] - ETA: 46:15 - loss: 0.6293 - regression_loss: 0.5083 - classification_loss: 0.1211
 3928/10000 [==========>...................] - ETA: 46:15 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1211
 3929/10000 [==========>...................] - ETA: 46:14 - loss: 0.6294 - regression_loss: 0.5083 - classification_loss: 0.1211
 3930/10000 [==========>...................] - ETA: 46:14 - loss: 0.6294 - regression_loss: 0.5083 - classification_loss: 0.1211
 3931/10000 [==========>...................] - ETA: 46:13 - loss: 0.6293 - regression_loss: 0.5083 - classification_loss: 0.1210
 3932/10000 [==========>...................] - ETA: 46:13 - loss: 0.6294 - regression_loss: 0.5083 - classification_loss: 0.1211
 3933/10000 [==========>...................] - ETA: 46:12 - loss: 0.6293 - regression_loss: 0.5082 - classification_loss: 0.1210
 3934/10000 [==========>...................] - ETA: 46:12 - loss: 0.6293 - regression_loss: 0.5083 - classification_loss: 0.1210
 3935/10000 [==========>...................] - ETA: 46:11 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1210
 3936/10000 [==========>...................] - ETA: 46:11 - loss: 0.6293 - regression_loss: 0.5083 - classification_loss: 0.1210
 3937/10000 [==========>...................] - ETA: 46:10 - loss: 0.6294 - regression_loss: 0.5084 - classification_loss: 0.1210
 3938/10000 [==========>...................] - ETA: 46:10 - loss: 0.6293 - regression_loss: 0.5083 - classification_loss: 0.1210
 3939/10000 [==========>...................] - ETA: 46:09 - loss: 0.6293 - regression_loss: 0.5083 - classification_loss: 0.1210
 3940/10000 [==========>...................] - ETA: 46:09 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1210
 3941/10000 [==========>...................] - ETA: 46:08 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1210
 3942/10000 [==========>...................] - ETA: 46:08 - loss: 0.6291 - regression_loss: 0.5081 - classification_loss: 0.1209
 3943/10000 [==========>...................] - ETA: 46:08 - loss: 0.6289 - regression_loss: 0.5080 - classification_loss: 0.1209
 3944/10000 [==========>...................] - ETA: 46:07 - loss: 0.6289 - regression_loss: 0.5080 - classification_loss: 0.1209
 3945/10000 [==========>...................] - ETA: 46:07 - loss: 0.6287 - regression_loss: 0.5079 - classification_loss: 0.1209
 3946/10000 [==========>...................] - ETA: 46:06 - loss: 0.6288 - regression_loss: 0.5079 - classification_loss: 0.1209
 3947/10000 [==========>...................] - ETA: 46:06 - loss: 0.6288 - regression_loss: 0.5079 - classification_loss: 0.1209
 3948/10000 [==========>...................] - ETA: 46:05 - loss: 0.6289 - regression_loss: 0.5079 - classification_loss: 0.1209
 3949/10000 [==========>...................] - ETA: 46:05 - loss: 0.6290 - regression_loss: 0.5081 - classification_loss: 0.1209
 3950/10000 [==========>...................] - ETA: 46:05 - loss: 0.6290 - regression_loss: 0.5081 - classification_loss: 0.1209
 3951/10000 [==========>...................] - ETA: 46:04 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1210
 3952/10000 [==========>...................] - ETA: 46:04 - loss: 0.6291 - regression_loss: 0.5081 - classification_loss: 0.1209
 3953/10000 [==========>...................] - ETA: 46:03 - loss: 0.6290 - regression_loss: 0.5081 - classification_loss: 0.1209
 3954/10000 [==========>...................] - ETA: 46:03 - loss: 0.6289 - regression_loss: 0.5080 - classification_loss: 0.1209
 3955/10000 [==========>...................] - ETA: 46:02 - loss: 0.6288 - regression_loss: 0.5079 - classification_loss: 0.1209
 3956/10000 [==========>...................] - ETA: 46:02 - loss: 0.6289 - regression_loss: 0.5080 - classification_loss: 0.1209
 3957/10000 [==========>...................] - ETA: 46:01 - loss: 0.6290 - regression_loss: 0.5081 - classification_loss: 0.1209
 3958/10000 [==========>...................] - ETA: 46:01 - loss: 0.6291 - regression_loss: 0.5081 - classification_loss: 0.1209
 3959/10000 [==========>...................] - ETA: 46:00 - loss: 0.6290 - regression_loss: 0.5080 - classification_loss: 0.1209
 3960/10000 [==========>...................] - ETA: 46:00 - loss: 0.6289 - regression_loss: 0.5080 - classification_loss: 0.1209
 3961/10000 [==========>...................] - ETA: 45:59 - loss: 0.6288 - regression_loss: 0.5079 - classification_loss: 0.1209
 3962/10000 [==========>...................] - ETA: 45:59 - loss: 0.6288 - regression_loss: 0.5079 - classification_loss: 0.1209
 3963/10000 [==========>...................] - ETA: 45:58 - loss: 0.6290 - regression_loss: 0.5081 - classification_loss: 0.1209
 3964/10000 [==========>...................] - ETA: 45:58 - loss: 0.6289 - regression_loss: 0.5080 - classification_loss: 0.1209
 3965/10000 [==========>...................] - ETA: 45:57 - loss: 0.6288 - regression_loss: 0.5079 - classification_loss: 0.1209
 3966/10000 [==========>...................] - ETA: 45:57 - loss: 0.6289 - regression_loss: 0.5080 - classification_loss: 0.1209
 3967/10000 [==========>...................] - ETA: 45:57 - loss: 0.6289 - regression_loss: 0.5079 - classification_loss: 0.1209
 3968/10000 [==========>...................] - ETA: 45:56 - loss: 0.6288 - regression_loss: 0.5078 - classification_loss: 0.1210
 3969/10000 [==========>...................] - ETA: 45:56 - loss: 0.6287 - regression_loss: 0.5077 - classification_loss: 0.1209
 3970/10000 [==========>...................] - ETA: 45:55 - loss: 0.6287 - regression_loss: 0.5077 - classification_loss: 0.1209
 3971/10000 [==========>...................] - ETA: 45:55 - loss: 0.6286 - regression_loss: 0.5077 - classification_loss: 0.1209
 3972/10000 [==========>...................] - ETA: 45:54 - loss: 0.6288 - regression_loss: 0.5078 - classification_loss: 0.1209
 3973/10000 [==========>...................] - ETA: 45:54 - loss: 0.6287 - regression_loss: 0.5078 - classification_loss: 0.1210
 3974/10000 [==========>...................] - ETA: 45:53 - loss: 0.6288 - regression_loss: 0.5078 - classification_loss: 0.1210
 3975/10000 [==========>...................] - ETA: 45:53 - loss: 0.6288 - regression_loss: 0.5078 - classification_loss: 0.1210
 3976/10000 [==========>...................] - ETA: 45:52 - loss: 0.6289 - regression_loss: 0.5079 - classification_loss: 0.1210
 3977/10000 [==========>...................] - ETA: 45:52 - loss: 0.6289 - regression_loss: 0.5079 - classification_loss: 0.1210
 3978/10000 [==========>...................] - ETA: 45:51 - loss: 0.6290 - regression_loss: 0.5080 - classification_loss: 0.1210
 3979/10000 [==========>...................] - ETA: 45:51 - loss: 0.6289 - regression_loss: 0.5079 - classification_loss: 0.1210
 3980/10000 [==========>...................] - ETA: 45:50 - loss: 0.6290 - regression_loss: 0.5080 - classification_loss: 0.1210
 3981/10000 [==========>...................] - ETA: 45:50 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1210
 3982/10000 [==========>...................] - ETA: 45:50 - loss: 0.6293 - regression_loss: 0.5083 - classification_loss: 0.1210
 3983/10000 [==========>...................] - ETA: 45:49 - loss: 0.6295 - regression_loss: 0.5085 - classification_loss: 0.1211
 3984/10000 [==========>...................] - ETA: 45:49 - loss: 0.6294 - regression_loss: 0.5084 - classification_loss: 0.1210
 3985/10000 [==========>...................] - ETA: 45:48 - loss: 0.6294 - regression_loss: 0.5083 - classification_loss: 0.1210
 3986/10000 [==========>...................] - ETA: 45:48 - loss: 0.6294 - regression_loss: 0.5083 - classification_loss: 0.1210
 3987/10000 [==========>...................] - ETA: 45:47 - loss: 0.6293 - regression_loss: 0.5082 - classification_loss: 0.1210
 3988/10000 [==========>...................] - ETA: 45:47 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1210
 3989/10000 [==========>...................] - ETA: 45:47 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1210
 3990/10000 [==========>...................] - ETA: 45:46 - loss: 0.6291 - regression_loss: 0.5082 - classification_loss: 0.1210
 3991/10000 [==========>...................] - ETA: 45:46 - loss: 0.6290 - regression_loss: 0.5081 - classification_loss: 0.1209
 3992/10000 [==========>...................] - ETA: 45:45 - loss: 0.6290 - regression_loss: 0.5081 - classification_loss: 0.1209
 3993/10000 [==========>...................] - ETA: 45:45 - loss: 0.6289 - regression_loss: 0.5080 - classification_loss: 0.1209
 3994/10000 [==========>...................] - ETA: 45:44 - loss: 0.6288 - regression_loss: 0.5079 - classification_loss: 0.1209
 3995/10000 [==========>...................] - ETA: 45:44 - loss: 0.6286 - regression_loss: 0.5078 - classification_loss: 0.1209
 3996/10000 [==========>...................] - ETA: 45:43 - loss: 0.6287 - regression_loss: 0.5079 - classification_loss: 0.1209
 3997/10000 [==========>...................] - ETA: 45:43 - loss: 0.6287 - regression_loss: 0.5078 - classification_loss: 0.1208
 3998/10000 [==========>...................] - ETA: 45:42 - loss: 0.6287 - regression_loss: 0.5079 - classification_loss: 0.1209
 3999/10000 [==========>...................] - ETA: 45:42 - loss: 0.6287 - regression_loss: 0.5078 - classification_loss: 0.1208
 4000/10000 [===========>..................] - ETA: 45:42 - loss: 0.6285 - regression_loss: 0.5077 - classification_loss: 0.1208
 4001/10000 [===========>..................] - ETA: 45:41 - loss: 0.6286 - regression_loss: 0.5078 - classification_loss: 0.1208
 4002/10000 [===========>..................] - ETA: 45:41 - loss: 0.6286 - regression_loss: 0.5078 - classification_loss: 0.1208
 4003/10000 [===========>..................] - ETA: 45:40 - loss: 0.6286 - regression_loss: 0.5078 - classification_loss: 0.1208
 4004/10000 [===========>..................] - ETA: 45:40 - loss: 0.6289 - regression_loss: 0.5080 - classification_loss: 0.1208
 4005/10000 [===========>..................] - ETA: 45:39 - loss: 0.6288 - regression_loss: 0.5080 - classification_loss: 0.1208
 4006/10000 [===========>..................] - ETA: 45:39 - loss: 0.6288 - regression_loss: 0.5080 - classification_loss: 0.1208
 4007/10000 [===========>..................] - ETA: 45:38 - loss: 0.6288 - regression_loss: 0.5080 - classification_loss: 0.1208
 4008/10000 [===========>..................] - ETA: 45:38 - loss: 0.6287 - regression_loss: 0.5079 - classification_loss: 0.1208
 4009/10000 [===========>..................] - ETA: 45:37 - loss: 0.6286 - regression_loss: 0.5078 - classification_loss: 0.1208
 4010/10000 [===========>..................] - ETA: 45:37 - loss: 0.6288 - regression_loss: 0.5079 - classification_loss: 0.1208
 4011/10000 [===========>..................] - ETA: 45:37 - loss: 0.6289 - regression_loss: 0.5079 - classification_loss: 0.1209
 4012/10000 [===========>..................] - ETA: 45:36 - loss: 0.6290 - regression_loss: 0.5080 - classification_loss: 0.1210
 4013/10000 [===========>..................] - ETA: 45:36 - loss: 0.6293 - regression_loss: 0.5082 - classification_loss: 0.1211
 4014/10000 [===========>..................] - ETA: 45:35 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1210
 4015/10000 [===========>..................] - ETA: 45:35 - loss: 0.6292 - regression_loss: 0.5081 - classification_loss: 0.1211
 4016/10000 [===========>..................] - ETA: 45:34 - loss: 0.6291 - regression_loss: 0.5081 - classification_loss: 0.1210
 4017/10000 [===========>..................] - ETA: 45:34 - loss: 0.6290 - regression_loss: 0.5080 - classification_loss: 0.1211
 4018/10000 [===========>..................] - ETA: 45:33 - loss: 0.6291 - regression_loss: 0.5080 - classification_loss: 0.1211
 4019/10000 [===========>..................] - ETA: 45:33 - loss: 0.6290 - regression_loss: 0.5079 - classification_loss: 0.1211
 4020/10000 [===========>..................] - ETA: 45:32 - loss: 0.6290 - regression_loss: 0.5080 - classification_loss: 0.1211
 4021/10000 [===========>..................] - ETA: 45:32 - loss: 0.6289 - regression_loss: 0.5079 - classification_loss: 0.1211
 4022/10000 [===========>..................] - ETA: 45:32 - loss: 0.6289 - regression_loss: 0.5078 - classification_loss: 0.1210
 4023/10000 [===========>..................] - ETA: 45:31 - loss: 0.6288 - regression_loss: 0.5078 - classification_loss: 0.1210
 4024/10000 [===========>..................] - ETA: 45:31 - loss: 0.6288 - regression_loss: 0.5078 - classification_loss: 0.1210
 4025/10000 [===========>..................] - ETA: 45:30 - loss: 0.6288 - regression_loss: 0.5078 - classification_loss: 0.1210
 4026/10000 [===========>..................] - ETA: 45:30 - loss: 0.6289 - regression_loss: 0.5079 - classification_loss: 0.1210
 4027/10000 [===========>..................] - ETA: 45:29 - loss: 0.6288 - regression_loss: 0.5078 - classification_loss: 0.1210
 4028/10000 [===========>..................] - ETA: 45:29 - loss: 0.6288 - regression_loss: 0.5077 - classification_loss: 0.1210
 4029/10000 [===========>..................] - ETA: 45:29 - loss: 0.6289 - regression_loss: 0.5079 - classification_loss: 0.1211
 4030/10000 [===========>..................] - ETA: 45:28 - loss: 0.6289 - regression_loss: 0.5079 - classification_loss: 0.1210
 4031/10000 [===========>..................] - ETA: 45:28 - loss: 0.6289 - regression_loss: 0.5079 - classification_loss: 0.1210
 4032/10000 [===========>..................] - ETA: 45:27 - loss: 0.6288 - regression_loss: 0.5078 - classification_loss: 0.1210
 4033/10000 [===========>..................] - ETA: 45:27 - loss: 0.6290 - regression_loss: 0.5079 - classification_loss: 0.1210
 4034/10000 [===========>..................] - ETA: 45:26 - loss: 0.6289 - regression_loss: 0.5079 - classification_loss: 0.1210
 4035/10000 [===========>..................] - ETA: 45:26 - loss: 0.6292 - regression_loss: 0.5081 - classification_loss: 0.1211
 4036/10000 [===========>..................] - ETA: 45:25 - loss: 0.6292 - regression_loss: 0.5081 - classification_loss: 0.1211
 4037/10000 [===========>..................] - ETA: 45:25 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1211
 4038/10000 [===========>..................] - ETA: 45:24 - loss: 0.6294 - regression_loss: 0.5083 - classification_loss: 0.1211
 4039/10000 [===========>..................] - ETA: 45:24 - loss: 0.6295 - regression_loss: 0.5083 - classification_loss: 0.1211
 4040/10000 [===========>..................] - ETA: 45:24 - loss: 0.6294 - regression_loss: 0.5083 - classification_loss: 0.1211
 4041/10000 [===========>..................] - ETA: 45:23 - loss: 0.6293 - regression_loss: 0.5082 - classification_loss: 0.1211
 4042/10000 [===========>..................] - ETA: 45:23 - loss: 0.6293 - regression_loss: 0.5083 - classification_loss: 0.1211
 4043/10000 [===========>..................] - ETA: 45:22 - loss: 0.6293 - regression_loss: 0.5082 - classification_loss: 0.1211
 4044/10000 [===========>..................] - ETA: 45:22 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1210
 4045/10000 [===========>..................] - ETA: 45:21 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1210
 4046/10000 [===========>..................] - ETA: 45:21 - loss: 0.6291 - regression_loss: 0.5081 - classification_loss: 0.1210
 4047/10000 [===========>..................] - ETA: 45:20 - loss: 0.6293 - regression_loss: 0.5082 - classification_loss: 0.1211
 4048/10000 [===========>..................] - ETA: 45:20 - loss: 0.6294 - regression_loss: 0.5083 - classification_loss: 0.1211
 4049/10000 [===========>..................] - ETA: 45:19 - loss: 0.6293 - regression_loss: 0.5082 - classification_loss: 0.1210
 4050/10000 [===========>..................] - ETA: 45:19 - loss: 0.6293 - regression_loss: 0.5083 - classification_loss: 0.1210
 4051/10000 [===========>..................] - ETA: 45:19 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1211
 4052/10000 [===========>..................] - ETA: 45:18 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1211
 4053/10000 [===========>..................] - ETA: 45:18 - loss: 0.6293 - regression_loss: 0.5082 - classification_loss: 0.1211
 4054/10000 [===========>..................] - ETA: 45:17 - loss: 0.6292 - regression_loss: 0.5081 - classification_loss: 0.1211
 4055/10000 [===========>..................] - ETA: 45:17 - loss: 0.6293 - regression_loss: 0.5082 - classification_loss: 0.1211
 4056/10000 [===========>..................] - ETA: 45:16 - loss: 0.6292 - regression_loss: 0.5081 - classification_loss: 0.1210
 4057/10000 [===========>..................] - ETA: 45:16 - loss: 0.6291 - regression_loss: 0.5081 - classification_loss: 0.1210
 4058/10000 [===========>..................] - ETA: 45:15 - loss: 0.6289 - regression_loss: 0.5079 - classification_loss: 0.1210
 4059/10000 [===========>..................] - ETA: 45:15 - loss: 0.6290 - regression_loss: 0.5081 - classification_loss: 0.1210
 4060/10000 [===========>..................] - ETA: 45:14 - loss: 0.6291 - regression_loss: 0.5081 - classification_loss: 0.1210
 4061/10000 [===========>..................] - ETA: 45:14 - loss: 0.6290 - regression_loss: 0.5080 - classification_loss: 0.1210
 4062/10000 [===========>..................] - ETA: 45:13 - loss: 0.6290 - regression_loss: 0.5080 - classification_loss: 0.1210
 4063/10000 [===========>..................] - ETA: 45:13 - loss: 0.6290 - regression_loss: 0.5080 - classification_loss: 0.1210
 4064/10000 [===========>..................] - ETA: 45:13 - loss: 0.6290 - regression_loss: 0.5080 - classification_loss: 0.1210
 4065/10000 [===========>..................] - ETA: 45:12 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1210
 4066/10000 [===========>..................] - ETA: 45:12 - loss: 0.6293 - regression_loss: 0.5083 - classification_loss: 0.1210
 4067/10000 [===========>..................] - ETA: 45:11 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1210
 4068/10000 [===========>..................] - ETA: 45:11 - loss: 0.6291 - regression_loss: 0.5082 - classification_loss: 0.1210
 4069/10000 [===========>..................] - ETA: 45:10 - loss: 0.6293 - regression_loss: 0.5082 - classification_loss: 0.1210
 4070/10000 [===========>..................] - ETA: 45:10 - loss: 0.6293 - regression_loss: 0.5083 - classification_loss: 0.1210
 4071/10000 [===========>..................] - ETA: 45:09 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1210
 4072/10000 [===========>..................] - ETA: 45:09 - loss: 0.6291 - regression_loss: 0.5081 - classification_loss: 0.1210
 4073/10000 [===========>..................] - ETA: 45:08 - loss: 0.6291 - regression_loss: 0.5081 - classification_loss: 0.1210
 4074/10000 [===========>..................] - ETA: 45:08 - loss: 0.6289 - regression_loss: 0.5079 - classification_loss: 0.1210
 4075/10000 [===========>..................] - ETA: 45:08 - loss: 0.6291 - regression_loss: 0.5081 - classification_loss: 0.1210
 4076/10000 [===========>..................] - ETA: 45:07 - loss: 0.6290 - regression_loss: 0.5080 - classification_loss: 0.1210
 4077/10000 [===========>..................] - ETA: 45:07 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1210
 4078/10000 [===========>..................] - ETA: 45:06 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1210
 4079/10000 [===========>..................] - ETA: 45:06 - loss: 0.6294 - regression_loss: 0.5083 - classification_loss: 0.1211
 4080/10000 [===========>..................] - ETA: 45:05 - loss: 0.6296 - regression_loss: 0.5084 - classification_loss: 0.1211
 4081/10000 [===========>..................] - ETA: 45:05 - loss: 0.6296 - regression_loss: 0.5085 - classification_loss: 0.1211
 4082/10000 [===========>..................] - ETA: 45:05 - loss: 0.6297 - regression_loss: 0.5085 - classification_loss: 0.1211
 4083/10000 [===========>..................] - ETA: 45:04 - loss: 0.6296 - regression_loss: 0.5085 - classification_loss: 0.1211
 4084/10000 [===========>..................] - ETA: 45:04 - loss: 0.6296 - regression_loss: 0.5085 - classification_loss: 0.1211
 4085/10000 [===========>..................] - ETA: 45:03 - loss: 0.6295 - regression_loss: 0.5084 - classification_loss: 0.1211
 4086/10000 [===========>..................] - ETA: 45:03 - loss: 0.6295 - regression_loss: 0.5084 - classification_loss: 0.1211
 4087/10000 [===========>..................] - ETA: 45:02 - loss: 0.6295 - regression_loss: 0.5084 - classification_loss: 0.1211
 4088/10000 [===========>..................] - ETA: 45:02 - loss: 0.6294 - regression_loss: 0.5083 - classification_loss: 0.1211
 4089/10000 [===========>..................] - ETA: 45:01 - loss: 0.6294 - regression_loss: 0.5083 - classification_loss: 0.1211
 4090/10000 [===========>..................] - ETA: 45:01 - loss: 0.6294 - regression_loss: 0.5083 - classification_loss: 0.1211
 4091/10000 [===========>..................] - ETA: 45:00 - loss: 0.6294 - regression_loss: 0.5083 - classification_loss: 0.1211
 4092/10000 [===========>..................] - ETA: 45:00 - loss: 0.6296 - regression_loss: 0.5085 - classification_loss: 0.1211
 4093/10000 [===========>..................] - ETA: 44:59 - loss: 0.6295 - regression_loss: 0.5084 - classification_loss: 0.1211
 4094/10000 [===========>..................] - ETA: 44:59 - loss: 0.6295 - regression_loss: 0.5084 - classification_loss: 0.1211
 4095/10000 [===========>..................] - ETA: 44:58 - loss: 0.6294 - regression_loss: 0.5083 - classification_loss: 0.1211
 4096/10000 [===========>..................] - ETA: 44:58 - loss: 0.6295 - regression_loss: 0.5084 - classification_loss: 0.1211
 4097/10000 [===========>..................] - ETA: 44:57 - loss: 0.6294 - regression_loss: 0.5084 - classification_loss: 0.1211
 4098/10000 [===========>..................] - ETA: 44:57 - loss: 0.6294 - regression_loss: 0.5083 - classification_loss: 0.1211
 4099/10000 [===========>..................] - ETA: 44:57 - loss: 0.6295 - regression_loss: 0.5084 - classification_loss: 0.1211
 4100/10000 [===========>..................] - ETA: 44:56 - loss: 0.6297 - regression_loss: 0.5085 - classification_loss: 0.1212
 4101/10000 [===========>..................] - ETA: 44:56 - loss: 0.6297 - regression_loss: 0.5086 - classification_loss: 0.1211
 4102/10000 [===========>..................] - ETA: 44:55 - loss: 0.6297 - regression_loss: 0.5085 - classification_loss: 0.1212
 4103/10000 [===========>..................] - ETA: 44:55 - loss: 0.6296 - regression_loss: 0.5085 - classification_loss: 0.1211
 4104/10000 [===========>..................] - ETA: 44:54 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1212
 4105/10000 [===========>..................] - ETA: 44:54 - loss: 0.6298 - regression_loss: 0.5086 - classification_loss: 0.1212
 4106/10000 [===========>..................] - ETA: 44:53 - loss: 0.6298 - regression_loss: 0.5086 - classification_loss: 0.1212
 4107/10000 [===========>..................] - ETA: 44:53 - loss: 0.6297 - regression_loss: 0.5085 - classification_loss: 0.1212
 4108/10000 [===========>..................] - ETA: 44:52 - loss: 0.6297 - regression_loss: 0.5085 - classification_loss: 0.1211
 4109/10000 [===========>..................] - ETA: 44:52 - loss: 0.6298 - regression_loss: 0.5086 - classification_loss: 0.1212
 4110/10000 [===========>..................] - ETA: 44:52 - loss: 0.6297 - regression_loss: 0.5086 - classification_loss: 0.1211
 4111/10000 [===========>..................] - ETA: 44:51 - loss: 0.6300 - regression_loss: 0.5087 - classification_loss: 0.1212
 4112/10000 [===========>..................] - ETA: 44:51 - loss: 0.6299 - regression_loss: 0.5087 - classification_loss: 0.1212
 4113/10000 [===========>..................] - ETA: 44:50 - loss: 0.6299 - regression_loss: 0.5087 - classification_loss: 0.1212
 4114/10000 [===========>..................] - ETA: 44:50 - loss: 0.6299 - regression_loss: 0.5087 - classification_loss: 0.1212
 4115/10000 [===========>..................] - ETA: 44:49 - loss: 0.6298 - regression_loss: 0.5086 - classification_loss: 0.1212
 4116/10000 [===========>..................] - ETA: 44:49 - loss: 0.6299 - regression_loss: 0.5087 - classification_loss: 0.1212
 4117/10000 [===========>..................] - ETA: 44:48 - loss: 0.6298 - regression_loss: 0.5086 - classification_loss: 0.1212
 4118/10000 [===========>..................] - ETA: 44:48 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1212
 4119/10000 [===========>..................] - ETA: 44:47 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4120/10000 [===========>..................] - ETA: 44:47 - loss: 0.6300 - regression_loss: 0.5088 - classification_loss: 0.1212
 4121/10000 [===========>..................] - ETA: 44:46 - loss: 0.6300 - regression_loss: 0.5088 - classification_loss: 0.1212
 4122/10000 [===========>..................] - ETA: 44:46 - loss: 0.6301 - regression_loss: 0.5089 - classification_loss: 0.1212
 4123/10000 [===========>..................] - ETA: 44:45 - loss: 0.6300 - regression_loss: 0.5088 - classification_loss: 0.1212
 4124/10000 [===========>..................] - ETA: 44:45 - loss: 0.6300 - regression_loss: 0.5088 - classification_loss: 0.1211
 4125/10000 [===========>..................] - ETA: 44:45 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4126/10000 [===========>..................] - ETA: 44:44 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4127/10000 [===========>..................] - ETA: 44:44 - loss: 0.6297 - regression_loss: 0.5086 - classification_loss: 0.1211
 4128/10000 [===========>..................] - ETA: 44:43 - loss: 0.6297 - regression_loss: 0.5086 - classification_loss: 0.1211
 4129/10000 [===========>..................] - ETA: 44:43 - loss: 0.6296 - regression_loss: 0.5085 - classification_loss: 0.1211
 4130/10000 [===========>..................] - ETA: 44:42 - loss: 0.6295 - regression_loss: 0.5084 - classification_loss: 0.1211
 4131/10000 [===========>..................] - ETA: 44:42 - loss: 0.6294 - regression_loss: 0.5083 - classification_loss: 0.1211
 4132/10000 [===========>..................] - ETA: 44:41 - loss: 0.6294 - regression_loss: 0.5084 - classification_loss: 0.1211
 4133/10000 [===========>..................] - ETA: 44:41 - loss: 0.6294 - regression_loss: 0.5083 - classification_loss: 0.1210
 4134/10000 [===========>..................] - ETA: 44:40 - loss: 0.6296 - regression_loss: 0.5085 - classification_loss: 0.1211
 4135/10000 [===========>..................] - ETA: 44:40 - loss: 0.6295 - regression_loss: 0.5085 - classification_loss: 0.1211
 4136/10000 [===========>..................] - ETA: 44:40 - loss: 0.6295 - regression_loss: 0.5085 - classification_loss: 0.1211
 4137/10000 [===========>..................] - ETA: 44:39 - loss: 0.6295 - regression_loss: 0.5085 - classification_loss: 0.1210
 4138/10000 [===========>..................] - ETA: 44:39 - loss: 0.6296 - regression_loss: 0.5085 - classification_loss: 0.1211
 4139/10000 [===========>..................] - ETA: 44:38 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1212
 4140/10000 [===========>..................] - ETA: 44:38 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1212
 4141/10000 [===========>..................] - ETA: 44:37 - loss: 0.6300 - regression_loss: 0.5089 - classification_loss: 0.1212
 4142/10000 [===========>..................] - ETA: 44:37 - loss: 0.6301 - regression_loss: 0.5090 - classification_loss: 0.1212
 4143/10000 [===========>..................] - ETA: 44:36 - loss: 0.6301 - regression_loss: 0.5090 - classification_loss: 0.1212
 4144/10000 [===========>..................] - ETA: 44:36 - loss: 0.6304 - regression_loss: 0.5093 - classification_loss: 0.1212
 4145/10000 [===========>..................] - ETA: 44:35 - loss: 0.6304 - regression_loss: 0.5092 - classification_loss: 0.1212
 4146/10000 [===========>..................] - ETA: 44:35 - loss: 0.6304 - regression_loss: 0.5092 - classification_loss: 0.1212
 4147/10000 [===========>..................] - ETA: 44:35 - loss: 0.6303 - regression_loss: 0.5091 - classification_loss: 0.1212
 4148/10000 [===========>..................] - ETA: 44:34 - loss: 0.6304 - regression_loss: 0.5092 - classification_loss: 0.1212
 4149/10000 [===========>..................] - ETA: 44:34 - loss: 0.6305 - regression_loss: 0.5093 - classification_loss: 0.1212
 4150/10000 [===========>..................] - ETA: 44:33 - loss: 0.6305 - regression_loss: 0.5093 - classification_loss: 0.1212
 4151/10000 [===========>..................] - ETA: 44:33 - loss: 0.6305 - regression_loss: 0.5093 - classification_loss: 0.1212
 4152/10000 [===========>..................] - ETA: 44:32 - loss: 0.6304 - regression_loss: 0.5092 - classification_loss: 0.1212
 4153/10000 [===========>..................] - ETA: 44:32 - loss: 0.6303 - regression_loss: 0.5092 - classification_loss: 0.1211
 4154/10000 [===========>..................] - ETA: 44:31 - loss: 0.6304 - regression_loss: 0.5092 - classification_loss: 0.1212
 4155/10000 [===========>..................] - ETA: 44:31 - loss: 0.6303 - regression_loss: 0.5092 - classification_loss: 0.1212
 4156/10000 [===========>..................] - ETA: 44:30 - loss: 0.6303 - regression_loss: 0.5092 - classification_loss: 0.1212
 4157/10000 [===========>..................] - ETA: 44:30 - loss: 0.6303 - regression_loss: 0.5091 - classification_loss: 0.1212
 4158/10000 [===========>..................] - ETA: 44:29 - loss: 0.6304 - regression_loss: 0.5092 - classification_loss: 0.1212
 4159/10000 [===========>..................] - ETA: 44:29 - loss: 0.6304 - regression_loss: 0.5092 - classification_loss: 0.1212
 4160/10000 [===========>..................] - ETA: 44:29 - loss: 0.6304 - regression_loss: 0.5092 - classification_loss: 0.1212
 4161/10000 [===========>..................] - ETA: 44:28 - loss: 0.6306 - regression_loss: 0.5093 - classification_loss: 0.1212
 4162/10000 [===========>..................] - ETA: 44:28 - loss: 0.6305 - regression_loss: 0.5093 - classification_loss: 0.1212
 4163/10000 [===========>..................] - ETA: 44:27 - loss: 0.6305 - regression_loss: 0.5092 - classification_loss: 0.1212
 4164/10000 [===========>..................] - ETA: 44:27 - loss: 0.6304 - regression_loss: 0.5092 - classification_loss: 0.1212
 4165/10000 [===========>..................] - ETA: 44:26 - loss: 0.6306 - regression_loss: 0.5093 - classification_loss: 0.1213
 4166/10000 [===========>..................] - ETA: 44:26 - loss: 0.6306 - regression_loss: 0.5093 - classification_loss: 0.1213
 4167/10000 [===========>..................] - ETA: 44:25 - loss: 0.6306 - regression_loss: 0.5093 - classification_loss: 0.1213
 4168/10000 [===========>..................] - ETA: 44:25 - loss: 0.6305 - regression_loss: 0.5092 - classification_loss: 0.1212
 4169/10000 [===========>..................] - ETA: 44:24 - loss: 0.6304 - regression_loss: 0.5091 - classification_loss: 0.1212
 4170/10000 [===========>..................] - ETA: 44:24 - loss: 0.6304 - regression_loss: 0.5091 - classification_loss: 0.1212
 4171/10000 [===========>..................] - ETA: 44:23 - loss: 0.6305 - regression_loss: 0.5093 - classification_loss: 0.1213
 4172/10000 [===========>..................] - ETA: 44:23 - loss: 0.6304 - regression_loss: 0.5092 - classification_loss: 0.1212
 4173/10000 [===========>..................] - ETA: 44:22 - loss: 0.6303 - regression_loss: 0.5091 - classification_loss: 0.1212
 4174/10000 [===========>..................] - ETA: 44:22 - loss: 0.6302 - regression_loss: 0.5090 - classification_loss: 0.1212
 4175/10000 [===========>..................] - ETA: 44:21 - loss: 0.6304 - regression_loss: 0.5091 - classification_loss: 0.1212
 4176/10000 [===========>..................] - ETA: 44:21 - loss: 0.6303 - regression_loss: 0.5091 - classification_loss: 0.1212
 4177/10000 [===========>..................] - ETA: 44:21 - loss: 0.6303 - regression_loss: 0.5091 - classification_loss: 0.1212
 4178/10000 [===========>..................] - ETA: 44:20 - loss: 0.6302 - regression_loss: 0.5090 - classification_loss: 0.1212
 4179/10000 [===========>..................] - ETA: 44:20 - loss: 0.6303 - regression_loss: 0.5091 - classification_loss: 0.1212
 4180/10000 [===========>..................] - ETA: 44:19 - loss: 0.6302 - regression_loss: 0.5090 - classification_loss: 0.1212
 4181/10000 [===========>..................] - ETA: 44:19 - loss: 0.6301 - regression_loss: 0.5089 - classification_loss: 0.1211
 4182/10000 [===========>..................] - ETA: 44:18 - loss: 0.6300 - regression_loss: 0.5089 - classification_loss: 0.1211
 4183/10000 [===========>..................] - ETA: 44:18 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4184/10000 [===========>..................] - ETA: 44:17 - loss: 0.6300 - regression_loss: 0.5088 - classification_loss: 0.1211
 4185/10000 [===========>..................] - ETA: 44:17 - loss: 0.6300 - regression_loss: 0.5088 - classification_loss: 0.1211
 4186/10000 [===========>..................] - ETA: 44:16 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4187/10000 [===========>..................] - ETA: 44:16 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4188/10000 [===========>..................] - ETA: 44:15 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4189/10000 [===========>..................] - ETA: 44:15 - loss: 0.6297 - regression_loss: 0.5087 - classification_loss: 0.1211
 4190/10000 [===========>..................] - ETA: 44:15 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1211
 4191/10000 [===========>..................] - ETA: 44:14 - loss: 0.6296 - regression_loss: 0.5085 - classification_loss: 0.1211
 4192/10000 [===========>..................] - ETA: 44:14 - loss: 0.6298 - regression_loss: 0.5086 - classification_loss: 0.1211
 4193/10000 [===========>..................] - ETA: 44:13 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1212
 4194/10000 [===========>..................] - ETA: 44:13 - loss: 0.6297 - regression_loss: 0.5086 - classification_loss: 0.1211
 4195/10000 [===========>..................] - ETA: 44:12 - loss: 0.6298 - regression_loss: 0.5086 - classification_loss: 0.1211
 4196/10000 [===========>..................] - ETA: 44:12 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4197/10000 [===========>..................] - ETA: 44:11 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4198/10000 [===========>..................] - ETA: 44:11 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4199/10000 [===========>..................] - ETA: 44:11 - loss: 0.6299 - regression_loss: 0.5087 - classification_loss: 0.1211
 4200/10000 [===========>..................] - ETA: 44:10 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4201/10000 [===========>..................] - ETA: 44:10 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4202/10000 [===========>..................] - ETA: 44:09 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4203/10000 [===========>..................] - ETA: 44:09 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4204/10000 [===========>..................] - ETA: 44:08 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4205/10000 [===========>..................] - ETA: 44:08 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4206/10000 [===========>..................] - ETA: 44:07 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4207/10000 [===========>..................] - ETA: 44:07 - loss: 0.6299 - regression_loss: 0.5089 - classification_loss: 0.1211
 4208/10000 [===========>..................] - ETA: 44:06 - loss: 0.6300 - regression_loss: 0.5090 - classification_loss: 0.1211
 4209/10000 [===========>..................] - ETA: 44:06 - loss: 0.6299 - regression_loss: 0.5089 - classification_loss: 0.1211
 4210/10000 [===========>..................] - ETA: 44:06 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4211/10000 [===========>..................] - ETA: 44:05 - loss: 0.6299 - regression_loss: 0.5089 - classification_loss: 0.1211
 4212/10000 [===========>..................] - ETA: 44:05 - loss: 0.6298 - regression_loss: 0.5088 - classification_loss: 0.1210
 4213/10000 [===========>..................] - ETA: 44:04 - loss: 0.6297 - regression_loss: 0.5087 - classification_loss: 0.1210
 4214/10000 [===========>..................] - ETA: 44:04 - loss: 0.6297 - regression_loss: 0.5087 - classification_loss: 0.1210
 4215/10000 [===========>..................] - ETA: 44:03 - loss: 0.6298 - regression_loss: 0.5088 - classification_loss: 0.1210
 4216/10000 [===========>..................] - ETA: 44:03 - loss: 0.6297 - regression_loss: 0.5087 - classification_loss: 0.1210
 4217/10000 [===========>..................] - ETA: 44:02 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1210
 4218/10000 [===========>..................] - ETA: 44:02 - loss: 0.6297 - regression_loss: 0.5086 - classification_loss: 0.1210
 4219/10000 [===========>..................] - ETA: 44:01 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1210
 4220/10000 [===========>..................] - ETA: 44:01 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4221/10000 [===========>..................] - ETA: 44:00 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4222/10000 [===========>..................] - ETA: 44:00 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4223/10000 [===========>..................] - ETA: 43:59 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4224/10000 [===========>..................] - ETA: 43:59 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4225/10000 [===========>..................] - ETA: 43:59 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4226/10000 [===========>..................] - ETA: 43:58 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4227/10000 [===========>..................] - ETA: 43:58 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1211
 4228/10000 [===========>..................] - ETA: 43:57 - loss: 0.6297 - regression_loss: 0.5086 - classification_loss: 0.1211
 4229/10000 [===========>..................] - ETA: 43:57 - loss: 0.6296 - regression_loss: 0.5085 - classification_loss: 0.1211
 4230/10000 [===========>..................] - ETA: 43:56 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4231/10000 [===========>..................] - ETA: 43:56 - loss: 0.6297 - regression_loss: 0.5086 - classification_loss: 0.1211
 4232/10000 [===========>..................] - ETA: 43:55 - loss: 0.6296 - regression_loss: 0.5085 - classification_loss: 0.1211
 4233/10000 [===========>..................] - ETA: 43:55 - loss: 0.6297 - regression_loss: 0.5086 - classification_loss: 0.1211
 4234/10000 [===========>..................] - ETA: 43:54 - loss: 0.6296 - regression_loss: 0.5085 - classification_loss: 0.1210
 4235/10000 [===========>..................] - ETA: 43:54 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4236/10000 [===========>..................] - ETA: 43:54 - loss: 0.6297 - regression_loss: 0.5086 - classification_loss: 0.1211
 4237/10000 [===========>..................] - ETA: 43:53 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1211
 4238/10000 [===========>..................] - ETA: 43:53 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4239/10000 [===========>..................] - ETA: 43:52 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4240/10000 [===========>..................] - ETA: 43:52 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4241/10000 [===========>..................] - ETA: 43:51 - loss: 0.6300 - regression_loss: 0.5089 - classification_loss: 0.1211
 4242/10000 [===========>..................] - ETA: 43:51 - loss: 0.6300 - regression_loss: 0.5089 - classification_loss: 0.1212
 4243/10000 [===========>..................] - ETA: 43:50 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1212
 4244/10000 [===========>..................] - ETA: 43:50 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4245/10000 [===========>..................] - ETA: 43:49 - loss: 0.6300 - regression_loss: 0.5089 - classification_loss: 0.1211
 4246/10000 [===========>..................] - ETA: 43:49 - loss: 0.6300 - regression_loss: 0.5089 - classification_loss: 0.1211
 4247/10000 [===========>..................] - ETA: 43:48 - loss: 0.6302 - regression_loss: 0.5091 - classification_loss: 0.1212
 4248/10000 [===========>..................] - ETA: 43:48 - loss: 0.6304 - regression_loss: 0.5093 - classification_loss: 0.1212
 4249/10000 [===========>..................] - ETA: 43:48 - loss: 0.6305 - regression_loss: 0.5093 - classification_loss: 0.1212
 4250/10000 [===========>..................] - ETA: 43:47 - loss: 0.6304 - regression_loss: 0.5093 - classification_loss: 0.1211
 4251/10000 [===========>..................] - ETA: 43:47 - loss: 0.6303 - regression_loss: 0.5092 - classification_loss: 0.1211
 4252/10000 [===========>..................] - ETA: 43:46 - loss: 0.6304 - regression_loss: 0.5093 - classification_loss: 0.1211
 4253/10000 [===========>..................] - ETA: 43:46 - loss: 0.6304 - regression_loss: 0.5093 - classification_loss: 0.1211
 4254/10000 [===========>..................] - ETA: 43:45 - loss: 0.6303 - regression_loss: 0.5092 - classification_loss: 0.1211
 4255/10000 [===========>..................] - ETA: 43:45 - loss: 0.6301 - regression_loss: 0.5091 - classification_loss: 0.1211
 4256/10000 [===========>..................] - ETA: 43:44 - loss: 0.6301 - regression_loss: 0.5090 - classification_loss: 0.1210
 4257/10000 [===========>..................] - ETA: 43:44 - loss: 0.6300 - regression_loss: 0.5090 - classification_loss: 0.1210
 4258/10000 [===========>..................] - ETA: 43:44 - loss: 0.6299 - regression_loss: 0.5089 - classification_loss: 0.1210
 4259/10000 [===========>..................] - ETA: 43:43 - loss: 0.6299 - regression_loss: 0.5089 - classification_loss: 0.1210
 4260/10000 [===========>..................] - ETA: 43:43 - loss: 0.6298 - regression_loss: 0.5089 - classification_loss: 0.1210
 4261/10000 [===========>..................] - ETA: 43:42 - loss: 0.6297 - regression_loss: 0.5088 - classification_loss: 0.1209
 4262/10000 [===========>..................] - ETA: 43:42 - loss: 0.6296 - regression_loss: 0.5087 - classification_loss: 0.1209
 4263/10000 [===========>..................] - ETA: 43:41 - loss: 0.6296 - regression_loss: 0.5087 - classification_loss: 0.1209
 4264/10000 [===========>..................] - ETA: 43:41 - loss: 0.6295 - regression_loss: 0.5086 - classification_loss: 0.1209
 4265/10000 [===========>..................] - ETA: 43:40 - loss: 0.6294 - regression_loss: 0.5085 - classification_loss: 0.1209
 4266/10000 [===========>..................] - ETA: 43:40 - loss: 0.6293 - regression_loss: 0.5084 - classification_loss: 0.1209
 4267/10000 [===========>..................] - ETA: 43:39 - loss: 0.6293 - regression_loss: 0.5084 - classification_loss: 0.1209
 4268/10000 [===========>..................] - ETA: 43:39 - loss: 0.6294 - regression_loss: 0.5085 - classification_loss: 0.1209
 4269/10000 [===========>..................] - ETA: 43:38 - loss: 0.6294 - regression_loss: 0.5084 - classification_loss: 0.1209
 4270/10000 [===========>..................] - ETA: 43:38 - loss: 0.6294 - regression_loss: 0.5085 - classification_loss: 0.1209
 4271/10000 [===========>..................] - ETA: 43:38 - loss: 0.6294 - regression_loss: 0.5085 - classification_loss: 0.1210
 4272/10000 [===========>..................] - ETA: 43:37 - loss: 0.6293 - regression_loss: 0.5084 - classification_loss: 0.1209
 4273/10000 [===========>..................] - ETA: 43:37 - loss: 0.6292 - regression_loss: 0.5083 - classification_loss: 0.1209
 4274/10000 [===========>..................] - ETA: 43:36 - loss: 0.6292 - regression_loss: 0.5083 - classification_loss: 0.1209
 4275/10000 [===========>..................] - ETA: 43:36 - loss: 0.6292 - regression_loss: 0.5083 - classification_loss: 0.1209
 4276/10000 [===========>..................] - ETA: 43:35 - loss: 0.6291 - regression_loss: 0.5082 - classification_loss: 0.1209
 4277/10000 [===========>..................] - ETA: 43:35 - loss: 0.6290 - regression_loss: 0.5082 - classification_loss: 0.1208
 4278/10000 [===========>..................] - ETA: 43:34 - loss: 0.6290 - regression_loss: 0.5081 - classification_loss: 0.1209
 4279/10000 [===========>..................] - ETA: 43:34 - loss: 0.6289 - regression_loss: 0.5080 - classification_loss: 0.1209
 4280/10000 [===========>..................] - ETA: 43:33 - loss: 0.6288 - regression_loss: 0.5080 - classification_loss: 0.1209
 4281/10000 [===========>..................] - ETA: 43:33 - loss: 0.6288 - regression_loss: 0.5079 - classification_loss: 0.1208
 4282/10000 [===========>..................] - ETA: 43:33 - loss: 0.6287 - regression_loss: 0.5079 - classification_loss: 0.1208
 4283/10000 [===========>..................] - ETA: 43:32 - loss: 0.6288 - regression_loss: 0.5080 - classification_loss: 0.1208
 4284/10000 [===========>..................] - ETA: 43:32 - loss: 0.6288 - regression_loss: 0.5079 - classification_loss: 0.1208
 4285/10000 [===========>..................] - ETA: 43:31 - loss: 0.6287 - regression_loss: 0.5079 - classification_loss: 0.1208
 4286/10000 [===========>..................] - ETA: 43:31 - loss: 0.6287 - regression_loss: 0.5078 - classification_loss: 0.1208
 4287/10000 [===========>..................] - ETA: 43:30 - loss: 0.6286 - regression_loss: 0.5078 - classification_loss: 0.1208
 4288/10000 [===========>..................] - ETA: 43:30 - loss: 0.6287 - regression_loss: 0.5079 - classification_loss: 0.1208
 4289/10000 [===========>..................] - ETA: 43:29 - loss: 0.6288 - regression_loss: 0.5079 - classification_loss: 0.1208
 4290/10000 [===========>..................] - ETA: 43:29 - loss: 0.6289 - regression_loss: 0.5080 - classification_loss: 0.1208
 4291/10000 [===========>..................] - ETA: 43:28 - loss: 0.6289 - regression_loss: 0.5081 - classification_loss: 0.1208
 4292/10000 [===========>..................] - ETA: 43:28 - loss: 0.6289 - regression_loss: 0.5081 - classification_loss: 0.1208
 4293/10000 [===========>..................] - ETA: 43:28 - loss: 0.6288 - regression_loss: 0.5080 - classification_loss: 0.1208
 4294/10000 [===========>..................] - ETA: 43:27 - loss: 0.6288 - regression_loss: 0.5080 - classification_loss: 0.1208
 4295/10000 [===========>..................] - ETA: 43:27 - loss: 0.6288 - regression_loss: 0.5080 - classification_loss: 0.1208
 4296/10000 [===========>..................] - ETA: 43:26 - loss: 0.6288 - regression_loss: 0.5080 - classification_loss: 0.1208
 4297/10000 [===========>..................] - ETA: 43:26 - loss: 0.6288 - regression_loss: 0.5080 - classification_loss: 0.1208
 4298/10000 [===========>..................] - ETA: 43:25 - loss: 0.6289 - regression_loss: 0.5081 - classification_loss: 0.1208
 4299/10000 [===========>..................] - ETA: 43:25 - loss: 0.6291 - regression_loss: 0.5082 - classification_loss: 0.1208
 4300/10000 [===========>..................] - ETA: 43:24 - loss: 0.6290 - regression_loss: 0.5082 - classification_loss: 0.1208
 4301/10000 [===========>..................] - ETA: 43:24 - loss: 0.6290 - regression_loss: 0.5081 - classification_loss: 0.1209
 4302/10000 [===========>..................] - ETA: 43:23 - loss: 0.6290 - regression_loss: 0.5081 - classification_loss: 0.1210
 4303/10000 [===========>..................] - ETA: 43:23 - loss: 0.6291 - regression_loss: 0.5081 - classification_loss: 0.1210
 4304/10000 [===========>..................] - ETA: 43:23 - loss: 0.6290 - regression_loss: 0.5081 - classification_loss: 0.1210
 4305/10000 [===========>..................] - ETA: 43:22 - loss: 0.6290 - regression_loss: 0.5080 - classification_loss: 0.1210
 4306/10000 [===========>..................] - ETA: 43:22 - loss: 0.6290 - regression_loss: 0.5081 - classification_loss: 0.1210
 4307/10000 [===========>..................] - ETA: 43:21 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1210
 4308/10000 [===========>..................] - ETA: 43:21 - loss: 0.6293 - regression_loss: 0.5083 - classification_loss: 0.1210
 4309/10000 [===========>..................] - ETA: 43:20 - loss: 0.6293 - regression_loss: 0.5082 - classification_loss: 0.1210
 4310/10000 [===========>..................] - ETA: 43:20 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1210
 4311/10000 [===========>..................] - ETA: 43:19 - loss: 0.6293 - regression_loss: 0.5083 - classification_loss: 0.1210
 4312/10000 [===========>..................] - ETA: 43:19 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1210
 4313/10000 [===========>..................] - ETA: 43:19 - loss: 0.6291 - regression_loss: 0.5081 - classification_loss: 0.1210
 4314/10000 [===========>..................] - ETA: 43:18 - loss: 0.6290 - regression_loss: 0.5080 - classification_loss: 0.1209
 4315/10000 [===========>..................] - ETA: 43:18 - loss: 0.6290 - regression_loss: 0.5080 - classification_loss: 0.1209
 4316/10000 [===========>..................] - ETA: 43:17 - loss: 0.6291 - regression_loss: 0.5081 - classification_loss: 0.1209
 4317/10000 [===========>..................] - ETA: 43:17 - loss: 0.6289 - regression_loss: 0.5080 - classification_loss: 0.1209
 4318/10000 [===========>..................] - ETA: 43:16 - loss: 0.6289 - regression_loss: 0.5079 - classification_loss: 0.1209
 4319/10000 [===========>..................] - ETA: 43:16 - loss: 0.6287 - regression_loss: 0.5078 - classification_loss: 0.1209
 4320/10000 [===========>..................] - ETA: 43:15 - loss: 0.6287 - regression_loss: 0.5078 - classification_loss: 0.1209
 4321/10000 [===========>..................] - ETA: 43:15 - loss: 0.6288 - regression_loss: 0.5079 - classification_loss: 0.1209
 4322/10000 [===========>..................] - ETA: 43:15 - loss: 0.6287 - regression_loss: 0.5078 - classification_loss: 0.1209
 4323/10000 [===========>..................] - ETA: 43:14 - loss: 0.6287 - regression_loss: 0.5079 - classification_loss: 0.1209
 4324/10000 [===========>..................] - ETA: 43:14 - loss: 0.6288 - regression_loss: 0.5079 - classification_loss: 0.1209
 4325/10000 [===========>..................] - ETA: 43:13 - loss: 0.6287 - regression_loss: 0.5079 - classification_loss: 0.1209
 4326/10000 [===========>..................] - ETA: 43:13 - loss: 0.6288 - regression_loss: 0.5079 - classification_loss: 0.1209
 4327/10000 [===========>..................] - ETA: 43:12 - loss: 0.6287 - regression_loss: 0.5079 - classification_loss: 0.1208
 4328/10000 [===========>..................] - ETA: 43:12 - loss: 0.6287 - regression_loss: 0.5078 - classification_loss: 0.1209
 4329/10000 [===========>..................] - ETA: 43:11 - loss: 0.6287 - regression_loss: 0.5078 - classification_loss: 0.1209
 4330/10000 [===========>..................] - ETA: 43:11 - loss: 0.6287 - regression_loss: 0.5079 - classification_loss: 0.1209
 4331/10000 [===========>..................] - ETA: 43:10 - loss: 0.6289 - regression_loss: 0.5080 - classification_loss: 0.1209
 4332/10000 [===========>..................] - ETA: 43:10 - loss: 0.6290 - regression_loss: 0.5081 - classification_loss: 0.1209
 4333/10000 [===========>..................] - ETA: 43:09 - loss: 0.6289 - regression_loss: 0.5080 - classification_loss: 0.1209
 4334/10000 [============>.................] - ETA: 43:09 - loss: 0.6288 - regression_loss: 0.5079 - classification_loss: 0.1209
 4335/10000 [============>.................] - ETA: 43:08 - loss: 0.6288 - regression_loss: 0.5079 - classification_loss: 0.1209
 4336/10000 [============>.................] - ETA: 43:08 - loss: 0.6287 - regression_loss: 0.5079 - classification_loss: 0.1209
 4337/10000 [============>.................] - ETA: 43:07 - loss: 0.6288 - regression_loss: 0.5079 - classification_loss: 0.1209
 4338/10000 [============>.................] - ETA: 43:07 - loss: 0.6288 - regression_loss: 0.5079 - classification_loss: 0.1209
 4339/10000 [============>.................] - ETA: 43:07 - loss: 0.6287 - regression_loss: 0.5079 - classification_loss: 0.1209
 4340/10000 [============>.................] - ETA: 43:06 - loss: 0.6290 - regression_loss: 0.5080 - classification_loss: 0.1210
 4341/10000 [============>.................] - ETA: 43:06 - loss: 0.6290 - regression_loss: 0.5080 - classification_loss: 0.1210
 4342/10000 [============>.................] - ETA: 43:05 - loss: 0.6290 - regression_loss: 0.5080 - classification_loss: 0.1210
 4343/10000 [============>.................] - ETA: 43:05 - loss: 0.6293 - regression_loss: 0.5082 - classification_loss: 0.1211
 4344/10000 [============>.................] - ETA: 43:04 - loss: 0.6293 - regression_loss: 0.5082 - classification_loss: 0.1211
 4345/10000 [============>.................] - ETA: 43:04 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1210
 4346/10000 [============>.................] - ETA: 43:03 - loss: 0.6292 - regression_loss: 0.5081 - classification_loss: 0.1210
 4347/10000 [============>.................] - ETA: 43:03 - loss: 0.6292 - regression_loss: 0.5081 - classification_loss: 0.1210
 4348/10000 [============>.................] - ETA: 43:02 - loss: 0.6291 - regression_loss: 0.5081 - classification_loss: 0.1210
 4349/10000 [============>.................] - ETA: 43:02 - loss: 0.6291 - regression_loss: 0.5081 - classification_loss: 0.1210
 4350/10000 [============>.................] - ETA: 43:02 - loss: 0.6291 - regression_loss: 0.5081 - classification_loss: 0.1210
 4351/10000 [============>.................] - ETA: 43:01 - loss: 0.6291 - regression_loss: 0.5081 - classification_loss: 0.1210
 4352/10000 [============>.................] - ETA: 43:01 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1210
 4353/10000 [============>.................] - ETA: 43:00 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1210
 4354/10000 [============>.................] - ETA: 43:00 - loss: 0.6292 - regression_loss: 0.5081 - classification_loss: 0.1210
 4355/10000 [============>.................] - ETA: 42:59 - loss: 0.6291 - regression_loss: 0.5081 - classification_loss: 0.1210
 4356/10000 [============>.................] - ETA: 42:59 - loss: 0.6290 - regression_loss: 0.5080 - classification_loss: 0.1210
 4357/10000 [============>.................] - ETA: 42:58 - loss: 0.6290 - regression_loss: 0.5080 - classification_loss: 0.1210
 4358/10000 [============>.................] - ETA: 42:58 - loss: 0.6290 - regression_loss: 0.5080 - classification_loss: 0.1210
 4359/10000 [============>.................] - ETA: 42:57 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1210
 4360/10000 [============>.................] - ETA: 42:57 - loss: 0.6291 - regression_loss: 0.5081 - classification_loss: 0.1210
 4361/10000 [============>.................] - ETA: 42:56 - loss: 0.6290 - regression_loss: 0.5081 - classification_loss: 0.1210
 4362/10000 [============>.................] - ETA: 42:56 - loss: 0.6292 - regression_loss: 0.5082 - classification_loss: 0.1210
 4363/10000 [============>.................] - ETA: 42:56 - loss: 0.6294 - regression_loss: 0.5084 - classification_loss: 0.1210
 4364/10000 [============>.................] - ETA: 42:55 - loss: 0.6294 - regression_loss: 0.5084 - classification_loss: 0.1210
 4365/10000 [============>.................] - ETA: 42:55 - loss: 0.6295 - regression_loss: 0.5085 - classification_loss: 0.1210
 4366/10000 [============>.................] - ETA: 42:54 - loss: 0.6294 - regression_loss: 0.5084 - classification_loss: 0.1210
 4367/10000 [============>.................] - ETA: 42:54 - loss: 0.6294 - regression_loss: 0.5083 - classification_loss: 0.1210
 4368/10000 [============>.................] - ETA: 42:53 - loss: 0.6294 - regression_loss: 0.5084 - classification_loss: 0.1210
 4369/10000 [============>.................] - ETA: 42:53 - loss: 0.6294 - regression_loss: 0.5084 - classification_loss: 0.1210
 4370/10000 [============>.................] - ETA: 42:52 - loss: 0.6294 - regression_loss: 0.5083 - classification_loss: 0.1210
 4371/10000 [============>.................] - ETA: 42:52 - loss: 0.6293 - regression_loss: 0.5083 - classification_loss: 0.1210
 4372/10000 [============>.................] - ETA: 42:51 - loss: 0.6295 - regression_loss: 0.5085 - classification_loss: 0.1210
 4373/10000 [============>.................] - ETA: 42:51 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1210
 4374/10000 [============>.................] - ETA: 42:50 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1210
 4375/10000 [============>.................] - ETA: 42:50 - loss: 0.6295 - regression_loss: 0.5085 - classification_loss: 0.1210
 4376/10000 [============>.................] - ETA: 42:50 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1210
 4377/10000 [============>.................] - ETA: 42:49 - loss: 0.6296 - regression_loss: 0.5085 - classification_loss: 0.1210
 4378/10000 [============>.................] - ETA: 42:49 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4379/10000 [============>.................] - ETA: 42:48 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4380/10000 [============>.................] - ETA: 42:48 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4381/10000 [============>.................] - ETA: 42:47 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4382/10000 [============>.................] - ETA: 42:47 - loss: 0.6298 - regression_loss: 0.5088 - classification_loss: 0.1211
 4383/10000 [============>.................] - ETA: 42:46 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4384/10000 [============>.................] - ETA: 42:46 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4385/10000 [============>.................] - ETA: 42:46 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4386/10000 [============>.................] - ETA: 42:45 - loss: 0.6297 - regression_loss: 0.5087 - classification_loss: 0.1210
 4387/10000 [============>.................] - ETA: 42:45 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1210
 4388/10000 [============>.................] - ETA: 42:44 - loss: 0.6297 - regression_loss: 0.5087 - classification_loss: 0.1210
 4389/10000 [============>.................] - ETA: 42:44 - loss: 0.6299 - regression_loss: 0.5089 - classification_loss: 0.1211
 4390/10000 [============>.................] - ETA: 42:43 - loss: 0.6301 - regression_loss: 0.5090 - classification_loss: 0.1211
 4391/10000 [============>.................] - ETA: 42:43 - loss: 0.6301 - regression_loss: 0.5091 - classification_loss: 0.1211
 4392/10000 [============>.................] - ETA: 42:42 - loss: 0.6302 - regression_loss: 0.5090 - classification_loss: 0.1211
 4393/10000 [============>.................] - ETA: 42:42 - loss: 0.6301 - regression_loss: 0.5090 - classification_loss: 0.1211
 4394/10000 [============>.................] - ETA: 42:41 - loss: 0.6302 - regression_loss: 0.5091 - classification_loss: 0.1211
 4395/10000 [============>.................] - ETA: 42:41 - loss: 0.6302 - regression_loss: 0.5090 - classification_loss: 0.1211
 4396/10000 [============>.................] - ETA: 42:41 - loss: 0.6301 - regression_loss: 0.5090 - classification_loss: 0.1211
 4397/10000 [============>.................] - ETA: 42:40 - loss: 0.6300 - regression_loss: 0.5089 - classification_loss: 0.1211
 4398/10000 [============>.................] - ETA: 42:40 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4399/10000 [============>.................] - ETA: 42:39 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1210
 4400/10000 [============>.................] - ETA: 42:39 - loss: 0.6298 - regression_loss: 0.5088 - classification_loss: 0.1210
 4401/10000 [============>.................] - ETA: 42:38 - loss: 0.6299 - regression_loss: 0.5089 - classification_loss: 0.1210
 4402/10000 [============>.................] - ETA: 42:38 - loss: 0.6299 - regression_loss: 0.5089 - classification_loss: 0.1210
 4403/10000 [============>.................] - ETA: 42:37 - loss: 0.6300 - regression_loss: 0.5089 - classification_loss: 0.1211
 4404/10000 [============>.................] - ETA: 42:37 - loss: 0.6300 - regression_loss: 0.5089 - classification_loss: 0.1211
 4405/10000 [============>.................] - ETA: 42:37 - loss: 0.6300 - regression_loss: 0.5089 - classification_loss: 0.1211
 4406/10000 [============>.................] - ETA: 42:36 - loss: 0.6299 - regression_loss: 0.5089 - classification_loss: 0.1211
 4407/10000 [============>.................] - ETA: 42:36 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4408/10000 [============>.................] - ETA: 42:35 - loss: 0.6298 - regression_loss: 0.5088 - classification_loss: 0.1211
 4409/10000 [============>.................] - ETA: 42:35 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4410/10000 [============>.................] - ETA: 42:34 - loss: 0.6298 - regression_loss: 0.5088 - classification_loss: 0.1211
 4411/10000 [============>.................] - ETA: 42:34 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4412/10000 [============>.................] - ETA: 42:33 - loss: 0.6297 - regression_loss: 0.5087 - classification_loss: 0.1210
 4413/10000 [============>.................] - ETA: 42:33 - loss: 0.6297 - regression_loss: 0.5086 - classification_loss: 0.1210
 4414/10000 [============>.................] - ETA: 42:32 - loss: 0.6297 - regression_loss: 0.5086 - classification_loss: 0.1210
 4415/10000 [============>.................] - ETA: 42:32 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1210
 4416/10000 [============>.................] - ETA: 42:31 - loss: 0.6297 - regression_loss: 0.5086 - classification_loss: 0.1210
 4417/10000 [============>.................] - ETA: 42:31 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4418/10000 [============>.................] - ETA: 42:31 - loss: 0.6297 - regression_loss: 0.5087 - classification_loss: 0.1210
 4419/10000 [============>.................] - ETA: 42:30 - loss: 0.6297 - regression_loss: 0.5087 - classification_loss: 0.1210
 4420/10000 [============>.................] - ETA: 42:30 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1210
 4421/10000 [============>.................] - ETA: 42:29 - loss: 0.6298 - regression_loss: 0.5088 - classification_loss: 0.1210
 4422/10000 [============>.................] - ETA: 42:29 - loss: 0.6297 - regression_loss: 0.5087 - classification_loss: 0.1210
 4423/10000 [============>.................] - ETA: 42:28 - loss: 0.6296 - regression_loss: 0.5087 - classification_loss: 0.1210
 4424/10000 [============>.................] - ETA: 42:28 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1210
 4425/10000 [============>.................] - ETA: 42:27 - loss: 0.6295 - regression_loss: 0.5086 - classification_loss: 0.1210
 4426/10000 [============>.................] - ETA: 42:27 - loss: 0.6295 - regression_loss: 0.5085 - classification_loss: 0.1210
 4427/10000 [============>.................] - ETA: 42:27 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1210
 4428/10000 [============>.................] - ETA: 42:26 - loss: 0.6295 - regression_loss: 0.5085 - classification_loss: 0.1210
 4429/10000 [============>.................] - ETA: 42:26 - loss: 0.6295 - regression_loss: 0.5086 - classification_loss: 0.1210
 4430/10000 [============>.................] - ETA: 42:25 - loss: 0.6295 - regression_loss: 0.5086 - classification_loss: 0.1210
 4431/10000 [============>.................] - ETA: 42:25 - loss: 0.6295 - regression_loss: 0.5085 - classification_loss: 0.1210
 4432/10000 [============>.................] - ETA: 42:24 - loss: 0.6295 - regression_loss: 0.5085 - classification_loss: 0.1210
 4433/10000 [============>.................] - ETA: 42:24 - loss: 0.6295 - regression_loss: 0.5085 - classification_loss: 0.1210
 4434/10000 [============>.................] - ETA: 42:23 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1210
 4435/10000 [============>.................] - ETA: 42:23 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1210
 4436/10000 [============>.................] - ETA: 42:22 - loss: 0.6297 - regression_loss: 0.5087 - classification_loss: 0.1210
 4437/10000 [============>.................] - ETA: 42:22 - loss: 0.6297 - regression_loss: 0.5087 - classification_loss: 0.1210
 4438/10000 [============>.................] - ETA: 42:21 - loss: 0.6297 - regression_loss: 0.5087 - classification_loss: 0.1210
 4439/10000 [============>.................] - ETA: 42:21 - loss: 0.6296 - regression_loss: 0.5087 - classification_loss: 0.1210
 4440/10000 [============>.................] - ETA: 42:20 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1210
 4441/10000 [============>.................] - ETA: 42:20 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1210
 4442/10000 [============>.................] - ETA: 42:20 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1210
 4443/10000 [============>.................] - ETA: 42:19 - loss: 0.6295 - regression_loss: 0.5085 - classification_loss: 0.1210
 4444/10000 [============>.................] - ETA: 42:19 - loss: 0.6294 - regression_loss: 0.5084 - classification_loss: 0.1209
 4445/10000 [============>.................] - ETA: 42:18 - loss: 0.6294 - regression_loss: 0.5084 - classification_loss: 0.1209
 4446/10000 [============>.................] - ETA: 42:18 - loss: 0.6294 - regression_loss: 0.5084 - classification_loss: 0.1209
 4447/10000 [============>.................] - ETA: 42:17 - loss: 0.6293 - regression_loss: 0.5084 - classification_loss: 0.1209
 4448/10000 [============>.................] - ETA: 42:17 - loss: 0.6293 - regression_loss: 0.5084 - classification_loss: 0.1209
 4449/10000 [============>.................] - ETA: 42:16 - loss: 0.6295 - regression_loss: 0.5086 - classification_loss: 0.1209
 4450/10000 [============>.................] - ETA: 42:16 - loss: 0.6295 - regression_loss: 0.5086 - classification_loss: 0.1209
 4451/10000 [============>.................] - ETA: 42:15 - loss: 0.6295 - regression_loss: 0.5085 - classification_loss: 0.1210
 4452/10000 [============>.................] - ETA: 42:15 - loss: 0.6295 - regression_loss: 0.5086 - classification_loss: 0.1210
 4453/10000 [============>.................] - ETA: 42:15 - loss: 0.6297 - regression_loss: 0.5087 - classification_loss: 0.1209
 4454/10000 [============>.................] - ETA: 42:14 - loss: 0.6297 - regression_loss: 0.5087 - classification_loss: 0.1209
 4455/10000 [============>.................] - ETA: 42:14 - loss: 0.6298 - regression_loss: 0.5088 - classification_loss: 0.1210
 4456/10000 [============>.................] - ETA: 42:13 - loss: 0.6297 - regression_loss: 0.5087 - classification_loss: 0.1210
 4457/10000 [============>.................] - ETA: 42:13 - loss: 0.6297 - regression_loss: 0.5086 - classification_loss: 0.1210
 4458/10000 [============>.................] - ETA: 42:12 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1210
 4459/10000 [============>.................] - ETA: 42:12 - loss: 0.6297 - regression_loss: 0.5087 - classification_loss: 0.1210
 4460/10000 [============>.................] - ETA: 42:11 - loss: 0.6297 - regression_loss: 0.5087 - classification_loss: 0.1210
 4461/10000 [============>.................] - ETA: 42:11 - loss: 0.6296 - regression_loss: 0.5087 - classification_loss: 0.1210
 4462/10000 [============>.................] - ETA: 42:10 - loss: 0.6298 - regression_loss: 0.5088 - classification_loss: 0.1210
 4463/10000 [============>.................] - ETA: 42:10 - loss: 0.6298 - regression_loss: 0.5088 - classification_loss: 0.1210
 4464/10000 [============>.................] - ETA: 42:09 - loss: 0.6300 - regression_loss: 0.5089 - classification_loss: 0.1211
 4465/10000 [============>.................] - ETA: 42:09 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4466/10000 [============>.................] - ETA: 42:09 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1210
 4467/10000 [============>.................] - ETA: 42:08 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1210
 4468/10000 [============>.................] - ETA: 42:08 - loss: 0.6298 - regression_loss: 0.5088 - classification_loss: 0.1210
 4469/10000 [============>.................] - ETA: 42:07 - loss: 0.6297 - regression_loss: 0.5087 - classification_loss: 0.1210
 4470/10000 [============>.................] - ETA: 42:07 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1210
 4471/10000 [============>.................] - ETA: 42:06 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1210
 4472/10000 [============>.................] - ETA: 42:06 - loss: 0.6297 - regression_loss: 0.5087 - classification_loss: 0.1210
 4473/10000 [============>.................] - ETA: 42:05 - loss: 0.6298 - regression_loss: 0.5088 - classification_loss: 0.1211
 4474/10000 [============>.................] - ETA: 42:05 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4475/10000 [============>.................] - ETA: 42:04 - loss: 0.6300 - regression_loss: 0.5089 - classification_loss: 0.1211
 4476/10000 [============>.................] - ETA: 42:04 - loss: 0.6299 - regression_loss: 0.5089 - classification_loss: 0.1211
 4477/10000 [============>.................] - ETA: 42:04 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4478/10000 [============>.................] - ETA: 42:03 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4479/10000 [============>.................] - ETA: 42:03 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4480/10000 [============>.................] - ETA: 42:02 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4481/10000 [============>.................] - ETA: 42:02 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1210
 4482/10000 [============>.................] - ETA: 42:01 - loss: 0.6297 - regression_loss: 0.5087 - classification_loss: 0.1210
 4483/10000 [============>.................] - ETA: 42:01 - loss: 0.6297 - regression_loss: 0.5086 - classification_loss: 0.1211
 4484/10000 [============>.................] - ETA: 42:00 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1211
 4485/10000 [============>.................] - ETA: 42:00 - loss: 0.6296 - regression_loss: 0.5085 - classification_loss: 0.1210
 4486/10000 [============>.................] - ETA: 41:59 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1211
 4487/10000 [============>.................] - ETA: 41:59 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4488/10000 [============>.................] - ETA: 41:58 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4489/10000 [============>.................] - ETA: 41:58 - loss: 0.6297 - regression_loss: 0.5086 - classification_loss: 0.1211
 4490/10000 [============>.................] - ETA: 41:58 - loss: 0.6297 - regression_loss: 0.5086 - classification_loss: 0.1211
 4491/10000 [============>.................] - ETA: 41:57 - loss: 0.6297 - regression_loss: 0.5086 - classification_loss: 0.1210
 4492/10000 [============>.................] - ETA: 41:57 - loss: 0.6297 - regression_loss: 0.5086 - classification_loss: 0.1210
 4493/10000 [============>.................] - ETA: 41:56 - loss: 0.6297 - regression_loss: 0.5087 - classification_loss: 0.1210
 4494/10000 [============>.................] - ETA: 41:56 - loss: 0.6298 - regression_loss: 0.5088 - classification_loss: 0.1210
 4495/10000 [============>.................] - ETA: 41:55 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1210
 4496/10000 [============>.................] - ETA: 41:55 - loss: 0.6295 - regression_loss: 0.5086 - classification_loss: 0.1210
 4497/10000 [============>.................] - ETA: 41:54 - loss: 0.6295 - regression_loss: 0.5085 - classification_loss: 0.1210
 4498/10000 [============>.................] - ETA: 41:54 - loss: 0.6294 - regression_loss: 0.5084 - classification_loss: 0.1210
 4499/10000 [============>.................] - ETA: 41:53 - loss: 0.6294 - regression_loss: 0.5084 - classification_loss: 0.1210
 4500/10000 [============>.................] - ETA: 41:53 - loss: 0.6295 - regression_loss: 0.5085 - classification_loss: 0.1210
 4501/10000 [============>.................] - ETA: 41:53 - loss: 0.6294 - regression_loss: 0.5085 - classification_loss: 0.1210
 4502/10000 [============>.................] - ETA: 41:52 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1210
 4503/10000 [============>.................] - ETA: 41:52 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1210
 4504/10000 [============>.................] - ETA: 41:51 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1210
 4505/10000 [============>.................] - ETA: 41:51 - loss: 0.6296 - regression_loss: 0.5085 - classification_loss: 0.1210
 4506/10000 [============>.................] - ETA: 41:50 - loss: 0.6295 - regression_loss: 0.5084 - classification_loss: 0.1210
 4507/10000 [============>.................] - ETA: 41:50 - loss: 0.6294 - regression_loss: 0.5084 - classification_loss: 0.1210
 4508/10000 [============>.................] - ETA: 41:49 - loss: 0.6293 - regression_loss: 0.5083 - classification_loss: 0.1210
 4509/10000 [============>.................] - ETA: 41:49 - loss: 0.6295 - regression_loss: 0.5085 - classification_loss: 0.1210
 4510/10000 [============>.................] - ETA: 41:49 - loss: 0.6295 - regression_loss: 0.5084 - classification_loss: 0.1210
 4511/10000 [============>.................] - ETA: 41:48 - loss: 0.6294 - regression_loss: 0.5084 - classification_loss: 0.1210
 4512/10000 [============>.................] - ETA: 41:48 - loss: 0.6295 - regression_loss: 0.5085 - classification_loss: 0.1210
 4513/10000 [============>.................] - ETA: 41:47 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1210
 4514/10000 [============>.................] - ETA: 41:47 - loss: 0.6296 - regression_loss: 0.5086 - classification_loss: 0.1210
 4515/10000 [============>.................] - ETA: 41:46 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4516/10000 [============>.................] - ETA: 41:46 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4517/10000 [============>.................] - ETA: 41:45 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4518/10000 [============>.................] - ETA: 41:45 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4519/10000 [============>.................] - ETA: 41:44 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4520/10000 [============>.................] - ETA: 41:44 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4521/10000 [============>.................] - ETA: 41:43 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4522/10000 [============>.................] - ETA: 41:43 - loss: 0.6297 - regression_loss: 0.5087 - classification_loss: 0.1210
 4523/10000 [============>.................] - ETA: 41:43 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4524/10000 [============>.................] - ETA: 41:42 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4525/10000 [============>.................] - ETA: 41:42 - loss: 0.6299 - regression_loss: 0.5087 - classification_loss: 0.1211
 4526/10000 [============>.................] - ETA: 41:41 - loss: 0.6299 - regression_loss: 0.5087 - classification_loss: 0.1212
 4527/10000 [============>.................] - ETA: 41:41 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1212
 4528/10000 [============>.................] - ETA: 41:40 - loss: 0.6298 - regression_loss: 0.5087 - classification_loss: 0.1211
 4529/10000 [============>.................] - ETA: 41:40 - loss: 0.6299 - regression_loss: 0.5088 - classification_loss: 0.1211
 4530/10000 [============>.................] - ETA: 41:39 - loss: 0.6301 - regression_loss: 0.5090 - classification_loss: 0.1212
 4531/10000 [============>.................] - ETA: 41:39 - loss: 0.6302 - regression_loss: 0.5091 - classification_loss: 0.1212
 4532/10000 [============>.................] - ETA: 41:38 - loss: 0.6302 - regression_loss: 0.5091 - classification_loss: 0.1212
 4533/10000 [============>.................] - ETA: 41:38 - loss: 0.6302 - regression_loss: 0.5090 - classification_loss: 0.1211
 4534/10000 [============>.................] - ETA: 41:37 - loss: 0.6302 - regression_loss: 0.5090 - classification_loss: 0.1212
 4535/10000 [============>.................] - ETA: 41:37 - loss: 0.6302 - regression_loss: 0.5090 - classification_loss: 0.1212
 4536/10000 [============>.................] - ETA: 41:36 - loss: 0.6302 - regression_loss: 0.5090 - classification_loss: 0.1212
 4537/10000 [============>.................] - ETA: 41:36 - loss: 0.6301 - regression_loss: 0.5089 - classification_loss: 0.1212
 4538/10000 [============>.................] - ETA: 41:35 - loss: 0.6304 - regression_loss: 0.5091 - classification_loss: 0.1212
 4539/10000 [============>.................] - ETA: 41:35 - loss: 0.6305 - regression_loss: 0.5092 - classification_loss: 0.1213
 4540/10000 [============>.................] - ETA: 41:35 - loss: 0.6304 - regression_loss: 0.5092 - classification_loss: 0.1213
 4541/10000 [============>.................] - ETA: 41:34 - loss: 0.6304 - regression_loss: 0.5092 - classification_loss: 0.1213
 4542/10000 [============>.................] - ETA: 41:34 - loss: 0.6304 - regression_loss: 0.5091 - classification_loss: 0.1212
 4543/10000 [============>.................] - ETA: 41:33 - loss: 0.6303 - regression_loss: 0.5091 - classification_loss: 0.1212
 4544/10000 [============>.................] - ETA: 41:33 - loss: 0.6303 - regression_loss: 0.5090 - classification_loss: 0.1212
 4545/10000 [============>.................] - ETA: 41:32 - loss: 0.6302 - regression_loss: 0.5090 - classification_loss: 0.1212
 4546/10000 [============>.................] - ETA: 41:32 - loss: 0.6301 - regression_loss: 0.5089 - classification_loss: 0.1212
 4547/10000 [============>.................] - ETA: 41:31 - loss: 0.6301 - regression_loss: 0.5089 - classification_loss: 0.1212
 4548/10000 [============>.................] - ETA: 41:31 - loss: 0.6303 - regression_loss: 0.5090 - classification_loss: 0.1213
 4549/10000 [============>.................] - ETA: 41:30 - loss: 0.6303 - regression_loss: 0.5090 - classification_loss: 0.1213
 4550/10000 [============>.................] - ETA: 41:30 - loss: 0.6306 - regression_loss: 0.5093 - classification_loss: 0.1213
 4551/10000 [============>.................] - ETA: 41:30 - loss: 0.6306 - regression_loss: 0.5093 - classification_loss: 0.1213
 4552/10000 [============>.................] - ETA: 41:29 - loss: 0.6308 - regression_loss: 0.5095 - classification_loss: 0.1213
 4553/10000 [============>.................] - ETA: 41:29 - loss: 0.6309 - regression_loss: 0.5095 - classification_loss: 0.1213
 4554/10000 [============>.................] - ETA: 41:28 - loss: 0.6308 - regression_loss: 0.5095 - classification_loss: 0.1213
 4555/10000 [============>.................] - ETA: 41:28 - loss: 0.6308 - regression_loss: 0.5095 - classification_loss: 0.1213
 4556/10000 [============>.................] - ETA: 41:27 - loss: 0.6307 - regression_loss: 0.5094 - classification_loss: 0.1213
 4557/10000 [============>.................] - ETA: 41:27 - loss: 0.6309 - regression_loss: 0.5096 - classification_loss: 0.1213
 4558/10000 [============>.................] - ETA: 41:26 - loss: 0.6308 - regression_loss: 0.5095 - classification_loss: 0.1213
 4559/10000 [============>.................] - ETA: 41:26 - loss: 0.6308 - regression_loss: 0.5095 - classification_loss: 0.1213
 4560/10000 [============>.................] - ETA: 41:25 - loss: 0.6308 - regression_loss: 0.5095 - classification_loss: 0.1213
 4561/10000 [============>.................] - ETA: 41:25 - loss: 0.6307 - regression_loss: 0.5094 - classification_loss: 0.1213
 4562/10000 [============>.................] - ETA: 41:25 - loss: 0.6309 - regression_loss: 0.5095 - classification_loss: 0.1213
 4563/10000 [============>.................] - ETA: 41:24 - loss: 0.6310 - regression_loss: 0.5097 - classification_loss: 0.1213
 4564/10000 [============>.................] - ETA: 41:24 - loss: 0.6309 - regression_loss: 0.5096 - classification_loss: 0.1213
 4565/10000 [============>.................] - ETA: 41:23 - loss: 0.6309 - regression_loss: 0.5096 - classification_loss: 0.1213
 4566/10000 [============>.................] - ETA: 41:23 - loss: 0.6310 - regression_loss: 0.5097 - classification_loss: 0.1214
 4567/10000 [============>.................] - ETA: 41:22 - loss: 0.6310 - regression_loss: 0.5096 - classification_loss: 0.1214
 4568/10000 [============>.................] - ETA: 41:22 - loss: 0.6309 - regression_loss: 0.5096 - classification_loss: 0.1214
 4569/10000 [============>.................] - ETA: 41:21 - loss: 0.6309 - regression_loss: 0.5095 - classification_loss: 0.1214
 4570/10000 [============>.................] - ETA: 41:21 - loss: 0.6308 - regression_loss: 0.5094 - classification_loss: 0.1214
 4571/10000 [============>.................] - ETA: 41:20 - loss: 0.6307 - regression_loss: 0.5094 - classification_loss: 0.1213
 4572/10000 [============>.................] - ETA: 41:20 - loss: 0.6307 - regression_loss: 0.5093 - classification_loss: 0.1213
 4573/10000 [============>.................] - ETA: 41:19 - loss: 0.6307 - regression_loss: 0.5094 - classification_loss: 0.1214
 4574/10000 [============>.................] - ETA: 41:19 - loss: 0.6308 - regression_loss: 0.5094 - classification_loss: 0.1214
 4575/10000 [============>.................] - ETA: 41:19 - loss: 0.6308 - regression_loss: 0.5095 - classification_loss: 0.1214
 4576/10000 [============>.................] - ETA: 41:18 - loss: 0.6308 - regression_loss: 0.5095 - classification_loss: 0.1214
 4577/10000 [============>.................] - ETA: 41:18 - loss: 0.6309 - regression_loss: 0.5095 - classification_loss: 0.1214
 4578/10000 [============>.................] - ETA: 41:17 - loss: 0.6309 - regression_loss: 0.5095 - classification_loss: 0.1214
 4579/10000 [============>.................] - ETA: 41:17 - loss: 0.6309 - regression_loss: 0.5095 - classification_loss: 0.1214
 4580/10000 [============>.................] - ETA: 41:16 - loss: 0.6308 - regression_loss: 0.5094 - classification_loss: 0.1213
 4581/10000 [============>.................] - ETA: 41:16 - loss: 0.6309 - regression_loss: 0.5095 - classification_loss: 0.1213
 4582/10000 [============>.................] - ETA: 41:15 - loss: 0.6309 - regression_loss: 0.5096 - classification_loss: 0.1213
 4583/10000 [============>.................] - ETA: 41:15 - loss: 0.6309 - regression_loss: 0.5096 - classification_loss: 0.1213
 4584/10000 [============>.................] - ETA: 41:14 - loss: 0.6308 - regression_loss: 0.5095 - classification_loss: 0.1213
 4585/10000 [============>.................] - ETA: 41:14 - loss: 0.6309 - regression_loss: 0.5096 - classification_loss: 0.1213
 4586/10000 [============>.................] - ETA: 41:14 - loss: 0.6310 - regression_loss: 0.5097 - classification_loss: 0.1213
 4587/10000 [============>.................] - ETA: 41:13 - loss: 0.6310 - regression_loss: 0.5097 - classification_loss: 0.1213
 4588/10000 [============>.................] - ETA: 41:13 - loss: 0.6310 - regression_loss: 0.5097 - classification_loss: 0.1213
 4589/10000 [============>.................] - ETA: 41:12 - loss: 0.6309 - regression_loss: 0.5096 - classification_loss: 0.1213
 4590/10000 [============>.................] - ETA: 41:12 - loss: 0.6310 - regression_loss: 0.5097 - classification_loss: 0.1213
 4591/10000 [============>.................] - ETA: 41:11 - loss: 0.6309 - regression_loss: 0.5096 - classification_loss: 0.1213
 4592/10000 [============>.................] - ETA: 41:11 - loss: 0.6308 - regression_loss: 0.5095 - classification_loss: 0.1213
 4593/10000 [============>.................] - ETA: 41:11 - loss: 0.6308 - regression_loss: 0.5095 - classification_loss: 0.1213
 4594/10000 [============>.................] - ETA: 41:10 - loss: 0.6307 - regression_loss: 0.5095 - classification_loss: 0.1213
 4595/10000 [============>.................] - ETA: 41:10 - loss: 0.6308 - regression_loss: 0.5095 - classification_loss: 0.1213
 4596/10000 [============>.................] - ETA: 41:09 - loss: 0.6310 - regression_loss: 0.5097 - classification_loss: 0.1213
 4597/10000 [============>.................] - ETA: 41:09 - loss: 0.6309 - regression_loss: 0.5096 - classification_loss: 0.1213
 4598/10000 [============>.................] - ETA: 41:08 - loss: 0.6309 - regression_loss: 0.5096 - classification_loss: 0.1213
 4599/10000 [============>.................] - ETA: 41:08 - loss: 0.6309 - regression_loss: 0.5096 - classification_loss: 0.1213
 4600/10000 [============>.................] - ETA: 41:07 - loss: 0.6308 - regression_loss: 0.5095 - classification_loss: 0.1213
 4601/10000 [============>.................] - ETA: 41:07 - loss: 0.6307 - regression_loss: 0.5094 - classification_loss: 0.1213
 4602/10000 [============>.................] - ETA: 41:07 - loss: 0.6307 - regression_loss: 0.5094 - classification_loss: 0.1213
 4603/10000 [============>.................] - ETA: 41:06 - loss: 0.6309 - regression_loss: 0.5096 - classification_loss: 0.1213
 4604/10000 [============>.................] - ETA: 41:06 - loss: 0.6309 - regression_loss: 0.5096 - classification_loss: 0.1213
 4605/10000 [============>.................] - ETA: 41:05 - loss: 0.6309 - regression_loss: 0.5096 - classification_loss: 0.1213
 4606/10000 [============>.................] - ETA: 41:05 - loss: 0.6310 - regression_loss: 0.5097 - classification_loss: 0.1213
 4607/10000 [============>.................] - ETA: 41:04 - loss: 0.6311 - regression_loss: 0.5097 - classification_loss: 0.1213
 4608/10000 [============>.................] - ETA: 41:04 - loss: 0.6310 - regression_loss: 0.5097 - classification_loss: 0.1213
 4609/10000 [============>.................] - ETA: 41:03 - loss: 0.6312 - regression_loss: 0.5099 - classification_loss: 0.1213
 4610/10000 [============>.................] - ETA: 41:03 - loss: 0.6313 - regression_loss: 0.5099 - classification_loss: 0.1214
 4611/10000 [============>.................] - ETA: 41:02 - loss: 0.6312 - regression_loss: 0.5098 - classification_loss: 0.1213
 4612/10000 [============>.................] - ETA: 41:02 - loss: 0.6311 - regression_loss: 0.5097 - classification_loss: 0.1213
 4613/10000 [============>.................] - ETA: 41:01 - loss: 0.6311 - regression_loss: 0.5098 - classification_loss: 0.1213
 4614/10000 [============>.................] - ETA: 41:01 - loss: 0.6313 - regression_loss: 0.5099 - classification_loss: 0.1213
 4615/10000 [============>.................] - ETA: 41:01 - loss: 0.6314 - regression_loss: 0.5100 - classification_loss: 0.1214
 4616/10000 [============>.................] - ETA: 41:00 - loss: 0.6314 - regression_loss: 0.5101 - classification_loss: 0.1213
 4617/10000 [============>.................] - ETA: 41:00 - loss: 0.6314 - regression_loss: 0.5100 - classification_loss: 0.1213
 4618/10000 [============>.................] - ETA: 40:59 - loss: 0.6315 - regression_loss: 0.5102 - classification_loss: 0.1213
 4619/10000 [============>.................] - ETA: 40:59 - loss: 0.6315 - regression_loss: 0.5101 - classification_loss: 0.1213
 4620/10000 [============>.................] - ETA: 40:58 - loss: 0.6314 - regression_loss: 0.5101 - classification_loss: 0.1213
 4621/10000 [============>.................] - ETA: 40:58 - loss: 0.6314 - regression_loss: 0.5101 - classification_loss: 0.1213
 4622/10000 [============>.................] - ETA: 40:57 - loss: 0.6314 - regression_loss: 0.5101 - classification_loss: 0.1213
 4623/10000 [============>.................] - ETA: 40:57 - loss: 0.6316 - regression_loss: 0.5103 - classification_loss: 0.1214
 4624/10000 [============>.................] - ETA: 40:56 - loss: 0.6316 - regression_loss: 0.5102 - classification_loss: 0.1213
 4625/10000 [============>.................] - ETA: 40:56 - loss: 0.6315 - regression_loss: 0.5102 - classification_loss: 0.1213
 4626/10000 [============>.................] - ETA: 40:56 - loss: 0.6315 - regression_loss: 0.5101 - classification_loss: 0.1213
 4627/10000 [============>.................] - ETA: 40:55 - loss: 0.6314 - regression_loss: 0.5101 - classification_loss: 0.1213
 4628/10000 [============>.................] - ETA: 40:55 - loss: 0.6315 - regression_loss: 0.5101 - classification_loss: 0.1213
 4629/10000 [============>.................] - ETA: 40:54 - loss: 0.6316 - regression_loss: 0.5102 - classification_loss: 0.1214
 4630/10000 [============>.................] - ETA: 40:54 - loss: 0.6316 - regression_loss: 0.5103 - classification_loss: 0.1213
 4631/10000 [============>.................] - ETA: 40:53 - loss: 0.6317 - regression_loss: 0.5104 - classification_loss: 0.1214
 4632/10000 [============>.................] - ETA: 40:53 - loss: 0.6317 - regression_loss: 0.5103 - classification_loss: 0.1213
 4633/10000 [============>.................] - ETA: 40:52 - loss: 0.6316 - regression_loss: 0.5103 - classification_loss: 0.1213
 4634/10000 [============>.................] - ETA: 40:52 - loss: 0.6316 - regression_loss: 0.5103 - classification_loss: 0.1213
 4635/10000 [============>.................] - ETA: 40:51 - loss: 0.6316 - regression_loss: 0.5103 - classification_loss: 0.1213
 4636/10000 [============>.................] - ETA: 40:51 - loss: 0.6317 - regression_loss: 0.5103 - classification_loss: 0.1213
 4637/10000 [============>.................] - ETA: 40:50 - loss: 0.6318 - regression_loss: 0.5104 - classification_loss: 0.1213
 4638/10000 [============>.................] - ETA: 40:50 - loss: 0.6317 - regression_loss: 0.5104 - classification_loss: 0.1213
 4639/10000 [============>.................] - ETA: 40:50 - loss: 0.6318 - regression_loss: 0.5104 - classification_loss: 0.1213
 4640/10000 [============>.................] - ETA: 40:49 - loss: 0.6317 - regression_loss: 0.5104 - classification_loss: 0.1213
 4641/10000 [============>.................] - ETA: 40:49 - loss: 0.6316 - regression_loss: 0.5103 - classification_loss: 0.1213
 4642/10000 [============>.................] - ETA: 40:48 - loss: 0.6316 - regression_loss: 0.5103 - classification_loss: 0.1213
 4643/10000 [============>.................] - ETA: 40:48 - loss: 0.6318 - regression_loss: 0.5104 - classification_loss: 0.1213
 4644/10000 [============>.................] - ETA: 40:47 - loss: 0.6318 - regression_loss: 0.5104 - classification_loss: 0.1213
 4645/10000 [============>.................] - ETA: 40:47 - loss: 0.6317 - regression_loss: 0.5104 - classification_loss: 0.1213
 4646/10000 [============>.................] - ETA: 40:46 - loss: 0.6317 - regression_loss: 0.5103 - classification_loss: 0.1213
 4647/10000 [============>.................] - ETA: 40:46 - loss: 0.6316 - regression_loss: 0.5103 - classification_loss: 0.1213
 4648/10000 [============>.................] - ETA: 40:45 - loss: 0.6315 - regression_loss: 0.5102 - classification_loss: 0.1213
 4649/10000 [============>.................] - ETA: 40:45 - loss: 0.6314 - regression_loss: 0.5101 - classification_loss: 0.1213
 4650/10000 [============>.................] - ETA: 40:44 - loss: 0.6315 - regression_loss: 0.5102 - classification_loss: 0.1213
 4651/10000 [============>.................] - ETA: 40:44 - loss: 0.6314 - regression_loss: 0.5101 - classification_loss: 0.1213
 4652/10000 [============>.................] - ETA: 40:44 - loss: 0.6315 - regression_loss: 0.5102 - classification_loss: 0.1213
 4653/10000 [============>.................] - ETA: 40:43 - loss: 0.6314 - regression_loss: 0.5101 - classification_loss: 0.1213
 4654/10000 [============>.................] - ETA: 40:43 - loss: 0.6314 - regression_loss: 0.5101 - classification_loss: 0.1213
 4655/10000 [============>.................] - ETA: 40:42 - loss: 0.6314 - regression_loss: 0.5101 - classification_loss: 0.1213
 4656/10000 [============>.................] - ETA: 40:42 - loss: 0.6313 - regression_loss: 0.5100 - classification_loss: 0.1213
 4657/10000 [============>.................] - ETA: 40:41 - loss: 0.6316 - regression_loss: 0.5102 - classification_loss: 0.1214
 4658/10000 [============>.................] - ETA: 40:41 - loss: 0.6317 - regression_loss: 0.5103 - classification_loss: 0.1214
 4659/10000 [============>.................] - ETA: 40:40 - loss: 0.6317 - regression_loss: 0.5103 - classification_loss: 0.1214
 4660/10000 [============>.................] - ETA: 40:40 - loss: 0.6317 - regression_loss: 0.5103 - classification_loss: 0.1214
 4661/10000 [============>.................] - ETA: 40:39 - loss: 0.6318 - regression_loss: 0.5104 - classification_loss: 0.1214
 4662/10000 [============>.................] - ETA: 40:39 - loss: 0.6317 - regression_loss: 0.5103 - classification_loss: 0.1214
 4663/10000 [============>.................] - ETA: 40:38 - loss: 0.6317 - regression_loss: 0.5103 - classification_loss: 0.1214
 4664/10000 [============>.................] - ETA: 40:38 - loss: 0.6318 - regression_loss: 0.5104 - classification_loss: 0.1214
 4665/10000 [============>.................] - ETA: 40:37 - loss: 0.6317 - regression_loss: 0.5103 - classification_loss: 0.1214
 4666/10000 [============>.................] - ETA: 40:37 - loss: 0.6318 - regression_loss: 0.5103 - classification_loss: 0.1214
 4667/10000 [=============>................] - ETA: 40:36 - loss: 0.6319 - regression_loss: 0.5104 - classification_loss: 0.1215
 4668/10000 [=============>................] - ETA: 40:36 - loss: 0.6319 - regression_loss: 0.5105 - classification_loss: 0.1215
 4669/10000 [=============>................] - ETA: 40:36 - loss: 0.6319 - regression_loss: 0.5104 - classification_loss: 0.1215
 4670/10000 [=============>................] - ETA: 40:35 - loss: 0.6319 - regression_loss: 0.5104 - classification_loss: 0.1215
 4671/10000 [=============>................] - ETA: 40:35 - loss: 0.6319 - regression_loss: 0.5105 - classification_loss: 0.1215
 4672/10000 [=============>................] - ETA: 40:34 - loss: 0.6320 - regression_loss: 0.5105 - classification_loss: 0.1215
 4673/10000 [=============>................] - ETA: 40:34 - loss: 0.6319 - regression_loss: 0.5105 - classification_loss: 0.1215
 4674/10000 [=============>................] - ETA: 40:33 - loss: 0.6321 - regression_loss: 0.5106 - classification_loss: 0.1215
 4675/10000 [=============>................] - ETA: 40:33 - loss: 0.6320 - regression_loss: 0.5105 - classification_loss: 0.1215
 4676/10000 [=============>................] - ETA: 40:32 - loss: 0.6319 - regression_loss: 0.5104 - classification_loss: 0.1215
 4677/10000 [=============>................] - ETA: 40:32 - loss: 0.6319 - regression_loss: 0.5104 - classification_loss: 0.1215
 4678/10000 [=============>................] - ETA: 40:32 - loss: 0.6320 - regression_loss: 0.5105 - classification_loss: 0.1215
 4679/10000 [=============>................] - ETA: 40:31 - loss: 0.6321 - regression_loss: 0.5106 - classification_loss: 0.1215
 4680/10000 [=============>................] - ETA: 40:31 - loss: 0.6322 - regression_loss: 0.5107 - classification_loss: 0.1215
 4681/10000 [=============>................] - ETA: 40:30 - loss: 0.6321 - regression_loss: 0.5106 - classification_loss: 0.1215
 4682/10000 [=============>................] - ETA: 40:30 - loss: 0.6321 - regression_loss: 0.5106 - classification_loss: 0.1215
 4683/10000 [=============>................] - ETA: 40:29 - loss: 0.6322 - regression_loss: 0.5106 - classification_loss: 0.1215
 4684/10000 [=============>................] - ETA: 40:29 - loss: 0.6322 - regression_loss: 0.5107 - classification_loss: 0.1215
 4685/10000 [=============>................] - ETA: 40:28 - loss: 0.6323 - regression_loss: 0.5108 - classification_loss: 0.1216
 4686/10000 [=============>................] - ETA: 40:28 - loss: 0.6324 - regression_loss: 0.5108 - classification_loss: 0.1216
 4687/10000 [=============>................] - ETA: 40:27 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1216
 4688/10000 [=============>................] - ETA: 40:27 - loss: 0.6323 - regression_loss: 0.5108 - classification_loss: 0.1216
 4689/10000 [=============>................] - ETA: 40:26 - loss: 0.6322 - regression_loss: 0.5107 - classification_loss: 0.1216
 4690/10000 [=============>................] - ETA: 40:26 - loss: 0.6322 - regression_loss: 0.5106 - classification_loss: 0.1215
 4691/10000 [=============>................] - ETA: 40:26 - loss: 0.6322 - regression_loss: 0.5106 - classification_loss: 0.1216
 4692/10000 [=============>................] - ETA: 40:25 - loss: 0.6321 - regression_loss: 0.5105 - classification_loss: 0.1215
 4693/10000 [=============>................] - ETA: 40:25 - loss: 0.6321 - regression_loss: 0.5106 - classification_loss: 0.1215
 4694/10000 [=============>................] - ETA: 40:24 - loss: 0.6320 - regression_loss: 0.5105 - classification_loss: 0.1215
 4695/10000 [=============>................] - ETA: 40:24 - loss: 0.6321 - regression_loss: 0.5106 - classification_loss: 0.1215
 4696/10000 [=============>................] - ETA: 40:23 - loss: 0.6321 - regression_loss: 0.5106 - classification_loss: 0.1215
 4697/10000 [=============>................] - ETA: 40:23 - loss: 0.6322 - regression_loss: 0.5107 - classification_loss: 0.1215
 4698/10000 [=============>................] - ETA: 40:22 - loss: 0.6322 - regression_loss: 0.5106 - classification_loss: 0.1215
 4699/10000 [=============>................] - ETA: 40:22 - loss: 0.6321 - regression_loss: 0.5106 - classification_loss: 0.1215
 4700/10000 [=============>................] - ETA: 40:22 - loss: 0.6320 - regression_loss: 0.5106 - classification_loss: 0.1215
 4701/10000 [=============>................] - ETA: 40:21 - loss: 0.6320 - regression_loss: 0.5105 - classification_loss: 0.1215
 4702/10000 [=============>................] - ETA: 40:21 - loss: 0.6320 - regression_loss: 0.5105 - classification_loss: 0.1215
 4703/10000 [=============>................] - ETA: 40:20 - loss: 0.6321 - regression_loss: 0.5106 - classification_loss: 0.1215
 4704/10000 [=============>................] - ETA: 40:20 - loss: 0.6323 - regression_loss: 0.5108 - classification_loss: 0.1216
 4705/10000 [=============>................] - ETA: 40:19 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1215
 4706/10000 [=============>................] - ETA: 40:19 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1216
 4707/10000 [=============>................] - ETA: 40:18 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1216
 4708/10000 [=============>................] - ETA: 40:18 - loss: 0.6325 - regression_loss: 0.5109 - classification_loss: 0.1216
 4709/10000 [=============>................] - ETA: 40:17 - loss: 0.6324 - regression_loss: 0.5108 - classification_loss: 0.1216
 4710/10000 [=============>................] - ETA: 40:17 - loss: 0.6324 - regression_loss: 0.5108 - classification_loss: 0.1216
 4711/10000 [=============>................] - ETA: 40:16 - loss: 0.6324 - regression_loss: 0.5108 - classification_loss: 0.1216
 4712/10000 [=============>................] - ETA: 40:16 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1216
 4713/10000 [=============>................] - ETA: 40:16 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1216
 4714/10000 [=============>................] - ETA: 40:15 - loss: 0.6322 - regression_loss: 0.5106 - classification_loss: 0.1216
 4715/10000 [=============>................] - ETA: 40:15 - loss: 0.6321 - regression_loss: 0.5106 - classification_loss: 0.1216
 4716/10000 [=============>................] - ETA: 40:14 - loss: 0.6320 - regression_loss: 0.5105 - classification_loss: 0.1216
 4717/10000 [=============>................] - ETA: 40:14 - loss: 0.6319 - regression_loss: 0.5104 - classification_loss: 0.1215
 4718/10000 [=============>................] - ETA: 40:13 - loss: 0.6319 - regression_loss: 0.5104 - classification_loss: 0.1215
 4719/10000 [=============>................] - ETA: 40:13 - loss: 0.6320 - regression_loss: 0.5104 - classification_loss: 0.1215
 4720/10000 [=============>................] - ETA: 40:12 - loss: 0.6321 - regression_loss: 0.5105 - classification_loss: 0.1216
 4721/10000 [=============>................] - ETA: 40:12 - loss: 0.6320 - regression_loss: 0.5104 - classification_loss: 0.1216
 4722/10000 [=============>................] - ETA: 40:11 - loss: 0.6319 - regression_loss: 0.5104 - classification_loss: 0.1216
 4723/10000 [=============>................] - ETA: 40:11 - loss: 0.6319 - regression_loss: 0.5103 - classification_loss: 0.1215
 4724/10000 [=============>................] - ETA: 40:10 - loss: 0.6318 - regression_loss: 0.5102 - classification_loss: 0.1215
 4725/10000 [=============>................] - ETA: 40:10 - loss: 0.6317 - regression_loss: 0.5102 - classification_loss: 0.1215
 4726/10000 [=============>................] - ETA: 40:10 - loss: 0.6317 - regression_loss: 0.5102 - classification_loss: 0.1215
 4727/10000 [=============>................] - ETA: 40:09 - loss: 0.6318 - regression_loss: 0.5103 - classification_loss: 0.1215
 4728/10000 [=============>................] - ETA: 40:09 - loss: 0.6318 - regression_loss: 0.5103 - classification_loss: 0.1215
 4729/10000 [=============>................] - ETA: 40:08 - loss: 0.6319 - regression_loss: 0.5104 - classification_loss: 0.1215
 4730/10000 [=============>................] - ETA: 40:08 - loss: 0.6319 - regression_loss: 0.5103 - classification_loss: 0.1216
 4731/10000 [=============>................] - ETA: 40:07 - loss: 0.6318 - regression_loss: 0.5102 - classification_loss: 0.1216
 4732/10000 [=============>................] - ETA: 40:07 - loss: 0.6318 - regression_loss: 0.5102 - classification_loss: 0.1216
 4733/10000 [=============>................] - ETA: 40:06 - loss: 0.6317 - regression_loss: 0.5101 - classification_loss: 0.1215
 4734/10000 [=============>................] - ETA: 40:06 - loss: 0.6316 - regression_loss: 0.5101 - classification_loss: 0.1215
 4735/10000 [=============>................] - ETA: 40:06 - loss: 0.6318 - regression_loss: 0.5102 - classification_loss: 0.1215
 4736/10000 [=============>................] - ETA: 40:05 - loss: 0.6318 - regression_loss: 0.5103 - classification_loss: 0.1215
 4737/10000 [=============>................] - ETA: 40:05 - loss: 0.6318 - regression_loss: 0.5103 - classification_loss: 0.1215
 4738/10000 [=============>................] - ETA: 40:04 - loss: 0.6318 - regression_loss: 0.5103 - classification_loss: 0.1215
 4739/10000 [=============>................] - ETA: 40:04 - loss: 0.6317 - regression_loss: 0.5102 - classification_loss: 0.1215
 4740/10000 [=============>................] - ETA: 40:03 - loss: 0.6318 - regression_loss: 0.5102 - classification_loss: 0.1216
 4741/10000 [=============>................] - ETA: 40:03 - loss: 0.6317 - regression_loss: 0.5102 - classification_loss: 0.1216
 4742/10000 [=============>................] - ETA: 40:02 - loss: 0.6317 - regression_loss: 0.5102 - classification_loss: 0.1216
 4743/10000 [=============>................] - ETA: 40:02 - loss: 0.6317 - regression_loss: 0.5102 - classification_loss: 0.1215
 4744/10000 [=============>................] - ETA: 40:01 - loss: 0.6316 - regression_loss: 0.5101 - classification_loss: 0.1216
 4745/10000 [=============>................] - ETA: 40:01 - loss: 0.6315 - regression_loss: 0.5100 - classification_loss: 0.1215
 4746/10000 [=============>................] - ETA: 40:00 - loss: 0.6315 - regression_loss: 0.5099 - classification_loss: 0.1215
 4747/10000 [=============>................] - ETA: 40:00 - loss: 0.6316 - regression_loss: 0.5100 - classification_loss: 0.1216
 4748/10000 [=============>................] - ETA: 39:59 - loss: 0.6318 - regression_loss: 0.5102 - classification_loss: 0.1216
 4749/10000 [=============>................] - ETA: 39:59 - loss: 0.6319 - regression_loss: 0.5103 - classification_loss: 0.1216
 4750/10000 [=============>................] - ETA: 39:58 - loss: 0.6319 - regression_loss: 0.5104 - classification_loss: 0.1216
 4751/10000 [=============>................] - ETA: 39:58 - loss: 0.6320 - regression_loss: 0.5104 - classification_loss: 0.1216
 4752/10000 [=============>................] - ETA: 39:58 - loss: 0.6320 - regression_loss: 0.5104 - classification_loss: 0.1216
 4753/10000 [=============>................] - ETA: 39:57 - loss: 0.6321 - regression_loss: 0.5105 - classification_loss: 0.1216
 4754/10000 [=============>................] - ETA: 39:57 - loss: 0.6321 - regression_loss: 0.5105 - classification_loss: 0.1216
 4755/10000 [=============>................] - ETA: 39:56 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1217
 4756/10000 [=============>................] - ETA: 39:56 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1216
 4757/10000 [=============>................] - ETA: 39:55 - loss: 0.6323 - regression_loss: 0.5106 - classification_loss: 0.1216
 4758/10000 [=============>................] - ETA: 39:55 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1216
 4759/10000 [=============>................] - ETA: 39:54 - loss: 0.6324 - regression_loss: 0.5108 - classification_loss: 0.1216
 4760/10000 [=============>................] - ETA: 39:54 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1216
 4761/10000 [=============>................] - ETA: 39:53 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1216
 4762/10000 [=============>................] - ETA: 39:53 - loss: 0.6324 - regression_loss: 0.5107 - classification_loss: 0.1216
 4763/10000 [=============>................] - ETA: 39:52 - loss: 0.6324 - regression_loss: 0.5108 - classification_loss: 0.1216
 4764/10000 [=============>................] - ETA: 39:52 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1216
 4765/10000 [=============>................] - ETA: 39:52 - loss: 0.6324 - regression_loss: 0.5108 - classification_loss: 0.1216
 4766/10000 [=============>................] - ETA: 39:51 - loss: 0.6325 - regression_loss: 0.5108 - classification_loss: 0.1216
 4767/10000 [=============>................] - ETA: 39:51 - loss: 0.6325 - regression_loss: 0.5108 - classification_loss: 0.1216
 4768/10000 [=============>................] - ETA: 39:50 - loss: 0.6326 - regression_loss: 0.5109 - classification_loss: 0.1217
 4769/10000 [=============>................] - ETA: 39:50 - loss: 0.6325 - regression_loss: 0.5108 - classification_loss: 0.1217
 4770/10000 [=============>................] - ETA: 39:49 - loss: 0.6324 - regression_loss: 0.5107 - classification_loss: 0.1216
 4771/10000 [=============>................] - ETA: 39:49 - loss: 0.6325 - regression_loss: 0.5109 - classification_loss: 0.1217
 4772/10000 [=============>................] - ETA: 39:48 - loss: 0.6325 - regression_loss: 0.5108 - classification_loss: 0.1217
 4773/10000 [=============>................] - ETA: 39:48 - loss: 0.6325 - regression_loss: 0.5108 - classification_loss: 0.1216
 4774/10000 [=============>................] - ETA: 39:47 - loss: 0.6325 - regression_loss: 0.5108 - classification_loss: 0.1216
 4775/10000 [=============>................] - ETA: 39:47 - loss: 0.6324 - regression_loss: 0.5108 - classification_loss: 0.1216
 4776/10000 [=============>................] - ETA: 39:46 - loss: 0.6324 - regression_loss: 0.5108 - classification_loss: 0.1216
 4777/10000 [=============>................] - ETA: 39:46 - loss: 0.6324 - regression_loss: 0.5107 - classification_loss: 0.1216
 4778/10000 [=============>................] - ETA: 39:45 - loss: 0.6324 - regression_loss: 0.5108 - classification_loss: 0.1216
 4779/10000 [=============>................] - ETA: 39:45 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1216
 4780/10000 [=============>................] - ETA: 39:44 - loss: 0.6325 - regression_loss: 0.5109 - classification_loss: 0.1216
 4781/10000 [=============>................] - ETA: 39:44 - loss: 0.6326 - regression_loss: 0.5110 - classification_loss: 0.1217
 4782/10000 [=============>................] - ETA: 39:44 - loss: 0.6326 - regression_loss: 0.5109 - classification_loss: 0.1216
 4783/10000 [=============>................] - ETA: 39:43 - loss: 0.6325 - regression_loss: 0.5109 - classification_loss: 0.1216
 4784/10000 [=============>................] - ETA: 39:43 - loss: 0.6324 - regression_loss: 0.5108 - classification_loss: 0.1216
 4785/10000 [=============>................] - ETA: 39:42 - loss: 0.6324 - regression_loss: 0.5108 - classification_loss: 0.1216
 4786/10000 [=============>................] - ETA: 39:42 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1216
 4787/10000 [=============>................] - ETA: 39:41 - loss: 0.6324 - regression_loss: 0.5107 - classification_loss: 0.1216
 4788/10000 [=============>................] - ETA: 39:41 - loss: 0.6324 - regression_loss: 0.5108 - classification_loss: 0.1216
 4789/10000 [=============>................] - ETA: 39:40 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1216
 4790/10000 [=============>................] - ETA: 39:40 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1216
 4791/10000 [=============>................] - ETA: 39:39 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1216
 4792/10000 [=============>................] - ETA: 39:39 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1216
 4793/10000 [=============>................] - ETA: 39:39 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1216
 4794/10000 [=============>................] - ETA: 39:38 - loss: 0.6323 - regression_loss: 0.5108 - classification_loss: 0.1216
 4795/10000 [=============>................] - ETA: 39:38 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1216
 4796/10000 [=============>................] - ETA: 39:37 - loss: 0.6322 - regression_loss: 0.5107 - classification_loss: 0.1216
 4797/10000 [=============>................] - ETA: 39:37 - loss: 0.6322 - regression_loss: 0.5106 - classification_loss: 0.1216
 4798/10000 [=============>................] - ETA: 39:36 - loss: 0.6322 - regression_loss: 0.5106 - classification_loss: 0.1215
 4799/10000 [=============>................] - ETA: 39:36 - loss: 0.6321 - regression_loss: 0.5106 - classification_loss: 0.1215
 4800/10000 [=============>................] - ETA: 39:35 - loss: 0.6321 - regression_loss: 0.5106 - classification_loss: 0.1215
 4801/10000 [=============>................] - ETA: 39:35 - loss: 0.6322 - regression_loss: 0.5107 - classification_loss: 0.1215
 4802/10000 [=============>................] - ETA: 39:34 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1215
 4803/10000 [=============>................] - ETA: 39:34 - loss: 0.6322 - regression_loss: 0.5107 - classification_loss: 0.1215
 4804/10000 [=============>................] - ETA: 39:34 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1216
 4805/10000 [=============>................] - ETA: 39:33 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1216
 4806/10000 [=============>................] - ETA: 39:33 - loss: 0.6323 - regression_loss: 0.5108 - classification_loss: 0.1216
 4807/10000 [=============>................] - ETA: 39:32 - loss: 0.6324 - regression_loss: 0.5108 - classification_loss: 0.1216
 4808/10000 [=============>................] - ETA: 39:32 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1215
 4809/10000 [=============>................] - ETA: 39:31 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1215
 4810/10000 [=============>................] - ETA: 39:31 - loss: 0.6323 - regression_loss: 0.5108 - classification_loss: 0.1215
 4811/10000 [=============>................] - ETA: 39:31 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1215
 4812/10000 [=============>................] - ETA: 39:30 - loss: 0.6323 - regression_loss: 0.5107 - classification_loss: 0.1215
 4813/10000 [=============>................] - ETA: 39:30 - loss: 0.6322 - regression_loss: 0.5106 - classification_loss: 0.1215
 4814/10000 [=============>................] - ETA: 39:29 - loss: 0.6323 - regression_loss: 0.5108 - classification_loss: 0.1215
 4815/10000 [=============>................] - ETA: 39:29 - loss: 0.6322 - regression_loss: 0.5107 - classification_loss: 0.1215
 4816/10000 [=============>................] - ETA: 39:28 - loss: 0.6325 - regression_loss: 0.5109 - classification_loss: 0.1216
 4817/10000 [=============>................] - ETA: 39:28 - loss: 0.6327 - regression_loss: 0.5110 - classification_loss: 0.1217
 4818/10000 [=============>................] - ETA: 39:27 - loss: 0.6329 - regression_loss: 0.5112 - classification_loss: 0.1217
 4819/10000 [=============>................] - ETA: 39:27 - loss: 0.6328 - regression_loss: 0.5111 - classification_loss: 0.1217
 4820/10000 [=============>................] - ETA: 39:26 - loss: 0.6327 - regression_loss: 0.5110 - classification_loss: 0.1217
 4821/10000 [=============>................] - ETA: 39:26 - loss: 0.6327 - regression_loss: 0.5110 - classification_loss: 0.1217
 4822/10000 [=============>................] - ETA: 39:26 - loss: 0.6326 - regression_loss: 0.5109 - classification_loss: 0.1217
 4823/10000 [=============>................] - ETA: 39:25 - loss: 0.6326 - regression_loss: 0.5109 - classification_loss: 0.1217
 4824/10000 [=============>................] - ETA: 39:25 - loss: 0.6326 - regression_loss: 0.5109 - classification_loss: 0.1217
 4825/10000 [=============>................] - ETA: 39:24 - loss: 0.6326 - regression_loss: 0.5109 - classification_loss: 0.1217
 4826/10000 [=============>................] - ETA: 39:24 - loss: 0.6325 - regression_loss: 0.5109 - classification_loss: 0.1217
 4827/10000 [=============>................] - ETA: 39:23 - loss: 0.6325 - regression_loss: 0.5108 - classification_loss: 0.1217
 4828/10000 [=============>................] - ETA: 39:23 - loss: 0.6324 - regression_loss: 0.5108 - classification_loss: 0.1216
 4829/10000 [=============>................] - ETA: 39:22 - loss: 0.6324 - regression_loss: 0.5108 - classification_loss: 0.1216
 4830/10000 [=============>................] - ETA: 39:22 - loss: 0.6325 - regression_loss: 0.5109 - classification_loss: 0.1217
 4831/10000 [=============>................] - ETA: 39:21 - loss: 0.6324 - regression_loss: 0.5108 - classification_loss: 0.1216
 4832/10000 [=============>................] - ETA: 39:21 - loss: 0.6324 - regression_loss: 0.5107 - classification_loss: 0.1216
 4833/10000 [=============>................] - ETA: 39:20 - loss: 0.6325 - regression_loss: 0.5109 - classification_loss: 0.1217
 4834/10000 [=============>................] - ETA: 39:20 - loss: 0.6326 - regression_loss: 0.5109 - classification_loss: 0.1217
 4835/10000 [=============>................] - ETA: 39:20 - loss: 0.6326 - regression_loss: 0.5109 - classification_loss: 0.1217
 4836/10000 [=============>................] - ETA: 39:19 - loss: 0.6328 - regression_loss: 0.5111 - classification_loss: 0.1217
 4837/10000 [=============>................] - ETA: 39:19 - loss: 0.6328 - regression_loss: 0.5111 - classification_loss: 0.1218
 4838/10000 [=============>................] - ETA: 39:18 - loss: 0.6330 - regression_loss: 0.5113 - classification_loss: 0.1218
 4839/10000 [=============>................] - ETA: 39:18 - loss: 0.6330 - regression_loss: 0.5112 - classification_loss: 0.1218
 4840/10000 [=============>................] - ETA: 39:17 - loss: 0.6330 - regression_loss: 0.5112 - classification_loss: 0.1218
 4841/10000 [=============>................] - ETA: 39:17 - loss: 0.6329 - regression_loss: 0.5111 - classification_loss: 0.1217
 4842/10000 [=============>................] - ETA: 39:16 - loss: 0.6329 - regression_loss: 0.5111 - classification_loss: 0.1218
 4843/10000 [=============>................] - ETA: 39:16 - loss: 0.6329 - regression_loss: 0.5111 - classification_loss: 0.1218
 4844/10000 [=============>................] - ETA: 39:15 - loss: 0.6330 - regression_loss: 0.5113 - classification_loss: 0.1218
 4845/10000 [=============>................] - ETA: 39:15 - loss: 0.6329 - regression_loss: 0.5112 - classification_loss: 0.1218
 4846/10000 [=============>................] - ETA: 39:15 - loss: 0.6329 - regression_loss: 0.5112 - classification_loss: 0.1217
 4847/10000 [=============>................] - ETA: 39:14 - loss: 0.6329 - regression_loss: 0.5111 - classification_loss: 0.1217
 4848/10000 [=============>................] - ETA: 39:14 - loss: 0.6328 - regression_loss: 0.5111 - classification_loss: 0.1217
 4849/10000 [=============>................] - ETA: 39:13 - loss: 0.6328 - regression_loss: 0.5111 - classification_loss: 0.1217
 4850/10000 [=============>................] - ETA: 39:13 - loss: 0.6329 - regression_loss: 0.5112 - classification_loss: 0.1218
 4851/10000 [=============>................] - ETA: 39:12 - loss: 0.6331 - regression_loss: 0.5114 - classification_loss: 0.1218
 4852/10000 [=============>................] - ETA: 39:12 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 4853/10000 [=============>................] - ETA: 39:11 - loss: 0.6330 - regression_loss: 0.5112 - classification_loss: 0.1217
 4854/10000 [=============>................] - ETA: 39:11 - loss: 0.6330 - regression_loss: 0.5112 - classification_loss: 0.1217
 4855/10000 [=============>................] - ETA: 39:10 - loss: 0.6330 - regression_loss: 0.5112 - classification_loss: 0.1217
 4856/10000 [=============>................] - ETA: 39:10 - loss: 0.6330 - regression_loss: 0.5113 - classification_loss: 0.1217
 4857/10000 [=============>................] - ETA: 39:09 - loss: 0.6329 - regression_loss: 0.5112 - classification_loss: 0.1217
 4858/10000 [=============>................] - ETA: 39:09 - loss: 0.6329 - regression_loss: 0.5112 - classification_loss: 0.1218
 4859/10000 [=============>................] - ETA: 39:08 - loss: 0.6328 - regression_loss: 0.5111 - classification_loss: 0.1218
 4860/10000 [=============>................] - ETA: 39:08 - loss: 0.6328 - regression_loss: 0.5110 - classification_loss: 0.1218
 4861/10000 [=============>................] - ETA: 39:07 - loss: 0.6328 - regression_loss: 0.5111 - classification_loss: 0.1218
 4862/10000 [=============>................] - ETA: 39:07 - loss: 0.6329 - regression_loss: 0.5112 - classification_loss: 0.1218
 4863/10000 [=============>................] - ETA: 39:07 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 4864/10000 [=============>................] - ETA: 39:06 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 4865/10000 [=============>................] - ETA: 39:06 - loss: 0.6330 - regression_loss: 0.5112 - classification_loss: 0.1218
 4866/10000 [=============>................] - ETA: 39:05 - loss: 0.6330 - regression_loss: 0.5112 - classification_loss: 0.1218
 4867/10000 [=============>................] - ETA: 39:05 - loss: 0.6330 - regression_loss: 0.5112 - classification_loss: 0.1218
 4868/10000 [=============>................] - ETA: 39:04 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 4869/10000 [=============>................] - ETA: 39:04 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 4870/10000 [=============>................] - ETA: 39:03 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 4871/10000 [=============>................] - ETA: 39:03 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 4872/10000 [=============>................] - ETA: 39:02 - loss: 0.6331 - regression_loss: 0.5114 - classification_loss: 0.1218
 4873/10000 [=============>................] - ETA: 39:02 - loss: 0.6331 - regression_loss: 0.5114 - classification_loss: 0.1218
 4874/10000 [=============>................] - ETA: 39:02 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 4875/10000 [=============>................] - ETA: 39:01 - loss: 0.6331 - regression_loss: 0.5114 - classification_loss: 0.1218
 4876/10000 [=============>................] - ETA: 39:01 - loss: 0.6332 - regression_loss: 0.5115 - classification_loss: 0.1218
 4877/10000 [=============>................] - ETA: 39:00 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 4878/10000 [=============>................] - ETA: 39:00 - loss: 0.6331 - regression_loss: 0.5114 - classification_loss: 0.1217
 4879/10000 [=============>................] - ETA: 38:59 - loss: 0.6331 - regression_loss: 0.5114 - classification_loss: 0.1217
 4880/10000 [=============>................] - ETA: 38:59 - loss: 0.6330 - regression_loss: 0.5113 - classification_loss: 0.1217
 4881/10000 [=============>................] - ETA: 38:58 - loss: 0.6330 - regression_loss: 0.5113 - classification_loss: 0.1217
 4882/10000 [=============>................] - ETA: 38:58 - loss: 0.6329 - regression_loss: 0.5112 - classification_loss: 0.1217
 4883/10000 [=============>................] - ETA: 38:57 - loss: 0.6329 - regression_loss: 0.5112 - classification_loss: 0.1217
 4884/10000 [=============>................] - ETA: 38:57 - loss: 0.6328 - regression_loss: 0.5112 - classification_loss: 0.1217
 4885/10000 [=============>................] - ETA: 38:57 - loss: 0.6328 - regression_loss: 0.5111 - classification_loss: 0.1217
 4886/10000 [=============>................] - ETA: 38:56 - loss: 0.6329 - regression_loss: 0.5112 - classification_loss: 0.1217
 4887/10000 [=============>................] - ETA: 38:56 - loss: 0.6328 - regression_loss: 0.5111 - classification_loss: 0.1217
 4888/10000 [=============>................] - ETA: 38:55 - loss: 0.6328 - regression_loss: 0.5111 - classification_loss: 0.1217
 4889/10000 [=============>................] - ETA: 38:55 - loss: 0.6328 - regression_loss: 0.5111 - classification_loss: 0.1217
 4890/10000 [=============>................] - ETA: 38:54 - loss: 0.6327 - regression_loss: 0.5111 - classification_loss: 0.1216
 4891/10000 [=============>................] - ETA: 38:54 - loss: 0.6328 - regression_loss: 0.5111 - classification_loss: 0.1216
 4892/10000 [=============>................] - ETA: 38:53 - loss: 0.6329 - regression_loss: 0.5112 - classification_loss: 0.1217
 4893/10000 [=============>................] - ETA: 38:53 - loss: 0.6328 - regression_loss: 0.5112 - classification_loss: 0.1216
 4894/10000 [=============>................] - ETA: 38:52 - loss: 0.6327 - regression_loss: 0.5111 - classification_loss: 0.1216
 4895/10000 [=============>................] - ETA: 38:52 - loss: 0.6327 - regression_loss: 0.5110 - classification_loss: 0.1217
 4896/10000 [=============>................] - ETA: 38:51 - loss: 0.6328 - regression_loss: 0.5111 - classification_loss: 0.1218
 4897/10000 [=============>................] - ETA: 38:51 - loss: 0.6328 - regression_loss: 0.5110 - classification_loss: 0.1218
 4898/10000 [=============>................] - ETA: 38:51 - loss: 0.6327 - regression_loss: 0.5109 - classification_loss: 0.1217
 4899/10000 [=============>................] - ETA: 38:50 - loss: 0.6327 - regression_loss: 0.5109 - classification_loss: 0.1217
 4900/10000 [=============>................] - ETA: 38:50 - loss: 0.6326 - regression_loss: 0.5109 - classification_loss: 0.1217
 4901/10000 [=============>................] - ETA: 38:49 - loss: 0.6326 - regression_loss: 0.5109 - classification_loss: 0.1217
 4902/10000 [=============>................] - ETA: 38:49 - loss: 0.6327 - regression_loss: 0.5110 - classification_loss: 0.1217
 4903/10000 [=============>................] - ETA: 38:48 - loss: 0.6326 - regression_loss: 0.5109 - classification_loss: 0.1217
 4904/10000 [=============>................] - ETA: 38:48 - loss: 0.6325 - regression_loss: 0.5108 - classification_loss: 0.1217
 4905/10000 [=============>................] - ETA: 38:47 - loss: 0.6326 - regression_loss: 0.5109 - classification_loss: 0.1217
 4906/10000 [=============>................] - ETA: 38:47 - loss: 0.6326 - regression_loss: 0.5109 - classification_loss: 0.1217
 4907/10000 [=============>................] - ETA: 38:47 - loss: 0.6329 - regression_loss: 0.5111 - classification_loss: 0.1218
 4908/10000 [=============>................] - ETA: 38:46 - loss: 0.6329 - regression_loss: 0.5112 - classification_loss: 0.1218
 4909/10000 [=============>................] - ETA: 38:46 - loss: 0.6328 - regression_loss: 0.5111 - classification_loss: 0.1217
 4910/10000 [=============>................] - ETA: 38:45 - loss: 0.6328 - regression_loss: 0.5111 - classification_loss: 0.1217
 4911/10000 [=============>................] - ETA: 38:45 - loss: 0.6328 - regression_loss: 0.5110 - classification_loss: 0.1218
 4912/10000 [=============>................] - ETA: 38:44 - loss: 0.6329 - regression_loss: 0.5111 - classification_loss: 0.1218
 4913/10000 [=============>................] - ETA: 38:44 - loss: 0.6330 - regression_loss: 0.5112 - classification_loss: 0.1218
 4914/10000 [=============>................] - ETA: 38:43 - loss: 0.6329 - regression_loss: 0.5111 - classification_loss: 0.1218
 4915/10000 [=============>................] - ETA: 38:43 - loss: 0.6329 - regression_loss: 0.5111 - classification_loss: 0.1218
 4916/10000 [=============>................] - ETA: 38:42 - loss: 0.6331 - regression_loss: 0.5112 - classification_loss: 0.1218
 4917/10000 [=============>................] - ETA: 38:42 - loss: 0.6330 - regression_loss: 0.5112 - classification_loss: 0.1218
 4918/10000 [=============>................] - ETA: 38:41 - loss: 0.6333 - regression_loss: 0.5114 - classification_loss: 0.1219
 4919/10000 [=============>................] - ETA: 38:41 - loss: 0.6333 - regression_loss: 0.5114 - classification_loss: 0.1219
 4920/10000 [=============>................] - ETA: 38:41 - loss: 0.6332 - regression_loss: 0.5113 - classification_loss: 0.1218
 4921/10000 [=============>................] - ETA: 38:40 - loss: 0.6333 - regression_loss: 0.5114 - classification_loss: 0.1219
 4922/10000 [=============>................] - ETA: 38:40 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1219
 4923/10000 [=============>................] - ETA: 38:39 - loss: 0.6332 - regression_loss: 0.5113 - classification_loss: 0.1219
 4924/10000 [=============>................] - ETA: 38:39 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 4925/10000 [=============>................] - ETA: 38:38 - loss: 0.6331 - regression_loss: 0.5112 - classification_loss: 0.1218
 4926/10000 [=============>................] - ETA: 38:38 - loss: 0.6331 - regression_loss: 0.5112 - classification_loss: 0.1219
 4927/10000 [=============>................] - ETA: 38:37 - loss: 0.6330 - regression_loss: 0.5111 - classification_loss: 0.1218
 4928/10000 [=============>................] - ETA: 38:37 - loss: 0.6329 - regression_loss: 0.5111 - classification_loss: 0.1218
 4929/10000 [=============>................] - ETA: 38:37 - loss: 0.6328 - regression_loss: 0.5110 - classification_loss: 0.1218
 4930/10000 [=============>................] - ETA: 38:36 - loss: 0.6330 - regression_loss: 0.5112 - classification_loss: 0.1218
 4931/10000 [=============>................] - ETA: 38:36 - loss: 0.6329 - regression_loss: 0.5111 - classification_loss: 0.1218
 4932/10000 [=============>................] - ETA: 38:35 - loss: 0.6332 - regression_loss: 0.5113 - classification_loss: 0.1219
 4933/10000 [=============>................] - ETA: 38:35 - loss: 0.6332 - regression_loss: 0.5113 - classification_loss: 0.1219
 4934/10000 [=============>................] - ETA: 38:34 - loss: 0.6334 - regression_loss: 0.5115 - classification_loss: 0.1219
 4935/10000 [=============>................] - ETA: 38:34 - loss: 0.6333 - regression_loss: 0.5114 - classification_loss: 0.1219
 4936/10000 [=============>................] - ETA: 38:33 - loss: 0.6334 - regression_loss: 0.5115 - classification_loss: 0.1219
 4937/10000 [=============>................] - ETA: 38:33 - loss: 0.6334 - regression_loss: 0.5115 - classification_loss: 0.1219
 4938/10000 [=============>................] - ETA: 38:32 - loss: 0.6333 - regression_loss: 0.5115 - classification_loss: 0.1219
 4939/10000 [=============>................] - ETA: 38:32 - loss: 0.6333 - regression_loss: 0.5115 - classification_loss: 0.1218
 4940/10000 [=============>................] - ETA: 38:32 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 4941/10000 [=============>................] - ETA: 38:31 - loss: 0.6333 - regression_loss: 0.5115 - classification_loss: 0.1218
 4942/10000 [=============>................] - ETA: 38:31 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 4943/10000 [=============>................] - ETA: 38:30 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 4944/10000 [=============>................] - ETA: 38:30 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 4945/10000 [=============>................] - ETA: 38:29 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 4946/10000 [=============>................] - ETA: 38:29 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 4947/10000 [=============>................] - ETA: 38:28 - loss: 0.6330 - regression_loss: 0.5112 - classification_loss: 0.1218
 4948/10000 [=============>................] - ETA: 38:28 - loss: 0.6330 - regression_loss: 0.5112 - classification_loss: 0.1218
 4949/10000 [=============>................] - ETA: 38:27 - loss: 0.6330 - regression_loss: 0.5112 - classification_loss: 0.1218
 4950/10000 [=============>................] - ETA: 38:27 - loss: 0.6329 - regression_loss: 0.5111 - classification_loss: 0.1218
 4951/10000 [=============>................] - ETA: 38:26 - loss: 0.6330 - regression_loss: 0.5112 - classification_loss: 0.1218
 4952/10000 [=============>................] - ETA: 38:26 - loss: 0.6330 - regression_loss: 0.5112 - classification_loss: 0.1218
 4953/10000 [=============>................] - ETA: 38:26 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1219
 4954/10000 [=============>................] - ETA: 38:25 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1219
 4955/10000 [=============>................] - ETA: 38:25 - loss: 0.6331 - regression_loss: 0.5112 - classification_loss: 0.1219
 4956/10000 [=============>................] - ETA: 38:24 - loss: 0.6332 - regression_loss: 0.5113 - classification_loss: 0.1219
 4957/10000 [=============>................] - ETA: 38:24 - loss: 0.6333 - regression_loss: 0.5114 - classification_loss: 0.1219
 4958/10000 [=============>................] - ETA: 38:23 - loss: 0.6333 - regression_loss: 0.5114 - classification_loss: 0.1219
 4959/10000 [=============>................] - ETA: 38:23 - loss: 0.6334 - regression_loss: 0.5114 - classification_loss: 0.1219
 4960/10000 [=============>................] - ETA: 38:22 - loss: 0.6336 - regression_loss: 0.5116 - classification_loss: 0.1220
 4961/10000 [=============>................] - ETA: 38:22 - loss: 0.6336 - regression_loss: 0.5116 - classification_loss: 0.1220
 4962/10000 [=============>................] - ETA: 38:21 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1220
 4963/10000 [=============>................] - ETA: 38:21 - loss: 0.6336 - regression_loss: 0.5116 - classification_loss: 0.1220
 4964/10000 [=============>................] - ETA: 38:21 - loss: 0.6336 - regression_loss: 0.5116 - classification_loss: 0.1220
 4965/10000 [=============>................] - ETA: 38:20 - loss: 0.6335 - regression_loss: 0.5116 - classification_loss: 0.1219
 4966/10000 [=============>................] - ETA: 38:20 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1220
 4967/10000 [=============>................] - ETA: 38:19 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1219
 4968/10000 [=============>................] - ETA: 38:19 - loss: 0.6336 - regression_loss: 0.5116 - classification_loss: 0.1219
 4969/10000 [=============>................] - ETA: 38:18 - loss: 0.6337 - regression_loss: 0.5118 - classification_loss: 0.1219
 4970/10000 [=============>................] - ETA: 38:18 - loss: 0.6338 - regression_loss: 0.5119 - classification_loss: 0.1220
 4971/10000 [=============>................] - ETA: 38:17 - loss: 0.6337 - regression_loss: 0.5118 - classification_loss: 0.1219
 4972/10000 [=============>................] - ETA: 38:17 - loss: 0.6338 - regression_loss: 0.5118 - classification_loss: 0.1219
 4973/10000 [=============>................] - ETA: 38:17 - loss: 0.6337 - regression_loss: 0.5118 - classification_loss: 0.1219
 4974/10000 [=============>................] - ETA: 38:16 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1219
 4975/10000 [=============>................] - ETA: 38:16 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1219
 4976/10000 [=============>................] - ETA: 38:15 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1219
 4977/10000 [=============>................] - ETA: 38:15 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1219
 4978/10000 [=============>................] - ETA: 38:14 - loss: 0.6336 - regression_loss: 0.5116 - classification_loss: 0.1219
 4979/10000 [=============>................] - ETA: 38:14 - loss: 0.6335 - regression_loss: 0.5116 - classification_loss: 0.1219
 4980/10000 [=============>................] - ETA: 38:13 - loss: 0.6335 - regression_loss: 0.5116 - classification_loss: 0.1219
 4981/10000 [=============>................] - ETA: 38:13 - loss: 0.6335 - regression_loss: 0.5116 - classification_loss: 0.1219
 4982/10000 [=============>................] - ETA: 38:12 - loss: 0.6336 - regression_loss: 0.5116 - classification_loss: 0.1219
 4983/10000 [=============>................] - ETA: 38:12 - loss: 0.6335 - regression_loss: 0.5116 - classification_loss: 0.1219
 4984/10000 [=============>................] - ETA: 38:11 - loss: 0.6335 - regression_loss: 0.5116 - classification_loss: 0.1219
 4985/10000 [=============>................] - ETA: 38:11 - loss: 0.6335 - regression_loss: 0.5116 - classification_loss: 0.1219
 4986/10000 [=============>................] - ETA: 38:11 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1219
 4987/10000 [=============>................] - ETA: 38:10 - loss: 0.6335 - regression_loss: 0.5116 - classification_loss: 0.1219
 4988/10000 [=============>................] - ETA: 38:10 - loss: 0.6334 - regression_loss: 0.5116 - classification_loss: 0.1219
 4989/10000 [=============>................] - ETA: 38:09 - loss: 0.6334 - regression_loss: 0.5115 - classification_loss: 0.1218
 4990/10000 [=============>................] - ETA: 38:09 - loss: 0.6333 - regression_loss: 0.5115 - classification_loss: 0.1218
 4991/10000 [=============>................] - ETA: 38:08 - loss: 0.6334 - regression_loss: 0.5115 - classification_loss: 0.1219
 4992/10000 [=============>................] - ETA: 38:08 - loss: 0.6334 - regression_loss: 0.5115 - classification_loss: 0.1219
 4993/10000 [=============>................] - ETA: 38:08 - loss: 0.6334 - regression_loss: 0.5116 - classification_loss: 0.1219
 4994/10000 [=============>................] - ETA: 38:07 - loss: 0.6334 - regression_loss: 0.5115 - classification_loss: 0.1218
 4995/10000 [=============>................] - ETA: 38:07 - loss: 0.6334 - regression_loss: 0.5115 - classification_loss: 0.1219
 4996/10000 [=============>................] - ETA: 38:06 - loss: 0.6334 - regression_loss: 0.5115 - classification_loss: 0.1219
 4997/10000 [=============>................] - ETA: 38:06 - loss: 0.6334 - regression_loss: 0.5116 - classification_loss: 0.1219
 4998/10000 [=============>................] - ETA: 38:05 - loss: 0.6335 - regression_loss: 0.5116 - classification_loss: 0.1219
 4999/10000 [=============>................] - ETA: 38:05 - loss: 0.6334 - regression_loss: 0.5115 - classification_loss: 0.1219
 5000/10000 [==============>...............] - ETA: 38:04 - loss: 0.6334 - regression_loss: 0.5115 - classification_loss: 0.1219
 5001/10000 [==============>...............] - ETA: 38:04 - loss: 0.6333 - regression_loss: 0.5115 - classification_loss: 0.1219
 5002/10000 [==============>...............] - ETA: 38:03 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 5003/10000 [==============>...............] - ETA: 38:03 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 5004/10000 [==============>...............] - ETA: 38:02 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 5005/10000 [==============>...............] - ETA: 38:02 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 5006/10000 [==============>...............] - ETA: 38:02 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 5007/10000 [==============>...............] - ETA: 38:01 - loss: 0.6331 - regression_loss: 0.5112 - classification_loss: 0.1218
 5008/10000 [==============>...............] - ETA: 38:01 - loss: 0.6332 - regression_loss: 0.5113 - classification_loss: 0.1218
 5009/10000 [==============>...............] - ETA: 38:00 - loss: 0.6331 - regression_loss: 0.5112 - classification_loss: 0.1218
 5010/10000 [==============>...............] - ETA: 38:00 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 5011/10000 [==============>...............] - ETA: 37:59 - loss: 0.6330 - regression_loss: 0.5112 - classification_loss: 0.1218
 5012/10000 [==============>...............] - ETA: 37:59 - loss: 0.6330 - regression_loss: 0.5111 - classification_loss: 0.1218
 5013/10000 [==============>...............] - ETA: 37:58 - loss: 0.6329 - regression_loss: 0.5111 - classification_loss: 0.1218
 5014/10000 [==============>...............] - ETA: 37:58 - loss: 0.6330 - regression_loss: 0.5112 - classification_loss: 0.1218
 5015/10000 [==============>...............] - ETA: 37:57 - loss: 0.6330 - regression_loss: 0.5112 - classification_loss: 0.1218
 5016/10000 [==============>...............] - ETA: 37:57 - loss: 0.6330 - regression_loss: 0.5112 - classification_loss: 0.1218
 5017/10000 [==============>...............] - ETA: 37:57 - loss: 0.6329 - regression_loss: 0.5111 - classification_loss: 0.1218
 5018/10000 [==============>...............] - ETA: 37:56 - loss: 0.6328 - regression_loss: 0.5110 - classification_loss: 0.1218
 5019/10000 [==============>...............] - ETA: 37:56 - loss: 0.6328 - regression_loss: 0.5110 - classification_loss: 0.1218
 5020/10000 [==============>...............] - ETA: 37:55 - loss: 0.6327 - regression_loss: 0.5110 - classification_loss: 0.1217
 5021/10000 [==============>...............] - ETA: 37:55 - loss: 0.6327 - regression_loss: 0.5110 - classification_loss: 0.1217
 5022/10000 [==============>...............] - ETA: 37:54 - loss: 0.6326 - regression_loss: 0.5109 - classification_loss: 0.1217
 5023/10000 [==============>...............] - ETA: 37:54 - loss: 0.6326 - regression_loss: 0.5109 - classification_loss: 0.1217
 5024/10000 [==============>...............] - ETA: 37:53 - loss: 0.6325 - regression_loss: 0.5108 - classification_loss: 0.1217
 5025/10000 [==============>...............] - ETA: 37:53 - loss: 0.6328 - regression_loss: 0.5111 - classification_loss: 0.1217
 5026/10000 [==============>...............] - ETA: 37:52 - loss: 0.6330 - regression_loss: 0.5112 - classification_loss: 0.1218
 5027/10000 [==============>...............] - ETA: 37:52 - loss: 0.6329 - regression_loss: 0.5112 - classification_loss: 0.1218
 5028/10000 [==============>...............] - ETA: 37:52 - loss: 0.6330 - regression_loss: 0.5112 - classification_loss: 0.1218
 5029/10000 [==============>...............] - ETA: 37:51 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 5030/10000 [==============>...............] - ETA: 37:51 - loss: 0.6330 - regression_loss: 0.5113 - classification_loss: 0.1218
 5031/10000 [==============>...............] - ETA: 37:50 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 5032/10000 [==============>...............] - ETA: 37:50 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 5033/10000 [==============>...............] - ETA: 37:49 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 5034/10000 [==============>...............] - ETA: 37:49 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 5035/10000 [==============>...............] - ETA: 37:48 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 5036/10000 [==============>...............] - ETA: 37:48 - loss: 0.6334 - regression_loss: 0.5116 - classification_loss: 0.1218
 5037/10000 [==============>...............] - ETA: 37:47 - loss: 0.6333 - regression_loss: 0.5115 - classification_loss: 0.1218
 5038/10000 [==============>...............] - ETA: 37:47 - loss: 0.6334 - regression_loss: 0.5116 - classification_loss: 0.1218
 5039/10000 [==============>...............] - ETA: 37:47 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1219
 5040/10000 [==============>...............] - ETA: 37:46 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1219
 5041/10000 [==============>...............] - ETA: 37:46 - loss: 0.6335 - regression_loss: 0.5117 - classification_loss: 0.1218
 5042/10000 [==============>...............] - ETA: 37:45 - loss: 0.6336 - regression_loss: 0.5118 - classification_loss: 0.1218
 5043/10000 [==============>...............] - ETA: 37:45 - loss: 0.6337 - regression_loss: 0.5118 - classification_loss: 0.1220
 5044/10000 [==============>...............] - ETA: 37:44 - loss: 0.6337 - regression_loss: 0.5117 - classification_loss: 0.1219
 5045/10000 [==============>...............] - ETA: 37:44 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1219
 5046/10000 [==============>...............] - ETA: 37:43 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1219
 5047/10000 [==============>...............] - ETA: 37:43 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1219
 5048/10000 [==============>...............] - ETA: 37:42 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1219
 5049/10000 [==============>...............] - ETA: 37:42 - loss: 0.6337 - regression_loss: 0.5117 - classification_loss: 0.1219
 5050/10000 [==============>...............] - ETA: 37:42 - loss: 0.6337 - regression_loss: 0.5118 - classification_loss: 0.1219
 5051/10000 [==============>...............] - ETA: 37:41 - loss: 0.6337 - regression_loss: 0.5117 - classification_loss: 0.1219
 5052/10000 [==============>...............] - ETA: 37:41 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1219
 5053/10000 [==============>...............] - ETA: 37:40 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1219
 5054/10000 [==============>...............] - ETA: 37:40 - loss: 0.6335 - regression_loss: 0.5116 - classification_loss: 0.1219
 5055/10000 [==============>...............] - ETA: 37:39 - loss: 0.6335 - regression_loss: 0.5116 - classification_loss: 0.1219
 5056/10000 [==============>...............] - ETA: 37:39 - loss: 0.6335 - regression_loss: 0.5116 - classification_loss: 0.1219
 5057/10000 [==============>...............] - ETA: 37:38 - loss: 0.6335 - regression_loss: 0.5116 - classification_loss: 0.1219
 5058/10000 [==============>...............] - ETA: 37:38 - loss: 0.6334 - regression_loss: 0.5115 - classification_loss: 0.1219
 5059/10000 [==============>...............] - ETA: 37:37 - loss: 0.6335 - regression_loss: 0.5116 - classification_loss: 0.1219
 5060/10000 [==============>...............] - ETA: 37:37 - loss: 0.6335 - regression_loss: 0.5116 - classification_loss: 0.1219
 5061/10000 [==============>...............] - ETA: 37:37 - loss: 0.6335 - regression_loss: 0.5116 - classification_loss: 0.1219
 5062/10000 [==============>...............] - ETA: 37:36 - loss: 0.6334 - regression_loss: 0.5116 - classification_loss: 0.1218
 5063/10000 [==============>...............] - ETA: 37:36 - loss: 0.6335 - regression_loss: 0.5117 - classification_loss: 0.1219
 5064/10000 [==============>...............] - ETA: 37:35 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1219
 5065/10000 [==============>...............] - ETA: 37:35 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1219
 5066/10000 [==============>...............] - ETA: 37:34 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1219
 5067/10000 [==============>...............] - ETA: 37:34 - loss: 0.6335 - regression_loss: 0.5116 - classification_loss: 0.1219
 5068/10000 [==============>...............] - ETA: 37:33 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1219
 5069/10000 [==============>...............] - ETA: 37:33 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1219
 5070/10000 [==============>...............] - ETA: 37:32 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1219
 5071/10000 [==============>...............] - ETA: 37:32 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1219
 5072/10000 [==============>...............] - ETA: 37:31 - loss: 0.6335 - regression_loss: 0.5116 - classification_loss: 0.1219
 5073/10000 [==============>...............] - ETA: 37:31 - loss: 0.6335 - regression_loss: 0.5116 - classification_loss: 0.1219
 5074/10000 [==============>...............] - ETA: 37:31 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1219
 5075/10000 [==============>...............] - ETA: 37:30 - loss: 0.6335 - regression_loss: 0.5116 - classification_loss: 0.1219
 5076/10000 [==============>...............] - ETA: 37:30 - loss: 0.6334 - regression_loss: 0.5116 - classification_loss: 0.1219
 5077/10000 [==============>...............] - ETA: 37:29 - loss: 0.6335 - regression_loss: 0.5116 - classification_loss: 0.1219
 5078/10000 [==============>...............] - ETA: 37:29 - loss: 0.6334 - regression_loss: 0.5115 - classification_loss: 0.1218
 5079/10000 [==============>...............] - ETA: 37:28 - loss: 0.6334 - regression_loss: 0.5116 - classification_loss: 0.1218
 5080/10000 [==============>...............] - ETA: 37:28 - loss: 0.6333 - regression_loss: 0.5115 - classification_loss: 0.1218
 5081/10000 [==============>...............] - ETA: 37:27 - loss: 0.6333 - regression_loss: 0.5115 - classification_loss: 0.1218
 5082/10000 [==============>...............] - ETA: 37:27 - loss: 0.6333 - regression_loss: 0.5115 - classification_loss: 0.1218
 5083/10000 [==============>...............] - ETA: 37:26 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 5084/10000 [==============>...............] - ETA: 37:26 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 5085/10000 [==============>...............] - ETA: 37:26 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 5086/10000 [==============>...............] - ETA: 37:25 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 5087/10000 [==============>...............] - ETA: 37:25 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 5088/10000 [==============>...............] - ETA: 37:24 - loss: 0.6333 - regression_loss: 0.5115 - classification_loss: 0.1218
 5089/10000 [==============>...............] - ETA: 37:24 - loss: 0.6333 - regression_loss: 0.5115 - classification_loss: 0.1218
 5090/10000 [==============>...............] - ETA: 37:23 - loss: 0.6333 - regression_loss: 0.5115 - classification_loss: 0.1218
 5091/10000 [==============>...............] - ETA: 37:23 - loss: 0.6333 - regression_loss: 0.5115 - classification_loss: 0.1218
 5092/10000 [==============>...............] - ETA: 37:22 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 5093/10000 [==============>...............] - ETA: 37:22 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 5094/10000 [==============>...............] - ETA: 37:21 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 5095/10000 [==============>...............] - ETA: 37:21 - loss: 0.6332 - regression_loss: 0.5113 - classification_loss: 0.1218
 5096/10000 [==============>...............] - ETA: 37:20 - loss: 0.6331 - regression_loss: 0.5114 - classification_loss: 0.1218
 5097/10000 [==============>...............] - ETA: 37:20 - loss: 0.6334 - regression_loss: 0.5115 - classification_loss: 0.1218
 5098/10000 [==============>...............] - ETA: 37:20 - loss: 0.6335 - regression_loss: 0.5116 - classification_loss: 0.1218
 5099/10000 [==============>...............] - ETA: 37:19 - loss: 0.6337 - regression_loss: 0.5118 - classification_loss: 0.1219
 5100/10000 [==============>...............] - ETA: 37:19 - loss: 0.6337 - regression_loss: 0.5118 - classification_loss: 0.1218
 5101/10000 [==============>...............] - ETA: 37:18 - loss: 0.6336 - regression_loss: 0.5118 - classification_loss: 0.1218
 5102/10000 [==============>...............] - ETA: 37:18 - loss: 0.6337 - regression_loss: 0.5119 - classification_loss: 0.1219
 5103/10000 [==============>...............] - ETA: 37:17 - loss: 0.6339 - regression_loss: 0.5120 - classification_loss: 0.1219
 5104/10000 [==============>...............] - ETA: 37:17 - loss: 0.6338 - regression_loss: 0.5120 - classification_loss: 0.1219
 5105/10000 [==============>...............] - ETA: 37:16 - loss: 0.6338 - regression_loss: 0.5119 - classification_loss: 0.1219
 5106/10000 [==============>...............] - ETA: 37:16 - loss: 0.6337 - regression_loss: 0.5119 - classification_loss: 0.1218
 5107/10000 [==============>...............] - ETA: 37:15 - loss: 0.6338 - regression_loss: 0.5120 - classification_loss: 0.1219
 5108/10000 [==============>...............] - ETA: 37:15 - loss: 0.6337 - regression_loss: 0.5119 - classification_loss: 0.1218
 5109/10000 [==============>...............] - ETA: 37:15 - loss: 0.6336 - regression_loss: 0.5118 - classification_loss: 0.1218
 5110/10000 [==============>...............] - ETA: 37:14 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1218
 5111/10000 [==============>...............] - ETA: 37:14 - loss: 0.6335 - regression_loss: 0.5117 - classification_loss: 0.1218
 5112/10000 [==============>...............] - ETA: 37:13 - loss: 0.6336 - regression_loss: 0.5118 - classification_loss: 0.1218
 5113/10000 [==============>...............] - ETA: 37:13 - loss: 0.6337 - regression_loss: 0.5119 - classification_loss: 0.1218
 5114/10000 [==============>...............] - ETA: 37:12 - loss: 0.6337 - regression_loss: 0.5118 - classification_loss: 0.1218
 5115/10000 [==============>...............] - ETA: 37:12 - loss: 0.6336 - regression_loss: 0.5118 - classification_loss: 0.1218
 5116/10000 [==============>...............] - ETA: 37:11 - loss: 0.6337 - regression_loss: 0.5118 - classification_loss: 0.1219
 5117/10000 [==============>...............] - ETA: 37:11 - loss: 0.6337 - regression_loss: 0.5119 - classification_loss: 0.1219
 5118/10000 [==============>...............] - ETA: 37:11 - loss: 0.6337 - regression_loss: 0.5118 - classification_loss: 0.1219
 5119/10000 [==============>...............] - ETA: 37:10 - loss: 0.6336 - regression_loss: 0.5117 - classification_loss: 0.1219
 5120/10000 [==============>...............] - ETA: 37:10 - loss: 0.6335 - regression_loss: 0.5117 - classification_loss: 0.1218
 5121/10000 [==============>...............] - ETA: 37:09 - loss: 0.6334 - regression_loss: 0.5116 - classification_loss: 0.1218
 5122/10000 [==============>...............] - ETA: 37:09 - loss: 0.6335 - regression_loss: 0.5117 - classification_loss: 0.1218
 5123/10000 [==============>...............] - ETA: 37:08 - loss: 0.6335 - regression_loss: 0.5117 - classification_loss: 0.1218
 5124/10000 [==============>...............] - ETA: 37:08 - loss: 0.6335 - regression_loss: 0.5116 - classification_loss: 0.1218
 5125/10000 [==============>...............] - ETA: 37:07 - loss: 0.6335 - regression_loss: 0.5117 - classification_loss: 0.1218
 5126/10000 [==============>...............] - ETA: 37:07 - loss: 0.6335 - regression_loss: 0.5117 - classification_loss: 0.1219
 5127/10000 [==============>...............] - ETA: 37:06 - loss: 0.6335 - regression_loss: 0.5116 - classification_loss: 0.1218
 5128/10000 [==============>...............] - ETA: 37:06 - loss: 0.6335 - regression_loss: 0.5117 - classification_loss: 0.1218
 5129/10000 [==============>...............] - ETA: 37:05 - loss: 0.6335 - regression_loss: 0.5117 - classification_loss: 0.1218
 5130/10000 [==============>...............] - ETA: 37:05 - loss: 0.6334 - regression_loss: 0.5116 - classification_loss: 0.1218
 5131/10000 [==============>...............] - ETA: 37:05 - loss: 0.6334 - regression_loss: 0.5115 - classification_loss: 0.1218
 5132/10000 [==============>...............] - ETA: 37:04 - loss: 0.6333 - regression_loss: 0.5115 - classification_loss: 0.1218
 5133/10000 [==============>...............] - ETA: 37:04 - loss: 0.6333 - regression_loss: 0.5114 - classification_loss: 0.1218
 5134/10000 [==============>...............] - ETA: 37:03 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 5135/10000 [==============>...............] - ETA: 37:03 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 5136/10000 [==============>...............] - ETA: 37:02 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 5137/10000 [==============>...............] - ETA: 37:02 - loss: 0.6334 - regression_loss: 0.5115 - classification_loss: 0.1218
 5138/10000 [==============>...............] - ETA: 37:01 - loss: 0.6333 - regression_loss: 0.5115 - classification_loss: 0.1218
 5139/10000 [==============>...............] - ETA: 37:01 - loss: 0.6334 - regression_loss: 0.5116 - classification_loss: 0.1218
 5140/10000 [==============>...............] - ETA: 37:00 - loss: 0.6333 - regression_loss: 0.5116 - classification_loss: 0.1218
 5141/10000 [==============>...............] - ETA: 37:00 - loss: 0.6333 - regression_loss: 0.5115 - classification_loss: 0.1218
 5142/10000 [==============>...............] - ETA: 37:00 - loss: 0.6333 - regression_loss: 0.5115 - classification_loss: 0.1218
 5143/10000 [==============>...............] - ETA: 36:59 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 5144/10000 [==============>...............] - ETA: 36:59 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 5145/10000 [==============>...............] - ETA: 36:58 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 5146/10000 [==============>...............] - ETA: 36:58 - loss: 0.6333 - regression_loss: 0.5114 - classification_loss: 0.1218
 5147/10000 [==============>...............] - ETA: 36:57 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 5148/10000 [==============>...............] - ETA: 36:57 - loss: 0.6333 - regression_loss: 0.5115 - classification_loss: 0.1218
 5149/10000 [==============>...............] - ETA: 36:56 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 5150/10000 [==============>...............] - ETA: 36:56 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 5151/10000 [==============>...............] - ETA: 36:55 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 5152/10000 [==============>...............] - ETA: 36:55 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 5153/10000 [==============>...............] - ETA: 36:54 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 5154/10000 [==============>...............] - ETA: 36:54 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 5155/10000 [==============>...............] - ETA: 36:54 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 5156/10000 [==============>...............] - ETA: 36:53 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 5157/10000 [==============>...............] - ETA: 36:53 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 5158/10000 [==============>...............] - ETA: 36:52 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 5159/10000 [==============>...............] - ETA: 36:52 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 5160/10000 [==============>...............] - ETA: 36:51 - loss: 0.6331 - regression_loss: 0.5114 - classification_loss: 0.1218
 5161/10000 [==============>...............] - ETA: 36:51 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 5162/10000 [==============>...............] - ETA: 36:50 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1217
 5163/10000 [==============>...............] - ETA: 36:50 - loss: 0.6331 - regression_loss: 0.5114 - classification_loss: 0.1217
 5164/10000 [==============>...............] - ETA: 36:49 - loss: 0.6330 - regression_loss: 0.5113 - classification_loss: 0.1217
 5165/10000 [==============>...............] - ETA: 36:49 - loss: 0.6331 - regression_loss: 0.5114 - classification_loss: 0.1217
 5166/10000 [==============>...............] - ETA: 36:49 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1217
 5167/10000 [==============>...............] - ETA: 36:48 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1217
 5168/10000 [==============>...............] - ETA: 36:48 - loss: 0.6330 - regression_loss: 0.5113 - classification_loss: 0.1217
 5169/10000 [==============>...............] - ETA: 36:47 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 5170/10000 [==============>...............] - ETA: 36:47 - loss: 0.6331 - regression_loss: 0.5114 - classification_loss: 0.1218
 5171/10000 [==============>...............] - ETA: 36:46 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 5172/10000 [==============>...............] - ETA: 36:46 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 5173/10000 [==============>...............] - ETA: 36:45 - loss: 0.6331 - regression_loss: 0.5114 - classification_loss: 0.1218
 5174/10000 [==============>...............] - ETA: 36:45 - loss: 0.6331 - regression_loss: 0.5114 - classification_loss: 0.1217
 5175/10000 [==============>...............] - ETA: 36:44 - loss: 0.6332 - regression_loss: 0.5114 - classification_loss: 0.1218
 5176/10000 [==============>...............] - ETA: 36:44 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1218
 5177/10000 [==============>...............] - ETA: 36:44 - loss: 0.6331 - regression_loss: 0.5113 - classification_loss: 0.1217
 5178/10000 [==============>...............] - ETA: 36:43 - loss: 0.6330 - regression_loss: 0.5113 - classification_loss: 0.1217
 5179/10000 [==============>...............] - ETA: 36:43 - loss: 0.6329 - regression_loss: 0.5112 - classification_loss: 0.1217
 5180/10000 [==============>...............] - ETA: 36:42 - loss: 0.6329 - regression_loss: 0.5112 - classification_loss: 0.1217
 5181/10000 [==============>...............] - ETA: 36:42 - loss: 0.6328 - regression_loss: 0.5112 - classification_loss: 0.1217
 5182/10000 [==============>...............] - ETA: 36:41 - loss: 0.6329 - regression_loss: 0.5112 - classification_loss: 0.1217
 5183/10000 [==============>...............] - ETA: 36:41 - loss: 0.6330 - regression_loss: 0.5113 - classification_loss: 0.1217
 5184/10000 [==============>...............] - ETA: 36:40 - loss: 0.6330 - regression_loss: 0.5114 - classification_loss: 0.1217
 5185/10000 [==============>...............] - ETA: 36:40 - loss: 0.6330 - regression_loss: 0.5114 - classification_loss: 0.1217
 5186/10000 [==============>...............] - ETA: 36:40 - loss: 0.6330 - regression_loss: 0.5113 - classification_loss: 0.1217
 5187/10000 [==============>...............] - ETA: 36:39 - loss: 0.6330 - regression_loss: 0.5113 - classification_loss: 0.1217
 5188/10000 [==============>...............] - ETA: 36:39 - loss: 0.6329 - regression_loss: 0.5113 - classification_loss: 0.1217
 5189/10000 [==============>...............] - ETA: 36:38 - loss: 0.6330 - regression_loss: 0.5113 - classification_loss: 0.1217
 5190/10000 [==============>...............] - ETA: 36:38 - loss: 0.6330 - regression_loss: 0.5113 - classification_loss: 0.1217
 5191/10000 [==============>...............] - ETA: 36:37 - loss: 0.6329 - regression_loss: 0.5113 - classification_loss: 0.1217
 5192/10000 [==============>...............] - ETA: 36:37 - loss: 0.6329 - regression_loss: 0.5112 - classification_loss: 0.1217
 5193/10000 [==============>...............] - ETA: 36:36 - loss: 0.6329 - regression_loss: 0.5112 - classification_loss: 0.1217
 5194/10000 [==============>...............] - ETA: 36:36 - loss: 0.6328 - regression_loss: 0.5112 - classification_loss: 0.1217
 5195/10000 [==============>...............] - ETA: 36:35 - loss: 0.6328 - regression_loss: 0.5111 - classification_loss: 0.1216
 5196/10000 [==============>...............] - ETA: 36:35 - loss: 0.6328 - regression_loss: 0.5111 - classification_loss: 0.1217
 5197/10000 [==============>...............] - ETA: 36:34 - loss: 0.6327 - regression_loss: 0.5110 - classification_loss: 0.1216
 5198/10000 [==============>...............] - ETA: 36:34 - loss: 0.6326 - regression_loss: 0.5110 - classification_loss: 0.1216
 5199/10000 [==============>...............] - ETA: 36:34 - loss: 0.6326 - regression_loss: 0.5109 - classification_loss: 0.1216
 5200/10000 [==============>...............] - ETA: 36:33 - loss: 0.6325 - regression_loss: 0.5109 - classification_loss: 0.1216
 5201/10000 [==============>...............] - ETA: 36:33 - loss: 0.6326 - regression_loss: 0.5110 - classification_loss: 0.1216
 5202/10000 [==============>...............] - ETA: 36:32 - loss: 0.6325 - regression_loss: 0.5109 - classification_loss: 0.1216
 5203/10000 [==============>...............] - ETA: 36:32 - loss: 0.6326 - regression_loss: 0.5110 - classification_loss: 0.1216
 5204/10000 [==============>...............] - ETA: 36:31 - loss: 0.6325 - regression_loss: 0.5109 - classification_loss: 0.1216
 5205/10000 [==============>...............] - ETA: 36:31 - loss: 0.6326 - regression_loss: 0.5110 - classification_loss: 0.1216
 5206/10000 [==============>...............] - ETA: 36:30 - loss: 0.6326 - regression_loss: 0.5110 - classification_loss: 0.1216
 5207/10000 [==============>...............] - ETA: 36:30 - loss: 0.6326 - regression_loss: 0.5109 - classification_loss: 0.1216
 5208/10000 [==============>...............] - ETA: 36:29 - loss: 0.6325 - regression_loss: 0.5109 - classification_loss: 0.1216
 5209/10000 [==============>...............] - ETA: 36:29 - loss: 0.6325 - regression_loss: 0.5109 - classification_loss: 0.1216
 5210/10000 [==============>...............] - ETA: 36:29 - loss: 0.6325 - regression_loss: 0.5109 - classification_loss: 0.1216
 5211/10000 [==============>...............] - ETA: 36:28 - loss: 0.6326 - regression_loss: 0.5110 - classification_loss: 0.1216
 5212/10000 [==============>...............] - ETA: 36:28 - loss: 0.6326 - regression_loss: 0.5110 - classification_loss: 0.1216
 5213/10000 [==============>...............] - ETA: 36:27 - loss: 0.6327 - regression_loss: 0.5110 - classification_loss: 0.1217
 5214/10000 [==============>...............] - ETA: 36:27 - loss: 0.6326 - regression_loss: 0.5110 - classification_loss: 0.1216
 5215/10000 [==============>...............] - ETA: 36:26 - loss: 0.6327 - regression_loss: 0.5110 - classification_loss: 0.1216
 5216/10000 [==============>...............] - ETA: 36:26 - loss: 0.6327 - regression_loss: 0.5111 - classification_loss: 0.1216
 5217/10000 [==============>...............] - ETA: 36:25 - loss: 0.6326 - regression_loss: 0.5110 - classification_loss: 0.1216
 5218/10000 [==============>...............] - ETA: 36:25 - loss: 0.6327 - regression_loss: 0.5110 - classification_loss: 0.1216
 5219/10000 [==============>...............] - ETA: 36:24 - loss: 0.6327 - regression_loss: 0.5111 - classification_loss: 0.1216
 5220/10000 [==============>...............] - ETA: 36:24 - loss: 0.6327 - regression_loss: 0.5111 - classification_loss: 0.1216
 5221/10000 [==============>...............] - ETA: 36:23 - loss: 0.6328 - regression_loss: 0.5111 - classification_loss: 0.1216
 5222/10000 [==============>...............] - ETA: 36:23 - loss: 0.6328 - regression_loss: 0.5112 - classification_loss: 0.1216
 5223/10000 [==============>...............] - ETA: 36:23 - loss: 0.6328 - regression_loss: 0.5112 - classification_loss: 0.1216
 5224/10000 [==============>...............] - ETA: 36:22 - loss: 0.6328 - regression_loss: 0.5112 - classification_loss: 0.1216
 5225/10000 [==============>...............] - ETA: 36:22 - loss: 0.6327 - regression_loss: 0.5111 - classification_loss: 0.1216
 5226/10000 [==============>...............] - ETA: 36:21 - loss: 0.6326 - regression_loss: 0.5110 - classification_loss: 0.1216
 5227/10000 [==============>...............] - ETA: 36:21 - loss: 0.6328 - regression_loss: 0.5112 - classification_loss: 0.1217
 5228/10000 [==============>...............] - ETA: 36:20 - loss: 0.6328 - regression_loss: 0.5111 - classification_loss: 0.1216
 5229/10000 [==============>...............] - ETA: 36:20 - loss: 0.6330 - regression_loss: 0.5113 - classification_loss: 0.1217
 5230/10000 [==============>...............] - ETA: 36:19 - loss: 0.6331 - regression_loss: 0.5114 - classification_loss: 0.1217
 5231/10000 [==============>...............] - ETA: 36:19 - loss: 0.6331 - regression_loss: 0.5114 - classification_loss: 0.1217
 5232/10000 [==============>...............] - ETA: 36:18 - loss: 0.6331 - regression_loss: 0.5114 - classification_loss: 0.1217
 5233/10000 [==============>...............] - ETA: 36:18 - loss: 0.6331 - regression_loss: 0.5114 - classification_loss: 0.1217
 5234/10000 [==============>...............] - ETA: 36:18 - loss: 0.6330 - regression_loss: 0.5113 - classification_loss: 0.1217
 5235/10000 [==============>...............] - ETA: 36:17 - loss: 0.6330 - regression_loss: 0.5113 - classification_loss: 0.1217
 5236/10000 [==============>...............] - ETA: 36:17 - loss: 0.6329 - regression_loss: 0.5113 - classification_loss: 0.1217
 5237/10000 [==============>...............] - ETA: 36:16 - loss: 0.6329 - regression_loss: 0.5112 - classification_loss: 0.1216
 5238/10000 [==============>...............] - ETA: 36:16 - loss: 0.6329 - regression_loss: 0.5113 - classification_loss: 0.1216
 5239/10000 [==============>...............] - ETA: 36:15 - loss: 0.6329 - regression_loss: 0.5112 - classification_loss: 0.1216
 5240/10000 [==============>...............] - ETA: 36:15 - loss: 0.6330 - regression_loss: 0.5113 - classification_loss: 0.1216
 5241/10000 [==============>...............] - ETA: 36:14 - loss: 0.6330 - regression_loss: 0.5113 - classification_loss: 0.1216
 5242/10000 [==============>...............] - ETA: 36:14 - loss: 0.6330 - regression_loss: 0.5113 - classification_loss: 0.1216
 5243/10000 [==============>...............] - ETA: 36:13 - loss: 0.6329 - regression_loss: 0.5113 - classification_loss: 0.1216
 5244/10000 [==============>...............] - ETA: 36:13 - loss: 0.6328 - regression_loss: 0.5112 - classification_loss: 0.1216
 5245/10000 [==============>...............] - ETA: 36:12 - loss: 0.6328 - regression_loss: 0.5112 - classification_loss: 0.1216
 5246/10000 [==============>...............] - ETA: 36:12 - loss: 0.6328 - regression_loss: 0.5112 - classification_loss: 0.1216
 5247/10000 [==============>...............] - ETA: 36:12 - loss: 0.6328 - regression_loss: 0.5113 - classification_loss: 0.1216
 5248/10000 [==============>...............] - ETA: 36:11 - loss: 0.6328 - regression_loss: 0.5112 - classification_loss: 0.1216
 5249/10000 [==============>...............] - ETA: 36:11 - loss: 0.6327 - regression_loss: 0.5112 - classification_loss: 0.1216
 5250/10000 [==============>...............] - ETA: 36:10 - loss: 0.6326 - regression_loss: 0.5111 - classification_loss: 0.1215
 5251/10000 [==============>...............] - ETA: 36:10 - loss: 0.6326 - regression_loss: 0.5111 - classification_loss: 0.1215
 5252/10000 [==============>...............] - ETA: 36:09 - loss: 0.6326 - regression_loss: 0.5111 - classification_loss: 0.1215
 5253/10000 [==============>...............] - ETA: 36:09 - loss: 0.6325 - regression_loss: 0.5110 - classification_loss: 0.1215
 5254/10000 [==============>...............] - ETA: 36:08 - loss: 0.6326 - regression_loss: 0.5111 - classification_loss: 0.1215
 5255/10000 [==============>...............] - ETA: 36:08 - loss: 0.6326 - regression_loss: 0.5111 - classification_loss: 0.1215
 5256/10000 [==============>...............] - ETA: 36:08 - loss: 0.6325 - regression_loss: 0.5110 - classification_loss: 0.1215
 5257/10000 [==============>...............] - ETA: 36:07 - loss: 0.6324 - regression_loss: 0.5109 - classification_loss: 0.1215
 5258/10000 [==============>...............] - ETA: 36:07 - loss: 0.6326 - regression_loss: 0.5111 - classification_loss: 0.1215
 5259/10000 [==============>...............] - ETA: 36:06 - loss: 0.6326 - regression_loss: 0.5111 - classification_loss: 0.1215
 5260/10000 [==============>...............] - ETA: 36:06 - loss: 0.6325 - regression_loss: 0.5110 - classification_loss: 0.1215
 5261/10000 [==============>...............] - ETA: 36:05 - loss: 0.6325 - regression_loss: 0.5110 - classification_loss: 0.1215
 5262/10000 [==============>...............] - ETA: 36:05 - loss: 0.6324 - regression_loss: 0.5109 - classification_loss: 0.1215
 5263/10000 [==============>...............] - ETA: 36:04 - loss: 0.6323 - regression_loss: 0.5109 - classification_loss: 0.1214
 5264/10000 [==============>...............] - ETA: 36:04 - loss: 0.6323 - regression_loss: 0.5109 - classification_loss: 0.1214
 5265/10000 [==============>...............] - ETA: 36:03 - loss: 0.6323 - regression_loss: 0.5109 - classification_loss: 0.1214
 5266/10000 [==============>...............] - ETA: 36:03 - loss: 0.6324 - regression_loss: 0.5110 - classification_loss: 0.1214
 5267/10000 [==============>...............] - ETA: 36:02 - loss: 0.6324 - regression_loss: 0.5109 - classification_loss: 0.1214
 5268/10000 [==============>...............] - ETA: 36:02 - loss: 0.6325 - regression_loss: 0.5110 - classification_loss: 0.1214
 5269/10000 [==============>...............] - ETA: 36:02 - loss: 0.6324 - regression_loss: 0.5110 - classification_loss: 0.1214
 5270/10000 [==============>...............] - ETA: 36:01 - loss: 0.6324 - regression_loss: 0.5110 - classification_loss: 0.1214
 5271/10000 [==============>...............] - ETA: 36:01 - loss: 0.6323 - regression_loss: 0.5110 - classification_loss: 0.1214
 5272/10000 [==============>...............] - ETA: 36:00 - loss: 0.6324 - regression_loss: 0.5111 - classification_loss: 0.1214
 5273/10000 [==============>...............] - ETA: 36:00 - loss: 0.6324 - regression_loss: 0.5110 - classification_loss: 0.1214
 5274/10000 [==============>...............] - ETA: 35:59 - loss: 0.6324 - regression_loss: 0.5110 - classification_loss: 0.1214
 5275/10000 [==============>...............] - ETA: 35:59 - loss: 0.6323 - regression_loss: 0.5109 - classification_loss: 0.1214
 5276/10000 [==============>...............] - ETA: 35:58 - loss: 0.6322 - regression_loss: 0.5109 - classification_loss: 0.1213
 5277/10000 [==============>...............] - ETA: 35:58 - loss: 0.6321 - regression_loss: 0.5108 - classification_loss: 0.1213
 5278/10000 [==============>...............] - ETA: 35:57 - loss: 0.6322 - regression_loss: 0.5109 - classification_loss: 0.1213
 5279/10000 [==============>...............] - ETA: 35:57 - loss: 0.6323 - regression_loss: 0.5109 - classification_loss: 0.1213
 5280/10000 [==============>...............] - ETA: 35:56 - loss: 0.6323 - regression_loss: 0.5110 - classification_loss: 0.1213
 5281/10000 [==============>...............] - ETA: 35:56 - loss: 0.6322 - regression_loss: 0.5109 - classification_loss: 0.1213
 5282/10000 [==============>...............] - ETA: 35:56 - loss: 0.6324 - regression_loss: 0.5110 - classification_loss: 0.1214
 5283/10000 [==============>...............] - ETA: 35:55 - loss: 0.6323 - regression_loss: 0.5110 - classification_loss: 0.1214
 5284/10000 [==============>...............] - ETA: 35:55 - loss: 0.6324 - regression_loss: 0.5110 - classification_loss: 0.1214
 5285/10000 [==============>...............] - ETA: 35:54 - loss: 0.6325 - regression_loss: 0.5111 - classification_loss: 0.1214
 5286/10000 [==============>...............] - ETA: 35:54 - loss: 0.6325 - regression_loss: 0.5111 - classification_loss: 0.1214
 5287/10000 [==============>...............] - ETA: 35:53 - loss: 0.6324 - regression_loss: 0.5110 - classification_loss: 0.1214
 5288/10000 [==============>...............] - ETA: 35:53 - loss: 0.6323 - regression_loss: 0.5110 - classification_loss: 0.1214
 5289/10000 [==============>...............] - ETA: 35:52 - loss: 0.6323 - regression_loss: 0.5109 - classification_loss: 0.1214
 5290/10000 [==============>...............] - ETA: 35:52 - loss: 0.6324 - regression_loss: 0.5110 - classification_loss: 0.1214
 5291/10000 [==============>...............] - ETA: 35:51 - loss: 0.6324 - regression_loss: 0.5110 - classification_loss: 0.1214
 5292/10000 [==============>...............] - ETA: 35:51 - loss: 0.6323 - regression_loss: 0.5110 - classification_loss: 0.1214
 5293/10000 [==============>...............] - ETA: 35:50 - loss: 0.6324 - regression_loss: 0.5110 - classification_loss: 0.1214
 5294/10000 [==============>...............] - ETA: 35:50 - loss: 0.6323 - regression_loss: 0.5110 - classification_loss: 0.1214
 5295/10000 [==============>...............] - ETA: 35:50 - loss: 0.6325 - regression_loss: 0.5111 - classification_loss: 0.1214
 5296/10000 [==============>...............] - ETA: 35:49 - loss: 0.6323 - regression_loss: 0.5110 - classification_loss: 0.1214
 5297/10000 [==============>...............] - ETA: 35:49 - loss: 0.6323 - regression_loss: 0.5109 - classification_loss: 0.1214
 5298/10000 [==============>...............] - ETA: 35:48 - loss: 0.6322 - regression_loss: 0.5109 - classification_loss: 0.1213
 5299/10000 [==============>...............] - ETA: 35:48 - loss: 0.6322 - regression_loss: 0.5109 - classification_loss: 0.1213
 5300/10000 [==============>...............] - ETA: 35:47 - loss: 0.6322 - regression_loss: 0.5109 - classification_loss: 0.1213
 5301/10000 [==============>...............] - ETA: 35:47 - loss: 0.6323 - regression_loss: 0.5110 - classification_loss: 0.1213
 5302/10000 [==============>...............] - ETA: 35:46 - loss: 0.6322 - regression_loss: 0.5109 - classification_loss: 0.1213
 5303/10000 [==============>...............] - ETA: 35:46 - loss: 0.6322 - regression_loss: 0.5109 - classification_loss: 0.1213
 5304/10000 [==============>...............] - ETA: 35:45 - loss: 0.6321 - regression_loss: 0.5108 - classification_loss: 0.1213
 5305/10000 [==============>...............] - ETA: 35:45 - loss: 0.6321 - regression_loss: 0.5108 - classification_loss: 0.1213
 5306/10000 [==============>...............] - ETA: 35:44 - loss: 0.6321 - regression_loss: 0.5108 - classification_loss: 0.1213
 5307/10000 [==============>...............] - ETA: 35:44 - loss: 0.6321 - regression_loss: 0.5108 - classification_loss: 0.1213
 5308/10000 [==============>...............] - ETA: 35:44 - loss: 0.6323 - regression_loss: 0.5110 - classification_loss: 0.1213
 5309/10000 [==============>...............] - ETA: 35:43 - loss: 0.6323 - regression_loss: 0.5110 - classification_loss: 0.1213
 5310/10000 [==============>...............] - ETA: 35:43 - loss: 0.6323 - regression_loss: 0.5110 - classification_loss: 0.1213
 5311/10000 [==============>...............] - ETA: 35:42 - loss: 0.6324 - regression_loss: 0.5111 - classification_loss: 0.1213
 5312/10000 [==============>...............] - ETA: 35:42 - loss: 0.6325 - regression_loss: 0.5112 - classification_loss: 0.1213
 5313/10000 [==============>...............] - ETA: 35:41 - loss: 0.6325 - regression_loss: 0.5112 - classification_loss: 0.1213
 5314/10000 [==============>...............] - ETA: 35:41 - loss: 0.6324 - regression_loss: 0.5111 - classification_loss: 0.1213
 5315/10000 [==============>...............] - ETA: 35:40 - loss: 0.6324 - regression_loss: 0.5112 - classification_loss: 0.1213
 5316/10000 [==============>...............] - ETA: 35:40 - loss: 0.6324 - regression_loss: 0.5111 - classification_loss: 0.1213
 5317/10000 [==============>...............] - ETA: 35:39 - loss: 0.6323 - regression_loss: 0.5110 - classification_loss: 0.1213
 5318/10000 [==============>...............] - ETA: 35:39 - loss: 0.6322 - regression_loss: 0.5110 - classification_loss: 0.1213
 5319/10000 [==============>...............] - ETA: 35:39 - loss: 0.6323 - regression_loss: 0.5110 - classification_loss: 0.1213
 5320/10000 [==============>...............] - ETA: 35:38 - loss: 0.6322 - regression_loss: 0.5109 - classification_loss: 0.1213
 5321/10000 [==============>...............] - ETA: 35:38 - loss: 0.6321 - regression_loss: 0.5109 - classification_loss: 0.1212
 5322/10000 [==============>...............] - ETA: 35:37 - loss: 0.6322 - regression_loss: 0.5110 - classification_loss: 0.1213
 5323/10000 [==============>...............] - ETA: 35:37 - loss: 0.6322 - regression_loss: 0.5109 - classification_loss: 0.1213
 5324/10000 [==============>...............] - ETA: 35:36 - loss: 0.6321 - regression_loss: 0.5109 - classification_loss: 0.1212
 5325/10000 [==============>...............] - ETA: 35:36 - loss: 0.6321 - regression_loss: 0.5109 - classification_loss: 0.1212
 5326/10000 [==============>...............] - ETA: 35:35 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5327/10000 [==============>...............] - ETA: 35:35 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5328/10000 [==============>...............] - ETA: 35:34 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5329/10000 [==============>...............] - ETA: 35:34 - loss: 0.6321 - regression_loss: 0.5109 - classification_loss: 0.1212
 5330/10000 [==============>...............] - ETA: 35:33 - loss: 0.6321 - regression_loss: 0.5109 - classification_loss: 0.1212
 5331/10000 [==============>...............] - ETA: 35:33 - loss: 0.6321 - regression_loss: 0.5108 - classification_loss: 0.1212
 5332/10000 [==============>...............] - ETA: 35:33 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5333/10000 [==============>...............] - ETA: 35:32 - loss: 0.6321 - regression_loss: 0.5109 - classification_loss: 0.1212
 5334/10000 [===============>..............] - ETA: 35:32 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5335/10000 [===============>..............] - ETA: 35:31 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5336/10000 [===============>..............] - ETA: 35:31 - loss: 0.6320 - regression_loss: 0.5107 - classification_loss: 0.1212
 5337/10000 [===============>..............] - ETA: 35:30 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5338/10000 [===============>..............] - ETA: 35:30 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5339/10000 [===============>..............] - ETA: 35:29 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5340/10000 [===============>..............] - ETA: 35:29 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5341/10000 [===============>..............] - ETA: 35:29 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5342/10000 [===============>..............] - ETA: 35:28 - loss: 0.6321 - regression_loss: 0.5109 - classification_loss: 0.1212
 5343/10000 [===============>..............] - ETA: 35:28 - loss: 0.6321 - regression_loss: 0.5108 - classification_loss: 0.1212
 5344/10000 [===============>..............] - ETA: 35:27 - loss: 0.6321 - regression_loss: 0.5109 - classification_loss: 0.1212
 5345/10000 [===============>..............] - ETA: 35:27 - loss: 0.6321 - regression_loss: 0.5109 - classification_loss: 0.1212
 5346/10000 [===============>..............] - ETA: 35:26 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5347/10000 [===============>..............] - ETA: 35:26 - loss: 0.6319 - regression_loss: 0.5107 - classification_loss: 0.1212
 5348/10000 [===============>..............] - ETA: 35:25 - loss: 0.6319 - regression_loss: 0.5107 - classification_loss: 0.1212
 5349/10000 [===============>..............] - ETA: 35:25 - loss: 0.6318 - regression_loss: 0.5106 - classification_loss: 0.1212
 5350/10000 [===============>..............] - ETA: 35:24 - loss: 0.6318 - regression_loss: 0.5106 - classification_loss: 0.1212
 5351/10000 [===============>..............] - ETA: 35:24 - loss: 0.6319 - regression_loss: 0.5107 - classification_loss: 0.1212
 5352/10000 [===============>..............] - ETA: 35:23 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5353/10000 [===============>..............] - ETA: 35:23 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5354/10000 [===============>..............] - ETA: 35:23 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5355/10000 [===============>..............] - ETA: 35:22 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5356/10000 [===============>..............] - ETA: 35:22 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5357/10000 [===============>..............] - ETA: 35:21 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5358/10000 [===============>..............] - ETA: 35:21 - loss: 0.6320 - regression_loss: 0.5109 - classification_loss: 0.1212
 5359/10000 [===============>..............] - ETA: 35:20 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5360/10000 [===============>..............] - ETA: 35:20 - loss: 0.6319 - regression_loss: 0.5107 - classification_loss: 0.1212
 5361/10000 [===============>..............] - ETA: 35:19 - loss: 0.6319 - regression_loss: 0.5107 - classification_loss: 0.1212
 5362/10000 [===============>..............] - ETA: 35:19 - loss: 0.6323 - regression_loss: 0.5110 - classification_loss: 0.1213
 5363/10000 [===============>..............] - ETA: 35:18 - loss: 0.6323 - regression_loss: 0.5110 - classification_loss: 0.1213
 5364/10000 [===============>..............] - ETA: 35:18 - loss: 0.6323 - regression_loss: 0.5110 - classification_loss: 0.1213
 5365/10000 [===============>..............] - ETA: 35:17 - loss: 0.6322 - regression_loss: 0.5109 - classification_loss: 0.1213
 5366/10000 [===============>..............] - ETA: 35:17 - loss: 0.6322 - regression_loss: 0.5109 - classification_loss: 0.1213
 5367/10000 [===============>..............] - ETA: 35:17 - loss: 0.6321 - regression_loss: 0.5108 - classification_loss: 0.1212
 5368/10000 [===============>..............] - ETA: 35:16 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5369/10000 [===============>..............] - ETA: 35:16 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5370/10000 [===============>..............] - ETA: 35:15 - loss: 0.6322 - regression_loss: 0.5109 - classification_loss: 0.1212
 5371/10000 [===============>..............] - ETA: 35:15 - loss: 0.6322 - regression_loss: 0.5109 - classification_loss: 0.1213
 5372/10000 [===============>..............] - ETA: 35:14 - loss: 0.6321 - regression_loss: 0.5108 - classification_loss: 0.1212
 5373/10000 [===============>..............] - ETA: 35:14 - loss: 0.6321 - regression_loss: 0.5109 - classification_loss: 0.1212
 5374/10000 [===============>..............] - ETA: 35:13 - loss: 0.6321 - regression_loss: 0.5108 - classification_loss: 0.1213
 5375/10000 [===============>..............] - ETA: 35:13 - loss: 0.6321 - regression_loss: 0.5108 - classification_loss: 0.1213
 5376/10000 [===============>..............] - ETA: 35:12 - loss: 0.6321 - regression_loss: 0.5108 - classification_loss: 0.1213
 5377/10000 [===============>..............] - ETA: 35:12 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5378/10000 [===============>..............] - ETA: 35:12 - loss: 0.6321 - regression_loss: 0.5108 - classification_loss: 0.1212
 5379/10000 [===============>..............] - ETA: 35:11 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5380/10000 [===============>..............] - ETA: 35:11 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5381/10000 [===============>..............] - ETA: 35:10 - loss: 0.6320 - regression_loss: 0.5107 - classification_loss: 0.1212
 5382/10000 [===============>..............] - ETA: 35:10 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5383/10000 [===============>..............] - ETA: 35:09 - loss: 0.6322 - regression_loss: 0.5109 - classification_loss: 0.1212
 5384/10000 [===============>..............] - ETA: 35:09 - loss: 0.6322 - regression_loss: 0.5109 - classification_loss: 0.1212
 5385/10000 [===============>..............] - ETA: 35:08 - loss: 0.6322 - regression_loss: 0.5109 - classification_loss: 0.1212
 5386/10000 [===============>..............] - ETA: 35:08 - loss: 0.6321 - regression_loss: 0.5109 - classification_loss: 0.1212
 5387/10000 [===============>..............] - ETA: 35:07 - loss: 0.6322 - regression_loss: 0.5109 - classification_loss: 0.1212
 5388/10000 [===============>..............] - ETA: 35:07 - loss: 0.6321 - regression_loss: 0.5109 - classification_loss: 0.1212
 5389/10000 [===============>..............] - ETA: 35:07 - loss: 0.6321 - regression_loss: 0.5109 - classification_loss: 0.1212
 5390/10000 [===============>..............] - ETA: 35:06 - loss: 0.6321 - regression_loss: 0.5108 - classification_loss: 0.1212
 5391/10000 [===============>..............] - ETA: 35:06 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5392/10000 [===============>..............] - ETA: 35:05 - loss: 0.6319 - regression_loss: 0.5107 - classification_loss: 0.1212
 5393/10000 [===============>..............] - ETA: 35:05 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5394/10000 [===============>..............] - ETA: 35:04 - loss: 0.6319 - regression_loss: 0.5107 - classification_loss: 0.1212
 5395/10000 [===============>..............] - ETA: 35:04 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5396/10000 [===============>..............] - ETA: 35:03 - loss: 0.6319 - regression_loss: 0.5107 - classification_loss: 0.1212
 5397/10000 [===============>..............] - ETA: 35:03 - loss: 0.6319 - regression_loss: 0.5107 - classification_loss: 0.1212
 5398/10000 [===============>..............] - ETA: 35:03 - loss: 0.6319 - regression_loss: 0.5107 - classification_loss: 0.1212
 5399/10000 [===============>..............] - ETA: 35:02 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5400/10000 [===============>..............] - ETA: 35:02 - loss: 0.6319 - regression_loss: 0.5107 - classification_loss: 0.1212
 5401/10000 [===============>..............] - ETA: 35:01 - loss: 0.6318 - regression_loss: 0.5107 - classification_loss: 0.1211
 5402/10000 [===============>..............] - ETA: 35:01 - loss: 0.6318 - regression_loss: 0.5106 - classification_loss: 0.1211
 5403/10000 [===============>..............] - ETA: 35:00 - loss: 0.6319 - regression_loss: 0.5107 - classification_loss: 0.1211
 5404/10000 [===============>..............] - ETA: 35:00 - loss: 0.6319 - regression_loss: 0.5107 - classification_loss: 0.1212
 5405/10000 [===============>..............] - ETA: 34:59 - loss: 0.6318 - regression_loss: 0.5107 - classification_loss: 0.1211
 5406/10000 [===============>..............] - ETA: 34:59 - loss: 0.6318 - regression_loss: 0.5107 - classification_loss: 0.1211
 5407/10000 [===============>..............] - ETA: 34:58 - loss: 0.6318 - regression_loss: 0.5107 - classification_loss: 0.1211
 5408/10000 [===============>..............] - ETA: 34:58 - loss: 0.6319 - regression_loss: 0.5107 - classification_loss: 0.1211
 5409/10000 [===============>..............] - ETA: 34:57 - loss: 0.6318 - regression_loss: 0.5107 - classification_loss: 0.1211
 5410/10000 [===============>..............] - ETA: 34:57 - loss: 0.6319 - regression_loss: 0.5108 - classification_loss: 0.1211
 5411/10000 [===============>..............] - ETA: 34:56 - loss: 0.6321 - regression_loss: 0.5109 - classification_loss: 0.1212
 5412/10000 [===============>..............] - ETA: 34:56 - loss: 0.6321 - regression_loss: 0.5110 - classification_loss: 0.1212
 5413/10000 [===============>..............] - ETA: 34:55 - loss: 0.6321 - regression_loss: 0.5109 - classification_loss: 0.1212
 5414/10000 [===============>..............] - ETA: 34:55 - loss: 0.6321 - regression_loss: 0.5109 - classification_loss: 0.1211
 5415/10000 [===============>..............] - ETA: 34:55 - loss: 0.6320 - regression_loss: 0.5109 - classification_loss: 0.1211
 5416/10000 [===============>..............] - ETA: 34:54 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1211
 5417/10000 [===============>..............] - ETA: 34:54 - loss: 0.6319 - regression_loss: 0.5108 - classification_loss: 0.1211
 5418/10000 [===============>..............] - ETA: 34:53 - loss: 0.6319 - regression_loss: 0.5108 - classification_loss: 0.1211
 5419/10000 [===============>..............] - ETA: 34:53 - loss: 0.6319 - regression_loss: 0.5108 - classification_loss: 0.1211
 5420/10000 [===============>..............] - ETA: 34:52 - loss: 0.6318 - regression_loss: 0.5107 - classification_loss: 0.1211
 5421/10000 [===============>..............] - ETA: 34:52 - loss: 0.6318 - regression_loss: 0.5107 - classification_loss: 0.1211
 5422/10000 [===============>..............] - ETA: 34:51 - loss: 0.6318 - regression_loss: 0.5107 - classification_loss: 0.1211
 5423/10000 [===============>..............] - ETA: 34:51 - loss: 0.6317 - regression_loss: 0.5106 - classification_loss: 0.1211
 5424/10000 [===============>..............] - ETA: 34:51 - loss: 0.6316 - regression_loss: 0.5106 - classification_loss: 0.1211
 5425/10000 [===============>..............] - ETA: 34:50 - loss: 0.6316 - regression_loss: 0.5106 - classification_loss: 0.1211
 5426/10000 [===============>..............] - ETA: 34:50 - loss: 0.6316 - regression_loss: 0.5105 - classification_loss: 0.1211
 5427/10000 [===============>..............] - ETA: 34:49 - loss: 0.6316 - regression_loss: 0.5105 - classification_loss: 0.1211
 5428/10000 [===============>..............] - ETA: 34:49 - loss: 0.6315 - regression_loss: 0.5104 - classification_loss: 0.1211
 5429/10000 [===============>..............] - ETA: 34:48 - loss: 0.6315 - regression_loss: 0.5104 - classification_loss: 0.1211
 5430/10000 [===============>..............] - ETA: 34:48 - loss: 0.6314 - regression_loss: 0.5104 - classification_loss: 0.1211
 5431/10000 [===============>..............] - ETA: 34:47 - loss: 0.6314 - regression_loss: 0.5103 - classification_loss: 0.1211
 5432/10000 [===============>..............] - ETA: 34:47 - loss: 0.6316 - regression_loss: 0.5104 - classification_loss: 0.1211
 5433/10000 [===============>..............] - ETA: 34:46 - loss: 0.6317 - regression_loss: 0.5106 - classification_loss: 0.1211
 5434/10000 [===============>..............] - ETA: 34:46 - loss: 0.6317 - regression_loss: 0.5106 - classification_loss: 0.1211
 5435/10000 [===============>..............] - ETA: 34:45 - loss: 0.6318 - regression_loss: 0.5106 - classification_loss: 0.1211
 5436/10000 [===============>..............] - ETA: 34:45 - loss: 0.6317 - regression_loss: 0.5106 - classification_loss: 0.1211
 5437/10000 [===============>..............] - ETA: 34:44 - loss: 0.6317 - regression_loss: 0.5106 - classification_loss: 0.1211
 5438/10000 [===============>..............] - ETA: 34:44 - loss: 0.6317 - regression_loss: 0.5105 - classification_loss: 0.1211
 5439/10000 [===============>..............] - ETA: 34:44 - loss: 0.6317 - regression_loss: 0.5106 - classification_loss: 0.1211
 5440/10000 [===============>..............] - ETA: 34:43 - loss: 0.6318 - regression_loss: 0.5106 - classification_loss: 0.1211
 5441/10000 [===============>..............] - ETA: 34:43 - loss: 0.6318 - regression_loss: 0.5107 - classification_loss: 0.1211
 5442/10000 [===============>..............] - ETA: 34:42 - loss: 0.6318 - regression_loss: 0.5106 - classification_loss: 0.1211
 5443/10000 [===============>..............] - ETA: 34:42 - loss: 0.6318 - regression_loss: 0.5106 - classification_loss: 0.1211
 5444/10000 [===============>..............] - ETA: 34:41 - loss: 0.6318 - regression_loss: 0.5106 - classification_loss: 0.1211
 5445/10000 [===============>..............] - ETA: 34:41 - loss: 0.6317 - regression_loss: 0.5106 - classification_loss: 0.1211
 5446/10000 [===============>..............] - ETA: 34:40 - loss: 0.6316 - regression_loss: 0.5106 - classification_loss: 0.1211
 5447/10000 [===============>..............] - ETA: 34:40 - loss: 0.6316 - regression_loss: 0.5105 - classification_loss: 0.1211
 5448/10000 [===============>..............] - ETA: 34:40 - loss: 0.6316 - regression_loss: 0.5105 - classification_loss: 0.1211
 5449/10000 [===============>..............] - ETA: 34:39 - loss: 0.6318 - regression_loss: 0.5107 - classification_loss: 0.1211
 5450/10000 [===============>..............] - ETA: 34:39 - loss: 0.6317 - regression_loss: 0.5106 - classification_loss: 0.1211
 5451/10000 [===============>..............] - ETA: 34:38 - loss: 0.6317 - regression_loss: 0.5106 - classification_loss: 0.1211
 5452/10000 [===============>..............] - ETA: 34:38 - loss: 0.6318 - regression_loss: 0.5106 - classification_loss: 0.1212
 5453/10000 [===============>..............] - ETA: 34:37 - loss: 0.6317 - regression_loss: 0.5106 - classification_loss: 0.1212
 5454/10000 [===============>..............] - ETA: 34:37 - loss: 0.6317 - regression_loss: 0.5106 - classification_loss: 0.1211
 5455/10000 [===============>..............] - ETA: 34:36 - loss: 0.6316 - regression_loss: 0.5105 - classification_loss: 0.1211
 5456/10000 [===============>..............] - ETA: 34:36 - loss: 0.6318 - regression_loss: 0.5106 - classification_loss: 0.1211
 5457/10000 [===============>..............] - ETA: 34:35 - loss: 0.6318 - regression_loss: 0.5107 - classification_loss: 0.1211
 5458/10000 [===============>..............] - ETA: 34:35 - loss: 0.6318 - regression_loss: 0.5107 - classification_loss: 0.1211
 5459/10000 [===============>..............] - ETA: 34:35 - loss: 0.6317 - regression_loss: 0.5106 - classification_loss: 0.1211
 5460/10000 [===============>..............] - ETA: 34:34 - loss: 0.6317 - regression_loss: 0.5106 - classification_loss: 0.1211
 5461/10000 [===============>..............] - ETA: 34:34 - loss: 0.6316 - regression_loss: 0.5105 - classification_loss: 0.1211
 5462/10000 [===============>..............] - ETA: 34:33 - loss: 0.6317 - regression_loss: 0.5106 - classification_loss: 0.1211
 5463/10000 [===============>..............] - ETA: 34:33 - loss: 0.6317 - regression_loss: 0.5105 - classification_loss: 0.1211
 5464/10000 [===============>..............] - ETA: 34:32 - loss: 0.6316 - regression_loss: 0.5105 - classification_loss: 0.1211
 5465/10000 [===============>..............] - ETA: 34:32 - loss: 0.6315 - regression_loss: 0.5105 - classification_loss: 0.1211
 5466/10000 [===============>..............] - ETA: 34:31 - loss: 0.6316 - regression_loss: 0.5105 - classification_loss: 0.1211
 5467/10000 [===============>..............] - ETA: 34:31 - loss: 0.6317 - regression_loss: 0.5106 - classification_loss: 0.1211
 5468/10000 [===============>..............] - ETA: 34:30 - loss: 0.6317 - regression_loss: 0.5106 - classification_loss: 0.1211
 5469/10000 [===============>..............] - ETA: 34:30 - loss: 0.6316 - regression_loss: 0.5105 - classification_loss: 0.1211
 5470/10000 [===============>..............] - ETA: 34:29 - loss: 0.6316 - regression_loss: 0.5105 - classification_loss: 0.1211
 5471/10000 [===============>..............] - ETA: 34:29 - loss: 0.6315 - regression_loss: 0.5104 - classification_loss: 0.1211
 5472/10000 [===============>..............] - ETA: 34:29 - loss: 0.6315 - regression_loss: 0.5104 - classification_loss: 0.1211
 5473/10000 [===============>..............] - ETA: 34:28 - loss: 0.6315 - regression_loss: 0.5104 - classification_loss: 0.1211
 5474/10000 [===============>..............] - ETA: 34:28 - loss: 0.6316 - regression_loss: 0.5105 - classification_loss: 0.1211
 5475/10000 [===============>..............] - ETA: 34:27 - loss: 0.6318 - regression_loss: 0.5106 - classification_loss: 0.1211
 5476/10000 [===============>..............] - ETA: 34:27 - loss: 0.6319 - regression_loss: 0.5107 - classification_loss: 0.1212
 5477/10000 [===============>..............] - ETA: 34:26 - loss: 0.6319 - regression_loss: 0.5107 - classification_loss: 0.1212
 5478/10000 [===============>..............] - ETA: 34:26 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5479/10000 [===============>..............] - ETA: 34:25 - loss: 0.6320 - regression_loss: 0.5108 - classification_loss: 0.1212
 5480/10000 [===============>..............] - ETA: 34:25 - loss: 0.6319 - regression_loss: 0.5107 - classification_loss: 0.1212
 5481/10000 [===============>..............] - ETA: 34:25 - loss: 0.6319 - regression_loss: 0.5107 - classification_loss: 0.1212
 5482/10000 [===============>..............] - ETA: 34:24 - loss: 0.6318 - regression_loss: 0.5107 - classification_loss: 0.1212
 5483/10000 [===============>..............] - ETA: 34:24 - loss: 0.6318 - regression_loss: 0.5106 - classification_loss: 0.1212
 5484/10000 [===============>..............] - ETA: 34:23 - loss: 0.6318 - regression_loss: 0.5106 - classification_loss: 0.1212
 5485/10000 [===============>..............] - ETA: 34:23 - loss: 0.6317 - regression_loss: 0.5105 - classification_loss: 0.1212
 5486/10000 [===============>..............] - ETA: 34:22 - loss: 0.6316 - regression_loss: 0.5104 - classification_loss: 0.1212
 5487/10000 [===============>..............] - ETA: 34:22 - loss: 0.6316 - regression_loss: 0.5104 - classification_loss: 0.1212
 5488/10000 [===============>..............] - ETA: 34:21 - loss: 0.6315 - regression_loss: 0.5103 - classification_loss: 0.1212
 5489/10000 [===============>..............] - ETA: 34:21 - loss: 0.6316 - regression_loss: 0.5104 - classification_loss: 0.1212
 5490/10000 [===============>..............] - ETA: 34:21 - loss: 0.6316 - regression_loss: 0.5104 - classification_loss: 0.1212
 5491/10000 [===============>..............] - ETA: 34:20 - loss: 0.6316 - regression_loss: 0.5104 - classification_loss: 0.1212
 5492/10000 [===============>..............] - ETA: 34:20 - loss: 0.6316 - regression_loss: 0.5104 - classification_loss: 0.1212
 5493/10000 [===============>..............] - ETA: 34:19 - loss: 0.6315 - regression_loss: 0.5103 - classification_loss: 0.1212
 5494/10000 [===============>..............] - ETA: 34:19 - loss: 0.6314 - regression_loss: 0.5102 - classification_loss: 0.1212
 5495/10000 [===============>..............] - ETA: 34:18 - loss: 0.6314 - regression_loss: 0.5102 - classification_loss: 0.1212
 5496/10000 [===============>..............] - ETA: 34:18 - loss: 0.6314 - regression_loss: 0.5103 - classification_loss: 0.1212
 5497/10000 [===============>..............] - ETA: 34:17 - loss: 0.6315 - regression_loss: 0.5103 - classification_loss: 0.1212
 5498/10000 [===============>..............] - ETA: 34:17 - loss: 0.6315 - regression_loss: 0.5103 - classification_loss: 0.1212
 5499/10000 [===============>..............] - ETA: 34:16 - loss: 0.6315 - regression_loss: 0.5103 - classification_loss: 0.1212
 5500/10000 [===============>..............] - ETA: 34:16 - loss: 0.6314 - regression_loss: 0.5102 - classification_loss: 0.1212
 5501/10000 [===============>..............] - ETA: 34:15 - loss: 0.6315 - regression_loss: 0.5103 - classification_loss: 0.1212
 5502/10000 [===============>..............] - ETA: 34:15 - loss: 0.6316 - regression_loss: 0.5104 - classification_loss: 0.1212
 5503/10000 [===============>..............] - ETA: 34:15 - loss: 0.6317 - regression_loss: 0.5105 - classification_loss: 0.1212
 5504/10000 [===============>..............] - ETA: 34:14 - loss: 0.6316 - regression_loss: 0.5104 - classification_loss: 0.1212
 5505/10000 [===============>..............] - ETA: 34:14 - loss: 0.6317 - regression_loss: 0.5105 - classification_loss: 0.1212
 5506/10000 [===============>..............] - ETA: 34:13 - loss: 0.6318 - regression_loss: 0.5105 - classification_loss: 0.1212
 5507/10000 [===============>..............] - ETA: 34:13 - loss: 0.6318 - regression_loss: 0.5105 - classification_loss: 0.1212
 5508/10000 [===============>..............] - ETA: 34:12 - loss: 0.6317 - regression_loss: 0.5105 - classification_loss: 0.1212
 5509/10000 [===============>..............] - ETA: 34:12 - loss: 0.6317 - regression_loss: 0.5105 - classification_loss: 0.1212
 5510/10000 [===============>..............] - ETA: 34:11 - loss: 0.6316 - regression_loss: 0.5104 - classification_loss: 0.1212
 5511/10000 [===============>..............] - ETA: 34:11 - loss: 0.6316 - regression_loss: 0.5104 - classification_loss: 0.1212
 5512/10000 [===============>..............] - ETA: 34:10 - loss: 0.6316 - regression_loss: 0.5104 - classification_loss: 0.1212
 5513/10000 [===============>..............] - ETA: 34:10 - loss: 0.6315 - regression_loss: 0.5104 - classification_loss: 0.1212
 5514/10000 [===============>..............] - ETA: 34:10 - loss: 0.6316 - regression_loss: 0.5104 - classification_loss: 0.1212
 5515/10000 [===============>..............] - ETA: 34:09 - loss: 0.6316 - regression_loss: 0.5104 - classification_loss: 0.1212
 5516/10000 [===============>..............] - ETA: 34:09 - loss: 0.6316 - regression_loss: 0.5104 - classification_loss: 0.1212
 5517/10000 [===============>..............] - ETA: 34:08 - loss: 0.6315 - regression_loss: 0.5104 - classification_loss: 0.1212
 5518/10000 [===============>..............] - ETA: 34:08 - loss: 0.6315 - regression_loss: 0.5103 - classification_loss: 0.1211
 5519/10000 [===============>..............] - ETA: 34:07 - loss: 0.6316 - regression_loss: 0.5104 - classification_loss: 0.1212
 5520/10000 [===============>..............] - ETA: 34:07 - loss: 0.6315 - regression_loss: 0.5104 - classification_loss: 0.1212
 5521/10000 [===============>..............] - ETA: 34:06 - loss: 0.6315 - regression_loss: 0.5104 - classification_loss: 0.1211
 5522/10000 [===============>..............] - ETA: 34:06 - loss: 0.6315 - regression_loss: 0.5104 - classification_loss: 0.1212
 5523/10000 [===============>..............] - ETA: 34:05 - loss: 0.6316 - regression_loss: 0.5104 - classification_loss: 0.1212
 5524/10000 [===============>..............] - ETA: 34:05 - loss: 0.6315 - regression_loss: 0.5103 - classification_loss: 0.1212
 5525/10000 [===============>..............] - ETA: 34:04 - loss: 0.6315 - regression_loss: 0.5103 - classification_loss: 0.1212
 5526/10000 [===============>..............] - ETA: 34:04 - loss: 0.6314 - regression_loss: 0.5103 - classification_loss: 0.1211
 5527/10000 [===============>..............] - ETA: 34:04 - loss: 0.6313 - regression_loss: 0.5102 - classification_loss: 0.1211
 5528/10000 [===============>..............] - ETA: 34:03 - loss: 0.6313 - regression_loss: 0.5102 - classification_loss: 0.1211
 5529/10000 [===============>..............] - ETA: 34:03 - loss: 0.6313 - regression_loss: 0.5102 - classification_loss: 0.1211
 5530/10000 [===============>..............] - ETA: 34:02 - loss: 0.6312 - regression_loss: 0.5101 - classification_loss: 0.1211
 5531/10000 [===============>..............] - ETA: 34:02 - loss: 0.6312 - regression_loss: 0.5101 - classification_loss: 0.1211
 5532/10000 [===============>..............] - ETA: 34:01 - loss: 0.6314 - regression_loss: 0.5102 - classification_loss: 0.1211
 5533/10000 [===============>..............] - ETA: 34:01 - loss: 0.6314 - regression_loss: 0.5102 - classification_loss: 0.1212
 5534/10000 [===============>..............] - ETA: 34:00 - loss: 0.6313 - regression_loss: 0.5101 - classification_loss: 0.1211
 5535/10000 [===============>..............] - ETA: 34:00 - loss: 0.6313 - regression_loss: 0.5101 - classification_loss: 0.1211
 5536/10000 [===============>..............] - ETA: 33:59 - loss: 0.6313 - regression_loss: 0.5102 - classification_loss: 0.1212
 5537/10000 [===============>..............] - ETA: 33:59 - loss: 0.6313 - regression_loss: 0.5102 - classification_loss: 0.1212
 5538/10000 [===============>..............] - ETA: 33:58 - loss: 0.6313 - regression_loss: 0.5102 - classification_loss: 0.1212
 5539/10000 [===============>..............] - ETA: 33:58 - loss: 0.6312 - regression_loss: 0.5101 - classification_loss: 0.1211
 5540/10000 [===============>..............] - ETA: 33:57 - loss: 0.6312 - regression_loss: 0.5101 - classification_loss: 0.1212
 5541/10000 [===============>..............] - ETA: 33:57 - loss: 0.6313 - regression_loss: 0.5101 - classification_loss: 0.1212
 5542/10000 [===============>..............] - ETA: 33:57 - loss: 0.6313 - regression_loss: 0.5101 - classification_loss: 0.1212
 5543/10000 [===============>..............] - ETA: 33:56 - loss: 0.6312 - regression_loss: 0.5101 - classification_loss: 0.1212
 5544/10000 [===============>..............] - ETA: 33:56 - loss: 0.6312 - regression_loss: 0.5101 - classification_loss: 0.1212
 5545/10000 [===============>..............] - ETA: 33:55 - loss: 0.6312 - regression_loss: 0.5100 - classification_loss: 0.1212
 5546/10000 [===============>..............] - ETA: 33:55 - loss: 0.6311 - regression_loss: 0.5100 - classification_loss: 0.1211
 5547/10000 [===============>..............] - ETA: 33:54 - loss: 0.6313 - regression_loss: 0.5102 - classification_loss: 0.1211
 5548/10000 [===============>..............] - ETA: 33:54 - loss: 0.6313 - regression_loss: 0.5102 - classification_loss: 0.1211
 5549/10000 [===============>..............] - ETA: 33:53 - loss: 0.6312 - regression_loss: 0.5101 - classification_loss: 0.1211
 5550/10000 [===============>..............] - ETA: 33:53 - loss: 0.6312 - regression_loss: 0.5101 - classification_loss: 0.1211
 5551/10000 [===============>..............] - ETA: 33:53 - loss: 0.6312 - regression_loss: 0.5101 - classification_loss: 0.1211
 5552/10000 [===============>..............] - ETA: 33:52 - loss: 0.6314 - regression_loss: 0.5103 - classification_loss: 0.1211
 5553/10000 [===============>..............] - ETA: 33:52 - loss: 0.6316 - regression_loss: 0.5105 - classification_loss: 0.1211
 5554/10000 [===============>..............] - ETA: 33:51 - loss: 0.6316 - regression_loss: 0.5104 - classification_loss: 0.1211
 5555/10000 [===============>..............] - ETA: 33:51 - loss: 0.6316 - regression_loss: 0.5105 - classification_loss: 0.1211
 5556/10000 [===============>..............] - ETA: 33:50 - loss: 0.6315 - regression_loss: 0.5104 - classification_loss: 0.1211
 5557/10000 [===============>..............] - ETA: 33:50 - loss: 0.6315 - regression_loss: 0.5104 - classification_loss: 0.1211
 5558/10000 [===============>..............] - ETA: 33:49 - loss: 0.6315 - regression_loss: 0.5104 - classification_loss: 0.1211
 5559/10000 [===============>..............] - ETA: 33:49 - loss: 0.6315 - regression_loss: 0.5104 - classification_loss: 0.1211
 5560/10000 [===============>..............] - ETA: 33:48 - loss: 0.6316 - regression_loss: 0.5105 - classification_loss: 0.1211
 5561/10000 [===============>..............] - ETA: 33:48 - loss: 0.6317 - regression_loss: 0.5106 - classification_loss: 0.1211
 5562/10000 [===============>..............] - ETA: 33:48 - loss: 0.6318 - regression_loss: 0.5106 - classification_loss: 0.1212
 5563/10000 [===============>..............] - ETA: 33:47 - loss: 0.6319 - regression_loss: 0.5107 - classification_loss: 0.1212
 5564/10000 [===============>..............] - ETA: 33:47 - loss: 0.6319 - regression_loss: 0.5107 - classification_loss: 0.1212
 5565/10000 [===============>..............] - ETA: 33:46 - loss: 0.6318 - regression_loss: 0.5107 - classification_loss: 0.1212
 5566/10000 [===============>..............] - ETA: 33:46 - loss: 0.6319 - regression_loss: 0.5107 - classification_loss: 0.1212
 5567/10000 [===============>..............] - ETA: 33:45 - loss: 0.6319 - regression_loss: 0.5107 - classification_loss: 0.1212
 5568/10000 [===============>..............] - ETA: 33:45 - loss: 0.6319 - regression_loss: 0.5107 - classification_loss: 0.1212
 5569/10000 [===============>..............] - ETA: 33:44 - loss: 0.6318 - regression_loss: 0.5107 - classification_loss: 0.1212
 5570/10000 [===============>..............] - ETA: 33:44 - loss: 0.6319 - regression_loss: 0.5107 - classification_loss: 0.1212
 5571/10000 [===============>..............] - ETA: 33:44 - loss: 0.6318 - regression_loss: 0.5107 - classification_loss: 0.1212
 5572/10000 [===============>..............] - ETA: 33:43 - loss: 0.6318 - regression_loss: 0.5107 - classification_loss: 0.1212
 5573/10000 [===============>..............] - ETA: 33:43 - loss: 0.6317 - regression_loss: 0.5106 - classification_loss: 0.1211
 5574/10000 [===============>..............] - ETA: 33:42 - loss: 0.6318 - regression_loss: 0.5107 - classification_loss: 0.1211
 5575/10000 [===============>..............] - ETA: 33:42 - loss: 0.6318 - regression_loss: 0.5107 - classification_loss: 0.1211
 5576/10000 [===============>..............] - ETA: 33:41 - loss: 0.6318 - regression_loss: 0.5107 - classification_loss: 0.1211
 5577/10000 [===============>..............] - ETA: 33:41 - loss: 0.6317 - regression_loss: 0.5106 - classification_loss: 0.1211
 5578/10000 [===============>..............] - ETA: 33:40 - loss: 0.6317 - regression_loss: 0.5106 - classification_loss: 0.1211
 5579/10000 [===============>..............] - ETA: 33:40 - loss: 0.6318 - regression_loss: 0.5107 - classification_loss: 0.1211
 5580/10000 [===============>..............] - ETA: 33:39 - loss: 0.6317 - regression_loss: 0.5106 - classification_loss: 0.1211
 5581/10000 [===============>..............] - ETA: 33:39 - loss: 0.6317 - regression_loss: 0.5106 - classification_loss: 0.1211
 5582/10000 [===============>..............] - ETA: 33:38 - loss: 0.6316 - regression_loss: 0.5106 - classification_loss: 0.1211
 5583/10000 [===============>..............] - ETA: 33:38 - loss: 0.6316 - regression_loss: 0.5106 - classification_loss: 0.1211
 5584/10000 [===============>..............] - ETA: 33:38 - loss: 0.6316 - regression_loss: 0.5105 - classification_loss: 0.1211
 5585/10000 [===============>..............] - ETA: 33:37 - loss: 0.6316 - regression_loss: 0.5105 - classification_loss: 0.1211
 5586/10000 [===============>..............] - ETA: 33:37 - loss: 0.6316 - regression_loss: 0.5105 - classification_loss: 0.1210
 5587/10000 [===============>..............] - ETA: 33:36 - loss: 0.6315 - regression_loss: 0.5105 - classification_loss: 0.1210
 5588/10000 [===============>..............] - ETA: 33:36 - loss: 0.6315 - regression_loss: 0.5105 - classification_loss: 0.1210
 5589/10000 [===============>..............] - ETA: 33:35 - loss: 0.6314 - regression_loss: 0.5104 - classification_loss: 0.1210
 5590/10000 [===============>..............] - ETA: 33:35 - loss: 0.6316 - regression_loss: 0.5106 - classification_loss: 0.1210
 5591/10000 [===============>..............] - ETA: 33:34 - loss: 0.6316 - regression_loss: 0.5106 - classification_loss: 0.1210
 5592/10000 [===============>..............] - ETA: 33:34 - loss: 0.6317 - regression_loss: 0.5107 - classification_loss: 0.1210
 5593/10000 [===============>..............] - ETA: 33:33 - loss: 0.6317 - regression_loss: 0.5107 - classification_loss: 0.1211
 5594/10000 [===============>..............] - ETA: 33:33 - loss: 0.6318 - regression_loss: 0.5108 - classification_loss: 0.1211
 5595/10000 [===============>..............] - ETA: 33:32 - loss: 0.6318 - regression_loss: 0.5107 - classification_loss: 0.1211
 5596/10000 [===============>..............] - ETA: 33:32 - loss: 0.6317 - regression_loss: 0.5107 - classification_loss: 0.1210
 5597/10000 [===============>..............] - ETA: 33:32 - loss: 0.6317 - regression_loss: 0.5106 - classification_loss: 0.1210
 5598/10000 [===============>..............] - ETA: 33:31 - loss: 0.6316 - regression_loss: 0.5106 - classification_loss: 0.1210
 5599/10000 [===============>..............] - ETA: 33:31 - loss: 0.6316 - regression_loss: 0.5105 - classification_loss: 0.1210
 5600/10000 [===============>..............] - ETA: 33:30 - loss: 0.6315 - regression_loss: 0.5105 - classification_loss: 0.1210
 5601/10000 [===============>..............] - ETA: 33:30 - loss: 0.6315 - regression_loss: 0.5105 - classification_loss: 0.1210
 5602/10000 [===============>..............] - ETA: 33:29 - loss: 0.6314 - regression_loss: 0.5104 - classification_loss: 0.1210
 5603/10000 [===============>..............] - ETA: 33:29 - loss: 0.6314 - regression_loss: 0.5104 - classification_loss: 0.1210
 5604/10000 [===============>..............] - ETA: 33:28 - loss: 0.6314 - regression_loss: 0.5104 - classification_loss: 0.1210
 5605/10000 [===============>..............] - ETA: 33:28 - loss: 0.6314 - regression_loss: 0.5104 - classification_loss: 0.1210
 5606/10000 [===============>..............] - ETA: 33:27 - loss: 0.6313 - regression_loss: 0.5104 - classification_loss: 0.1210
 5607/10000 [===============>..............] - ETA: 33:27 - loss: 0.6313 - regression_loss: 0.5103 - classification_loss: 0.1210
 5608/10000 [===============>..............] - ETA: 33:26 - loss: 0.6314 - regression_loss: 0.5104 - classification_loss: 0.1210
 5609/10000 [===============>..............] - ETA: 33:26 - loss: 0.6314 - regression_loss: 0.5104 - classification_loss: 0.1210
 5610/10000 [===============>..............] - ETA: 33:26 - loss: 0.6313 - regression_loss: 0.5103 - classification_loss: 0.1210
 5611/10000 [===============>..............] - ETA: 33:25 - loss: 0.6314 - regression_loss: 0.5105 - classification_loss: 0.1210
 5612/10000 [===============>..............] - ETA: 33:25 - loss: 0.6314 - regression_loss: 0.5104 - classification_loss: 0.1210
 5613/10000 [===============>..............] - ETA: 33:24 - loss: 0.6312 - regression_loss: 0.5103 - classification_loss: 0.1209
 5614/10000 [===============>..............] - ETA: 33:24 - loss: 0.6312 - regression_loss: 0.5103 - classification_loss: 0.1209
 5615/10000 [===============>..............] - ETA: 33:23 - loss: 0.6311 - regression_loss: 0.5102 - classification_loss: 0.1209
 5616/10000 [===============>..............] - ETA: 33:23 - loss: 0.6311 - regression_loss: 0.5102 - classification_loss: 0.1209
 5617/10000 [===============>..............] - ETA: 33:22 - loss: 0.6313 - regression_loss: 0.5104 - classification_loss: 0.1209
 5618/10000 [===============>..............] - ETA: 33:22 - loss: 0.6314 - regression_loss: 0.5104 - classification_loss: 0.1209
 5619/10000 [===============>..............] - ETA: 33:21 - loss: 0.6315 - regression_loss: 0.5105 - classification_loss: 0.1210
 5620/10000 [===============>..............] - ETA: 33:21 - loss: 0.6314 - regression_loss: 0.5105 - classification_loss: 0.1209
 5621/10000 [===============>..............] - ETA: 33:21 - loss: 0.6313 - regression_loss: 0.5104 - classification_loss: 0.1209
 5622/10000 [===============>..............] - ETA: 33:20 - loss: 0.6314 - regression_loss: 0.5104 - classification_loss: 0.1209
 5623/10000 [===============>..............] - ETA: 33:20 - loss: 0.6314 - regression_loss: 0.5104 - classification_loss: 0.1209
 5624/10000 [===============>..............] - ETA: 33:19 - loss: 0.6313 - regression_loss: 0.5104 - classification_loss: 0.1209
 5625/10000 [===============>..............] - ETA: 33:19 - loss: 0.6314 - regression_loss: 0.5105 - classification_loss: 0.1209
 5626/10000 [===============>..............] - ETA: 33:18 - loss: 0.6316 - regression_loss: 0.5106 - classification_loss: 0.1210
 5627/10000 [===============>..............] - ETA: 33:18 - loss: 0.6315 - regression_loss: 0.5106 - classification_loss: 0.1209
 5628/10000 [===============>..............] - ETA: 33:17 - loss: 0.6316 - regression_loss: 0.5107 - classification_loss: 0.1209
 5629/10000 [===============>..............] - ETA: 33:17 - loss: 0.6315 - regression_loss: 0.5106 - classification_loss: 0.1209
 5630/10000 [===============>..............] - ETA: 33:16 - loss: 0.6317 - regression_loss: 0.5107 - classification_loss: 0.1210
 5631/10000 [===============>..............] - ETA: 33:16 - loss: 0.6317 - regression_loss: 0.5107 - classification_loss: 0.1210
 5632/10000 [===============>..............] - ETA: 33:16 - loss: 0.6317 - regression_loss: 0.5107 - classification_loss: 0.1209
 5633/10000 [===============>..............] - ETA: 33:15 - loss: 0.6316 - regression_loss: 0.5107 - classification_loss: 0.1209
 5634/10000 [===============>..............] - ETA: 33:15 - loss: 0.6316 - regression_loss: 0.5106 - classification_loss: 0.1209
 5635/10000 [===============>..............] - ETA: 33:14 - loss: 0.6316 - regression_loss: 0.5107 - classification_loss: 0.1209
 5636/10000 [===============>..............] - ETA: 33:14 - loss: 0.6315 - regression_loss: 0.5106 - classification_loss: 0.1209
 5637/10000 [===============>..............] - ETA: 33:13 - loss: 0.6315 - regression_loss: 0.5106 - classification_loss: 0.1209
 5638/10000 [===============>..............] - ETA: 33:13 - loss: 0.6316 - regression_loss: 0.5106 - classification_loss: 0.1209
 5639/10000 [===============>..............] - ETA: 33:12 - loss: 0.6316 - regression_loss: 0.5107 - classification_loss: 0.1209
 5640/10000 [===============>..............] - ETA: 33:12 - loss: 0.6315 - regression_loss: 0.5106 - classification_loss: 0.1209
 5641/10000 [===============>..............] - ETA: 33:11 - loss: 0.6315 - regression_loss: 0.5106 - classification_loss: 0.1209
 5642/10000 [===============>..............] - ETA: 33:11 - loss: 0.6315 - regression_loss: 0.5106 - classification_loss: 0.1209
 5643/10000 [===============>..............] - ETA: 33:11 - loss: 0.6316 - regression_loss: 0.5107 - classification_loss: 0.1209
 5644/10000 [===============>..............] - ETA: 33:10 - loss: 0.6316 - regression_loss: 0.5107 - classification_loss: 0.1209
 5645/10000 [===============>..............] - ETA: 33:10 - loss: 0.6317 - regression_loss: 0.5108 - classification_loss: 0.1209
 5646/10000 [===============>..............] - ETA: 33:09 - loss: 0.6317 - regression_loss: 0.5107 - classification_loss: 0.1209
 5647/10000 [===============>..............] - ETA: 33:09 - loss: 0.6316 - regression_loss: 0.5107 - classification_loss: 0.1209
 5648/10000 [===============>..............] - ETA: 33:08 - loss: 0.6315 - regression_loss: 0.5106 - classification_loss: 0.1209
 5649/10000 [===============>..............] - ETA: 33:08 - loss: 0.6316 - regression_loss: 0.5106 - classification_loss: 0.1209
 5650/10000 [===============>..............] - ETA: 33:07 - loss: 0.6315 - regression_loss: 0.5106 - classification_loss: 0.1209
 5651/10000 [===============>..............] - ETA: 33:07 - loss: 0.6317 - regression_loss: 0.5107 - classification_loss: 0.1209
 5652/10000 [===============>..............] - ETA: 33:06 - loss: 0.6318 - regression_loss: 0.5108 - classification_loss: 0.1209
 5653/10000 [===============>..............] - ETA: 33:06 - loss: 0.6317 - regression_loss: 0.5108 - classification_loss: 0.1209
 5654/10000 [===============>..............] - ETA: 33:06 - loss: 0.6318 - regression_loss: 0.5108 - classification_loss: 0.1210
 5655/10000 [===============>..............] - ETA: 33:05 - loss: 0.6317 - regression_loss: 0.5108 - classification_loss: 0.1209
 5656/10000 [===============>..............] - ETA: 33:05 - loss: 0.6318 - regression_loss: 0.5109 - classification_loss: 0.1209
 5657/10000 [===============>..............] - ETA: 33:04 - loss: 0.6318 - regression_loss: 0.5109 - classification_loss: 0.1209
 5658/10000 [===============>..............] - ETA: 33:04 - loss: 0.6318 - regression_loss: 0.5109 - classification_loss: 0.1209
 5659/10000 [===============>..............] - ETA: 33:03 - loss: 0.6318 - regression_loss: 0.5109 - classification_loss: 0.1209
 5660/10000 [===============>..............] - ETA: 33:03 - loss: 0.6318 - regression_loss: 0.5109 - classification_loss: 0.1209
 5661/10000 [===============>..............] - ETA: 33:02 - loss: 0.6320 - regression_loss: 0.5111 - classification_loss: 0.1209
 5662/10000 [===============>..............] - ETA: 33:02 - loss: 0.6320 - regression_loss: 0.5110 - classification_loss: 0.1209
 5663/10000 [===============>..............] - ETA: 33:02 - loss: 0.6319 - regression_loss: 0.5110 - classification_loss: 0.1209
 5664/10000 [===============>..............] - ETA: 33:01 - loss: 0.6318 - regression_loss: 0.5109 - classification_loss: 0.1209
 5665/10000 [===============>..............] - ETA: 33:01 - loss: 0.6319 - regression_loss: 0.5110 - classification_loss: 0.1209
 5666/10000 [===============>..............] - ETA: 33:00 - loss: 0.6320 - regression_loss: 0.5111 - classification_loss: 0.1209
 5667/10000 [================>.............] - ETA: 33:00 - loss: 0.6320 - regression_loss: 0.5110 - classification_loss: 0.1209
 5668/10000 [================>.............] - ETA: 32:59 - loss: 0.6320 - regression_loss: 0.5111 - classification_loss: 0.1209
 5669/10000 [================>.............] - ETA: 32:59 - loss: 0.6319 - regression_loss: 0.5110 - classification_loss: 0.1209
 5670/10000 [================>.............] - ETA: 32:58 - loss: 0.6320 - regression_loss: 0.5111 - classification_loss: 0.1209
 5671/10000 [================>.............] - ETA: 32:58 - loss: 0.6319 - regression_loss: 0.5110 - classification_loss: 0.1209
 5672/10000 [================>.............] - ETA: 32:57 - loss: 0.6320 - regression_loss: 0.5110 - classification_loss: 0.1209
 5673/10000 [================>.............] - ETA: 32:57 - loss: 0.6320 - regression_loss: 0.5111 - classification_loss: 0.1209
 5674/10000 [================>.............] - ETA: 32:56 - loss: 0.6320 - regression_loss: 0.5111 - classification_loss: 0.1209
 5675/10000 [================>.............] - ETA: 32:56 - loss: 0.6320 - regression_loss: 0.5111 - classification_loss: 0.1209
 5676/10000 [================>.............] - ETA: 32:56 - loss: 0.6322 - regression_loss: 0.5112 - classification_loss: 0.1210
 5677/10000 [================>.............] - ETA: 32:55 - loss: 0.6321 - regression_loss: 0.5112 - classification_loss: 0.1209
 5678/10000 [================>.............] - ETA: 32:55 - loss: 0.6321 - regression_loss: 0.5112 - classification_loss: 0.1209
 5679/10000 [================>.............] - ETA: 32:54 - loss: 0.6322 - regression_loss: 0.5113 - classification_loss: 0.1209
 5680/10000 [================>.............] - ETA: 32:54 - loss: 0.6321 - regression_loss: 0.5112 - classification_loss: 0.1209
 5681/10000 [================>.............] - ETA: 32:53 - loss: 0.6321 - regression_loss: 0.5112 - classification_loss: 0.1209
 5682/10000 [================>.............] - ETA: 32:53 - loss: 0.6320 - regression_loss: 0.5111 - classification_loss: 0.1209
 5683/10000 [================>.............] - ETA: 32:52 - loss: 0.6320 - regression_loss: 0.5111 - classification_loss: 0.1209
 5684/10000 [================>.............] - ETA: 32:52 - loss: 0.6319 - regression_loss: 0.5110 - classification_loss: 0.1209
 5685/10000 [================>.............] - ETA: 32:51 - loss: 0.6318 - regression_loss: 0.5110 - classification_loss: 0.1209
 5686/10000 [================>.............] - ETA: 32:51 - loss: 0.6318 - regression_loss: 0.5109 - classification_loss: 0.1209
 5687/10000 [================>.............] - ETA: 32:50 - loss: 0.6319 - regression_loss: 0.5110 - classification_loss: 0.1209
 5688/10000 [================>.............] - ETA: 32:50 - loss: 0.6320 - regression_loss: 0.5110 - classification_loss: 0.1209
 5689/10000 [================>.............] - ETA: 32:49 - loss: 0.6319 - regression_loss: 0.5110 - classification_loss: 0.1209
 5690/10000 [================>.............] - ETA: 32:49 - loss: 0.6318 - regression_loss: 0.5110 - classification_loss: 0.1209
 5691/10000 [================>.............] - ETA: 32:49 - loss: 0.6318 - regression_loss: 0.5109 - classification_loss: 0.1209
 5692/10000 [================>.............] - ETA: 32:48 - loss: 0.6318 - regression_loss: 0.5109 - classification_loss: 0.1209
 5693/10000 [================>.............] - ETA: 32:48 - loss: 0.6318 - regression_loss: 0.5109 - classification_loss: 0.1209
 5694/10000 [================>.............] - ETA: 32:47 - loss: 0.6319 - regression_loss: 0.5110 - classification_loss: 0.1209
 5695/10000 [================>.............] - ETA: 32:47 - loss: 0.6320 - regression_loss: 0.5111 - classification_loss: 0.1209
 5696/10000 [================>.............] - ETA: 32:46 - loss: 0.6319 - regression_loss: 0.5111 - classification_loss: 0.1209
 5697/10000 [================>.............] - ETA: 32:46 - loss: 0.6319 - regression_loss: 0.5110 - classification_loss: 0.1209
 5698/10000 [================>.............] - ETA: 32:45 - loss: 0.6318 - regression_loss: 0.5110 - classification_loss: 0.1208
 5699/10000 [================>.............] - ETA: 32:45 - loss: 0.6318 - regression_loss: 0.5110 - classification_loss: 0.1208
 5700/10000 [================>.............] - ETA: 32:45 - loss: 0.6317 - regression_loss: 0.5109 - classification_loss: 0.1208
 5701/10000 [================>.............] - ETA: 32:44 - loss: 0.6318 - regression_loss: 0.5110 - classification_loss: 0.1208
 5702/10000 [================>.............] - ETA: 32:44 - loss: 0.6318 - regression_loss: 0.5109 - classification_loss: 0.1209
 5703/10000 [================>.............] - ETA: 32:43 - loss: 0.6317 - regression_loss: 0.5108 - classification_loss: 0.1209
 5704/10000 [================>.............] - ETA: 32:43 - loss: 0.6318 - regression_loss: 0.5109 - classification_loss: 0.1209
 5705/10000 [================>.............] - ETA: 32:42 - loss: 0.6318 - regression_loss: 0.5109 - classification_loss: 0.1209
 5706/10000 [================>.............] - ETA: 32:42 - loss: 0.6317 - regression_loss: 0.5109 - classification_loss: 0.1209
 5707/10000 [================>.............] - ETA: 32:41 - loss: 0.6317 - regression_loss: 0.5108 - classification_loss: 0.1208
 5708/10000 [================>.............] - ETA: 32:41 - loss: 0.6316 - regression_loss: 0.5108 - classification_loss: 0.1208
 5709/10000 [================>.............] - ETA: 32:40 - loss: 0.6316 - regression_loss: 0.5107 - classification_loss: 0.1208
 5710/10000 [================>.............] - ETA: 32:40 - loss: 0.6315 - regression_loss: 0.5107 - classification_loss: 0.1208
 5711/10000 [================>.............] - ETA: 32:40 - loss: 0.6316 - regression_loss: 0.5107 - classification_loss: 0.1209
 5712/10000 [================>.............] - ETA: 32:39 - loss: 0.6315 - regression_loss: 0.5106 - classification_loss: 0.1209
 5713/10000 [================>.............] - ETA: 32:39 - loss: 0.6314 - regression_loss: 0.5106 - classification_loss: 0.1209
 5714/10000 [================>.............] - ETA: 32:38 - loss: 0.6314 - regression_loss: 0.5105 - classification_loss: 0.1208
 5715/10000 [================>.............] - ETA: 32:38 - loss: 0.6314 - regression_loss: 0.5105 - classification_loss: 0.1208
 5716/10000 [================>.............] - ETA: 32:37 - loss: 0.6314 - regression_loss: 0.5106 - classification_loss: 0.1208
 5717/10000 [================>.............] - ETA: 32:37 - loss: 0.6316 - regression_loss: 0.5107 - classification_loss: 0.1208
 5718/10000 [================>.............] - ETA: 32:36 - loss: 0.6315 - regression_loss: 0.5107 - classification_loss: 0.1208
 5719/10000 [================>.............] - ETA: 32:36 - loss: 0.6315 - regression_loss: 0.5106 - classification_loss: 0.1208
 5720/10000 [================>.............] - ETA: 32:35 - loss: 0.6314 - regression_loss: 0.5106 - classification_loss: 0.1208
 5721/10000 [================>.............] - ETA: 32:35 - loss: 0.6313 - regression_loss: 0.5106 - classification_loss: 0.1208
 5722/10000 [================>.............] - ETA: 32:34 - loss: 0.6313 - regression_loss: 0.5105 - classification_loss: 0.1208
 5723/10000 [================>.............] - ETA: 32:34 - loss: 0.6313 - regression_loss: 0.5105 - classification_loss: 0.1208
 5724/10000 [================>.............] - ETA: 32:34 - loss: 0.6312 - regression_loss: 0.5104 - classification_loss: 0.1208
 5725/10000 [================>.............] - ETA: 32:33 - loss: 0.6312 - regression_loss: 0.5104 - classification_loss: 0.1208
 5726/10000 [================>.............] - ETA: 32:33 - loss: 0.6312 - regression_loss: 0.5104 - classification_loss: 0.1208
 5727/10000 [================>.............] - ETA: 32:32 - loss: 0.6311 - regression_loss: 0.5104 - classification_loss: 0.1208
 5728/10000 [================>.............] - ETA: 32:32 - loss: 0.6311 - regression_loss: 0.5103 - classification_loss: 0.1208
 5729/10000 [================>.............] - ETA: 32:31 - loss: 0.6310 - regression_loss: 0.5102 - classification_loss: 0.1208
 5730/10000 [================>.............] - ETA: 32:31 - loss: 0.6309 - regression_loss: 0.5102 - classification_loss: 0.1207
 5731/10000 [================>.............] - ETA: 32:30 - loss: 0.6308 - regression_loss: 0.5101 - classification_loss: 0.1207
 5732/10000 [================>.............] - ETA: 32:30 - loss: 0.6308 - regression_loss: 0.5101 - classification_loss: 0.1207
 5733/10000 [================>.............] - ETA: 32:30 - loss: 0.6308 - regression_loss: 0.5101 - classification_loss: 0.1207
 5734/10000 [================>.............] - ETA: 32:29 - loss: 0.6307 - regression_loss: 0.5100 - classification_loss: 0.1207
 5735/10000 [================>.............] - ETA: 32:29 - loss: 0.6308 - regression_loss: 0.5101 - classification_loss: 0.1207
 5736/10000 [================>.............] - ETA: 32:28 - loss: 0.6309 - regression_loss: 0.5101 - classification_loss: 0.1207
 5737/10000 [================>.............] - ETA: 32:28 - loss: 0.6309 - regression_loss: 0.5102 - classification_loss: 0.1207
 5738/10000 [================>.............] - ETA: 32:27 - loss: 0.6309 - regression_loss: 0.5102 - classification_loss: 0.1207
 5739/10000 [================>.............] - ETA: 32:27 - loss: 0.6309 - regression_loss: 0.5102 - classification_loss: 0.1207
 5740/10000 [================>.............] - ETA: 32:26 - loss: 0.6309 - regression_loss: 0.5102 - classification_loss: 0.1207
 5741/10000 [================>.............] - ETA: 32:26 - loss: 0.6309 - regression_loss: 0.5102 - classification_loss: 0.1207
 5742/10000 [================>.............] - ETA: 32:25 - loss: 0.6309 - regression_loss: 0.5102 - classification_loss: 0.1207
 5743/10000 [================>.............] - ETA: 32:25 - loss: 0.6309 - regression_loss: 0.5102 - classification_loss: 0.1207
 5744/10000 [================>.............] - ETA: 32:24 - loss: 0.6308 - regression_loss: 0.5101 - classification_loss: 0.1207
 5745/10000 [================>.............] - ETA: 32:24 - loss: 0.6308 - regression_loss: 0.5101 - classification_loss: 0.1207
 5746/10000 [================>.............] - ETA: 32:24 - loss: 0.6308 - regression_loss: 0.5101 - classification_loss: 0.1207
 5747/10000 [================>.............] - ETA: 32:23 - loss: 0.6307 - regression_loss: 0.5101 - classification_loss: 0.1207
 5748/10000 [================>.............] - ETA: 32:23 - loss: 0.6308 - regression_loss: 0.5101 - classification_loss: 0.1207
 5749/10000 [================>.............] - ETA: 32:22 - loss: 0.6308 - regression_loss: 0.5101 - classification_loss: 0.1207
 5750/10000 [================>.............] - ETA: 32:22 - loss: 0.6308 - regression_loss: 0.5101 - classification_loss: 0.1206
 5751/10000 [================>.............] - ETA: 32:21 - loss: 0.6307 - regression_loss: 0.5101 - classification_loss: 0.1206
 5752/10000 [================>.............] - ETA: 32:21 - loss: 0.6307 - regression_loss: 0.5101 - classification_loss: 0.1206
 5753/10000 [================>.............] - ETA: 32:20 - loss: 0.6307 - regression_loss: 0.5101 - classification_loss: 0.1206
 5754/10000 [================>.............] - ETA: 32:20 - loss: 0.6306 - regression_loss: 0.5100 - classification_loss: 0.1206
 5755/10000 [================>.............] - ETA: 32:19 - loss: 0.6307 - regression_loss: 0.5101 - classification_loss: 0.1206
 5756/10000 [================>.............] - ETA: 32:19 - loss: 0.6307 - regression_loss: 0.5100 - classification_loss: 0.1206
 5757/10000 [================>.............] - ETA: 32:18 - loss: 0.6306 - regression_loss: 0.5100 - classification_loss: 0.1206
 5758/10000 [================>.............] - ETA: 32:18 - loss: 0.6307 - regression_loss: 0.5101 - classification_loss: 0.1207
 5759/10000 [================>.............] - ETA: 32:18 - loss: 0.6308 - regression_loss: 0.5102 - classification_loss: 0.1207
 5760/10000 [================>.............] - ETA: 32:17 - loss: 0.6308 - regression_loss: 0.5101 - classification_loss: 0.1207
 5761/10000 [================>.............] - ETA: 32:17 - loss: 0.6308 - regression_loss: 0.5101 - classification_loss: 0.1206
 5762/10000 [================>.............] - ETA: 32:16 - loss: 0.6308 - regression_loss: 0.5102 - classification_loss: 0.1207
 5763/10000 [================>.............] - ETA: 32:16 - loss: 0.6308 - regression_loss: 0.5101 - classification_loss: 0.1206
 5764/10000 [================>.............] - ETA: 32:15 - loss: 0.6308 - regression_loss: 0.5101 - classification_loss: 0.1206
 5765/10000 [================>.............] - ETA: 32:15 - loss: 0.6307 - regression_loss: 0.5100 - classification_loss: 0.1206
 5766/10000 [================>.............] - ETA: 32:14 - loss: 0.6306 - regression_loss: 0.5100 - classification_loss: 0.1206
 5767/10000 [================>.............] - ETA: 32:14 - loss: 0.6306 - regression_loss: 0.5100 - classification_loss: 0.1206
 5768/10000 [================>.............] - ETA: 32:13 - loss: 0.6306 - regression_loss: 0.5100 - classification_loss: 0.1206
 5769/10000 [================>.............] - ETA: 32:13 - loss: 0.6307 - regression_loss: 0.5101 - classification_loss: 0.1206
 5770/10000 [================>.............] - ETA: 32:13 - loss: 0.6307 - regression_loss: 0.5100 - classification_loss: 0.1208
 5771/10000 [================>.............] - ETA: 32:12 - loss: 0.6308 - regression_loss: 0.5100 - classification_loss: 0.1208
 5772/10000 [================>.............] - ETA: 32:12 - loss: 0.6308 - regression_loss: 0.5100 - classification_loss: 0.1207
 5773/10000 [================>.............] - ETA: 32:11 - loss: 0.6309 - regression_loss: 0.5101 - classification_loss: 0.1208
 5774/10000 [================>.............] - ETA: 32:11 - loss: 0.6309 - regression_loss: 0.5102 - classification_loss: 0.1208
 5775/10000 [================>.............] - ETA: 32:10 - loss: 0.6309 - regression_loss: 0.5101 - classification_loss: 0.1207
 5776/10000 [================>.............] - ETA: 32:10 - loss: 0.6309 - regression_loss: 0.5102 - classification_loss: 0.1207
 5777/10000 [================>.............] - ETA: 32:09 - loss: 0.6309 - regression_loss: 0.5102 - classification_loss: 0.1207
 5778/10000 [================>.............] - ETA: 32:09 - loss: 0.6310 - regression_loss: 0.5103 - classification_loss: 0.1207
 5779/10000 [================>.............] - ETA: 32:08 - loss: 0.6310 - regression_loss: 0.5102 - classification_loss: 0.1207
 5780/10000 [================>.............] - ETA: 32:08 - loss: 0.6310 - regression_loss: 0.5102 - classification_loss: 0.1207
 5781/10000 [================>.............] - ETA: 32:08 - loss: 0.6310 - regression_loss: 0.5103 - classification_loss: 0.1207
 5782/10000 [================>.............] - ETA: 32:07 - loss: 0.6309 - regression_loss: 0.5102 - classification_loss: 0.1207
 5783/10000 [================>.............] - ETA: 32:07 - loss: 0.6310 - regression_loss: 0.5102 - classification_loss: 0.1207
 5784/10000 [================>.............] - ETA: 32:06 - loss: 0.6310 - regression_loss: 0.5103 - classification_loss: 0.1207
 5785/10000 [================>.............] - ETA: 32:06 - loss: 0.6311 - regression_loss: 0.5104 - classification_loss: 0.1207
 5786/10000 [================>.............] - ETA: 32:05 - loss: 0.6311 - regression_loss: 0.5103 - classification_loss: 0.1207
 5787/10000 [================>.............] - ETA: 32:05 - loss: 0.6311 - regression_loss: 0.5104 - classification_loss: 0.1207
 5788/10000 [================>.............] - ETA: 32:04 - loss: 0.6311 - regression_loss: 0.5104 - classification_loss: 0.1207
 5789/10000 [================>.............] - ETA: 32:04 - loss: 0.6311 - regression_loss: 0.5104 - classification_loss: 0.1207
 5790/10000 [================>.............] - ETA: 32:03 - loss: 0.6311 - regression_loss: 0.5104 - classification_loss: 0.1207
 5791/10000 [================>.............] - ETA: 32:03 - loss: 0.6311 - regression_loss: 0.5104 - classification_loss: 0.1207
 5792/10000 [================>.............] - ETA: 32:02 - loss: 0.6310 - regression_loss: 0.5103 - classification_loss: 0.1207
 5793/10000 [================>.............] - ETA: 32:02 - loss: 0.6310 - regression_loss: 0.5103 - classification_loss: 0.1207
 5794/10000 [================>.............] - ETA: 32:01 - loss: 0.6310 - regression_loss: 0.5103 - classification_loss: 0.1207
 5795/10000 [================>.............] - ETA: 32:01 - loss: 0.6310 - regression_loss: 0.5103 - classification_loss: 0.1207
 5796/10000 [================>.............] - ETA: 32:01 - loss: 0.6309 - regression_loss: 0.5102 - classification_loss: 0.1207
 5797/10000 [================>.............] - ETA: 32:00 - loss: 0.6310 - regression_loss: 0.5103 - classification_loss: 0.1206
 5798/10000 [================>.............] - ETA: 32:00 - loss: 0.6311 - regression_loss: 0.5104 - classification_loss: 0.1207
 5799/10000 [================>.............] - ETA: 31:59 - loss: 0.6312 - regression_loss: 0.5105 - classification_loss: 0.1207
 5800/10000 [================>.............] - ETA: 31:59 - loss: 0.6312 - regression_loss: 0.5105 - classification_loss: 0.1207
 5801/10000 [================>.............] - ETA: 31:58 - loss: 0.6312 - regression_loss: 0.5105 - classification_loss: 0.1207
 5802/10000 [================>.............] - ETA: 31:58 - loss: 0.6313 - regression_loss: 0.5106 - classification_loss: 0.1207
 5803/10000 [================>.............] - ETA: 31:57 - loss: 0.6314 - regression_loss: 0.5107 - classification_loss: 0.1207
 5804/10000 [================>.............] - ETA: 31:57 - loss: 0.6314 - regression_loss: 0.5107 - classification_loss: 0.1207
 5805/10000 [================>.............] - ETA: 31:56 - loss: 0.6314 - regression_loss: 0.5108 - classification_loss: 0.1207
 5806/10000 [================>.............] - ETA: 31:56 - loss: 0.6314 - regression_loss: 0.5107 - classification_loss: 0.1207
 5807/10000 [================>.............] - ETA: 31:55 - loss: 0.6315 - regression_loss: 0.5108 - classification_loss: 0.1207
 5808/10000 [================>.............] - ETA: 31:55 - loss: 0.6315 - regression_loss: 0.5108 - classification_loss: 0.1207
 5809/10000 [================>.............] - ETA: 31:55 - loss: 0.6316 - regression_loss: 0.5109 - classification_loss: 0.1207
 5810/10000 [================>.............] - ETA: 31:54 - loss: 0.6316 - regression_loss: 0.5109 - classification_loss: 0.1207
 5811/10000 [================>.............] - ETA: 31:54 - loss: 0.6316 - regression_loss: 0.5109 - classification_loss: 0.1207
 5812/10000 [================>.............] - ETA: 31:53 - loss: 0.6317 - regression_loss: 0.5110 - classification_loss: 0.1207
 5813/10000 [================>.............] - ETA: 31:53 - loss: 0.6316 - regression_loss: 0.5109 - classification_loss: 0.1207
 5814/10000 [================>.............] - ETA: 31:52 - loss: 0.6317 - regression_loss: 0.5110 - classification_loss: 0.1207
 5815/10000 [================>.............] - ETA: 31:52 - loss: 0.6316 - regression_loss: 0.5109 - classification_loss: 0.1207
 5816/10000 [================>.............] - ETA: 31:51 - loss: 0.6316 - regression_loss: 0.5109 - classification_loss: 0.1207
 5817/10000 [================>.............] - ETA: 31:51 - loss: 0.6318 - regression_loss: 0.5110 - classification_loss: 0.1207
 5818/10000 [================>.............] - ETA: 31:50 - loss: 0.6319 - regression_loss: 0.5111 - classification_loss: 0.1208
 5819/10000 [================>.............] - ETA: 31:50 - loss: 0.6319 - regression_loss: 0.5112 - classification_loss: 0.1207
 5820/10000 [================>.............] - ETA: 31:50 - loss: 0.6319 - regression_loss: 0.5112 - classification_loss: 0.1207
 5821/10000 [================>.............] - ETA: 31:49 - loss: 0.6318 - regression_loss: 0.5111 - classification_loss: 0.1207
 5822/10000 [================>.............] - ETA: 31:49 - loss: 0.6318 - regression_loss: 0.5111 - classification_loss: 0.1207
 5823/10000 [================>.............] - ETA: 31:48 - loss: 0.6317 - regression_loss: 0.5110 - classification_loss: 0.1207
 5824/10000 [================>.............] - ETA: 31:48 - loss: 0.6318 - regression_loss: 0.5111 - classification_loss: 0.1207
 5825/10000 [================>.............] - ETA: 31:47 - loss: 0.6319 - regression_loss: 0.5112 - classification_loss: 0.1207
 5826/10000 [================>.............] - ETA: 31:47 - loss: 0.6319 - regression_loss: 0.5112 - classification_loss: 0.1207
 5827/10000 [================>.............] - ETA: 31:46 - loss: 0.6319 - regression_loss: 0.5111 - classification_loss: 0.1207
 5828/10000 [================>.............] - ETA: 31:46 - loss: 0.6319 - regression_loss: 0.5112 - classification_loss: 0.1207
 5829/10000 [================>.............] - ETA: 31:45 - loss: 0.6318 - regression_loss: 0.5111 - classification_loss: 0.1207
 5830/10000 [================>.............] - ETA: 31:45 - loss: 0.6318 - regression_loss: 0.5111 - classification_loss: 0.1207
 5831/10000 [================>.............] - ETA: 31:45 - loss: 0.6318 - regression_loss: 0.5111 - classification_loss: 0.1207
 5832/10000 [================>.............] - ETA: 31:44 - loss: 0.6317 - regression_loss: 0.5110 - classification_loss: 0.1207
 5833/10000 [================>.............] - ETA: 31:44 - loss: 0.6318 - regression_loss: 0.5111 - classification_loss: 0.1207
 5834/10000 [================>.............] - ETA: 31:43 - loss: 0.6318 - regression_loss: 0.5111 - classification_loss: 0.1207
 5835/10000 [================>.............] - ETA: 31:43 - loss: 0.6318 - regression_loss: 0.5111 - classification_loss: 0.1207
 5836/10000 [================>.............] - ETA: 31:42 - loss: 0.6317 - regression_loss: 0.5110 - classification_loss: 0.1206
 5837/10000 [================>.............] - ETA: 31:42 - loss: 0.6316 - regression_loss: 0.5110 - classification_loss: 0.1206
 5838/10000 [================>.............] - ETA: 31:41 - loss: 0.6316 - regression_loss: 0.5110 - classification_loss: 0.1206
 5839/10000 [================>.............] - ETA: 31:41 - loss: 0.6315 - regression_loss: 0.5109 - classification_loss: 0.1206
 5840/10000 [================>.............] - ETA: 31:40 - loss: 0.6316 - regression_loss: 0.5110 - classification_loss: 0.1206
 5841/10000 [================>.............] - ETA: 31:40 - loss: 0.6316 - regression_loss: 0.5110 - classification_loss: 0.1206
 5842/10000 [================>.............] - ETA: 31:40 - loss: 0.6315 - regression_loss: 0.5109 - classification_loss: 0.1206
 5843/10000 [================>.............] - ETA: 31:39 - loss: 0.6316 - regression_loss: 0.5110 - classification_loss: 0.1206
 5844/10000 [================>.............] - ETA: 31:39 - loss: 0.6315 - regression_loss: 0.5109 - classification_loss: 0.1206
 5845/10000 [================>.............] - ETA: 31:38 - loss: 0.6315 - regression_loss: 0.5109 - classification_loss: 0.1206
 5846/10000 [================>.............] - ETA: 31:38 - loss: 0.6315 - regression_loss: 0.5109 - classification_loss: 0.1206
 5847/10000 [================>.............] - ETA: 31:37 - loss: 0.6315 - regression_loss: 0.5109 - classification_loss: 0.1206
 5848/10000 [================>.............] - ETA: 31:37 - loss: 0.6315 - regression_loss: 0.5109 - classification_loss: 0.1206
 5849/10000 [================>.............] - ETA: 31:36 - loss: 0.6314 - regression_loss: 0.5109 - classification_loss: 0.1206
 5850/10000 [================>.............] - ETA: 31:36 - loss: 0.6313 - regression_loss: 0.5108 - classification_loss: 0.1205
 5851/10000 [================>.............] - ETA: 31:35 - loss: 0.6313 - regression_loss: 0.5108 - classification_loss: 0.1205
 5852/10000 [================>.............] - ETA: 31:35 - loss: 0.6313 - regression_loss: 0.5108 - classification_loss: 0.1205
 5853/10000 [================>.............] - ETA: 31:34 - loss: 0.6313 - regression_loss: 0.5108 - classification_loss: 0.1205
 5854/10000 [================>.............] - ETA: 31:34 - loss: 0.6313 - regression_loss: 0.5108 - classification_loss: 0.1205
 5855/10000 [================>.............] - ETA: 31:34 - loss: 0.6315 - regression_loss: 0.5110 - classification_loss: 0.1205
 5856/10000 [================>.............] - ETA: 31:33 - loss: 0.6315 - regression_loss: 0.5109 - classification_loss: 0.1205
 5857/10000 [================>.............] - ETA: 31:33 - loss: 0.6315 - regression_loss: 0.5110 - classification_loss: 0.1205
 5858/10000 [================>.............] - ETA: 31:32 - loss: 0.6315 - regression_loss: 0.5110 - classification_loss: 0.1205
 5859/10000 [================>.............] - ETA: 31:32 - loss: 0.6314 - regression_loss: 0.5109 - classification_loss: 0.1205
 5860/10000 [================>.............] - ETA: 31:31 - loss: 0.6314 - regression_loss: 0.5109 - classification_loss: 0.1205
 5861/10000 [================>.............] - ETA: 31:31 - loss: 0.6313 - regression_loss: 0.5108 - classification_loss: 0.1205
 5862/10000 [================>.............] - ETA: 31:30 - loss: 0.6314 - regression_loss: 0.5109 - classification_loss: 0.1205
 5863/10000 [================>.............] - ETA: 31:30 - loss: 0.6313 - regression_loss: 0.5108 - classification_loss: 0.1205
 5864/10000 [================>.............] - ETA: 31:29 - loss: 0.6313 - regression_loss: 0.5108 - classification_loss: 0.1205
 5865/10000 [================>.............] - ETA: 31:29 - loss: 0.6313 - regression_loss: 0.5108 - classification_loss: 0.1205
 5866/10000 [================>.............] - ETA: 31:28 - loss: 0.6313 - regression_loss: 0.5108 - classification_loss: 0.1205
 5867/10000 [================>.............] - ETA: 31:28 - loss: 0.6314 - regression_loss: 0.5109 - classification_loss: 0.1205
 5868/10000 [================>.............] - ETA: 31:27 - loss: 0.6314 - regression_loss: 0.5109 - classification_loss: 0.1205
 5869/10000 [================>.............] - ETA: 31:27 - loss: 0.6314 - regression_loss: 0.5109 - classification_loss: 0.1205
 5870/10000 [================>.............] - ETA: 31:27 - loss: 0.6314 - regression_loss: 0.5109 - classification_loss: 0.1205
 5871/10000 [================>.............] - ETA: 31:26 - loss: 0.6313 - regression_loss: 0.5108 - classification_loss: 0.1205
 5872/10000 [================>.............] - ETA: 31:26 - loss: 0.6313 - regression_loss: 0.5108 - classification_loss: 0.1205
 5873/10000 [================>.............] - ETA: 31:25 - loss: 0.6313 - regression_loss: 0.5108 - classification_loss: 0.1205
 5874/10000 [================>.............] - ETA: 31:25 - loss: 0.6313 - regression_loss: 0.5108 - classification_loss: 0.1205
 5875/10000 [================>.............] - ETA: 31:24 - loss: 0.6313 - regression_loss: 0.5108 - classification_loss: 0.1205
 5876/10000 [================>.............] - ETA: 31:24 - loss: 0.6313 - regression_loss: 0.5108 - classification_loss: 0.1205
 5877/10000 [================>.............] - ETA: 31:23 - loss: 0.6312 - regression_loss: 0.5107 - classification_loss: 0.1205
 5878/10000 [================>.............] - ETA: 31:23 - loss: 0.6312 - regression_loss: 0.5107 - classification_loss: 0.1205
 5879/10000 [================>.............] - ETA: 31:23 - loss: 0.6313 - regression_loss: 0.5108 - classification_loss: 0.1205
 5880/10000 [================>.............] - ETA: 31:22 - loss: 0.6312 - regression_loss: 0.5107 - classification_loss: 0.1205
 5881/10000 [================>.............] - ETA: 31:22 - loss: 0.6312 - regression_loss: 0.5107 - classification_loss: 0.1205
 5882/10000 [================>.............] - ETA: 31:21 - loss: 0.6312 - regression_loss: 0.5107 - classification_loss: 0.1205
 5883/10000 [================>.............] - ETA: 31:21 - loss: 0.6312 - regression_loss: 0.5107 - classification_loss: 0.1205
 5884/10000 [================>.............] - ETA: 31:20 - loss: 0.6312 - regression_loss: 0.5107 - classification_loss: 0.1205
 5885/10000 [================>.............] - ETA: 31:20 - loss: 0.6312 - regression_loss: 0.5107 - classification_loss: 0.1205
 5886/10000 [================>.............] - ETA: 31:19 - loss: 0.6311 - regression_loss: 0.5107 - classification_loss: 0.1204
 5887/10000 [================>.............] - ETA: 31:19 - loss: 0.6311 - regression_loss: 0.5107 - classification_loss: 0.1204
 5888/10000 [================>.............] - ETA: 31:18 - loss: 0.6310 - regression_loss: 0.5106 - classification_loss: 0.1204
 5889/10000 [================>.............] - ETA: 31:18 - loss: 0.6310 - regression_loss: 0.5106 - classification_loss: 0.1204
 5890/10000 [================>.............] - ETA: 31:18 - loss: 0.6309 - regression_loss: 0.5105 - classification_loss: 0.1204
 5891/10000 [================>.............] - ETA: 31:17 - loss: 0.6309 - regression_loss: 0.5106 - classification_loss: 0.1204
 5892/10000 [================>.............] - ETA: 31:17 - loss: 0.6309 - regression_loss: 0.5105 - classification_loss: 0.1204
 5893/10000 [================>.............] - ETA: 31:16 - loss: 0.6309 - regression_loss: 0.5105 - classification_loss: 0.1204
 5894/10000 [================>.............] - ETA: 31:16 - loss: 0.6309 - regression_loss: 0.5105 - classification_loss: 0.1204
 5895/10000 [================>.............] - ETA: 31:15 - loss: 0.6309 - regression_loss: 0.5106 - classification_loss: 0.1204
 5896/10000 [================>.............] - ETA: 31:15 - loss: 0.6308 - regression_loss: 0.5105 - classification_loss: 0.1203
 5897/10000 [================>.............] - ETA: 31:14 - loss: 0.6308 - regression_loss: 0.5105 - classification_loss: 0.1203
 5898/10000 [================>.............] - ETA: 31:14 - loss: 0.6308 - regression_loss: 0.5104 - classification_loss: 0.1203
 5899/10000 [================>.............] - ETA: 31:13 - loss: 0.6307 - regression_loss: 0.5104 - classification_loss: 0.1203
 5900/10000 [================>.............] - ETA: 31:13 - loss: 0.6308 - regression_loss: 0.5104 - classification_loss: 0.1204
 5901/10000 [================>.............] - ETA: 31:13 - loss: 0.6307 - regression_loss: 0.5104 - classification_loss: 0.1203
 5902/10000 [================>.............] - ETA: 31:12 - loss: 0.6308 - regression_loss: 0.5104 - classification_loss: 0.1203
 5903/10000 [================>.............] - ETA: 31:12 - loss: 0.6307 - regression_loss: 0.5104 - classification_loss: 0.1203
 5904/10000 [================>.............] - ETA: 31:11 - loss: 0.6307 - regression_loss: 0.5104 - classification_loss: 0.1203
 5905/10000 [================>.............] - ETA: 31:11 - loss: 0.6306 - regression_loss: 0.5103 - classification_loss: 0.1203
 5906/10000 [================>.............] - ETA: 31:10 - loss: 0.6306 - regression_loss: 0.5103 - classification_loss: 0.1203
 5907/10000 [================>.............] - ETA: 31:10 - loss: 0.6305 - regression_loss: 0.5102 - classification_loss: 0.1203
 5908/10000 [================>.............] - ETA: 31:09 - loss: 0.6304 - regression_loss: 0.5101 - classification_loss: 0.1203
 5909/10000 [================>.............] - ETA: 31:09 - loss: 0.6303 - regression_loss: 0.5101 - classification_loss: 0.1203
 5910/10000 [================>.............] - ETA: 31:09 - loss: 0.6304 - regression_loss: 0.5102 - classification_loss: 0.1203
 5911/10000 [================>.............] - ETA: 31:08 - loss: 0.6305 - regression_loss: 0.5103 - classification_loss: 0.1203
 5912/10000 [================>.............] - ETA: 31:08 - loss: 0.6305 - regression_loss: 0.5103 - classification_loss: 0.1203
 5913/10000 [================>.............] - ETA: 31:07 - loss: 0.6304 - regression_loss: 0.5102 - classification_loss: 0.1202
 5914/10000 [================>.............] - ETA: 31:07 - loss: 0.6305 - regression_loss: 0.5102 - classification_loss: 0.1203
 5915/10000 [================>.............] - ETA: 31:06 - loss: 0.6305 - regression_loss: 0.5103 - classification_loss: 0.1203
 5916/10000 [================>.............] - ETA: 31:06 - loss: 0.6305 - regression_loss: 0.5102 - classification_loss: 0.1203
 5917/10000 [================>.............] - ETA: 31:05 - loss: 0.6306 - regression_loss: 0.5103 - classification_loss: 0.1203
 5918/10000 [================>.............] - ETA: 31:05 - loss: 0.6306 - regression_loss: 0.5103 - classification_loss: 0.1203
 5919/10000 [================>.............] - ETA: 31:04 - loss: 0.6306 - regression_loss: 0.5103 - classification_loss: 0.1203
 5920/10000 [================>.............] - ETA: 31:04 - loss: 0.6307 - regression_loss: 0.5104 - classification_loss: 0.1203
 5921/10000 [================>.............] - ETA: 31:04 - loss: 0.6307 - regression_loss: 0.5104 - classification_loss: 0.1203
 5922/10000 [================>.............] - ETA: 31:03 - loss: 0.6307 - regression_loss: 0.5104 - classification_loss: 0.1203
 5923/10000 [================>.............] - ETA: 31:03 - loss: 0.6307 - regression_loss: 0.5104 - classification_loss: 0.1203
 5924/10000 [================>.............] - ETA: 31:02 - loss: 0.6306 - regression_loss: 0.5103 - classification_loss: 0.1203
 5925/10000 [================>.............] - ETA: 31:02 - loss: 0.6307 - regression_loss: 0.5103 - classification_loss: 0.1203
 5926/10000 [================>.............] - ETA: 31:01 - loss: 0.6306 - regression_loss: 0.5103 - classification_loss: 0.1203
 5927/10000 [================>.............] - ETA: 31:01 - loss: 0.6306 - regression_loss: 0.5103 - classification_loss: 0.1203
 5928/10000 [================>.............] - ETA: 31:00 - loss: 0.6306 - regression_loss: 0.5103 - classification_loss: 0.1203
 5929/10000 [================>.............] - ETA: 31:00 - loss: 0.6305 - regression_loss: 0.5102 - classification_loss: 0.1203
 5930/10000 [================>.............] - ETA: 30:59 - loss: 0.6305 - regression_loss: 0.5102 - classification_loss: 0.1203
 5931/10000 [================>.............] - ETA: 30:59 - loss: 0.6306 - regression_loss: 0.5103 - classification_loss: 0.1203
 5932/10000 [================>.............] - ETA: 30:59 - loss: 0.6307 - regression_loss: 0.5104 - classification_loss: 0.1203
 5933/10000 [================>.............] - ETA: 30:58 - loss: 0.6306 - regression_loss: 0.5103 - classification_loss: 0.1203
 5934/10000 [================>.............] - ETA: 30:58 - loss: 0.6307 - regression_loss: 0.5103 - classification_loss: 0.1203
 5935/10000 [================>.............] - ETA: 30:57 - loss: 0.6307 - regression_loss: 0.5103 - classification_loss: 0.1203
 5936/10000 [================>.............] - ETA: 30:57 - loss: 0.6306 - regression_loss: 0.5103 - classification_loss: 0.1203
 5937/10000 [================>.............] - ETA: 30:56 - loss: 0.6306 - regression_loss: 0.5102 - classification_loss: 0.1203
 5938/10000 [================>.............] - ETA: 30:56 - loss: 0.6305 - regression_loss: 0.5102 - classification_loss: 0.1203
 5939/10000 [================>.............] - ETA: 30:55 - loss: 0.6304 - regression_loss: 0.5102 - classification_loss: 0.1203
 5940/10000 [================>.............] - ETA: 30:55 - loss: 0.6304 - regression_loss: 0.5101 - classification_loss: 0.1203
 5941/10000 [================>.............] - ETA: 30:54 - loss: 0.6304 - regression_loss: 0.5101 - classification_loss: 0.1203
 5942/10000 [================>.............] - ETA: 30:54 - loss: 0.6304 - regression_loss: 0.5101 - classification_loss: 0.1203
 5943/10000 [================>.............] - ETA: 30:54 - loss: 0.6304 - regression_loss: 0.5101 - classification_loss: 0.1203
 5944/10000 [================>.............] - ETA: 30:53 - loss: 0.6305 - regression_loss: 0.5102 - classification_loss: 0.1203
 5945/10000 [================>.............] - ETA: 30:53 - loss: 0.6304 - regression_loss: 0.5101 - classification_loss: 0.1203
 5946/10000 [================>.............] - ETA: 30:52 - loss: 0.6304 - regression_loss: 0.5101 - classification_loss: 0.1203
 5947/10000 [================>.............] - ETA: 30:52 - loss: 0.6304 - regression_loss: 0.5101 - classification_loss: 0.1203
 5948/10000 [================>.............] - ETA: 30:51 - loss: 0.6305 - regression_loss: 0.5101 - classification_loss: 0.1203
 5949/10000 [================>.............] - ETA: 30:51 - loss: 0.6304 - regression_loss: 0.5101 - classification_loss: 0.1203
 5950/10000 [================>.............] - ETA: 30:50 - loss: 0.6303 - regression_loss: 0.5100 - classification_loss: 0.1203
 5951/10000 [================>.............] - ETA: 30:50 - loss: 0.6304 - regression_loss: 0.5101 - classification_loss: 0.1203
 5952/10000 [================>.............] - ETA: 30:49 - loss: 0.6305 - regression_loss: 0.5102 - classification_loss: 0.1203
 5953/10000 [================>.............] - ETA: 30:49 - loss: 0.6305 - regression_loss: 0.5102 - classification_loss: 0.1203
 5954/10000 [================>.............] - ETA: 30:49 - loss: 0.6304 - regression_loss: 0.5101 - classification_loss: 0.1203
 5955/10000 [================>.............] - ETA: 30:48 - loss: 0.6304 - regression_loss: 0.5101 - classification_loss: 0.1203
 5956/10000 [================>.............] - ETA: 30:48 - loss: 0.6303 - regression_loss: 0.5100 - classification_loss: 0.1203
 5957/10000 [================>.............] - ETA: 30:47 - loss: 0.6302 - regression_loss: 0.5100 - classification_loss: 0.1202
 5958/10000 [================>.............] - ETA: 30:47 - loss: 0.6302 - regression_loss: 0.5099 - classification_loss: 0.1202
 5959/10000 [================>.............] - ETA: 30:46 - loss: 0.6302 - regression_loss: 0.5099 - classification_loss: 0.1202
 5960/10000 [================>.............] - ETA: 30:46 - loss: 0.6301 - regression_loss: 0.5099 - classification_loss: 0.1202
 5961/10000 [================>.............] - ETA: 30:45 - loss: 0.6301 - regression_loss: 0.5099 - classification_loss: 0.1202
 5962/10000 [================>.............] - ETA: 30:45 - loss: 0.6302 - regression_loss: 0.5100 - classification_loss: 0.1202
 5963/10000 [================>.............] - ETA: 30:44 - loss: 0.6302 - regression_loss: 0.5099 - classification_loss: 0.1202
 5964/10000 [================>.............] - ETA: 30:44 - loss: 0.6302 - regression_loss: 0.5099 - classification_loss: 0.1202
 5965/10000 [================>.............] - ETA: 30:43 - loss: 0.6301 - regression_loss: 0.5099 - classification_loss: 0.1202
 5966/10000 [================>.............] - ETA: 30:43 - loss: 0.6301 - regression_loss: 0.5098 - classification_loss: 0.1202
 5967/10000 [================>.............] - ETA: 30:43 - loss: 0.6301 - regression_loss: 0.5099 - classification_loss: 0.1202
 5968/10000 [================>.............] - ETA: 30:42 - loss: 0.6300 - regression_loss: 0.5098 - classification_loss: 0.1202
 5969/10000 [================>.............] - ETA: 30:42 - loss: 0.6301 - regression_loss: 0.5098 - classification_loss: 0.1202
 5970/10000 [================>.............] - ETA: 30:41 - loss: 0.6300 - regression_loss: 0.5098 - classification_loss: 0.1202
 5971/10000 [================>.............] - ETA: 30:41 - loss: 0.6300 - regression_loss: 0.5098 - classification_loss: 0.1202
 5972/10000 [================>.............] - ETA: 30:40 - loss: 0.6301 - regression_loss: 0.5099 - classification_loss: 0.1202
 5973/10000 [================>.............] - ETA: 30:40 - loss: 0.6300 - regression_loss: 0.5098 - classification_loss: 0.1202
 5974/10000 [================>.............] - ETA: 30:39 - loss: 0.6300 - regression_loss: 0.5098 - classification_loss: 0.1202
 5975/10000 [================>.............] - ETA: 30:39 - loss: 0.6300 - regression_loss: 0.5098 - classification_loss: 0.1202
 5976/10000 [================>.............] - ETA: 30:38 - loss: 0.6300 - regression_loss: 0.5098 - classification_loss: 0.1202
 5977/10000 [================>.............] - ETA: 30:38 - loss: 0.6300 - regression_loss: 0.5098 - classification_loss: 0.1202
 5978/10000 [================>.............] - ETA: 30:38 - loss: 0.6301 - regression_loss: 0.5099 - classification_loss: 0.1202
 5979/10000 [================>.............] - ETA: 30:37 - loss: 0.6301 - regression_loss: 0.5099 - classification_loss: 0.1202
 5980/10000 [================>.............] - ETA: 30:37 - loss: 0.6300 - regression_loss: 0.5099 - classification_loss: 0.1202
 5981/10000 [================>.............] - ETA: 30:36 - loss: 0.6300 - regression_loss: 0.5098 - classification_loss: 0.1202
 5982/10000 [================>.............] - ETA: 30:36 - loss: 0.6299 - regression_loss: 0.5098 - classification_loss: 0.1201
 5983/10000 [================>.............] - ETA: 30:35 - loss: 0.6299 - regression_loss: 0.5097 - classification_loss: 0.1201
 5984/10000 [================>.............] - ETA: 30:35 - loss: 0.6299 - regression_loss: 0.5098 - classification_loss: 0.1201
 5985/10000 [================>.............] - ETA: 30:34 - loss: 0.6298 - regression_loss: 0.5097 - classification_loss: 0.1201
 5986/10000 [================>.............] - ETA: 30:34 - loss: 0.6298 - regression_loss: 0.5097 - classification_loss: 0.1201
 5987/10000 [================>.............] - ETA: 30:33 - loss: 0.6298 - regression_loss: 0.5097 - classification_loss: 0.1201
 5988/10000 [================>.............] - ETA: 30:33 - loss: 0.6298 - regression_loss: 0.5097 - classification_loss: 0.1201
 5989/10000 [================>.............] - ETA: 30:32 - loss: 0.6298 - regression_loss: 0.5097 - classification_loss: 0.1201
 5990/10000 [================>.............] - ETA: 30:32 - loss: 0.6298 - regression_loss: 0.5097 - classification_loss: 0.1201
 5991/10000 [================>.............] - ETA: 30:32 - loss: 0.6298 - regression_loss: 0.5097 - classification_loss: 0.1201
 5992/10000 [================>.............] - ETA: 30:31 - loss: 0.6298 - regression_loss: 0.5097 - classification_loss: 0.1201
 5993/10000 [================>.............] - ETA: 30:31 - loss: 0.6297 - regression_loss: 0.5096 - classification_loss: 0.1201
 5994/10000 [================>.............] - ETA: 30:30 - loss: 0.6299 - regression_loss: 0.5097 - classification_loss: 0.1202
 5995/10000 [================>.............] - ETA: 30:30 - loss: 0.6299 - regression_loss: 0.5098 - classification_loss: 0.1202
 5996/10000 [================>.............] - ETA: 30:29 - loss: 0.6299 - regression_loss: 0.5097 - classification_loss: 0.1202
 5997/10000 [================>.............] - ETA: 30:29 - loss: 0.6299 - regression_loss: 0.5097 - classification_loss: 0.1202
 5998/10000 [================>.............] - ETA: 30:28 - loss: 0.6299 - regression_loss: 0.5097 - classification_loss: 0.1202
 5999/10000 [================>.............] - ETA: 30:28 - loss: 0.6300 - regression_loss: 0.5098 - classification_loss: 0.1202
 6000/10000 [=================>............] - ETA: 30:27 - loss: 0.6300 - regression_loss: 0.5098 - classification_loss: 0.1202
 6001/10000 [=================>............] - ETA: 30:27 - loss: 0.6300 - regression_loss: 0.5098 - classification_loss: 0.1202
 6002/10000 [=================>............] - ETA: 30:26 - loss: 0.6301 - regression_loss: 0.5099 - classification_loss: 0.1202
 6003/10000 [=================>............] - ETA: 30:26 - loss: 0.6300 - regression_loss: 0.5098 - classification_loss: 0.1202
 6004/10000 [=================>............] - ETA: 30:26 - loss: 0.6300 - regression_loss: 0.5098 - classification_loss: 0.1202
 6005/10000 [=================>............] - ETA: 30:25 - loss: 0.6300 - regression_loss: 0.5098 - classification_loss: 0.1202
 6006/10000 [=================>............] - ETA: 30:25 - loss: 0.6299 - regression_loss: 0.5097 - classification_loss: 0.1202
 6007/10000 [=================>............] - ETA: 30:24 - loss: 0.6299 - regression_loss: 0.5097 - classification_loss: 0.1202
 6008/10000 [=================>............] - ETA: 30:24 - loss: 0.6301 - regression_loss: 0.5099 - classification_loss: 0.1202
 6009/10000 [=================>............] - ETA: 30:23 - loss: 0.6300 - regression_loss: 0.5099 - classification_loss: 0.1202
 6010/10000 [=================>............] - ETA: 30:23 - loss: 0.6300 - regression_loss: 0.5098 - classification_loss: 0.1202
 6011/10000 [=================>............] - ETA: 30:22 - loss: 0.6300 - regression_loss: 0.5098 - classification_loss: 0.1202
 6012/10000 [=================>............] - ETA: 30:22 - loss: 0.6300 - regression_loss: 0.5098 - classification_loss: 0.1202
 6013/10000 [=================>............] - ETA: 30:21 - loss: 0.6300 - regression_loss: 0.5098 - classification_loss: 0.1202
 6014/10000 [=================>............] - ETA: 30:21 - loss: 0.6300 - regression_loss: 0.5098 - classification_loss: 0.1202
 6015/10000 [=================>............] - ETA: 30:21 - loss: 0.6299 - regression_loss: 0.5097 - classification_loss: 0.1202
 6016/10000 [=================>............] - ETA: 30:20 - loss: 0.6299 - regression_loss: 0.5097 - classification_loss: 0.1202
 6017/10000 [=================>............] - ETA: 30:20 - loss: 0.6298 - regression_loss: 0.5096 - classification_loss: 0.1202
 6018/10000 [=================>............] - ETA: 30:19 - loss: 0.6299 - regression_loss: 0.5097 - classification_loss: 0.1202
 6019/10000 [=================>............] - ETA: 30:19 - loss: 0.6299 - regression_loss: 0.5097 - classification_loss: 0.1202
 6020/10000 [=================>............] - ETA: 30:18 - loss: 0.6298 - regression_loss: 0.5096 - classification_loss: 0.1202
 6021/10000 [=================>............] - ETA: 30:18 - loss: 0.6298 - regression_loss: 0.5096 - classification_loss: 0.1202
 6022/10000 [=================>............] - ETA: 30:17 - loss: 0.6298 - regression_loss: 0.5096 - classification_loss: 0.1202
 6023/10000 [=================>............] - ETA: 30:17 - loss: 0.6298 - regression_loss: 0.5096 - classification_loss: 0.1202
 6024/10000 [=================>............] - ETA: 30:16 - loss: 0.6298 - regression_loss: 0.5096 - classification_loss: 0.1202
 6025/10000 [=================>............] - ETA: 30:16 - loss: 0.6297 - regression_loss: 0.5095 - classification_loss: 0.1202
 6026/10000 [=================>............] - ETA: 30:16 - loss: 0.6296 - regression_loss: 0.5095 - classification_loss: 0.1202
 6027/10000 [=================>............] - ETA: 30:15 - loss: 0.6297 - regression_loss: 0.5095 - classification_loss: 0.1202
 6028/10000 [=================>............] - ETA: 30:15 - loss: 0.6297 - regression_loss: 0.5095 - classification_loss: 0.1202
 6029/10000 [=================>............] - ETA: 30:14 - loss: 0.6296 - regression_loss: 0.5094 - classification_loss: 0.1202
 6030/10000 [=================>............] - ETA: 30:14 - loss: 0.6297 - regression_loss: 0.5095 - classification_loss: 0.1202
 6031/10000 [=================>............] - ETA: 30:13 - loss: 0.6298 - regression_loss: 0.5096 - classification_loss: 0.1202
 6032/10000 [=================>............] - ETA: 30:13 - loss: 0.6298 - regression_loss: 0.5096 - classification_loss: 0.1202
 6033/10000 [=================>............] - ETA: 30:12 - loss: 0.6298 - regression_loss: 0.5096 - classification_loss: 0.1202
 6034/10000 [=================>............] - ETA: 30:12 - loss: 0.6300 - regression_loss: 0.5098 - classification_loss: 0.1202
 6035/10000 [=================>............] - ETA: 30:11 - loss: 0.6300 - regression_loss: 0.5098 - classification_loss: 0.1202
 6036/10000 [=================>............] - ETA: 30:11 - loss: 0.6300 - regression_loss: 0.5097 - classification_loss: 0.1202
 6037/10000 [=================>............] - ETA: 30:10 - loss: 0.6299 - regression_loss: 0.5097 - classification_loss: 0.1202
 6038/10000 [=================>............] - ETA: 30:10 - loss: 0.6299 - regression_loss: 0.5097 - classification_loss: 0.1202
 6039/10000 [=================>............] - ETA: 30:10 - loss: 0.6299 - regression_loss: 0.5097 - classification_loss: 0.1202
 6040/10000 [=================>............] - ETA: 30:09 - loss: 0.6298 - regression_loss: 0.5096 - classification_loss: 0.1202
 6041/10000 [=================>............] - ETA: 30:09 - loss: 0.6297 - regression_loss: 0.5096 - classification_loss: 0.1202
 6042/10000 [=================>............] - ETA: 30:08 - loss: 0.6297 - regression_loss: 0.5096 - classification_loss: 0.1202
 6043/10000 [=================>............] - ETA: 30:08 - loss: 0.6296 - regression_loss: 0.5095 - classification_loss: 0.1201
 6044/10000 [=================>............] - ETA: 30:07 - loss: 0.6296 - regression_loss: 0.5094 - classification_loss: 0.1201
 6045/10000 [=================>............] - ETA: 30:07 - loss: 0.6296 - regression_loss: 0.5095 - classification_loss: 0.1201
 6046/10000 [=================>............] - ETA: 30:06 - loss: 0.6298 - regression_loss: 0.5096 - classification_loss: 0.1201
 6047/10000 [=================>............] - ETA: 30:06 - loss: 0.6297 - regression_loss: 0.5096 - classification_loss: 0.1201
 6048/10000 [=================>............] - ETA: 30:05 - loss: 0.6297 - regression_loss: 0.5095 - classification_loss: 0.1201
 6049/10000 [=================>............] - ETA: 30:05 - loss: 0.6297 - regression_loss: 0.5095 - classification_loss: 0.1201
 6050/10000 [=================>............] - ETA: 30:05 - loss: 0.6296 - regression_loss: 0.5095 - classification_loss: 0.1201
 6051/10000 [=================>............] - ETA: 30:04 - loss: 0.6296 - regression_loss: 0.5095 - classification_loss: 0.1201
 6052/10000 [=================>............] - ETA: 30:04 - loss: 0.6297 - regression_loss: 0.5095 - classification_loss: 0.1201
 6053/10000 [=================>............] - ETA: 30:03 - loss: 0.6296 - regression_loss: 0.5095 - classification_loss: 0.1201
 6054/10000 [=================>............] - ETA: 30:03 - loss: 0.6297 - regression_loss: 0.5095 - classification_loss: 0.1201
 6055/10000 [=================>............] - ETA: 30:02 - loss: 0.6296 - regression_loss: 0.5095 - classification_loss: 0.1201
 6056/10000 [=================>............] - ETA: 30:02 - loss: 0.6296 - regression_loss: 0.5095 - classification_loss: 0.1201
 6057/10000 [=================>............] - ETA: 30:01 - loss: 0.6296 - regression_loss: 0.5094 - classification_loss: 0.1201
 6058/10000 [=================>............] - ETA: 30:01 - loss: 0.6296 - regression_loss: 0.5095 - classification_loss: 0.1201
 6059/10000 [=================>............] - ETA: 30:00 - loss: 0.6296 - regression_loss: 0.5095 - classification_loss: 0.1201
 6060/10000 [=================>............] - ETA: 30:00 - loss: 0.6296 - regression_loss: 0.5095 - classification_loss: 0.1201
 6061/10000 [=================>............] - ETA: 29:59 - loss: 0.6296 - regression_loss: 0.5095 - classification_loss: 0.1201
 6062/10000 [=================>............] - ETA: 29:59 - loss: 0.6297 - regression_loss: 0.5096 - classification_loss: 0.1201
 6063/10000 [=================>............] - ETA: 29:59 - loss: 0.6297 - regression_loss: 0.5096 - classification_loss: 0.1201
 6064/10000 [=================>............] - ETA: 29:58 - loss: 0.6297 - regression_loss: 0.5095 - classification_loss: 0.1201
 6065/10000 [=================>............] - ETA: 29:58 - loss: 0.6298 - regression_loss: 0.5096 - classification_loss: 0.1201
 6066/10000 [=================>............] - ETA: 29:57 - loss: 0.6297 - regression_loss: 0.5096 - classification_loss: 0.1201
 6067/10000 [=================>............] - ETA: 29:57 - loss: 0.6298 - regression_loss: 0.5097 - classification_loss: 0.1201
 6068/10000 [=================>............] - ETA: 29:56 - loss: 0.6299 - regression_loss: 0.5097 - classification_loss: 0.1202
 6069/10000 [=================>............] - ETA: 29:56 - loss: 0.6298 - regression_loss: 0.5097 - classification_loss: 0.1201
 6070/10000 [=================>............] - ETA: 29:55 - loss: 0.6297 - regression_loss: 0.5096 - classification_loss: 0.1201
 6071/10000 [=================>............] - ETA: 29:55 - loss: 0.6297 - regression_loss: 0.5095 - classification_loss: 0.1201
 6072/10000 [=================>............] - ETA: 29:54 - loss: 0.6297 - regression_loss: 0.5095 - classification_loss: 0.1201
 6073/10000 [=================>............] - ETA: 29:54 - loss: 0.6296 - regression_loss: 0.5095 - classification_loss: 0.1201
 6074/10000 [=================>............] - ETA: 29:54 - loss: 0.6296 - regression_loss: 0.5095 - classification_loss: 0.1201
 6075/10000 [=================>............] - ETA: 29:53 - loss: 0.6296 - regression_loss: 0.5095 - classification_loss: 0.1201
 6076/10000 [=================>............] - ETA: 29:53 - loss: 0.6296 - regression_loss: 0.5095 - classification_loss: 0.1202
 6077/10000 [=================>............] - ETA: 29:52 - loss: 0.6296 - regression_loss: 0.5094 - classification_loss: 0.1202
 6078/10000 [=================>............] - ETA: 29:52 - loss: 0.6296 - regression_loss: 0.5094 - classification_loss: 0.1202
 6079/10000 [=================>............] - ETA: 29:51 - loss: 0.6295 - regression_loss: 0.5094 - classification_loss: 0.1202
 6080/10000 [=================>............] - ETA: 29:51 - loss: 0.6295 - regression_loss: 0.5093 - classification_loss: 0.1201
 6081/10000 [=================>............] - ETA: 29:50 - loss: 0.6294 - regression_loss: 0.5093 - classification_loss: 0.1201
 6082/10000 [=================>............] - ETA: 29:50 - loss: 0.6293 - regression_loss: 0.5092 - classification_loss: 0.1201
 6083/10000 [=================>............] - ETA: 29:49 - loss: 0.6293 - regression_loss: 0.5092 - classification_loss: 0.1201
 6084/10000 [=================>............] - ETA: 29:49 - loss: 0.6293 - regression_loss: 0.5092 - classification_loss: 0.1201
 6085/10000 [=================>............] - ETA: 29:48 - loss: 0.6293 - regression_loss: 0.5092 - classification_loss: 0.1201
 6086/10000 [=================>............] - ETA: 29:48 - loss: 0.6293 - regression_loss: 0.5092 - classification_loss: 0.1201
 6087/10000 [=================>............] - ETA: 29:47 - loss: 0.6293 - regression_loss: 0.5092 - classification_loss: 0.1201
 6088/10000 [=================>............] - ETA: 29:47 - loss: 0.6293 - regression_loss: 0.5092 - classification_loss: 0.1201
 6089/10000 [=================>............] - ETA: 29:47 - loss: 0.6294 - regression_loss: 0.5093 - classification_loss: 0.1201
 6090/10000 [=================>............] - ETA: 29:46 - loss: 0.6294 - regression_loss: 0.5093 - classification_loss: 0.1201
 6091/10000 [=================>............] - ETA: 29:46 - loss: 0.6294 - regression_loss: 0.5093 - classification_loss: 0.1201
 6092/10000 [=================>............] - ETA: 29:45 - loss: 0.6294 - regression_loss: 0.5093 - classification_loss: 0.1201
 6093/10000 [=================>............] - ETA: 29:45 - loss: 0.6294 - regression_loss: 0.5093 - classification_loss: 0.1201
 6094/10000 [=================>............] - ETA: 29:44 - loss: 0.6294 - regression_loss: 0.5093 - classification_loss: 0.1201
 6095/10000 [=================>............] - ETA: 29:44 - loss: 0.6294 - regression_loss: 0.5093 - classification_loss: 0.1201
 6096/10000 [=================>............] - ETA: 29:43 - loss: 0.6294 - regression_loss: 0.5093 - classification_loss: 0.1201
 6097/10000 [=================>............] - ETA: 29:43 - loss: 0.6294 - regression_loss: 0.5094 - classification_loss: 0.1201
 6098/10000 [=================>............] - ETA: 29:42 - loss: 0.6294 - regression_loss: 0.5093 - classification_loss: 0.1201
 6099/10000 [=================>............] - ETA: 29:42 - loss: 0.6294 - regression_loss: 0.5093 - classification_loss: 0.1200
 6100/10000 [=================>............] - ETA: 29:41 - loss: 0.6293 - regression_loss: 0.5092 - classification_loss: 0.1200
 6101/10000 [=================>............] - ETA: 29:41 - loss: 0.6292 - regression_loss: 0.5092 - classification_loss: 0.1200
 6102/10000 [=================>............] - ETA: 29:41 - loss: 0.6292 - regression_loss: 0.5091 - classification_loss: 0.1200
 6103/10000 [=================>............] - ETA: 29:40 - loss: 0.6291 - regression_loss: 0.5091 - classification_loss: 0.1200
 6104/10000 [=================>............] - ETA: 29:40 - loss: 0.6292 - regression_loss: 0.5092 - classification_loss: 0.1200
 6105/10000 [=================>............] - ETA: 29:39 - loss: 0.6291 - regression_loss: 0.5091 - classification_loss: 0.1200
 6106/10000 [=================>............] - ETA: 29:39 - loss: 0.6291 - regression_loss: 0.5091 - classification_loss: 0.1200
 6107/10000 [=================>............] - ETA: 29:38 - loss: 0.6291 - regression_loss: 0.5091 - classification_loss: 0.1200
 6108/10000 [=================>............] - ETA: 29:38 - loss: 0.6290 - regression_loss: 0.5091 - classification_loss: 0.1200
 6109/10000 [=================>............] - ETA: 29:37 - loss: 0.6291 - regression_loss: 0.5091 - classification_loss: 0.1200
 6110/10000 [=================>............] - ETA: 29:37 - loss: 0.6291 - regression_loss: 0.5091 - classification_loss: 0.1200
 6111/10000 [=================>............] - ETA: 29:37 - loss: 0.6291 - regression_loss: 0.5091 - classification_loss: 0.1200
 6112/10000 [=================>............] - ETA: 29:36 - loss: 0.6291 - regression_loss: 0.5091 - classification_loss: 0.1200
 6113/10000 [=================>............] - ETA: 29:36 - loss: 0.6290 - regression_loss: 0.5090 - classification_loss: 0.1200
 6114/10000 [=================>............] - ETA: 29:35 - loss: 0.6291 - regression_loss: 0.5091 - classification_loss: 0.1200
 6115/10000 [=================>............] - ETA: 29:35 - loss: 0.6290 - regression_loss: 0.5090 - classification_loss: 0.1200
 6116/10000 [=================>............] - ETA: 29:34 - loss: 0.6290 - regression_loss: 0.5090 - classification_loss: 0.1200
 6117/10000 [=================>............] - ETA: 29:34 - loss: 0.6290 - regression_loss: 0.5091 - classification_loss: 0.1200
 6118/10000 [=================>............] - ETA: 29:33 - loss: 0.6290 - regression_loss: 0.5090 - classification_loss: 0.1200
 6119/10000 [=================>............] - ETA: 29:33 - loss: 0.6290 - regression_loss: 0.5090 - classification_loss: 0.1200
 6120/10000 [=================>............] - ETA: 29:32 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1199
 6121/10000 [=================>............] - ETA: 29:32 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1200
 6122/10000 [=================>............] - ETA: 29:32 - loss: 0.6290 - regression_loss: 0.5090 - classification_loss: 0.1200
 6123/10000 [=================>............] - ETA: 29:31 - loss: 0.6290 - regression_loss: 0.5090 - classification_loss: 0.1200
 6124/10000 [=================>............] - ETA: 29:31 - loss: 0.6291 - regression_loss: 0.5091 - classification_loss: 0.1200
 6125/10000 [=================>............] - ETA: 29:30 - loss: 0.6290 - regression_loss: 0.5091 - classification_loss: 0.1200
 6126/10000 [=================>............] - ETA: 29:30 - loss: 0.6291 - regression_loss: 0.5091 - classification_loss: 0.1200
 6127/10000 [=================>............] - ETA: 29:29 - loss: 0.6291 - regression_loss: 0.5091 - classification_loss: 0.1200
 6128/10000 [=================>............] - ETA: 29:29 - loss: 0.6291 - regression_loss: 0.5091 - classification_loss: 0.1200
 6129/10000 [=================>............] - ETA: 29:28 - loss: 0.6290 - regression_loss: 0.5091 - classification_loss: 0.1200
 6130/10000 [=================>............] - ETA: 29:28 - loss: 0.6290 - regression_loss: 0.5091 - classification_loss: 0.1199
 6131/10000 [=================>............] - ETA: 29:27 - loss: 0.6290 - regression_loss: 0.5091 - classification_loss: 0.1199
 6132/10000 [=================>............] - ETA: 29:27 - loss: 0.6290 - regression_loss: 0.5091 - classification_loss: 0.1199
 6133/10000 [=================>............] - ETA: 29:26 - loss: 0.6290 - regression_loss: 0.5091 - classification_loss: 0.1199
 6134/10000 [=================>............] - ETA: 29:26 - loss: 0.6290 - regression_loss: 0.5091 - classification_loss: 0.1199
 6135/10000 [=================>............] - ETA: 29:26 - loss: 0.6290 - regression_loss: 0.5090 - classification_loss: 0.1199
 6136/10000 [=================>............] - ETA: 29:25 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1199
 6137/10000 [=================>............] - ETA: 29:25 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1199
 6138/10000 [=================>............] - ETA: 29:24 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1199
 6139/10000 [=================>............] - ETA: 29:24 - loss: 0.6288 - regression_loss: 0.5088 - classification_loss: 0.1199
 6140/10000 [=================>............] - ETA: 29:23 - loss: 0.6287 - regression_loss: 0.5088 - classification_loss: 0.1199
 6141/10000 [=================>............] - ETA: 29:23 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1200
 6142/10000 [=================>............] - ETA: 29:22 - loss: 0.6288 - regression_loss: 0.5088 - classification_loss: 0.1200
 6143/10000 [=================>............] - ETA: 29:22 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1200
 6144/10000 [=================>............] - ETA: 29:21 - loss: 0.6289 - regression_loss: 0.5089 - classification_loss: 0.1200
 6145/10000 [=================>............] - ETA: 29:21 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1200
 6146/10000 [=================>............] - ETA: 29:21 - loss: 0.6289 - regression_loss: 0.5089 - classification_loss: 0.1200
 6147/10000 [=================>............] - ETA: 29:20 - loss: 0.6289 - regression_loss: 0.5089 - classification_loss: 0.1200
 6148/10000 [=================>............] - ETA: 29:20 - loss: 0.6289 - regression_loss: 0.5089 - classification_loss: 0.1200
 6149/10000 [=================>............] - ETA: 29:19 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1200
 6150/10000 [=================>............] - ETA: 29:19 - loss: 0.6290 - regression_loss: 0.5090 - classification_loss: 0.1200
 6151/10000 [=================>............] - ETA: 29:18 - loss: 0.6292 - regression_loss: 0.5091 - classification_loss: 0.1200
 6152/10000 [=================>............] - ETA: 29:18 - loss: 0.6292 - regression_loss: 0.5092 - classification_loss: 0.1200
 6153/10000 [=================>............] - ETA: 29:17 - loss: 0.6291 - regression_loss: 0.5091 - classification_loss: 0.1200
 6154/10000 [=================>............] - ETA: 29:17 - loss: 0.6292 - regression_loss: 0.5092 - classification_loss: 0.1200
 6155/10000 [=================>............] - ETA: 29:16 - loss: 0.6292 - regression_loss: 0.5091 - classification_loss: 0.1200
 6156/10000 [=================>............] - ETA: 29:16 - loss: 0.6291 - regression_loss: 0.5091 - classification_loss: 0.1200
 6157/10000 [=================>............] - ETA: 29:16 - loss: 0.6291 - regression_loss: 0.5091 - classification_loss: 0.1200
 6158/10000 [=================>............] - ETA: 29:15 - loss: 0.6290 - regression_loss: 0.5091 - classification_loss: 0.1200
 6159/10000 [=================>............] - ETA: 29:15 - loss: 0.6291 - regression_loss: 0.5091 - classification_loss: 0.1200
 6160/10000 [=================>............] - ETA: 29:14 - loss: 0.6290 - regression_loss: 0.5090 - classification_loss: 0.1200
 6161/10000 [=================>............] - ETA: 29:14 - loss: 0.6290 - regression_loss: 0.5090 - classification_loss: 0.1200
 6162/10000 [=================>............] - ETA: 29:13 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1200
 6163/10000 [=================>............] - ETA: 29:13 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1200
 6164/10000 [=================>............] - ETA: 29:12 - loss: 0.6289 - regression_loss: 0.5089 - classification_loss: 0.1200
 6165/10000 [=================>............] - ETA: 29:12 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1199
 6166/10000 [=================>............] - ETA: 29:12 - loss: 0.6288 - regression_loss: 0.5088 - classification_loss: 0.1199
 6167/10000 [=================>............] - ETA: 29:11 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1199
 6168/10000 [=================>............] - ETA: 29:11 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1199
 6169/10000 [=================>............] - ETA: 29:10 - loss: 0.6287 - regression_loss: 0.5088 - classification_loss: 0.1199
 6170/10000 [=================>............] - ETA: 29:10 - loss: 0.6287 - regression_loss: 0.5088 - classification_loss: 0.1199
 6171/10000 [=================>............] - ETA: 29:09 - loss: 0.6288 - regression_loss: 0.5088 - classification_loss: 0.1199
 6172/10000 [=================>............] - ETA: 29:09 - loss: 0.6288 - regression_loss: 0.5088 - classification_loss: 0.1200
 6173/10000 [=================>............] - ETA: 29:08 - loss: 0.6288 - regression_loss: 0.5088 - classification_loss: 0.1199
 6174/10000 [=================>............] - ETA: 29:08 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1199
 6175/10000 [=================>............] - ETA: 29:07 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1199
 6176/10000 [=================>............] - ETA: 29:07 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1199
 6177/10000 [=================>............] - ETA: 29:07 - loss: 0.6287 - regression_loss: 0.5088 - classification_loss: 0.1199
 6178/10000 [=================>............] - ETA: 29:06 - loss: 0.6287 - regression_loss: 0.5088 - classification_loss: 0.1199
 6179/10000 [=================>............] - ETA: 29:06 - loss: 0.6287 - regression_loss: 0.5088 - classification_loss: 0.1199
 6180/10000 [=================>............] - ETA: 29:05 - loss: 0.6287 - regression_loss: 0.5088 - classification_loss: 0.1199
 6181/10000 [=================>............] - ETA: 29:05 - loss: 0.6287 - regression_loss: 0.5088 - classification_loss: 0.1199
 6182/10000 [=================>............] - ETA: 29:04 - loss: 0.6286 - regression_loss: 0.5088 - classification_loss: 0.1199
 6183/10000 [=================>............] - ETA: 29:04 - loss: 0.6287 - regression_loss: 0.5088 - classification_loss: 0.1199
 6184/10000 [=================>............] - ETA: 29:03 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1199
 6185/10000 [=================>............] - ETA: 29:03 - loss: 0.6288 - regression_loss: 0.5088 - classification_loss: 0.1199
 6186/10000 [=================>............] - ETA: 29:02 - loss: 0.6287 - regression_loss: 0.5088 - classification_loss: 0.1199
 6187/10000 [=================>............] - ETA: 29:02 - loss: 0.6287 - regression_loss: 0.5088 - classification_loss: 0.1199
 6188/10000 [=================>............] - ETA: 29:01 - loss: 0.6286 - regression_loss: 0.5087 - classification_loss: 0.1199
 6189/10000 [=================>............] - ETA: 29:01 - loss: 0.6286 - regression_loss: 0.5087 - classification_loss: 0.1199
 6190/10000 [=================>............] - ETA: 29:01 - loss: 0.6286 - regression_loss: 0.5087 - classification_loss: 0.1199
 6191/10000 [=================>............] - ETA: 29:00 - loss: 0.6286 - regression_loss: 0.5087 - classification_loss: 0.1199
 6192/10000 [=================>............] - ETA: 29:00 - loss: 0.6285 - regression_loss: 0.5086 - classification_loss: 0.1199
 6193/10000 [=================>............] - ETA: 28:59 - loss: 0.6285 - regression_loss: 0.5087 - classification_loss: 0.1199
 6194/10000 [=================>............] - ETA: 28:59 - loss: 0.6285 - regression_loss: 0.5087 - classification_loss: 0.1199
 6195/10000 [=================>............] - ETA: 28:58 - loss: 0.6285 - regression_loss: 0.5087 - classification_loss: 0.1199
 6196/10000 [=================>............] - ETA: 28:58 - loss: 0.6285 - regression_loss: 0.5086 - classification_loss: 0.1199
 6197/10000 [=================>............] - ETA: 28:57 - loss: 0.6285 - regression_loss: 0.5086 - classification_loss: 0.1198
 6198/10000 [=================>............] - ETA: 28:57 - loss: 0.6285 - regression_loss: 0.5087 - classification_loss: 0.1199
 6199/10000 [=================>............] - ETA: 28:56 - loss: 0.6287 - regression_loss: 0.5088 - classification_loss: 0.1199
 6200/10000 [=================>............] - ETA: 28:56 - loss: 0.6286 - regression_loss: 0.5088 - classification_loss: 0.1199
 6201/10000 [=================>............] - ETA: 28:55 - loss: 0.6287 - regression_loss: 0.5088 - classification_loss: 0.1199
 6202/10000 [=================>............] - ETA: 28:55 - loss: 0.6288 - regression_loss: 0.5090 - classification_loss: 0.1199
 6203/10000 [=================>............] - ETA: 28:55 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1199
 6204/10000 [=================>............] - ETA: 28:54 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1199
 6205/10000 [=================>............] - ETA: 28:54 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1199
 6206/10000 [=================>............] - ETA: 28:53 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1199
 6207/10000 [=================>............] - ETA: 28:53 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1199
 6208/10000 [=================>............] - ETA: 28:52 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1199
 6209/10000 [=================>............] - ETA: 28:52 - loss: 0.6289 - regression_loss: 0.5089 - classification_loss: 0.1199
 6210/10000 [=================>............] - ETA: 28:51 - loss: 0.6289 - regression_loss: 0.5089 - classification_loss: 0.1199
 6211/10000 [=================>............] - ETA: 28:51 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1199
 6212/10000 [=================>............] - ETA: 28:50 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1199
 6213/10000 [=================>............] - ETA: 28:50 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1199
 6214/10000 [=================>............] - ETA: 28:50 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1199
 6215/10000 [=================>............] - ETA: 28:49 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1199
 6216/10000 [=================>............] - ETA: 28:49 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1199
 6217/10000 [=================>............] - ETA: 28:48 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1199
 6218/10000 [=================>............] - ETA: 28:48 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1199
 6219/10000 [=================>............] - ETA: 28:47 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1199
 6220/10000 [=================>............] - ETA: 28:47 - loss: 0.6288 - regression_loss: 0.5090 - classification_loss: 0.1199
 6221/10000 [=================>............] - ETA: 28:46 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1199
 6222/10000 [=================>............] - ETA: 28:46 - loss: 0.6288 - regression_loss: 0.5090 - classification_loss: 0.1199
 6223/10000 [=================>............] - ETA: 28:45 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1199
 6224/10000 [=================>............] - ETA: 28:45 - loss: 0.6287 - regression_loss: 0.5089 - classification_loss: 0.1199
 6225/10000 [=================>............] - ETA: 28:44 - loss: 0.6287 - regression_loss: 0.5088 - classification_loss: 0.1198
 6226/10000 [=================>............] - ETA: 28:44 - loss: 0.6286 - regression_loss: 0.5088 - classification_loss: 0.1198
 6227/10000 [=================>............] - ETA: 28:43 - loss: 0.6287 - regression_loss: 0.5088 - classification_loss: 0.1199
 6228/10000 [=================>............] - ETA: 28:43 - loss: 0.6287 - regression_loss: 0.5088 - classification_loss: 0.1199
 6229/10000 [=================>............] - ETA: 28:43 - loss: 0.6286 - regression_loss: 0.5088 - classification_loss: 0.1199
 6230/10000 [=================>............] - ETA: 28:42 - loss: 0.6287 - regression_loss: 0.5088 - classification_loss: 0.1199
 6231/10000 [=================>............] - ETA: 28:42 - loss: 0.6287 - regression_loss: 0.5088 - classification_loss: 0.1199
 6232/10000 [=================>............] - ETA: 28:41 - loss: 0.6286 - regression_loss: 0.5088 - classification_loss: 0.1199
 6233/10000 [=================>............] - ETA: 28:41 - loss: 0.6287 - regression_loss: 0.5088 - classification_loss: 0.1199
 6234/10000 [=================>............] - ETA: 28:40 - loss: 0.6286 - regression_loss: 0.5088 - classification_loss: 0.1199
 6235/10000 [=================>............] - ETA: 28:40 - loss: 0.6286 - regression_loss: 0.5088 - classification_loss: 0.1199
 6236/10000 [=================>............] - ETA: 28:39 - loss: 0.6287 - regression_loss: 0.5088 - classification_loss: 0.1199
 6237/10000 [=================>............] - ETA: 28:39 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1199
 6238/10000 [=================>............] - ETA: 28:38 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1199
 6239/10000 [=================>............] - ETA: 28:38 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1199
 6240/10000 [=================>............] - ETA: 28:38 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1199
 6241/10000 [=================>............] - ETA: 28:37 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1199
 6242/10000 [=================>............] - ETA: 28:37 - loss: 0.6287 - regression_loss: 0.5089 - classification_loss: 0.1199
 6243/10000 [=================>............] - ETA: 28:36 - loss: 0.6287 - regression_loss: 0.5088 - classification_loss: 0.1199
 6244/10000 [=================>............] - ETA: 28:36 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1199
 6245/10000 [=================>............] - ETA: 28:35 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1199
 6246/10000 [=================>............] - ETA: 28:35 - loss: 0.6289 - regression_loss: 0.5091 - classification_loss: 0.1199
 6247/10000 [=================>............] - ETA: 28:34 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1199
 6248/10000 [=================>............] - ETA: 28:34 - loss: 0.6288 - regression_loss: 0.5090 - classification_loss: 0.1198
 6249/10000 [=================>............] - ETA: 28:33 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1198
 6250/10000 [=================>............] - ETA: 28:33 - loss: 0.6287 - regression_loss: 0.5089 - classification_loss: 0.1198
 6251/10000 [=================>............] - ETA: 28:33 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1198
 6252/10000 [=================>............] - ETA: 28:32 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1198
 6253/10000 [=================>............] - ETA: 28:32 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1199
 6254/10000 [=================>............] - ETA: 28:31 - loss: 0.6290 - regression_loss: 0.5092 - classification_loss: 0.1199
 6255/10000 [=================>............] - ETA: 28:31 - loss: 0.6290 - regression_loss: 0.5092 - classification_loss: 0.1199
 6256/10000 [=================>............] - ETA: 28:30 - loss: 0.6290 - regression_loss: 0.5092 - classification_loss: 0.1198
 6257/10000 [=================>............] - ETA: 28:30 - loss: 0.6290 - regression_loss: 0.5091 - classification_loss: 0.1198
 6258/10000 [=================>............] - ETA: 28:29 - loss: 0.6290 - regression_loss: 0.5091 - classification_loss: 0.1199
 6259/10000 [=================>............] - ETA: 28:29 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1199
 6260/10000 [=================>............] - ETA: 28:28 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1198
 6261/10000 [=================>............] - ETA: 28:28 - loss: 0.6288 - regression_loss: 0.5090 - classification_loss: 0.1198
 6262/10000 [=================>............] - ETA: 28:28 - loss: 0.6289 - regression_loss: 0.5091 - classification_loss: 0.1199
 6263/10000 [=================>............] - ETA: 28:27 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1199
 6264/10000 [=================>............] - ETA: 28:27 - loss: 0.6288 - regression_loss: 0.5090 - classification_loss: 0.1198
 6265/10000 [=================>............] - ETA: 28:26 - loss: 0.6287 - regression_loss: 0.5089 - classification_loss: 0.1198
 6266/10000 [=================>............] - ETA: 28:26 - loss: 0.6287 - regression_loss: 0.5089 - classification_loss: 0.1198
 6267/10000 [=================>............] - ETA: 28:25 - loss: 0.6286 - regression_loss: 0.5088 - classification_loss: 0.1198
 6268/10000 [=================>............] - ETA: 28:25 - loss: 0.6287 - regression_loss: 0.5089 - classification_loss: 0.1198
 6269/10000 [=================>............] - ETA: 28:24 - loss: 0.6287 - regression_loss: 0.5089 - classification_loss: 0.1198
 6270/10000 [=================>............] - ETA: 28:24 - loss: 0.6287 - regression_loss: 0.5089 - classification_loss: 0.1198
 6271/10000 [=================>............] - ETA: 28:23 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1198
 6272/10000 [=================>............] - ETA: 28:23 - loss: 0.6287 - regression_loss: 0.5089 - classification_loss: 0.1198
 6273/10000 [=================>............] - ETA: 28:22 - loss: 0.6286 - regression_loss: 0.5088 - classification_loss: 0.1198
 6274/10000 [=================>............] - ETA: 28:22 - loss: 0.6286 - regression_loss: 0.5088 - classification_loss: 0.1198
 6275/10000 [=================>............] - ETA: 28:22 - loss: 0.6285 - regression_loss: 0.5088 - classification_loss: 0.1198
 6276/10000 [=================>............] - ETA: 28:21 - loss: 0.6286 - regression_loss: 0.5088 - classification_loss: 0.1198
 6277/10000 [=================>............] - ETA: 28:21 - loss: 0.6286 - regression_loss: 0.5088 - classification_loss: 0.1198
 6278/10000 [=================>............] - ETA: 28:20 - loss: 0.6286 - regression_loss: 0.5088 - classification_loss: 0.1198
 6279/10000 [=================>............] - ETA: 28:20 - loss: 0.6286 - regression_loss: 0.5088 - classification_loss: 0.1198
 6280/10000 [=================>............] - ETA: 28:19 - loss: 0.6285 - regression_loss: 0.5087 - classification_loss: 0.1198
 6281/10000 [=================>............] - ETA: 28:19 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1198
 6282/10000 [=================>............] - ETA: 28:18 - loss: 0.6287 - regression_loss: 0.5089 - classification_loss: 0.1198
 6283/10000 [=================>............] - ETA: 28:18 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1198
 6284/10000 [=================>............] - ETA: 28:17 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1198
 6285/10000 [=================>............] - ETA: 28:17 - loss: 0.6288 - regression_loss: 0.5090 - classification_loss: 0.1198
 6286/10000 [=================>............] - ETA: 28:17 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1198
 6287/10000 [=================>............] - ETA: 28:16 - loss: 0.6289 - regression_loss: 0.5091 - classification_loss: 0.1198
 6288/10000 [=================>............] - ETA: 28:16 - loss: 0.6289 - regression_loss: 0.5090 - classification_loss: 0.1199
 6289/10000 [=================>............] - ETA: 28:15 - loss: 0.6288 - regression_loss: 0.5090 - classification_loss: 0.1199
 6290/10000 [=================>............] - ETA: 28:15 - loss: 0.6288 - regression_loss: 0.5090 - classification_loss: 0.1198
 6291/10000 [=================>............] - ETA: 28:14 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1198
 6292/10000 [=================>............] - ETA: 28:14 - loss: 0.6287 - regression_loss: 0.5089 - classification_loss: 0.1198
 6293/10000 [=================>............] - ETA: 28:13 - loss: 0.6287 - regression_loss: 0.5089 - classification_loss: 0.1198
 6294/10000 [=================>............] - ETA: 28:13 - loss: 0.6288 - regression_loss: 0.5089 - classification_loss: 0.1198
 6295/10000 [=================>............] - ETA: 28:13 - loss: 0.6287 - regression_loss: 0.5089 - classification_loss: 0.1198
 6296/10000 [=================>............] - ETA: 28:12 - loss: 0.6287 - regression_loss: 0.5089 - classification_loss: 0.1198
 6297/10000 [=================>............] - ETA: 28:12 - loss: 0.6286 - regression_loss: 0.5088 - classification_loss: 0.1198
 6298/10000 [=================>............] - ETA: 28:11 - loss: 0.6286 - regression_loss: 0.5088 - classification_loss: 0.1198
 6299/10000 [=================>............] - ETA: 28:11 - loss: 0.6286 - regression_loss: 0.5088 - classification_loss: 0.1198
 6300/10000 [=================>............] - ETA: 28:10 - loss: 0.6286 - regression_loss: 0.5088 - classification_loss: 0.1198
 6301/10000 [=================>............] - ETA: 28:10 - loss: 0.6287 - regression_loss: 0.5088 - classification_loss: 0.1198
 6302/10000 [=================>............] - ETA: 28:09 - loss: 0.6286 - regression_loss: 0.5088 - classification_loss: 0.1198
 6303/10000 [=================>............] - ETA: 28:09 - loss: 0.6286 - regression_loss: 0.5088 - classification_loss: 0.1198
 6304/10000 [=================>............] - ETA: 28:08 - loss: 0.6285 - regression_loss: 0.5087 - classification_loss: 0.1198
 6305/10000 [=================>............] - ETA: 28:08 - loss: 0.6285 - regression_loss: 0.5087 - classification_loss: 0.1198
 6306/10000 [=================>............] - ETA: 28:08 - loss: 0.6284 - regression_loss: 0.5087 - classification_loss: 0.1198
 6307/10000 [=================>............] - ETA: 28:07 - loss: 0.6284 - regression_loss: 0.5086 - classification_loss: 0.1198
 6308/10000 [=================>............] - ETA: 28:07 - loss: 0.6285 - regression_loss: 0.5087 - classification_loss: 0.1198
 6309/10000 [=================>............] - ETA: 28:06 - loss: 0.6286 - regression_loss: 0.5087 - classification_loss: 0.1198
 6310/10000 [=================>............] - ETA: 28:06 - loss: 0.6286 - regression_loss: 0.5087 - classification_loss: 0.1199
 6311/10000 [=================>............] - ETA: 28:05 - loss: 0.6286 - regression_loss: 0.5087 - classification_loss: 0.1199
 6312/10000 [=================>............] - ETA: 28:05 - loss: 0.6286 - regression_loss: 0.5087 - classification_loss: 0.1199
 6313/10000 [=================>............] - ETA: 28:04 - loss: 0.6286 - regression_loss: 0.5087 - classification_loss: 0.1199
 6314/10000 [=================>............] - ETA: 28:04 - loss: 0.6286 - regression_loss: 0.5087 - classification_loss: 0.1199
 6315/10000 [=================>............] - ETA: 28:03 - loss: 0.6285 - regression_loss: 0.5086 - classification_loss: 0.1199
 6316/10000 [=================>............] - ETA: 28:03 - loss: 0.6285 - regression_loss: 0.5086 - classification_loss: 0.1199
 6317/10000 [=================>............] - ETA: 28:03 - loss: 0.6284 - regression_loss: 0.5086 - classification_loss: 0.1199
 6318/10000 [=================>............] - ETA: 28:02 - loss: 0.6283 - regression_loss: 0.5085 - classification_loss: 0.1198
 6319/10000 [=================>............] - ETA: 28:02 - loss: 0.6283 - regression_loss: 0.5085 - classification_loss: 0.1198
 6320/10000 [=================>............] - ETA: 28:01 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1198
 6321/10000 [=================>............] - ETA: 28:01 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1198
 6322/10000 [=================>............] - ETA: 28:00 - loss: 0.6283 - regression_loss: 0.5085 - classification_loss: 0.1198
 6323/10000 [=================>............] - ETA: 28:00 - loss: 0.6283 - regression_loss: 0.5084 - classification_loss: 0.1198
 6324/10000 [=================>............] - ETA: 27:59 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1198
 6325/10000 [=================>............] - ETA: 27:59 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1198
 6326/10000 [=================>............] - ETA: 27:58 - loss: 0.6283 - regression_loss: 0.5084 - classification_loss: 0.1198
 6327/10000 [=================>............] - ETA: 27:58 - loss: 0.6283 - regression_loss: 0.5084 - classification_loss: 0.1199
 6328/10000 [=================>............] - ETA: 27:58 - loss: 0.6283 - regression_loss: 0.5084 - classification_loss: 0.1199
 6329/10000 [=================>............] - ETA: 27:57 - loss: 0.6283 - regression_loss: 0.5085 - classification_loss: 0.1199
 6330/10000 [=================>............] - ETA: 27:57 - loss: 0.6284 - regression_loss: 0.5085 - classification_loss: 0.1199
 6331/10000 [=================>............] - ETA: 27:56 - loss: 0.6283 - regression_loss: 0.5085 - classification_loss: 0.1199
 6332/10000 [=================>............] - ETA: 27:56 - loss: 0.6284 - regression_loss: 0.5085 - classification_loss: 0.1199
 6333/10000 [=================>............] - ETA: 27:55 - loss: 0.6284 - regression_loss: 0.5085 - classification_loss: 0.1199
 6334/10000 [==================>...........] - ETA: 27:55 - loss: 0.6283 - regression_loss: 0.5085 - classification_loss: 0.1198
 6335/10000 [==================>...........] - ETA: 27:54 - loss: 0.6284 - regression_loss: 0.5086 - classification_loss: 0.1199
 6336/10000 [==================>...........] - ETA: 27:54 - loss: 0.6284 - regression_loss: 0.5086 - classification_loss: 0.1199
 6337/10000 [==================>...........] - ETA: 27:53 - loss: 0.6284 - regression_loss: 0.5085 - classification_loss: 0.1198
 6338/10000 [==================>...........] - ETA: 27:53 - loss: 0.6284 - regression_loss: 0.5086 - classification_loss: 0.1198
 6339/10000 [==================>...........] - ETA: 27:53 - loss: 0.6283 - regression_loss: 0.5085 - classification_loss: 0.1198
 6340/10000 [==================>...........] - ETA: 27:52 - loss: 0.6283 - regression_loss: 0.5085 - classification_loss: 0.1198
 6341/10000 [==================>...........] - ETA: 27:52 - loss: 0.6283 - regression_loss: 0.5085 - classification_loss: 0.1198
 6342/10000 [==================>...........] - ETA: 27:51 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1198
 6343/10000 [==================>...........] - ETA: 27:51 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1198
 6344/10000 [==================>...........] - ETA: 27:50 - loss: 0.6281 - regression_loss: 0.5083 - classification_loss: 0.1198
 6345/10000 [==================>...........] - ETA: 27:50 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1198
 6346/10000 [==================>...........] - ETA: 27:49 - loss: 0.6281 - regression_loss: 0.5084 - classification_loss: 0.1198
 6347/10000 [==================>...........] - ETA: 27:49 - loss: 0.6281 - regression_loss: 0.5083 - classification_loss: 0.1198
 6348/10000 [==================>...........] - ETA: 27:48 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1198
 6349/10000 [==================>...........] - ETA: 27:48 - loss: 0.6281 - regression_loss: 0.5083 - classification_loss: 0.1198
 6350/10000 [==================>...........] - ETA: 27:48 - loss: 0.6281 - regression_loss: 0.5083 - classification_loss: 0.1198
 6351/10000 [==================>...........] - ETA: 27:47 - loss: 0.6281 - regression_loss: 0.5083 - classification_loss: 0.1198
 6352/10000 [==================>...........] - ETA: 27:47 - loss: 0.6281 - regression_loss: 0.5083 - classification_loss: 0.1198
 6353/10000 [==================>...........] - ETA: 27:46 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1198
 6354/10000 [==================>...........] - ETA: 27:46 - loss: 0.6283 - regression_loss: 0.5085 - classification_loss: 0.1198
 6355/10000 [==================>...........] - ETA: 27:45 - loss: 0.6284 - regression_loss: 0.5085 - classification_loss: 0.1198
 6356/10000 [==================>...........] - ETA: 27:45 - loss: 0.6284 - regression_loss: 0.5085 - classification_loss: 0.1198
 6357/10000 [==================>...........] - ETA: 27:44 - loss: 0.6284 - regression_loss: 0.5086 - classification_loss: 0.1199
 6358/10000 [==================>...........] - ETA: 27:44 - loss: 0.6284 - regression_loss: 0.5085 - classification_loss: 0.1199
 6359/10000 [==================>...........] - ETA: 27:43 - loss: 0.6283 - regression_loss: 0.5085 - classification_loss: 0.1198
 6360/10000 [==================>...........] - ETA: 27:43 - loss: 0.6283 - regression_loss: 0.5085 - classification_loss: 0.1198
 6361/10000 [==================>...........] - ETA: 27:42 - loss: 0.6283 - regression_loss: 0.5084 - classification_loss: 0.1198
 6362/10000 [==================>...........] - ETA: 27:42 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1198
 6363/10000 [==================>...........] - ETA: 27:42 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1198
 6364/10000 [==================>...........] - ETA: 27:41 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1198
 6365/10000 [==================>...........] - ETA: 27:41 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1198
 6366/10000 [==================>...........] - ETA: 27:40 - loss: 0.6283 - regression_loss: 0.5084 - classification_loss: 0.1198
 6367/10000 [==================>...........] - ETA: 27:40 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1198
 6368/10000 [==================>...........] - ETA: 27:39 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1198
 6369/10000 [==================>...........] - ETA: 27:39 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1198
 6370/10000 [==================>...........] - ETA: 27:38 - loss: 0.6281 - regression_loss: 0.5083 - classification_loss: 0.1198
 6371/10000 [==================>...........] - ETA: 27:38 - loss: 0.6281 - regression_loss: 0.5083 - classification_loss: 0.1198
 6372/10000 [==================>...........] - ETA: 27:37 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1198
 6373/10000 [==================>...........] - ETA: 27:37 - loss: 0.6281 - regression_loss: 0.5083 - classification_loss: 0.1198
 6374/10000 [==================>...........] - ETA: 27:37 - loss: 0.6284 - regression_loss: 0.5085 - classification_loss: 0.1198
 6375/10000 [==================>...........] - ETA: 27:36 - loss: 0.6283 - regression_loss: 0.5085 - classification_loss: 0.1198
 6376/10000 [==================>...........] - ETA: 27:36 - loss: 0.6283 - regression_loss: 0.5085 - classification_loss: 0.1198
 6377/10000 [==================>...........] - ETA: 27:35 - loss: 0.6283 - regression_loss: 0.5085 - classification_loss: 0.1198
 6378/10000 [==================>...........] - ETA: 27:35 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1198
 6379/10000 [==================>...........] - ETA: 27:34 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1198
 6380/10000 [==================>...........] - ETA: 27:34 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1198
 6381/10000 [==================>...........] - ETA: 27:33 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1198
 6382/10000 [==================>...........] - ETA: 27:33 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1198
 6383/10000 [==================>...........] - ETA: 27:32 - loss: 0.6281 - regression_loss: 0.5083 - classification_loss: 0.1198
 6384/10000 [==================>...........] - ETA: 27:32 - loss: 0.6281 - regression_loss: 0.5083 - classification_loss: 0.1198
 6385/10000 [==================>...........] - ETA: 27:32 - loss: 0.6281 - regression_loss: 0.5083 - classification_loss: 0.1198
 6386/10000 [==================>...........] - ETA: 27:31 - loss: 0.6281 - regression_loss: 0.5083 - classification_loss: 0.1198
 6387/10000 [==================>...........] - ETA: 27:31 - loss: 0.6281 - regression_loss: 0.5083 - classification_loss: 0.1198
 6388/10000 [==================>...........] - ETA: 27:30 - loss: 0.6281 - regression_loss: 0.5083 - classification_loss: 0.1198
 6389/10000 [==================>...........] - ETA: 27:30 - loss: 0.6280 - regression_loss: 0.5083 - classification_loss: 0.1198
 6390/10000 [==================>...........] - ETA: 27:29 - loss: 0.6280 - regression_loss: 0.5083 - classification_loss: 0.1197
 6391/10000 [==================>...........] - ETA: 27:29 - loss: 0.6279 - regression_loss: 0.5082 - classification_loss: 0.1197
 6392/10000 [==================>...........] - ETA: 27:28 - loss: 0.6280 - regression_loss: 0.5082 - classification_loss: 0.1198
 6393/10000 [==================>...........] - ETA: 27:28 - loss: 0.6279 - regression_loss: 0.5081 - classification_loss: 0.1197
 6394/10000 [==================>...........] - ETA: 27:28 - loss: 0.6278 - regression_loss: 0.5081 - classification_loss: 0.1197
 6395/10000 [==================>...........] - ETA: 27:27 - loss: 0.6278 - regression_loss: 0.5080 - classification_loss: 0.1197
 6396/10000 [==================>...........] - ETA: 27:27 - loss: 0.6278 - regression_loss: 0.5081 - classification_loss: 0.1197
 6397/10000 [==================>...........] - ETA: 27:26 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1197
 6398/10000 [==================>...........] - ETA: 27:26 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1197
 6399/10000 [==================>...........] - ETA: 27:25 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1197
 6400/10000 [==================>...........] - ETA: 27:25 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1197
 6401/10000 [==================>...........] - ETA: 27:24 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1197
 6402/10000 [==================>...........] - ETA: 27:24 - loss: 0.6276 - regression_loss: 0.5079 - classification_loss: 0.1197
 6403/10000 [==================>...........] - ETA: 27:23 - loss: 0.6276 - regression_loss: 0.5079 - classification_loss: 0.1197
 6404/10000 [==================>...........] - ETA: 27:23 - loss: 0.6276 - regression_loss: 0.5079 - classification_loss: 0.1197
 6405/10000 [==================>...........] - ETA: 27:22 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1197
 6406/10000 [==================>...........] - ETA: 27:22 - loss: 0.6278 - regression_loss: 0.5081 - classification_loss: 0.1197
 6407/10000 [==================>...........] - ETA: 27:22 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1197
 6408/10000 [==================>...........] - ETA: 27:21 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1197
 6409/10000 [==================>...........] - ETA: 27:21 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1197
 6410/10000 [==================>...........] - ETA: 27:20 - loss: 0.6276 - regression_loss: 0.5079 - classification_loss: 0.1197
 6411/10000 [==================>...........] - ETA: 27:20 - loss: 0.6276 - regression_loss: 0.5079 - classification_loss: 0.1197
 6412/10000 [==================>...........] - ETA: 27:19 - loss: 0.6276 - regression_loss: 0.5079 - classification_loss: 0.1197
 6413/10000 [==================>...........] - ETA: 27:19 - loss: 0.6276 - regression_loss: 0.5079 - classification_loss: 0.1197
 6414/10000 [==================>...........] - ETA: 27:18 - loss: 0.6276 - regression_loss: 0.5079 - classification_loss: 0.1197
 6415/10000 [==================>...........] - ETA: 27:18 - loss: 0.6276 - regression_loss: 0.5079 - classification_loss: 0.1197
 6416/10000 [==================>...........] - ETA: 27:17 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1197
 6417/10000 [==================>...........] - ETA: 27:17 - loss: 0.6276 - regression_loss: 0.5079 - classification_loss: 0.1197
 6418/10000 [==================>...........] - ETA: 27:17 - loss: 0.6276 - regression_loss: 0.5079 - classification_loss: 0.1197
 6419/10000 [==================>...........] - ETA: 27:16 - loss: 0.6276 - regression_loss: 0.5079 - classification_loss: 0.1197
 6420/10000 [==================>...........] - ETA: 27:16 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1197
 6421/10000 [==================>...........] - ETA: 27:15 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1197
 6422/10000 [==================>...........] - ETA: 27:15 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1197
 6423/10000 [==================>...........] - ETA: 27:14 - loss: 0.6276 - regression_loss: 0.5079 - classification_loss: 0.1197
 6424/10000 [==================>...........] - ETA: 27:14 - loss: 0.6276 - regression_loss: 0.5079 - classification_loss: 0.1197
 6425/10000 [==================>...........] - ETA: 27:13 - loss: 0.6275 - regression_loss: 0.5079 - classification_loss: 0.1196
 6426/10000 [==================>...........] - ETA: 27:13 - loss: 0.6274 - regression_loss: 0.5078 - classification_loss: 0.1196
 6427/10000 [==================>...........] - ETA: 27:12 - loss: 0.6274 - regression_loss: 0.5077 - classification_loss: 0.1196
 6428/10000 [==================>...........] - ETA: 27:12 - loss: 0.6273 - regression_loss: 0.5077 - classification_loss: 0.1196
 6429/10000 [==================>...........] - ETA: 27:11 - loss: 0.6273 - regression_loss: 0.5077 - classification_loss: 0.1196
 6430/10000 [==================>...........] - ETA: 27:11 - loss: 0.6274 - regression_loss: 0.5077 - classification_loss: 0.1196
 6431/10000 [==================>...........] - ETA: 27:11 - loss: 0.6273 - regression_loss: 0.5077 - classification_loss: 0.1196
 6432/10000 [==================>...........] - ETA: 27:10 - loss: 0.6272 - regression_loss: 0.5076 - classification_loss: 0.1196
 6433/10000 [==================>...........] - ETA: 27:10 - loss: 0.6273 - regression_loss: 0.5077 - classification_loss: 0.1196
 6434/10000 [==================>...........] - ETA: 27:09 - loss: 0.6272 - regression_loss: 0.5077 - classification_loss: 0.1196
 6435/10000 [==================>...........] - ETA: 27:09 - loss: 0.6273 - regression_loss: 0.5077 - classification_loss: 0.1196
 6436/10000 [==================>...........] - ETA: 27:08 - loss: 0.6273 - regression_loss: 0.5077 - classification_loss: 0.1196
 6437/10000 [==================>...........] - ETA: 27:08 - loss: 0.6273 - regression_loss: 0.5077 - classification_loss: 0.1196
 6438/10000 [==================>...........] - ETA: 27:07 - loss: 0.6273 - regression_loss: 0.5077 - classification_loss: 0.1196
 6439/10000 [==================>...........] - ETA: 27:07 - loss: 0.6273 - regression_loss: 0.5078 - classification_loss: 0.1196
 6440/10000 [==================>...........] - ETA: 27:06 - loss: 0.6274 - regression_loss: 0.5078 - classification_loss: 0.1196
 6441/10000 [==================>...........] - ETA: 27:06 - loss: 0.6274 - regression_loss: 0.5078 - classification_loss: 0.1196
 6442/10000 [==================>...........] - ETA: 27:05 - loss: 0.6274 - regression_loss: 0.5078 - classification_loss: 0.1196
 6443/10000 [==================>...........] - ETA: 27:05 - loss: 0.6275 - regression_loss: 0.5078 - classification_loss: 0.1196
 6444/10000 [==================>...........] - ETA: 27:04 - loss: 0.6274 - regression_loss: 0.5078 - classification_loss: 0.1196
 6445/10000 [==================>...........] - ETA: 27:04 - loss: 0.6275 - regression_loss: 0.5079 - classification_loss: 0.1196
 6446/10000 [==================>...........] - ETA: 27:04 - loss: 0.6275 - regression_loss: 0.5078 - classification_loss: 0.1196
 6447/10000 [==================>...........] - ETA: 27:03 - loss: 0.6275 - regression_loss: 0.5078 - classification_loss: 0.1196
 6448/10000 [==================>...........] - ETA: 27:03 - loss: 0.6274 - regression_loss: 0.5078 - classification_loss: 0.1196
 6449/10000 [==================>...........] - ETA: 27:02 - loss: 0.6273 - regression_loss: 0.5078 - classification_loss: 0.1196
 6450/10000 [==================>...........] - ETA: 27:02 - loss: 0.6273 - regression_loss: 0.5077 - classification_loss: 0.1196
 6451/10000 [==================>...........] - ETA: 27:01 - loss: 0.6273 - regression_loss: 0.5077 - classification_loss: 0.1196
 6452/10000 [==================>...........] - ETA: 27:01 - loss: 0.6273 - regression_loss: 0.5077 - classification_loss: 0.1196
 6453/10000 [==================>...........] - ETA: 27:00 - loss: 0.6274 - regression_loss: 0.5078 - classification_loss: 0.1196
 6454/10000 [==================>...........] - ETA: 27:00 - loss: 0.6273 - regression_loss: 0.5077 - classification_loss: 0.1196
 6455/10000 [==================>...........] - ETA: 26:59 - loss: 0.6273 - regression_loss: 0.5077 - classification_loss: 0.1196
 6456/10000 [==================>...........] - ETA: 26:59 - loss: 0.6275 - regression_loss: 0.5079 - classification_loss: 0.1196
 6457/10000 [==================>...........] - ETA: 26:59 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1197
 6458/10000 [==================>...........] - ETA: 26:58 - loss: 0.6278 - regression_loss: 0.5081 - classification_loss: 0.1197
 6459/10000 [==================>...........] - ETA: 26:58 - loss: 0.6278 - regression_loss: 0.5081 - classification_loss: 0.1197
 6460/10000 [==================>...........] - ETA: 26:57 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1197
 6461/10000 [==================>...........] - ETA: 26:57 - loss: 0.6280 - regression_loss: 0.5083 - classification_loss: 0.1197
 6462/10000 [==================>...........] - ETA: 26:56 - loss: 0.6280 - regression_loss: 0.5083 - classification_loss: 0.1197
 6463/10000 [==================>...........] - ETA: 26:56 - loss: 0.6280 - regression_loss: 0.5083 - classification_loss: 0.1197
 6464/10000 [==================>...........] - ETA: 26:55 - loss: 0.6279 - regression_loss: 0.5082 - classification_loss: 0.1197
 6465/10000 [==================>...........] - ETA: 26:55 - loss: 0.6278 - regression_loss: 0.5082 - classification_loss: 0.1197
 6466/10000 [==================>...........] - ETA: 26:54 - loss: 0.6278 - regression_loss: 0.5081 - classification_loss: 0.1197
 6467/10000 [==================>...........] - ETA: 26:54 - loss: 0.6278 - regression_loss: 0.5081 - classification_loss: 0.1196
 6468/10000 [==================>...........] - ETA: 26:54 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1196
 6469/10000 [==================>...........] - ETA: 26:53 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1196
 6470/10000 [==================>...........] - ETA: 26:53 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1196
 6471/10000 [==================>...........] - ETA: 26:52 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6472/10000 [==================>...........] - ETA: 26:52 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6473/10000 [==================>...........] - ETA: 26:51 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1196
 6474/10000 [==================>...........] - ETA: 26:51 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1196
 6475/10000 [==================>...........] - ETA: 26:50 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1196
 6476/10000 [==================>...........] - ETA: 26:50 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6477/10000 [==================>...........] - ETA: 26:49 - loss: 0.6275 - regression_loss: 0.5080 - classification_loss: 0.1196
 6478/10000 [==================>...........] - ETA: 26:49 - loss: 0.6275 - regression_loss: 0.5079 - classification_loss: 0.1196
 6479/10000 [==================>...........] - ETA: 26:49 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6480/10000 [==================>...........] - ETA: 26:48 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1196
 6481/10000 [==================>...........] - ETA: 26:48 - loss: 0.6276 - regression_loss: 0.5081 - classification_loss: 0.1196
 6482/10000 [==================>...........] - ETA: 26:47 - loss: 0.6276 - regression_loss: 0.5081 - classification_loss: 0.1196
 6483/10000 [==================>...........] - ETA: 26:47 - loss: 0.6276 - regression_loss: 0.5081 - classification_loss: 0.1195
 6484/10000 [==================>...........] - ETA: 26:46 - loss: 0.6276 - regression_loss: 0.5081 - classification_loss: 0.1195
 6485/10000 [==================>...........] - ETA: 26:46 - loss: 0.6276 - regression_loss: 0.5081 - classification_loss: 0.1195
 6486/10000 [==================>...........] - ETA: 26:45 - loss: 0.6276 - regression_loss: 0.5081 - classification_loss: 0.1195
 6487/10000 [==================>...........] - ETA: 26:45 - loss: 0.6276 - regression_loss: 0.5081 - classification_loss: 0.1195
 6488/10000 [==================>...........] - ETA: 26:44 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1195
 6489/10000 [==================>...........] - ETA: 26:44 - loss: 0.6276 - regression_loss: 0.5081 - classification_loss: 0.1195
 6490/10000 [==================>...........] - ETA: 26:43 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1195
 6491/10000 [==================>...........] - ETA: 26:43 - loss: 0.6276 - regression_loss: 0.5081 - classification_loss: 0.1195
 6492/10000 [==================>...........] - ETA: 26:42 - loss: 0.6276 - regression_loss: 0.5081 - classification_loss: 0.1195
 6493/10000 [==================>...........] - ETA: 26:42 - loss: 0.6276 - regression_loss: 0.5081 - classification_loss: 0.1195
 6494/10000 [==================>...........] - ETA: 26:42 - loss: 0.6275 - regression_loss: 0.5080 - classification_loss: 0.1195
 6495/10000 [==================>...........] - ETA: 26:41 - loss: 0.6274 - regression_loss: 0.5079 - classification_loss: 0.1195
 6496/10000 [==================>...........] - ETA: 26:41 - loss: 0.6275 - regression_loss: 0.5079 - classification_loss: 0.1195
 6497/10000 [==================>...........] - ETA: 26:40 - loss: 0.6274 - regression_loss: 0.5079 - classification_loss: 0.1195
 6498/10000 [==================>...........] - ETA: 26:40 - loss: 0.6274 - regression_loss: 0.5079 - classification_loss: 0.1195
 6499/10000 [==================>...........] - ETA: 26:39 - loss: 0.6274 - regression_loss: 0.5079 - classification_loss: 0.1195
 6500/10000 [==================>...........] - ETA: 26:39 - loss: 0.6273 - regression_loss: 0.5078 - classification_loss: 0.1195
 6501/10000 [==================>...........] - ETA: 26:38 - loss: 0.6273 - regression_loss: 0.5078 - classification_loss: 0.1195
 6502/10000 [==================>...........] - ETA: 26:38 - loss: 0.6272 - regression_loss: 0.5077 - classification_loss: 0.1195
 6503/10000 [==================>...........] - ETA: 26:37 - loss: 0.6271 - regression_loss: 0.5077 - classification_loss: 0.1195
 6504/10000 [==================>...........] - ETA: 26:37 - loss: 0.6272 - regression_loss: 0.5077 - classification_loss: 0.1195
 6505/10000 [==================>...........] - ETA: 26:36 - loss: 0.6272 - regression_loss: 0.5078 - classification_loss: 0.1195
 6506/10000 [==================>...........] - ETA: 26:36 - loss: 0.6272 - regression_loss: 0.5077 - classification_loss: 0.1195
 6507/10000 [==================>...........] - ETA: 26:36 - loss: 0.6272 - regression_loss: 0.5077 - classification_loss: 0.1195
 6508/10000 [==================>...........] - ETA: 26:35 - loss: 0.6272 - regression_loss: 0.5077 - classification_loss: 0.1195
 6509/10000 [==================>...........] - ETA: 26:35 - loss: 0.6272 - regression_loss: 0.5077 - classification_loss: 0.1195
 6510/10000 [==================>...........] - ETA: 26:34 - loss: 0.6271 - regression_loss: 0.5077 - classification_loss: 0.1194
 6511/10000 [==================>...........] - ETA: 26:34 - loss: 0.6271 - regression_loss: 0.5077 - classification_loss: 0.1194
 6512/10000 [==================>...........] - ETA: 26:33 - loss: 0.6271 - regression_loss: 0.5076 - classification_loss: 0.1194
 6513/10000 [==================>...........] - ETA: 26:33 - loss: 0.6271 - regression_loss: 0.5076 - classification_loss: 0.1194
 6514/10000 [==================>...........] - ETA: 26:32 - loss: 0.6270 - regression_loss: 0.5076 - classification_loss: 0.1194
 6515/10000 [==================>...........] - ETA: 26:32 - loss: 0.6269 - regression_loss: 0.5075 - classification_loss: 0.1194
 6516/10000 [==================>...........] - ETA: 26:31 - loss: 0.6269 - regression_loss: 0.5075 - classification_loss: 0.1194
 6517/10000 [==================>...........] - ETA: 26:31 - loss: 0.6270 - regression_loss: 0.5075 - classification_loss: 0.1195
 6518/10000 [==================>...........] - ETA: 26:31 - loss: 0.6271 - regression_loss: 0.5076 - classification_loss: 0.1195
 6519/10000 [==================>...........] - ETA: 26:30 - loss: 0.6271 - regression_loss: 0.5076 - classification_loss: 0.1195
 6520/10000 [==================>...........] - ETA: 26:30 - loss: 0.6271 - regression_loss: 0.5076 - classification_loss: 0.1195
 6521/10000 [==================>...........] - ETA: 26:29 - loss: 0.6271 - regression_loss: 0.5076 - classification_loss: 0.1195
 6522/10000 [==================>...........] - ETA: 26:29 - loss: 0.6271 - regression_loss: 0.5076 - classification_loss: 0.1195
 6523/10000 [==================>...........] - ETA: 26:28 - loss: 0.6272 - regression_loss: 0.5077 - classification_loss: 0.1195
 6524/10000 [==================>...........] - ETA: 26:28 - loss: 0.6273 - regression_loss: 0.5078 - classification_loss: 0.1196
 6525/10000 [==================>...........] - ETA: 26:27 - loss: 0.6273 - regression_loss: 0.5077 - classification_loss: 0.1196
 6526/10000 [==================>...........] - ETA: 26:27 - loss: 0.6272 - regression_loss: 0.5077 - classification_loss: 0.1195
 6527/10000 [==================>...........] - ETA: 26:26 - loss: 0.6272 - regression_loss: 0.5077 - classification_loss: 0.1195
 6528/10000 [==================>...........] - ETA: 26:26 - loss: 0.6272 - regression_loss: 0.5077 - classification_loss: 0.1195
 6529/10000 [==================>...........] - ETA: 26:25 - loss: 0.6272 - regression_loss: 0.5077 - classification_loss: 0.1195
 6530/10000 [==================>...........] - ETA: 26:25 - loss: 0.6272 - regression_loss: 0.5077 - classification_loss: 0.1195
 6531/10000 [==================>...........] - ETA: 26:25 - loss: 0.6272 - regression_loss: 0.5077 - classification_loss: 0.1195
 6532/10000 [==================>...........] - ETA: 26:24 - loss: 0.6273 - regression_loss: 0.5078 - classification_loss: 0.1195
 6533/10000 [==================>...........] - ETA: 26:24 - loss: 0.6273 - regression_loss: 0.5077 - classification_loss: 0.1195
 6534/10000 [==================>...........] - ETA: 26:23 - loss: 0.6272 - regression_loss: 0.5077 - classification_loss: 0.1195
 6535/10000 [==================>...........] - ETA: 26:23 - loss: 0.6272 - regression_loss: 0.5077 - classification_loss: 0.1195
 6536/10000 [==================>...........] - ETA: 26:22 - loss: 0.6272 - regression_loss: 0.5077 - classification_loss: 0.1195
 6537/10000 [==================>...........] - ETA: 26:22 - loss: 0.6271 - regression_loss: 0.5077 - classification_loss: 0.1195
 6538/10000 [==================>...........] - ETA: 26:21 - loss: 0.6272 - regression_loss: 0.5077 - classification_loss: 0.1195
 6539/10000 [==================>...........] - ETA: 26:21 - loss: 0.6271 - regression_loss: 0.5076 - classification_loss: 0.1195
 6540/10000 [==================>...........] - ETA: 26:20 - loss: 0.6272 - regression_loss: 0.5077 - classification_loss: 0.1195
 6541/10000 [==================>...........] - ETA: 26:20 - loss: 0.6272 - regression_loss: 0.5077 - classification_loss: 0.1195
 6542/10000 [==================>...........] - ETA: 26:20 - loss: 0.6272 - regression_loss: 0.5077 - classification_loss: 0.1195
 6543/10000 [==================>...........] - ETA: 26:19 - loss: 0.6272 - regression_loss: 0.5077 - classification_loss: 0.1195
 6544/10000 [==================>...........] - ETA: 26:19 - loss: 0.6271 - regression_loss: 0.5077 - classification_loss: 0.1195
 6545/10000 [==================>...........] - ETA: 26:18 - loss: 0.6272 - regression_loss: 0.5077 - classification_loss: 0.1195
 6546/10000 [==================>...........] - ETA: 26:18 - loss: 0.6273 - regression_loss: 0.5078 - classification_loss: 0.1195
 6547/10000 [==================>...........] - ETA: 26:17 - loss: 0.6273 - regression_loss: 0.5078 - classification_loss: 0.1195
 6548/10000 [==================>...........] - ETA: 26:17 - loss: 0.6272 - regression_loss: 0.5078 - classification_loss: 0.1195
 6549/10000 [==================>...........] - ETA: 26:16 - loss: 0.6273 - regression_loss: 0.5078 - classification_loss: 0.1195
 6550/10000 [==================>...........] - ETA: 26:16 - loss: 0.6273 - regression_loss: 0.5078 - classification_loss: 0.1195
 6551/10000 [==================>...........] - ETA: 26:15 - loss: 0.6274 - regression_loss: 0.5079 - classification_loss: 0.1195
 6552/10000 [==================>...........] - ETA: 26:15 - loss: 0.6275 - regression_loss: 0.5080 - classification_loss: 0.1195
 6553/10000 [==================>...........] - ETA: 26:14 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6554/10000 [==================>...........] - ETA: 26:14 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6555/10000 [==================>...........] - ETA: 26:14 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1196
 6556/10000 [==================>...........] - ETA: 26:13 - loss: 0.6278 - regression_loss: 0.5082 - classification_loss: 0.1196
 6557/10000 [==================>...........] - ETA: 26:13 - loss: 0.6279 - regression_loss: 0.5082 - classification_loss: 0.1196
 6558/10000 [==================>...........] - ETA: 26:12 - loss: 0.6279 - regression_loss: 0.5082 - classification_loss: 0.1196
 6559/10000 [==================>...........] - ETA: 26:12 - loss: 0.6278 - regression_loss: 0.5082 - classification_loss: 0.1196
 6560/10000 [==================>...........] - ETA: 26:11 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1196
 6561/10000 [==================>...........] - ETA: 26:11 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1196
 6562/10000 [==================>...........] - ETA: 26:10 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1196
 6563/10000 [==================>...........] - ETA: 26:10 - loss: 0.6278 - regression_loss: 0.5082 - classification_loss: 0.1196
 6564/10000 [==================>...........] - ETA: 26:09 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1196
 6565/10000 [==================>...........] - ETA: 26:09 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1196
 6566/10000 [==================>...........] - ETA: 26:09 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1196
 6567/10000 [==================>...........] - ETA: 26:08 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1196
 6568/10000 [==================>...........] - ETA: 26:08 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1196
 6569/10000 [==================>...........] - ETA: 26:07 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6570/10000 [==================>...........] - ETA: 26:07 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1196
 6571/10000 [==================>...........] - ETA: 26:06 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6572/10000 [==================>...........] - ETA: 26:06 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6573/10000 [==================>...........] - ETA: 26:05 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6574/10000 [==================>...........] - ETA: 26:05 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6575/10000 [==================>...........] - ETA: 26:04 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6576/10000 [==================>...........] - ETA: 26:04 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6577/10000 [==================>...........] - ETA: 26:03 - loss: 0.6275 - regression_loss: 0.5079 - classification_loss: 0.1196
 6578/10000 [==================>...........] - ETA: 26:03 - loss: 0.6275 - regression_loss: 0.5079 - classification_loss: 0.1196
 6579/10000 [==================>...........] - ETA: 26:03 - loss: 0.6275 - regression_loss: 0.5079 - classification_loss: 0.1196
 6580/10000 [==================>...........] - ETA: 26:02 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1196
 6581/10000 [==================>...........] - ETA: 26:02 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1196
 6582/10000 [==================>...........] - ETA: 26:01 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6583/10000 [==================>...........] - ETA: 26:01 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6584/10000 [==================>...........] - ETA: 26:00 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1196
 6585/10000 [==================>...........] - ETA: 26:00 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6586/10000 [==================>...........] - ETA: 25:59 - loss: 0.6275 - regression_loss: 0.5079 - classification_loss: 0.1196
 6587/10000 [==================>...........] - ETA: 25:59 - loss: 0.6275 - regression_loss: 0.5079 - classification_loss: 0.1196
 6588/10000 [==================>...........] - ETA: 25:58 - loss: 0.6274 - regression_loss: 0.5078 - classification_loss: 0.1196
 6589/10000 [==================>...........] - ETA: 25:58 - loss: 0.6274 - regression_loss: 0.5078 - classification_loss: 0.1196
 6590/10000 [==================>...........] - ETA: 25:58 - loss: 0.6273 - regression_loss: 0.5078 - classification_loss: 0.1196
 6591/10000 [==================>...........] - ETA: 25:57 - loss: 0.6274 - regression_loss: 0.5078 - classification_loss: 0.1196
 6592/10000 [==================>...........] - ETA: 25:57 - loss: 0.6274 - regression_loss: 0.5078 - classification_loss: 0.1196
 6593/10000 [==================>...........] - ETA: 25:56 - loss: 0.6274 - regression_loss: 0.5078 - classification_loss: 0.1196
 6594/10000 [==================>...........] - ETA: 25:56 - loss: 0.6273 - regression_loss: 0.5078 - classification_loss: 0.1195
 6595/10000 [==================>...........] - ETA: 25:55 - loss: 0.6274 - regression_loss: 0.5078 - classification_loss: 0.1196
 6596/10000 [==================>...........] - ETA: 25:55 - loss: 0.6275 - regression_loss: 0.5079 - classification_loss: 0.1196
 6597/10000 [==================>...........] - ETA: 25:54 - loss: 0.6275 - regression_loss: 0.5079 - classification_loss: 0.1196
 6598/10000 [==================>...........] - ETA: 25:54 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6599/10000 [==================>...........] - ETA: 25:53 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6600/10000 [==================>...........] - ETA: 25:53 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6601/10000 [==================>...........] - ETA: 25:53 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1196
 6602/10000 [==================>...........] - ETA: 25:52 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1196
 6603/10000 [==================>...........] - ETA: 25:52 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1197
 6604/10000 [==================>...........] - ETA: 25:51 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1197
 6605/10000 [==================>...........] - ETA: 25:51 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6606/10000 [==================>...........] - ETA: 25:50 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6607/10000 [==================>...........] - ETA: 25:50 - loss: 0.6276 - regression_loss: 0.5079 - classification_loss: 0.1196
 6608/10000 [==================>...........] - ETA: 25:49 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6609/10000 [==================>...........] - ETA: 25:49 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1196
 6610/10000 [==================>...........] - ETA: 25:48 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1196
 6611/10000 [==================>...........] - ETA: 25:48 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1196
 6612/10000 [==================>...........] - ETA: 25:48 - loss: 0.6278 - regression_loss: 0.5081 - classification_loss: 0.1196
 6613/10000 [==================>...........] - ETA: 25:47 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1196
 6614/10000 [==================>...........] - ETA: 25:47 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1196
 6615/10000 [==================>...........] - ETA: 25:46 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1196
 6616/10000 [==================>...........] - ETA: 25:46 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6617/10000 [==================>...........] - ETA: 25:45 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6618/10000 [==================>...........] - ETA: 25:45 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6619/10000 [==================>...........] - ETA: 25:44 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6620/10000 [==================>...........] - ETA: 25:44 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1196
 6621/10000 [==================>...........] - ETA: 25:43 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1196
 6622/10000 [==================>...........] - ETA: 25:43 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1196
 6623/10000 [==================>...........] - ETA: 25:43 - loss: 0.6277 - regression_loss: 0.5081 - classification_loss: 0.1196
 6624/10000 [==================>...........] - ETA: 25:42 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1196
 6625/10000 [==================>...........] - ETA: 25:42 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1196
 6626/10000 [==================>...........] - ETA: 25:41 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1196
 6627/10000 [==================>...........] - ETA: 25:41 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1196
 6628/10000 [==================>...........] - ETA: 25:40 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6629/10000 [==================>...........] - ETA: 25:40 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6630/10000 [==================>...........] - ETA: 25:39 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6631/10000 [==================>...........] - ETA: 25:39 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6632/10000 [==================>...........] - ETA: 25:38 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6633/10000 [==================>...........] - ETA: 25:38 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6634/10000 [==================>...........] - ETA: 25:38 - loss: 0.6276 - regression_loss: 0.5080 - classification_loss: 0.1196
 6635/10000 [==================>...........] - ETA: 25:37 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1196
 6636/10000 [==================>...........] - ETA: 25:37 - loss: 0.6278 - regression_loss: 0.5082 - classification_loss: 0.1197
 6637/10000 [==================>...........] - ETA: 25:36 - loss: 0.6279 - regression_loss: 0.5082 - classification_loss: 0.1197
 6638/10000 [==================>...........] - ETA: 25:36 - loss: 0.6278 - regression_loss: 0.5082 - classification_loss: 0.1197
 6639/10000 [==================>...........] - ETA: 25:35 - loss: 0.6278 - regression_loss: 0.5082 - classification_loss: 0.1197
 6640/10000 [==================>...........] - ETA: 25:35 - loss: 0.6279 - regression_loss: 0.5083 - classification_loss: 0.1197
 6641/10000 [==================>...........] - ETA: 25:34 - loss: 0.6279 - regression_loss: 0.5082 - classification_loss: 0.1197
 6642/10000 [==================>...........] - ETA: 25:34 - loss: 0.6279 - regression_loss: 0.5082 - classification_loss: 0.1197
 6643/10000 [==================>...........] - ETA: 25:33 - loss: 0.6279 - regression_loss: 0.5083 - classification_loss: 0.1196
 6644/10000 [==================>...........] - ETA: 25:33 - loss: 0.6279 - regression_loss: 0.5082 - classification_loss: 0.1196
 6645/10000 [==================>...........] - ETA: 25:32 - loss: 0.6279 - regression_loss: 0.5083 - classification_loss: 0.1196
 6646/10000 [==================>...........] - ETA: 25:32 - loss: 0.6279 - regression_loss: 0.5082 - classification_loss: 0.1196
 6647/10000 [==================>...........] - ETA: 25:32 - loss: 0.6280 - regression_loss: 0.5083 - classification_loss: 0.1197
 6648/10000 [==================>...........] - ETA: 25:31 - loss: 0.6280 - regression_loss: 0.5084 - classification_loss: 0.1197
 6649/10000 [==================>...........] - ETA: 25:31 - loss: 0.6280 - regression_loss: 0.5083 - classification_loss: 0.1196
 6650/10000 [==================>...........] - ETA: 25:30 - loss: 0.6279 - regression_loss: 0.5083 - classification_loss: 0.1196
 6651/10000 [==================>...........] - ETA: 25:30 - loss: 0.6280 - regression_loss: 0.5084 - classification_loss: 0.1197
 6652/10000 [==================>...........] - ETA: 25:29 - loss: 0.6281 - regression_loss: 0.5084 - classification_loss: 0.1197
 6653/10000 [==================>...........] - ETA: 25:29 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1197
 6654/10000 [==================>...........] - ETA: 25:28 - loss: 0.6281 - regression_loss: 0.5084 - classification_loss: 0.1197
 6655/10000 [==================>...........] - ETA: 25:28 - loss: 0.6281 - regression_loss: 0.5084 - classification_loss: 0.1197
 6656/10000 [==================>...........] - ETA: 25:27 - loss: 0.6281 - regression_loss: 0.5084 - classification_loss: 0.1197
 6657/10000 [==================>...........] - ETA: 25:27 - loss: 0.6280 - regression_loss: 0.5083 - classification_loss: 0.1197
 6658/10000 [==================>...........] - ETA: 25:27 - loss: 0.6280 - regression_loss: 0.5084 - classification_loss: 0.1197
 6659/10000 [==================>...........] - ETA: 25:26 - loss: 0.6280 - regression_loss: 0.5083 - classification_loss: 0.1197
 6660/10000 [==================>...........] - ETA: 25:26 - loss: 0.6279 - regression_loss: 0.5083 - classification_loss: 0.1197
 6661/10000 [==================>...........] - ETA: 25:25 - loss: 0.6279 - regression_loss: 0.5082 - classification_loss: 0.1197
 6662/10000 [==================>...........] - ETA: 25:25 - loss: 0.6279 - regression_loss: 0.5082 - classification_loss: 0.1196
 6663/10000 [==================>...........] - ETA: 25:24 - loss: 0.6278 - regression_loss: 0.5082 - classification_loss: 0.1196
 6664/10000 [==================>...........] - ETA: 25:24 - loss: 0.6280 - regression_loss: 0.5083 - classification_loss: 0.1197
 6665/10000 [==================>...........] - ETA: 25:23 - loss: 0.6280 - regression_loss: 0.5083 - classification_loss: 0.1197
 6666/10000 [==================>...........] - ETA: 25:23 - loss: 0.6280 - regression_loss: 0.5084 - classification_loss: 0.1197
 6667/10000 [===================>..........] - ETA: 25:22 - loss: 0.6281 - regression_loss: 0.5084 - classification_loss: 0.1197
 6668/10000 [===================>..........] - ETA: 25:22 - loss: 0.6280 - regression_loss: 0.5084 - classification_loss: 0.1197
 6669/10000 [===================>..........] - ETA: 25:21 - loss: 0.6280 - regression_loss: 0.5083 - classification_loss: 0.1197
 6670/10000 [===================>..........] - ETA: 25:21 - loss: 0.6280 - regression_loss: 0.5083 - classification_loss: 0.1197
 6671/10000 [===================>..........] - ETA: 25:21 - loss: 0.6280 - regression_loss: 0.5083 - classification_loss: 0.1197
 6672/10000 [===================>..........] - ETA: 25:20 - loss: 0.6281 - regression_loss: 0.5084 - classification_loss: 0.1198
 6673/10000 [===================>..........] - ETA: 25:20 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1198
 6674/10000 [===================>..........] - ETA: 25:19 - loss: 0.6282 - regression_loss: 0.5084 - classification_loss: 0.1198
 6675/10000 [===================>..........] - ETA: 25:19 - loss: 0.6281 - regression_loss: 0.5084 - classification_loss: 0.1197
 6676/10000 [===================>..........] - ETA: 25:18 - loss: 0.6281 - regression_loss: 0.5084 - classification_loss: 0.1197
 6677/10000 [===================>..........] - ETA: 25:18 - loss: 0.6280 - regression_loss: 0.5083 - classification_loss: 0.1197
 6678/10000 [===================>..........] - ETA: 25:17 - loss: 0.6280 - regression_loss: 0.5083 - classification_loss: 0.1197
 6679/10000 [===================>..........] - ETA: 25:17 - loss: 0.6279 - regression_loss: 0.5082 - classification_loss: 0.1197
 6680/10000 [===================>..........] - ETA: 25:16 - loss: 0.6280 - regression_loss: 0.5083 - classification_loss: 0.1198
 6681/10000 [===================>..........] - ETA: 25:16 - loss: 0.6280 - regression_loss: 0.5082 - classification_loss: 0.1197
 6682/10000 [===================>..........] - ETA: 25:16 - loss: 0.6281 - regression_loss: 0.5083 - classification_loss: 0.1198
 6683/10000 [===================>..........] - ETA: 25:15 - loss: 0.6280 - regression_loss: 0.5083 - classification_loss: 0.1197
 6684/10000 [===================>..........] - ETA: 25:15 - loss: 0.6280 - regression_loss: 0.5083 - classification_loss: 0.1197
 6685/10000 [===================>..........] - ETA: 25:14 - loss: 0.6280 - regression_loss: 0.5082 - classification_loss: 0.1197
 6686/10000 [===================>..........] - ETA: 25:14 - loss: 0.6279 - regression_loss: 0.5082 - classification_loss: 0.1197
 6687/10000 [===================>..........] - ETA: 25:13 - loss: 0.6279 - regression_loss: 0.5081 - classification_loss: 0.1197
 6688/10000 [===================>..........] - ETA: 25:13 - loss: 0.6278 - regression_loss: 0.5081 - classification_loss: 0.1197
 6689/10000 [===================>..........] - ETA: 25:12 - loss: 0.6278 - regression_loss: 0.5081 - classification_loss: 0.1197
 6690/10000 [===================>..........] - ETA: 25:12 - loss: 0.6279 - regression_loss: 0.5081 - classification_loss: 0.1197
 6691/10000 [===================>..........] - ETA: 25:11 - loss: 0.6279 - regression_loss: 0.5081 - classification_loss: 0.1197
 6692/10000 [===================>..........] - ETA: 25:11 - loss: 0.6278 - regression_loss: 0.5081 - classification_loss: 0.1197
 6693/10000 [===================>..........] - ETA: 25:11 - loss: 0.6278 - regression_loss: 0.5081 - classification_loss: 0.1197
 6694/10000 [===================>..........] - ETA: 25:10 - loss: 0.6278 - regression_loss: 0.5081 - classification_loss: 0.1197
 6695/10000 [===================>..........] - ETA: 25:10 - loss: 0.6279 - regression_loss: 0.5082 - classification_loss: 0.1198
 6696/10000 [===================>..........] - ETA: 25:09 - loss: 0.6279 - regression_loss: 0.5081 - classification_loss: 0.1198
 6697/10000 [===================>..........] - ETA: 25:09 - loss: 0.6279 - regression_loss: 0.5081 - classification_loss: 0.1198
 6698/10000 [===================>..........] - ETA: 25:08 - loss: 0.6279 - regression_loss: 0.5081 - classification_loss: 0.1198
 6699/10000 [===================>..........] - ETA: 25:08 - loss: 0.6279 - regression_loss: 0.5081 - classification_loss: 0.1198
 6700/10000 [===================>..........] - ETA: 25:07 - loss: 0.6278 - regression_loss: 0.5081 - classification_loss: 0.1197
 6701/10000 [===================>..........] - ETA: 25:07 - loss: 0.6278 - regression_loss: 0.5081 - classification_loss: 0.1197
 6702/10000 [===================>..........] - ETA: 25:06 - loss: 0.6279 - regression_loss: 0.5081 - classification_loss: 0.1198
 6703/10000 [===================>..........] - ETA: 25:06 - loss: 0.6278 - regression_loss: 0.5080 - classification_loss: 0.1198
 6704/10000 [===================>..........] - ETA: 25:06 - loss: 0.6277 - regression_loss: 0.5080 - classification_loss: 0.1198
 6705/10000 [===================>..........] - ETA: 25:05 - loss: 0.6277 - regression_loss: 0.5079 - classification_loss: 0.1197
 6706/10000 [===================>..........] - ETA: 25:05 - loss: 0.6276 - regression_loss: 0.5079 - classification_loss: 0.1197
 6707/10000 [===================>..........] - ETA: 25:04 - loss: 0.6276 - regression_loss: 0.5079 - classification_loss: 0.1197
 6708/10000 [===================>..........] - ETA: 25:04 - loss: 0.6276 - regression_loss: 0.5078 - classification_loss: 0.1197
 6709/10000 [===================>..........] - ETA: 25:03 - loss: 0.6275 - regression_loss: 0.5078 - classification_loss: 0.1197
 6710/10000 [===================>..........] - ETA: 25:03 - loss: 0.6275 - regression_loss: 0.5078 - classification_loss: 0.1197
 6711/10000 [===================>..........] - ETA: 25:02 - loss: 0.6274 - regression_loss: 0.5077 - classification_loss: 0.1197
 6712/10000 [===================>..........] - ETA: 25:02 - loss: 0.6274 - regression_loss: 0.5077 - classification_loss: 0.1197
 6713/10000 [===================>..........] - ETA: 25:01 - loss: 0.6274 - regression_loss: 0.5077 - classification_loss: 0.1197
 6714/10000 [===================>..........] - ETA: 25:01 - loss: 0.6273 - regression_loss: 0.5077 - classification_loss: 0.1197
 6715/10000 [===================>..........] - ETA: 25:00 - loss: 0.6274 - regression_loss: 0.5077 - classification_loss: 0.1197
 6716/10000 [===================>..........] - ETA: 25:00 - loss: 0.6274 - regression_loss: 0.5077 - classification_loss: 0.1197
 6717/10000 [===================>..........] - ETA: 25:00 - loss: 0.6274 - regression_loss: 0.5077 - classification_loss: 0.1197
 6718/10000 [===================>..........] - ETA: 24:59 - loss: 0.6274 - regression_loss: 0.5077 - classification_loss: 0.1197
 6719/10000 [===================>..........] - ETA: 24:59 - loss: 0.6273 - regression_loss: 0.5077 - classification_loss: 0.1197
 6720/10000 [===================>..........] - ETA: 24:58 - loss: 0.6273 - regression_loss: 0.5076 - classification_loss: 0.1197
 6721/10000 [===================>..........] - ETA: 24:58 - loss: 0.6273 - regression_loss: 0.5076 - classification_loss: 0.1197
 6722/10000 [===================>..........] - ETA: 24:57 - loss: 0.6272 - regression_loss: 0.5076 - classification_loss: 0.1197
 6723/10000 [===================>..........] - ETA: 24:57 - loss: 0.6272 - regression_loss: 0.5075 - classification_loss: 0.1197
 6724/10000 [===================>..........] - ETA: 24:56 - loss: 0.6271 - regression_loss: 0.5075 - classification_loss: 0.1196
 6725/10000 [===================>..........] - ETA: 24:56 - loss: 0.6271 - regression_loss: 0.5074 - classification_loss: 0.1196
 6726/10000 [===================>..........] - ETA: 24:55 - loss: 0.6271 - regression_loss: 0.5075 - classification_loss: 0.1196
 6727/10000 [===================>..........] - ETA: 24:55 - loss: 0.6271 - regression_loss: 0.5074 - classification_loss: 0.1196
 6728/10000 [===================>..........] - ETA: 24:55 - loss: 0.6270 - regression_loss: 0.5074 - classification_loss: 0.1196
 6729/10000 [===================>..........] - ETA: 24:54 - loss: 0.6270 - regression_loss: 0.5074 - classification_loss: 0.1196
 6730/10000 [===================>..........] - ETA: 24:54 - loss: 0.6271 - regression_loss: 0.5074 - classification_loss: 0.1197
 6731/10000 [===================>..........] - ETA: 24:53 - loss: 0.6270 - regression_loss: 0.5073 - classification_loss: 0.1197
 6732/10000 [===================>..........] - ETA: 24:53 - loss: 0.6270 - regression_loss: 0.5073 - classification_loss: 0.1197
 6733/10000 [===================>..........] - ETA: 24:52 - loss: 0.6270 - regression_loss: 0.5074 - classification_loss: 0.1197
 6734/10000 [===================>..........] - ETA: 24:52 - loss: 0.6270 - regression_loss: 0.5073 - classification_loss: 0.1197
 6735/10000 [===================>..........] - ETA: 24:51 - loss: 0.6269 - regression_loss: 0.5072 - classification_loss: 0.1197
 6736/10000 [===================>..........] - ETA: 24:51 - loss: 0.6269 - regression_loss: 0.5073 - classification_loss: 0.1196
 6737/10000 [===================>..........] - ETA: 24:50 - loss: 0.6270 - regression_loss: 0.5073 - classification_loss: 0.1197
 6738/10000 [===================>..........] - ETA: 24:50 - loss: 0.6270 - regression_loss: 0.5073 - classification_loss: 0.1197
 6739/10000 [===================>..........] - ETA: 24:50 - loss: 0.6269 - regression_loss: 0.5073 - classification_loss: 0.1196
 6740/10000 [===================>..........] - ETA: 24:49 - loss: 0.6270 - regression_loss: 0.5073 - classification_loss: 0.1196
 6741/10000 [===================>..........] - ETA: 24:49 - loss: 0.6269 - regression_loss: 0.5073 - classification_loss: 0.1196
 6742/10000 [===================>..........] - ETA: 24:48 - loss: 0.6269 - regression_loss: 0.5073 - classification_loss: 0.1196
 6743/10000 [===================>..........] - ETA: 24:48 - loss: 0.6269 - regression_loss: 0.5073 - classification_loss: 0.1196
 6744/10000 [===================>..........] - ETA: 24:47 - loss: 0.6269 - regression_loss: 0.5073 - classification_loss: 0.1196
 6745/10000 [===================>..........] - ETA: 24:47 - loss: 0.6269 - regression_loss: 0.5072 - classification_loss: 0.1196
 6746/10000 [===================>..........] - ETA: 24:46 - loss: 0.6268 - regression_loss: 0.5072 - classification_loss: 0.1196
 6747/10000 [===================>..........] - ETA: 24:46 - loss: 0.6268 - regression_loss: 0.5072 - classification_loss: 0.1196
 6748/10000 [===================>..........] - ETA: 24:45 - loss: 0.6267 - regression_loss: 0.5071 - classification_loss: 0.1196
 6749/10000 [===================>..........] - ETA: 24:45 - loss: 0.6267 - regression_loss: 0.5071 - classification_loss: 0.1196
 6750/10000 [===================>..........] - ETA: 24:44 - loss: 0.6266 - regression_loss: 0.5071 - classification_loss: 0.1196
 6751/10000 [===================>..........] - ETA: 24:44 - loss: 0.6267 - regression_loss: 0.5071 - classification_loss: 0.1196
 6752/10000 [===================>..........] - ETA: 24:44 - loss: 0.6266 - regression_loss: 0.5070 - classification_loss: 0.1196
 6753/10000 [===================>..........] - ETA: 24:43 - loss: 0.6266 - regression_loss: 0.5070 - classification_loss: 0.1196
 6754/10000 [===================>..........] - ETA: 24:43 - loss: 0.6265 - regression_loss: 0.5070 - classification_loss: 0.1196
 6755/10000 [===================>..........] - ETA: 24:42 - loss: 0.6265 - regression_loss: 0.5069 - classification_loss: 0.1196
 6756/10000 [===================>..........] - ETA: 24:42 - loss: 0.6264 - regression_loss: 0.5069 - classification_loss: 0.1195
 6757/10000 [===================>..........] - ETA: 24:41 - loss: 0.6264 - regression_loss: 0.5069 - classification_loss: 0.1195
 6758/10000 [===================>..........] - ETA: 24:41 - loss: 0.6263 - regression_loss: 0.5068 - classification_loss: 0.1195
 6759/10000 [===================>..........] - ETA: 24:40 - loss: 0.6264 - regression_loss: 0.5069 - classification_loss: 0.1195
 6760/10000 [===================>..........] - ETA: 24:40 - loss: 0.6264 - regression_loss: 0.5069 - classification_loss: 0.1195
 6761/10000 [===================>..........] - ETA: 24:39 - loss: 0.6264 - regression_loss: 0.5068 - classification_loss: 0.1195
 6762/10000 [===================>..........] - ETA: 24:39 - loss: 0.6263 - regression_loss: 0.5068 - classification_loss: 0.1195
 6763/10000 [===================>..........] - ETA: 24:39 - loss: 0.6262 - regression_loss: 0.5067 - classification_loss: 0.1195
 6764/10000 [===================>..........] - ETA: 24:38 - loss: 0.6262 - regression_loss: 0.5067 - classification_loss: 0.1195
 6765/10000 [===================>..........] - ETA: 24:38 - loss: 0.6263 - regression_loss: 0.5068 - classification_loss: 0.1195
 6766/10000 [===================>..........] - ETA: 24:37 - loss: 0.6262 - regression_loss: 0.5067 - classification_loss: 0.1195
 6767/10000 [===================>..........] - ETA: 24:37 - loss: 0.6262 - regression_loss: 0.5067 - classification_loss: 0.1195
 6768/10000 [===================>..........] - ETA: 24:36 - loss: 0.6261 - regression_loss: 0.5066 - classification_loss: 0.1195
 6769/10000 [===================>..........] - ETA: 24:36 - loss: 0.6261 - regression_loss: 0.5066 - classification_loss: 0.1195
 6770/10000 [===================>..........] - ETA: 24:35 - loss: 0.6262 - regression_loss: 0.5067 - classification_loss: 0.1195
 6771/10000 [===================>..........] - ETA: 24:35 - loss: 0.6261 - regression_loss: 0.5066 - classification_loss: 0.1195
 6772/10000 [===================>..........] - ETA: 24:34 - loss: 0.6261 - regression_loss: 0.5066 - classification_loss: 0.1195
 6773/10000 [===================>..........] - ETA: 24:34 - loss: 0.6260 - regression_loss: 0.5066 - classification_loss: 0.1195
 6774/10000 [===================>..........] - ETA: 24:33 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 6775/10000 [===================>..........] - ETA: 24:33 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 6776/10000 [===================>..........] - ETA: 24:33 - loss: 0.6259 - regression_loss: 0.5065 - classification_loss: 0.1195
 6777/10000 [===================>..........] - ETA: 24:32 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 6778/10000 [===================>..........] - ETA: 24:32 - loss: 0.6259 - regression_loss: 0.5065 - classification_loss: 0.1195
 6779/10000 [===================>..........] - ETA: 24:31 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 6780/10000 [===================>..........] - ETA: 24:31 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 6781/10000 [===================>..........] - ETA: 24:30 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 6782/10000 [===================>..........] - ETA: 24:30 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6783/10000 [===================>..........] - ETA: 24:29 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6784/10000 [===================>..........] - ETA: 24:29 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6785/10000 [===================>..........] - ETA: 24:28 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6786/10000 [===================>..........] - ETA: 24:28 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6787/10000 [===================>..........] - ETA: 24:28 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6788/10000 [===================>..........] - ETA: 24:27 - loss: 0.6258 - regression_loss: 0.5064 - classification_loss: 0.1195
 6789/10000 [===================>..........] - ETA: 24:27 - loss: 0.6258 - regression_loss: 0.5063 - classification_loss: 0.1194
 6790/10000 [===================>..........] - ETA: 24:26 - loss: 0.6258 - regression_loss: 0.5063 - classification_loss: 0.1194
 6791/10000 [===================>..........] - ETA: 24:26 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1194
 6792/10000 [===================>..........] - ETA: 24:25 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1194
 6793/10000 [===================>..........] - ETA: 24:25 - loss: 0.6257 - regression_loss: 0.5062 - classification_loss: 0.1194
 6794/10000 [===================>..........] - ETA: 24:24 - loss: 0.6257 - regression_loss: 0.5062 - classification_loss: 0.1194
 6795/10000 [===================>..........] - ETA: 24:24 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6796/10000 [===================>..........] - ETA: 24:23 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6797/10000 [===================>..........] - ETA: 24:23 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6798/10000 [===================>..........] - ETA: 24:23 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1194
 6799/10000 [===================>..........] - ETA: 24:22 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6800/10000 [===================>..........] - ETA: 24:22 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6801/10000 [===================>..........] - ETA: 24:21 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6802/10000 [===================>..........] - ETA: 24:21 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6803/10000 [===================>..........] - ETA: 24:20 - loss: 0.6255 - regression_loss: 0.5061 - classification_loss: 0.1194
 6804/10000 [===================>..........] - ETA: 24:20 - loss: 0.6254 - regression_loss: 0.5060 - classification_loss: 0.1194
 6805/10000 [===================>..........] - ETA: 24:19 - loss: 0.6254 - regression_loss: 0.5061 - classification_loss: 0.1194
 6806/10000 [===================>..........] - ETA: 24:19 - loss: 0.6254 - regression_loss: 0.5060 - classification_loss: 0.1194
 6807/10000 [===================>..........] - ETA: 24:18 - loss: 0.6254 - regression_loss: 0.5060 - classification_loss: 0.1194
 6808/10000 [===================>..........] - ETA: 24:18 - loss: 0.6254 - regression_loss: 0.5060 - classification_loss: 0.1194
 6809/10000 [===================>..........] - ETA: 24:17 - loss: 0.6254 - regression_loss: 0.5060 - classification_loss: 0.1194
 6810/10000 [===================>..........] - ETA: 24:17 - loss: 0.6253 - regression_loss: 0.5059 - classification_loss: 0.1194
 6811/10000 [===================>..........] - ETA: 24:17 - loss: 0.6253 - regression_loss: 0.5059 - classification_loss: 0.1194
 6812/10000 [===================>..........] - ETA: 24:16 - loss: 0.6253 - regression_loss: 0.5059 - classification_loss: 0.1194
 6813/10000 [===================>..........] - ETA: 24:16 - loss: 0.6253 - regression_loss: 0.5059 - classification_loss: 0.1194
 6814/10000 [===================>..........] - ETA: 24:15 - loss: 0.6254 - regression_loss: 0.5060 - classification_loss: 0.1194
 6815/10000 [===================>..........] - ETA: 24:15 - loss: 0.6255 - regression_loss: 0.5061 - classification_loss: 0.1194
 6816/10000 [===================>..........] - ETA: 24:14 - loss: 0.6255 - regression_loss: 0.5061 - classification_loss: 0.1194
 6817/10000 [===================>..........] - ETA: 24:14 - loss: 0.6255 - regression_loss: 0.5061 - classification_loss: 0.1194
 6818/10000 [===================>..........] - ETA: 24:13 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6819/10000 [===================>..........] - ETA: 24:13 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6820/10000 [===================>..........] - ETA: 24:12 - loss: 0.6255 - regression_loss: 0.5061 - classification_loss: 0.1194
 6821/10000 [===================>..........] - ETA: 24:12 - loss: 0.6255 - regression_loss: 0.5061 - classification_loss: 0.1194
 6822/10000 [===================>..........] - ETA: 24:12 - loss: 0.6255 - regression_loss: 0.5061 - classification_loss: 0.1194
 6823/10000 [===================>..........] - ETA: 24:11 - loss: 0.6255 - regression_loss: 0.5061 - classification_loss: 0.1194
 6824/10000 [===================>..........] - ETA: 24:11 - loss: 0.6255 - regression_loss: 0.5061 - classification_loss: 0.1194
 6825/10000 [===================>..........] - ETA: 24:10 - loss: 0.6255 - regression_loss: 0.5062 - classification_loss: 0.1194
 6826/10000 [===================>..........] - ETA: 24:10 - loss: 0.6255 - regression_loss: 0.5061 - classification_loss: 0.1194
 6827/10000 [===================>..........] - ETA: 24:09 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6828/10000 [===================>..........] - ETA: 24:09 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1194
 6829/10000 [===================>..........] - ETA: 24:08 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1194
 6830/10000 [===================>..........] - ETA: 24:08 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1194
 6831/10000 [===================>..........] - ETA: 24:07 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1194
 6832/10000 [===================>..........] - ETA: 24:07 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1194
 6833/10000 [===================>..........] - ETA: 24:07 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1194
 6834/10000 [===================>..........] - ETA: 24:06 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1194
 6835/10000 [===================>..........] - ETA: 24:06 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6836/10000 [===================>..........] - ETA: 24:05 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6837/10000 [===================>..........] - ETA: 24:05 - loss: 0.6257 - regression_loss: 0.5062 - classification_loss: 0.1194
 6838/10000 [===================>..........] - ETA: 24:04 - loss: 0.6258 - regression_loss: 0.5063 - classification_loss: 0.1194
 6839/10000 [===================>..........] - ETA: 24:04 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1194
 6840/10000 [===================>..........] - ETA: 24:03 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1194
 6841/10000 [===================>..........] - ETA: 24:03 - loss: 0.6257 - regression_loss: 0.5062 - classification_loss: 0.1194
 6842/10000 [===================>..........] - ETA: 24:02 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6843/10000 [===================>..........] - ETA: 24:02 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6844/10000 [===================>..........] - ETA: 24:02 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6845/10000 [===================>..........] - ETA: 24:01 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6846/10000 [===================>..........] - ETA: 24:01 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6847/10000 [===================>..........] - ETA: 24:00 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6848/10000 [===================>..........] - ETA: 24:00 - loss: 0.6257 - regression_loss: 0.5062 - classification_loss: 0.1194
 6849/10000 [===================>..........] - ETA: 23:59 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6850/10000 [===================>..........] - ETA: 23:59 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6851/10000 [===================>..........] - ETA: 23:58 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6852/10000 [===================>..........] - ETA: 23:58 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6853/10000 [===================>..........] - ETA: 23:57 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6854/10000 [===================>..........] - ETA: 23:57 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6855/10000 [===================>..........] - ETA: 23:57 - loss: 0.6255 - regression_loss: 0.5061 - classification_loss: 0.1194
 6856/10000 [===================>..........] - ETA: 23:56 - loss: 0.6255 - regression_loss: 0.5061 - classification_loss: 0.1194
 6857/10000 [===================>..........] - ETA: 23:56 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6858/10000 [===================>..........] - ETA: 23:55 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6859/10000 [===================>..........] - ETA: 23:55 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6860/10000 [===================>..........] - ETA: 23:54 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6861/10000 [===================>..........] - ETA: 23:54 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1194
 6862/10000 [===================>..........] - ETA: 23:53 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1194
 6863/10000 [===================>..........] - ETA: 23:53 - loss: 0.6258 - regression_loss: 0.5063 - classification_loss: 0.1194
 6864/10000 [===================>..........] - ETA: 23:52 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1194
 6865/10000 [===================>..........] - ETA: 23:52 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1194
 6866/10000 [===================>..........] - ETA: 23:51 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1194
 6867/10000 [===================>..........] - ETA: 23:51 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1194
 6868/10000 [===================>..........] - ETA: 23:50 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1194
 6869/10000 [===================>..........] - ETA: 23:50 - loss: 0.6256 - regression_loss: 0.5062 - classification_loss: 0.1194
 6870/10000 [===================>..........] - ETA: 23:50 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1194
 6871/10000 [===================>..........] - ETA: 23:49 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1194
 6872/10000 [===================>..........] - ETA: 23:49 - loss: 0.6258 - regression_loss: 0.5064 - classification_loss: 0.1194
 6873/10000 [===================>..........] - ETA: 23:48 - loss: 0.6259 - regression_loss: 0.5065 - classification_loss: 0.1195
 6874/10000 [===================>..........] - ETA: 23:48 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1194
 6875/10000 [===================>..........] - ETA: 23:47 - loss: 0.6259 - regression_loss: 0.5065 - classification_loss: 0.1194
 6876/10000 [===================>..........] - ETA: 23:47 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6877/10000 [===================>..........] - ETA: 23:46 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 6878/10000 [===================>..........] - ETA: 23:46 - loss: 0.6259 - regression_loss: 0.5065 - classification_loss: 0.1195
 6879/10000 [===================>..........] - ETA: 23:46 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6880/10000 [===================>..........] - ETA: 23:45 - loss: 0.6259 - regression_loss: 0.5065 - classification_loss: 0.1195
 6881/10000 [===================>..........] - ETA: 23:45 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6882/10000 [===================>..........] - ETA: 23:44 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6883/10000 [===================>..........] - ETA: 23:44 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6884/10000 [===================>..........] - ETA: 23:43 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6885/10000 [===================>..........] - ETA: 23:43 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6886/10000 [===================>..........] - ETA: 23:42 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6887/10000 [===================>..........] - ETA: 23:42 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6888/10000 [===================>..........] - ETA: 23:41 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 6889/10000 [===================>..........] - ETA: 23:41 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 6890/10000 [===================>..........] - ETA: 23:41 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6891/10000 [===================>..........] - ETA: 23:40 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6892/10000 [===================>..........] - ETA: 23:40 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6893/10000 [===================>..........] - ETA: 23:39 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6894/10000 [===================>..........] - ETA: 23:39 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 6895/10000 [===================>..........] - ETA: 23:38 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 6896/10000 [===================>..........] - ETA: 23:38 - loss: 0.6259 - regression_loss: 0.5065 - classification_loss: 0.1195
 6897/10000 [===================>..........] - ETA: 23:37 - loss: 0.6259 - regression_loss: 0.5065 - classification_loss: 0.1195
 6898/10000 [===================>..........] - ETA: 23:37 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6899/10000 [===================>..........] - ETA: 23:36 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6900/10000 [===================>..........] - ETA: 23:36 - loss: 0.6258 - regression_loss: 0.5064 - classification_loss: 0.1195
 6901/10000 [===================>..........] - ETA: 23:36 - loss: 0.6258 - regression_loss: 0.5064 - classification_loss: 0.1195
 6902/10000 [===================>..........] - ETA: 23:35 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6903/10000 [===================>..........] - ETA: 23:35 - loss: 0.6258 - regression_loss: 0.5063 - classification_loss: 0.1195
 6904/10000 [===================>..........] - ETA: 23:34 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1195
 6905/10000 [===================>..........] - ETA: 23:34 - loss: 0.6258 - regression_loss: 0.5063 - classification_loss: 0.1195
 6906/10000 [===================>..........] - ETA: 23:33 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6907/10000 [===================>..........] - ETA: 23:33 - loss: 0.6258 - regression_loss: 0.5063 - classification_loss: 0.1195
 6908/10000 [===================>..........] - ETA: 23:32 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1195
 6909/10000 [===================>..........] - ETA: 23:32 - loss: 0.6258 - regression_loss: 0.5064 - classification_loss: 0.1195
 6910/10000 [===================>..........] - ETA: 23:31 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6911/10000 [===================>..........] - ETA: 23:31 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6912/10000 [===================>..........] - ETA: 23:31 - loss: 0.6259 - regression_loss: 0.5065 - classification_loss: 0.1195
 6913/10000 [===================>..........] - ETA: 23:30 - loss: 0.6259 - regression_loss: 0.5065 - classification_loss: 0.1195
 6914/10000 [===================>..........] - ETA: 23:30 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 6915/10000 [===================>..........] - ETA: 23:29 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 6916/10000 [===================>..........] - ETA: 23:29 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6917/10000 [===================>..........] - ETA: 23:28 - loss: 0.6258 - regression_loss: 0.5064 - classification_loss: 0.1195
 6918/10000 [===================>..........] - ETA: 23:28 - loss: 0.6258 - regression_loss: 0.5063 - classification_loss: 0.1194
 6919/10000 [===================>..........] - ETA: 23:27 - loss: 0.6258 - regression_loss: 0.5064 - classification_loss: 0.1195
 6920/10000 [===================>..........] - ETA: 23:27 - loss: 0.6258 - regression_loss: 0.5064 - classification_loss: 0.1195
 6921/10000 [===================>..........] - ETA: 23:26 - loss: 0.6258 - regression_loss: 0.5064 - classification_loss: 0.1195
 6922/10000 [===================>..........] - ETA: 23:26 - loss: 0.6258 - regression_loss: 0.5063 - classification_loss: 0.1194
 6923/10000 [===================>..........] - ETA: 23:26 - loss: 0.6258 - regression_loss: 0.5063 - classification_loss: 0.1194
 6924/10000 [===================>..........] - ETA: 23:25 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1194
 6925/10000 [===================>..........] - ETA: 23:25 - loss: 0.6258 - regression_loss: 0.5064 - classification_loss: 0.1195
 6926/10000 [===================>..........] - ETA: 23:24 - loss: 0.6258 - regression_loss: 0.5064 - classification_loss: 0.1195
 6927/10000 [===================>..........] - ETA: 23:24 - loss: 0.6258 - regression_loss: 0.5064 - classification_loss: 0.1194
 6928/10000 [===================>..........] - ETA: 23:23 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6929/10000 [===================>..........] - ETA: 23:23 - loss: 0.6258 - regression_loss: 0.5063 - classification_loss: 0.1195
 6930/10000 [===================>..........] - ETA: 23:22 - loss: 0.6258 - regression_loss: 0.5063 - classification_loss: 0.1194
 6931/10000 [===================>..........] - ETA: 23:22 - loss: 0.6258 - regression_loss: 0.5063 - classification_loss: 0.1194
 6932/10000 [===================>..........] - ETA: 23:21 - loss: 0.6258 - regression_loss: 0.5063 - classification_loss: 0.1194
 6933/10000 [===================>..........] - ETA: 23:21 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1194
 6934/10000 [===================>..........] - ETA: 23:21 - loss: 0.6259 - regression_loss: 0.5065 - classification_loss: 0.1194
 6935/10000 [===================>..........] - ETA: 23:20 - loss: 0.6260 - regression_loss: 0.5066 - classification_loss: 0.1195
 6936/10000 [===================>..........] - ETA: 23:20 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 6937/10000 [===================>..........] - ETA: 23:19 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 6938/10000 [===================>..........] - ETA: 23:19 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 6939/10000 [===================>..........] - ETA: 23:18 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6940/10000 [===================>..........] - ETA: 23:18 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 6941/10000 [===================>..........] - ETA: 23:17 - loss: 0.6259 - regression_loss: 0.5065 - classification_loss: 0.1195
 6942/10000 [===================>..........] - ETA: 23:17 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 6943/10000 [===================>..........] - ETA: 23:16 - loss: 0.6259 - regression_loss: 0.5065 - classification_loss: 0.1195
 6944/10000 [===================>..........] - ETA: 23:16 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 6945/10000 [===================>..........] - ETA: 23:16 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 6946/10000 [===================>..........] - ETA: 23:15 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 6947/10000 [===================>..........] - ETA: 23:15 - loss: 0.6261 - regression_loss: 0.5066 - classification_loss: 0.1195
 6948/10000 [===================>..........] - ETA: 23:14 - loss: 0.6261 - regression_loss: 0.5066 - classification_loss: 0.1195
 6949/10000 [===================>..........] - ETA: 23:14 - loss: 0.6261 - regression_loss: 0.5066 - classification_loss: 0.1195
 6950/10000 [===================>..........] - ETA: 23:13 - loss: 0.6261 - regression_loss: 0.5066 - classification_loss: 0.1195
 6951/10000 [===================>..........] - ETA: 23:13 - loss: 0.6261 - regression_loss: 0.5066 - classification_loss: 0.1195
 6952/10000 [===================>..........] - ETA: 23:12 - loss: 0.6262 - regression_loss: 0.5067 - classification_loss: 0.1195
 6953/10000 [===================>..........] - ETA: 23:12 - loss: 0.6262 - regression_loss: 0.5067 - classification_loss: 0.1195
 6954/10000 [===================>..........] - ETA: 23:11 - loss: 0.6262 - regression_loss: 0.5067 - classification_loss: 0.1195
 6955/10000 [===================>..........] - ETA: 23:11 - loss: 0.6263 - regression_loss: 0.5067 - classification_loss: 0.1195
 6956/10000 [===================>..........] - ETA: 23:10 - loss: 0.6263 - regression_loss: 0.5068 - classification_loss: 0.1196
 6957/10000 [===================>..........] - ETA: 23:10 - loss: 0.6264 - regression_loss: 0.5069 - classification_loss: 0.1196
 6958/10000 [===================>..........] - ETA: 23:10 - loss: 0.6264 - regression_loss: 0.5068 - classification_loss: 0.1196
 6959/10000 [===================>..........] - ETA: 23:09 - loss: 0.6264 - regression_loss: 0.5068 - classification_loss: 0.1195
 6960/10000 [===================>..........] - ETA: 23:09 - loss: 0.6264 - regression_loss: 0.5068 - classification_loss: 0.1196
 6961/10000 [===================>..........] - ETA: 23:08 - loss: 0.6264 - regression_loss: 0.5069 - classification_loss: 0.1196
 6962/10000 [===================>..........] - ETA: 23:08 - loss: 0.6264 - regression_loss: 0.5068 - classification_loss: 0.1196
 6963/10000 [===================>..........] - ETA: 23:07 - loss: 0.6264 - regression_loss: 0.5068 - classification_loss: 0.1196
 6964/10000 [===================>..........] - ETA: 23:07 - loss: 0.6264 - regression_loss: 0.5068 - classification_loss: 0.1196
 6965/10000 [===================>..........] - ETA: 23:06 - loss: 0.6264 - regression_loss: 0.5069 - classification_loss: 0.1196
 6966/10000 [===================>..........] - ETA: 23:06 - loss: 0.6264 - regression_loss: 0.5069 - classification_loss: 0.1196
 6967/10000 [===================>..........] - ETA: 23:05 - loss: 0.6264 - regression_loss: 0.5068 - classification_loss: 0.1196
 6968/10000 [===================>..........] - ETA: 23:05 - loss: 0.6263 - regression_loss: 0.5068 - classification_loss: 0.1195
 6969/10000 [===================>..........] - ETA: 23:05 - loss: 0.6263 - regression_loss: 0.5068 - classification_loss: 0.1195
 6970/10000 [===================>..........] - ETA: 23:04 - loss: 0.6263 - regression_loss: 0.5068 - classification_loss: 0.1195
 6971/10000 [===================>..........] - ETA: 23:04 - loss: 0.6263 - regression_loss: 0.5068 - classification_loss: 0.1195
 6972/10000 [===================>..........] - ETA: 23:03 - loss: 0.6263 - regression_loss: 0.5067 - classification_loss: 0.1195
 6973/10000 [===================>..........] - ETA: 23:03 - loss: 0.6263 - regression_loss: 0.5068 - classification_loss: 0.1195
 6974/10000 [===================>..........] - ETA: 23:02 - loss: 0.6263 - regression_loss: 0.5068 - classification_loss: 0.1195
 6975/10000 [===================>..........] - ETA: 23:02 - loss: 0.6263 - regression_loss: 0.5068 - classification_loss: 0.1195
 6976/10000 [===================>..........] - ETA: 23:01 - loss: 0.6263 - regression_loss: 0.5067 - classification_loss: 0.1195
 6977/10000 [===================>..........] - ETA: 23:01 - loss: 0.6262 - regression_loss: 0.5067 - classification_loss: 0.1195
 6978/10000 [===================>..........] - ETA: 23:00 - loss: 0.6262 - regression_loss: 0.5067 - classification_loss: 0.1195
 6979/10000 [===================>..........] - ETA: 23:00 - loss: 0.6262 - regression_loss: 0.5067 - classification_loss: 0.1195
 6980/10000 [===================>..........] - ETA: 23:00 - loss: 0.6261 - regression_loss: 0.5066 - classification_loss: 0.1195
 6981/10000 [===================>..........] - ETA: 22:59 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 6982/10000 [===================>..........] - ETA: 22:59 - loss: 0.6261 - regression_loss: 0.5066 - classification_loss: 0.1195
 6983/10000 [===================>..........] - ETA: 22:58 - loss: 0.6261 - regression_loss: 0.5066 - classification_loss: 0.1195
 6984/10000 [===================>..........] - ETA: 22:58 - loss: 0.6261 - regression_loss: 0.5066 - classification_loss: 0.1195
 6985/10000 [===================>..........] - ETA: 22:57 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 6986/10000 [===================>..........] - ETA: 22:57 - loss: 0.6261 - regression_loss: 0.5066 - classification_loss: 0.1195
 6987/10000 [===================>..........] - ETA: 22:56 - loss: 0.6261 - regression_loss: 0.5066 - classification_loss: 0.1195
 6988/10000 [===================>..........] - ETA: 22:56 - loss: 0.6261 - regression_loss: 0.5066 - classification_loss: 0.1195
 6989/10000 [===================>..........] - ETA: 22:55 - loss: 0.6261 - regression_loss: 0.5066 - classification_loss: 0.1195
 6990/10000 [===================>..........] - ETA: 22:55 - loss: 0.6261 - regression_loss: 0.5066 - classification_loss: 0.1195
 6991/10000 [===================>..........] - ETA: 22:54 - loss: 0.6261 - regression_loss: 0.5066 - classification_loss: 0.1195
 6992/10000 [===================>..........] - ETA: 22:54 - loss: 0.6260 - regression_loss: 0.5066 - classification_loss: 0.1195
 6993/10000 [===================>..........] - ETA: 22:54 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 6994/10000 [===================>..........] - ETA: 22:53 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 6995/10000 [===================>..........] - ETA: 22:53 - loss: 0.6259 - regression_loss: 0.5065 - classification_loss: 0.1195
 6996/10000 [===================>..........] - ETA: 22:52 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1194
 6997/10000 [===================>..........] - ETA: 22:52 - loss: 0.6259 - regression_loss: 0.5065 - classification_loss: 0.1195
 6998/10000 [===================>..........] - ETA: 22:51 - loss: 0.6259 - regression_loss: 0.5065 - classification_loss: 0.1195
 6999/10000 [===================>..........] - ETA: 22:51 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1195
 7000/10000 [====================>.........] - ETA: 22:50 - loss: 0.6260 - regression_loss: 0.5065 - classification_loss: 0.1195
 7001/10000 [====================>.........] - ETA: 22:50 - loss: 0.6259 - regression_loss: 0.5065 - classification_loss: 0.1195
 7002/10000 [====================>.........] - ETA: 22:49 - loss: 0.6259 - regression_loss: 0.5065 - classification_loss: 0.1195
 7003/10000 [====================>.........] - ETA: 22:49 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1194
 7004/10000 [====================>.........] - ETA: 22:49 - loss: 0.6259 - regression_loss: 0.5064 - classification_loss: 0.1194
 7005/10000 [====================>.........] - ETA: 22:48 - loss: 0.6258 - regression_loss: 0.5064 - classification_loss: 0.1194
 7006/10000 [====================>.........] - ETA: 22:48 - loss: 0.6259 - regression_loss: 0.5065 - classification_loss: 0.1194
 7007/10000 [====================>.........] - ETA: 22:47 - loss: 0.6259 - regression_loss: 0.5065 - classification_loss: 0.1194
 7008/10000 [====================>.........] - ETA: 22:47 - loss: 0.6258 - regression_loss: 0.5064 - classification_loss: 0.1194
 7009/10000 [====================>.........] - ETA: 22:46 - loss: 0.6258 - regression_loss: 0.5064 - classification_loss: 0.1194
 7010/10000 [====================>.........] - ETA: 22:46 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1194
 7011/10000 [====================>.........] - ETA: 22:45 - loss: 0.6257 - regression_loss: 0.5063 - classification_loss: 0.1194
 7012/10000 [====================>.........] - ETA: 22:45 - loss: 0.6257 - regression_loss: 0.5064 - classification_loss: 0.1194
 7013/10000 [====================>.........] - ETA: 22:44 - loss: 0.6259 - regression_loss: 0.5065 - classification_loss: 0.1194
 7014/10000 [====================>.........] - ETA: 22:44 - loss: 0.6259 - regression_loss: 0.5066 - classification_loss: 0.1194
 7015/10000 [====================>.........] - ETA: 22:44 - loss: 0.6259 - regression_loss: 0.5065 - classification_loss: 0.1194
 7016/10000 [====================>.........] - ETA: 22:43 - loss: 0.6258 - regression_loss: 0.5065 - classification_loss: 0.1194
 7017/10000 [====================>.........] - ETA: 22:43 - loss: 0.6258 - regression_loss: 0.5065 - classification_loss: 0.1194
 7018/10000 [====================>.........] - ETA: 22:42 - loss: 0.6259 - regression_loss: 0.5065 - classification_loss: 0.1194
 7019/10000 [====================>.........] - ETA: 22:42 - loss: 0.6258 - regression_loss: 0.5065 - classification_loss: 0.1194
 7020/10000 [====================>.........] - ETA: 22:41 - loss: 0.6258 - regression_loss: 0.5064 - classification_loss: 0.1193
 7021/10000 [====================>.........] - ETA: 22:41 - loss: 0.6257 - regression_loss: 0.5064 - classification_loss: 0.1193
 7022/10000 [====================>.........] - ETA: 22:40 - loss: 0.6257 - regression_loss: 0.5064 - classification_loss: 0.1193
 7023/10000 [====================>.........] - ETA: 22:40 - loss: 0.6257 - regression_loss: 0.5064 - classification_loss: 0.1193
 7024/10000 [====================>.........] - ETA: 22:39 - loss: 0.6257 - regression_loss: 0.5064 - classification_loss: 0.1193
 7025/10000 [====================>.........] - ETA: 22:39 - loss: 0.6256 - regression_loss: 0.5063 - classification_loss: 0.1193
 7026/10000 [====================>.........] - ETA: 22:38 - loss: 0.6256 - regression_loss: 0.5063 - classification_loss: 0.1193
 7027/10000 [====================>.........] - ETA: 22:38 - loss: 0.6257 - regression_loss: 0.5064 - classification_loss: 0.1193
 7028/10000 [====================>.........] - ETA: 22:38 - loss: 0.6256 - regression_loss: 0.5063 - classification_loss: 0.1193
 7029/10000 [====================>.........] - ETA: 22:37 - loss: 0.6256 - regression_loss: 0.5063 - classification_loss: 0.1193
 7030/10000 [====================>.........] - ETA: 22:37 - loss: 0.6257 - regression_loss: 0.5064 - classification_loss: 0.1193
 7031/10000 [====================>.........] - ETA: 22:36 - loss: 0.6257 - regression_loss: 0.5064 - classification_loss: 0.1193
 7032/10000 [====================>.........] - ETA: 22:36 - loss: 0.6256 - regression_loss: 0.5063 - classification_loss: 0.1193
 7033/10000 [====================>.........] - ETA: 22:35 - loss: 0.6255 - regression_loss: 0.5063 - classification_loss: 0.1193
 7034/10000 [====================>.........] - ETA: 22:35 - loss: 0.6256 - regression_loss: 0.5063 - classification_loss: 0.1193
 7035/10000 [====================>.........] - ETA: 22:34 - loss: 0.6256 - regression_loss: 0.5063 - classification_loss: 0.1193
 7036/10000 [====================>.........] - ETA: 22:34 - loss: 0.6255 - regression_loss: 0.5063 - classification_loss: 0.1192
 7037/10000 [====================>.........] - ETA: 22:33 - loss: 0.6255 - regression_loss: 0.5062 - classification_loss: 0.1192
 7038/10000 [====================>.........] - ETA: 22:33 - loss: 0.6254 - regression_loss: 0.5062 - classification_loss: 0.1192
 7039/10000 [====================>.........] - ETA: 22:33 - loss: 0.6253 - regression_loss: 0.5061 - classification_loss: 0.1192
 7040/10000 [====================>.........] - ETA: 22:32 - loss: 0.6253 - regression_loss: 0.5061 - classification_loss: 0.1192
 7041/10000 [====================>.........] - ETA: 22:32 - loss: 0.6254 - regression_loss: 0.5062 - classification_loss: 0.1192
 7042/10000 [====================>.........] - ETA: 22:31 - loss: 0.6253 - regression_loss: 0.5061 - classification_loss: 0.1192
 7043/10000 [====================>.........] - ETA: 22:31 - loss: 0.6252 - regression_loss: 0.5061 - classification_loss: 0.1192
 7044/10000 [====================>.........] - ETA: 22:30 - loss: 0.6252 - regression_loss: 0.5061 - classification_loss: 0.1192
 7045/10000 [====================>.........] - ETA: 22:30 - loss: 0.6252 - regression_loss: 0.5060 - classification_loss: 0.1192
 7046/10000 [====================>.........] - ETA: 22:29 - loss: 0.6251 - regression_loss: 0.5060 - classification_loss: 0.1191
 7047/10000 [====================>.........] - ETA: 22:29 - loss: 0.6253 - regression_loss: 0.5061 - classification_loss: 0.1192
 7048/10000 [====================>.........] - ETA: 22:28 - loss: 0.6253 - regression_loss: 0.5061 - classification_loss: 0.1192
 7049/10000 [====================>.........] - ETA: 22:28 - loss: 0.6252 - regression_loss: 0.5061 - classification_loss: 0.1192
 7050/10000 [====================>.........] - ETA: 22:28 - loss: 0.6252 - regression_loss: 0.5060 - classification_loss: 0.1191
 7051/10000 [====================>.........] - ETA: 22:27 - loss: 0.6251 - regression_loss: 0.5060 - classification_loss: 0.1191
 7052/10000 [====================>.........] - ETA: 22:27 - loss: 0.6251 - regression_loss: 0.5060 - classification_loss: 0.1191
 7053/10000 [====================>.........] - ETA: 22:26 - loss: 0.6252 - regression_loss: 0.5061 - classification_loss: 0.1192
 7054/10000 [====================>.........] - ETA: 22:26 - loss: 0.6252 - regression_loss: 0.5060 - classification_loss: 0.1191
 7055/10000 [====================>.........] - ETA: 22:25 - loss: 0.6251 - regression_loss: 0.5060 - classification_loss: 0.1191
 7056/10000 [====================>.........] - ETA: 22:25 - loss: 0.6251 - regression_loss: 0.5060 - classification_loss: 0.1191
 7057/10000 [====================>.........] - ETA: 22:24 - loss: 0.6250 - regression_loss: 0.5059 - classification_loss: 0.1191
 7058/10000 [====================>.........] - ETA: 22:24 - loss: 0.6250 - regression_loss: 0.5059 - classification_loss: 0.1191
 7059/10000 [====================>.........] - ETA: 22:23 - loss: 0.6249 - regression_loss: 0.5058 - classification_loss: 0.1191
 7060/10000 [====================>.........] - ETA: 22:23 - loss: 0.6249 - regression_loss: 0.5058 - classification_loss: 0.1191
 7061/10000 [====================>.........] - ETA: 22:23 - loss: 0.6250 - regression_loss: 0.5059 - classification_loss: 0.1191
 7062/10000 [====================>.........] - ETA: 22:22 - loss: 0.6249 - regression_loss: 0.5059 - classification_loss: 0.1191
 7063/10000 [====================>.........] - ETA: 22:22 - loss: 0.6250 - regression_loss: 0.5059 - classification_loss: 0.1191
 7064/10000 [====================>.........] - ETA: 22:21 - loss: 0.6250 - regression_loss: 0.5059 - classification_loss: 0.1191
 7065/10000 [====================>.........] - ETA: 22:21 - loss: 0.6249 - regression_loss: 0.5058 - classification_loss: 0.1191
 7066/10000 [====================>.........] - ETA: 22:20 - loss: 0.6251 - regression_loss: 0.5059 - classification_loss: 0.1191
 7067/10000 [====================>.........] - ETA: 22:20 - loss: 0.6250 - regression_loss: 0.5059 - classification_loss: 0.1191
 7068/10000 [====================>.........] - ETA: 22:19 - loss: 0.6250 - regression_loss: 0.5059 - classification_loss: 0.1191
 7069/10000 [====================>.........] - ETA: 22:19 - loss: 0.6249 - regression_loss: 0.5058 - classification_loss: 0.1191
 7070/10000 [====================>.........] - ETA: 22:18 - loss: 0.6249 - regression_loss: 0.5058 - classification_loss: 0.1191
 7071/10000 [====================>.........] - ETA: 22:18 - loss: 0.6250 - regression_loss: 0.5059 - classification_loss: 0.1191
 7072/10000 [====================>.........] - ETA: 22:18 - loss: 0.6250 - regression_loss: 0.5058 - classification_loss: 0.1192
 7073/10000 [====================>.........] - ETA: 22:17 - loss: 0.6249 - regression_loss: 0.5058 - classification_loss: 0.1191
 7074/10000 [====================>.........] - ETA: 22:17 - loss: 0.6249 - regression_loss: 0.5058 - classification_loss: 0.1191
 7075/10000 [====================>.........] - ETA: 22:16 - loss: 0.6250 - regression_loss: 0.5058 - classification_loss: 0.1191
 7076/10000 [====================>.........] - ETA: 22:16 - loss: 0.6249 - regression_loss: 0.5058 - classification_loss: 0.1191
 7077/10000 [====================>.........] - ETA: 22:15 - loss: 0.6249 - regression_loss: 0.5058 - classification_loss: 0.1191
 7078/10000 [====================>.........] - ETA: 22:15 - loss: 0.6249 - regression_loss: 0.5058 - classification_loss: 0.1191
 7079/10000 [====================>.........] - ETA: 22:14 - loss: 0.6250 - regression_loss: 0.5059 - classification_loss: 0.1191
 7080/10000 [====================>.........] - ETA: 22:14 - loss: 0.6251 - regression_loss: 0.5059 - classification_loss: 0.1191
 7081/10000 [====================>.........] - ETA: 22:13 - loss: 0.6253 - regression_loss: 0.5061 - classification_loss: 0.1192
 7082/10000 [====================>.........] - ETA: 22:13 - loss: 0.6253 - regression_loss: 0.5061 - classification_loss: 0.1192
 7083/10000 [====================>.........] - ETA: 22:13 - loss: 0.6252 - regression_loss: 0.5060 - classification_loss: 0.1192
 7084/10000 [====================>.........] - ETA: 22:12 - loss: 0.6252 - regression_loss: 0.5060 - classification_loss: 0.1192
 7085/10000 [====================>.........] - ETA: 22:12 - loss: 0.6252 - regression_loss: 0.5060 - classification_loss: 0.1192
 7086/10000 [====================>.........] - ETA: 22:11 - loss: 0.6252 - regression_loss: 0.5060 - classification_loss: 0.1192
 7087/10000 [====================>.........] - ETA: 22:11 - loss: 0.6251 - regression_loss: 0.5059 - classification_loss: 0.1192
 7088/10000 [====================>.........] - ETA: 22:10 - loss: 0.6251 - regression_loss: 0.5059 - classification_loss: 0.1192
 7089/10000 [====================>.........] - ETA: 22:10 - loss: 0.6251 - regression_loss: 0.5059 - classification_loss: 0.1192
 7090/10000 [====================>.........] - ETA: 22:09 - loss: 0.6251 - regression_loss: 0.5059 - classification_loss: 0.1192
 7091/10000 [====================>.........] - ETA: 22:09 - loss: 0.6250 - regression_loss: 0.5059 - classification_loss: 0.1192
 7092/10000 [====================>.........] - ETA: 22:08 - loss: 0.6250 - regression_loss: 0.5059 - classification_loss: 0.1192
 7093/10000 [====================>.........] - ETA: 22:08 - loss: 0.6251 - regression_loss: 0.5059 - classification_loss: 0.1192
 7094/10000 [====================>.........] - ETA: 22:08 - loss: 0.6251 - regression_loss: 0.5059 - classification_loss: 0.1192
 7095/10000 [====================>.........] - ETA: 22:07 - loss: 0.6251 - regression_loss: 0.5059 - classification_loss: 0.1192
 7096/10000 [====================>.........] - ETA: 22:07 - loss: 0.6250 - regression_loss: 0.5058 - classification_loss: 0.1192
 7097/10000 [====================>.........] - ETA: 22:06 - loss: 0.6250 - regression_loss: 0.5058 - classification_loss: 0.1192
 7098/10000 [====================>.........] - ETA: 22:06 - loss: 0.6250 - regression_loss: 0.5058 - classification_loss: 0.1192
 7099/10000 [====================>.........] - ETA: 22:05 - loss: 0.6250 - regression_loss: 0.5058 - classification_loss: 0.1192
 7100/10000 [====================>.........] - ETA: 22:05 - loss: 0.6250 - regression_loss: 0.5058 - classification_loss: 0.1192
 7101/10000 [====================>.........] - ETA: 22:04 - loss: 0.6250 - regression_loss: 0.5058 - classification_loss: 0.1192
 7102/10000 [====================>.........] - ETA: 22:04 - loss: 0.6250 - regression_loss: 0.5058 - classification_loss: 0.1192
 7103/10000 [====================>.........] - ETA: 22:03 - loss: 0.6250 - regression_loss: 0.5058 - classification_loss: 0.1192
 7104/10000 [====================>.........] - ETA: 22:03 - loss: 0.6250 - regression_loss: 0.5058 - classification_loss: 0.1192
 7105/10000 [====================>.........] - ETA: 22:02 - loss: 0.6249 - regression_loss: 0.5057 - classification_loss: 0.1192
 7106/10000 [====================>.........] - ETA: 22:02 - loss: 0.6249 - regression_loss: 0.5057 - classification_loss: 0.1192
 7107/10000 [====================>.........] - ETA: 22:02 - loss: 0.6249 - regression_loss: 0.5057 - classification_loss: 0.1192
 7108/10000 [====================>.........] - ETA: 22:01 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7109/10000 [====================>.........] - ETA: 22:01 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7110/10000 [====================>.........] - ETA: 22:00 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7111/10000 [====================>.........] - ETA: 22:00 - loss: 0.6249 - regression_loss: 0.5057 - classification_loss: 0.1192
 7112/10000 [====================>.........] - ETA: 21:59 - loss: 0.6249 - regression_loss: 0.5057 - classification_loss: 0.1192
 7113/10000 [====================>.........] - ETA: 21:59 - loss: 0.6249 - regression_loss: 0.5056 - classification_loss: 0.1192
 7114/10000 [====================>.........] - ETA: 21:58 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7115/10000 [====================>.........] - ETA: 21:58 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7116/10000 [====================>.........] - ETA: 21:57 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7117/10000 [====================>.........] - ETA: 21:57 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7118/10000 [====================>.........] - ETA: 21:57 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7119/10000 [====================>.........] - ETA: 21:56 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7120/10000 [====================>.........] - ETA: 21:56 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7121/10000 [====================>.........] - ETA: 21:55 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7122/10000 [====================>.........] - ETA: 21:55 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7123/10000 [====================>.........] - ETA: 21:54 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7124/10000 [====================>.........] - ETA: 21:54 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7125/10000 [====================>.........] - ETA: 21:53 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7126/10000 [====================>.........] - ETA: 21:53 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7127/10000 [====================>.........] - ETA: 21:52 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7128/10000 [====================>.........] - ETA: 21:52 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7129/10000 [====================>.........] - ETA: 21:52 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7130/10000 [====================>.........] - ETA: 21:51 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7131/10000 [====================>.........] - ETA: 21:51 - loss: 0.6247 - regression_loss: 0.5056 - classification_loss: 0.1192
 7132/10000 [====================>.........] - ETA: 21:50 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7133/10000 [====================>.........] - ETA: 21:50 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7134/10000 [====================>.........] - ETA: 21:49 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7135/10000 [====================>.........] - ETA: 21:49 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7136/10000 [====================>.........] - ETA: 21:48 - loss: 0.6249 - regression_loss: 0.5057 - classification_loss: 0.1192
 7137/10000 [====================>.........] - ETA: 21:48 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7138/10000 [====================>.........] - ETA: 21:47 - loss: 0.6249 - regression_loss: 0.5057 - classification_loss: 0.1192
 7139/10000 [====================>.........] - ETA: 21:47 - loss: 0.6249 - regression_loss: 0.5057 - classification_loss: 0.1192
 7140/10000 [====================>.........] - ETA: 21:47 - loss: 0.6249 - regression_loss: 0.5057 - classification_loss: 0.1192
 7141/10000 [====================>.........] - ETA: 21:46 - loss: 0.6249 - regression_loss: 0.5057 - classification_loss: 0.1192
 7142/10000 [====================>.........] - ETA: 21:46 - loss: 0.6250 - regression_loss: 0.5058 - classification_loss: 0.1192
 7143/10000 [====================>.........] - ETA: 21:45 - loss: 0.6250 - regression_loss: 0.5058 - classification_loss: 0.1192
 7144/10000 [====================>.........] - ETA: 21:45 - loss: 0.6250 - regression_loss: 0.5058 - classification_loss: 0.1192
 7145/10000 [====================>.........] - ETA: 21:44 - loss: 0.6250 - regression_loss: 0.5058 - classification_loss: 0.1192
 7146/10000 [====================>.........] - ETA: 21:44 - loss: 0.6250 - regression_loss: 0.5058 - classification_loss: 0.1192
 7147/10000 [====================>.........] - ETA: 21:43 - loss: 0.6250 - regression_loss: 0.5058 - classification_loss: 0.1192
 7148/10000 [====================>.........] - ETA: 21:43 - loss: 0.6249 - regression_loss: 0.5058 - classification_loss: 0.1192
 7149/10000 [====================>.........] - ETA: 21:42 - loss: 0.6249 - regression_loss: 0.5058 - classification_loss: 0.1192
 7150/10000 [====================>.........] - ETA: 21:42 - loss: 0.6250 - regression_loss: 0.5058 - classification_loss: 0.1192
 7151/10000 [====================>.........] - ETA: 21:41 - loss: 0.6249 - regression_loss: 0.5058 - classification_loss: 0.1192
 7152/10000 [====================>.........] - ETA: 21:41 - loss: 0.6249 - regression_loss: 0.5058 - classification_loss: 0.1192
 7153/10000 [====================>.........] - ETA: 21:41 - loss: 0.6249 - regression_loss: 0.5057 - classification_loss: 0.1192
 7154/10000 [====================>.........] - ETA: 21:40 - loss: 0.6248 - regression_loss: 0.5057 - classification_loss: 0.1191
 7155/10000 [====================>.........] - ETA: 21:40 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1191
 7156/10000 [====================>.........] - ETA: 21:39 - loss: 0.6248 - regression_loss: 0.5057 - classification_loss: 0.1192
 7157/10000 [====================>.........] - ETA: 21:39 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1191
 7158/10000 [====================>.........] - ETA: 21:38 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7159/10000 [====================>.........] - ETA: 21:38 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7160/10000 [====================>.........] - ETA: 21:37 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7161/10000 [====================>.........] - ETA: 21:37 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7162/10000 [====================>.........] - ETA: 21:36 - loss: 0.6247 - regression_loss: 0.5056 - classification_loss: 0.1192
 7163/10000 [====================>.........] - ETA: 21:36 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7164/10000 [====================>.........] - ETA: 21:36 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7165/10000 [====================>.........] - ETA: 21:35 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7166/10000 [====================>.........] - ETA: 21:35 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7167/10000 [====================>.........] - ETA: 21:34 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7168/10000 [====================>.........] - ETA: 21:34 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7169/10000 [====================>.........] - ETA: 21:33 - loss: 0.6246 - regression_loss: 0.5055 - classification_loss: 0.1192
 7170/10000 [====================>.........] - ETA: 21:33 - loss: 0.6246 - regression_loss: 0.5055 - classification_loss: 0.1192
 7171/10000 [====================>.........] - ETA: 21:32 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7172/10000 [====================>.........] - ETA: 21:32 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7173/10000 [====================>.........] - ETA: 21:31 - loss: 0.6246 - regression_loss: 0.5055 - classification_loss: 0.1192
 7174/10000 [====================>.........] - ETA: 21:31 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7175/10000 [====================>.........] - ETA: 21:31 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7176/10000 [====================>.........] - ETA: 21:30 - loss: 0.6249 - regression_loss: 0.5057 - classification_loss: 0.1192
 7177/10000 [====================>.........] - ETA: 21:30 - loss: 0.6248 - regression_loss: 0.5057 - classification_loss: 0.1192
 7178/10000 [====================>.........] - ETA: 21:29 - loss: 0.6248 - regression_loss: 0.5057 - classification_loss: 0.1192
 7179/10000 [====================>.........] - ETA: 21:29 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7180/10000 [====================>.........] - ETA: 21:28 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7181/10000 [====================>.........] - ETA: 21:28 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7182/10000 [====================>.........] - ETA: 21:27 - loss: 0.6247 - regression_loss: 0.5056 - classification_loss: 0.1192
 7183/10000 [====================>.........] - ETA: 21:27 - loss: 0.6247 - regression_loss: 0.5056 - classification_loss: 0.1192
 7184/10000 [====================>.........] - ETA: 21:26 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7185/10000 [====================>.........] - ETA: 21:26 - loss: 0.6249 - regression_loss: 0.5057 - classification_loss: 0.1192
 7186/10000 [====================>.........] - ETA: 21:25 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7187/10000 [====================>.........] - ETA: 21:25 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7188/10000 [====================>.........] - ETA: 21:25 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7189/10000 [====================>.........] - ETA: 21:24 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7190/10000 [====================>.........] - ETA: 21:24 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7191/10000 [====================>.........] - ETA: 21:23 - loss: 0.6248 - regression_loss: 0.5057 - classification_loss: 0.1192
 7192/10000 [====================>.........] - ETA: 21:23 - loss: 0.6249 - regression_loss: 0.5057 - classification_loss: 0.1192
 7193/10000 [====================>.........] - ETA: 21:22 - loss: 0.6249 - regression_loss: 0.5057 - classification_loss: 0.1192
 7194/10000 [====================>.........] - ETA: 21:22 - loss: 0.6248 - regression_loss: 0.5057 - classification_loss: 0.1192
 7195/10000 [====================>.........] - ETA: 21:21 - loss: 0.6249 - regression_loss: 0.5057 - classification_loss: 0.1192
 7196/10000 [====================>.........] - ETA: 21:21 - loss: 0.6248 - regression_loss: 0.5057 - classification_loss: 0.1192
 7197/10000 [====================>.........] - ETA: 21:20 - loss: 0.6249 - regression_loss: 0.5057 - classification_loss: 0.1192
 7198/10000 [====================>.........] - ETA: 21:20 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7199/10000 [====================>.........] - ETA: 21:19 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7200/10000 [====================>.........] - ETA: 21:19 - loss: 0.6248 - regression_loss: 0.5056 - classification_loss: 0.1192
 7201/10000 [====================>.........] - ETA: 21:19 - loss: 0.6247 - regression_loss: 0.5056 - classification_loss: 0.1192
 7202/10000 [====================>.........] - ETA: 21:18 - loss: 0.6247 - regression_loss: 0.5056 - classification_loss: 0.1192
 7203/10000 [====================>.........] - ETA: 21:18 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1191
 7204/10000 [====================>.........] - ETA: 21:17 - loss: 0.6246 - regression_loss: 0.5055 - classification_loss: 0.1192
 7205/10000 [====================>.........] - ETA: 21:17 - loss: 0.6246 - regression_loss: 0.5055 - classification_loss: 0.1192
 7206/10000 [====================>.........] - ETA: 21:16 - loss: 0.6246 - regression_loss: 0.5055 - classification_loss: 0.1192
 7207/10000 [====================>.........] - ETA: 21:16 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7208/10000 [====================>.........] - ETA: 21:15 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7209/10000 [====================>.........] - ETA: 21:15 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7210/10000 [====================>.........] - ETA: 21:14 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7211/10000 [====================>.........] - ETA: 21:14 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7212/10000 [====================>.........] - ETA: 21:13 - loss: 0.6246 - regression_loss: 0.5055 - classification_loss: 0.1192
 7213/10000 [====================>.........] - ETA: 21:13 - loss: 0.6246 - regression_loss: 0.5054 - classification_loss: 0.1192
 7214/10000 [====================>.........] - ETA: 21:13 - loss: 0.6246 - regression_loss: 0.5055 - classification_loss: 0.1191
 7215/10000 [====================>.........] - ETA: 21:12 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7216/10000 [====================>.........] - ETA: 21:12 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7217/10000 [====================>.........] - ETA: 21:11 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7218/10000 [====================>.........] - ETA: 21:11 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7219/10000 [====================>.........] - ETA: 21:10 - loss: 0.6246 - regression_loss: 0.5055 - classification_loss: 0.1192
 7220/10000 [====================>.........] - ETA: 21:10 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7221/10000 [====================>.........] - ETA: 21:09 - loss: 0.6246 - regression_loss: 0.5055 - classification_loss: 0.1191
 7222/10000 [====================>.........] - ETA: 21:09 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7223/10000 [====================>.........] - ETA: 21:09 - loss: 0.6247 - regression_loss: 0.5056 - classification_loss: 0.1192
 7224/10000 [====================>.........] - ETA: 21:08 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7225/10000 [====================>.........] - ETA: 21:08 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7226/10000 [====================>.........] - ETA: 21:07 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7227/10000 [====================>.........] - ETA: 21:07 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7228/10000 [====================>.........] - ETA: 21:06 - loss: 0.6247 - regression_loss: 0.5055 - classification_loss: 0.1192
 7229/10000 [====================>.........] - ETA: 21:06 - loss: 0.6246 - regression_loss: 0.5055 - classification_loss: 0.1192
 7230/10000 [====================>.........] - ETA: 21:05 - loss: 0.6246 - regression_loss: 0.5055 - classification_loss: 0.1192
 7231/10000 [====================>.........] - ETA: 21:05 - loss: 0.6246 - regression_loss: 0.5054 - classification_loss: 0.1191
 7232/10000 [====================>.........] - ETA: 21:04 - loss: 0.6245 - regression_loss: 0.5054 - classification_loss: 0.1191
 7233/10000 [====================>.........] - ETA: 21:04 - loss: 0.6246 - regression_loss: 0.5055 - classification_loss: 0.1192
 7234/10000 [====================>.........] - ETA: 21:03 - loss: 0.6246 - regression_loss: 0.5054 - classification_loss: 0.1192
 7235/10000 [====================>.........] - ETA: 21:03 - loss: 0.6246 - regression_loss: 0.5054 - classification_loss: 0.1192
 7236/10000 [====================>.........] - ETA: 21:03 - loss: 0.6245 - regression_loss: 0.5054 - classification_loss: 0.1191
 7237/10000 [====================>.........] - ETA: 21:02 - loss: 0.6245 - regression_loss: 0.5053 - classification_loss: 0.1191
 7238/10000 [====================>.........] - ETA: 21:02 - loss: 0.6245 - regression_loss: 0.5053 - classification_loss: 0.1191
 7239/10000 [====================>.........] - ETA: 21:01 - loss: 0.6244 - regression_loss: 0.5053 - classification_loss: 0.1191
 7240/10000 [====================>.........] - ETA: 21:01 - loss: 0.6245 - regression_loss: 0.5054 - classification_loss: 0.1191
 7241/10000 [====================>.........] - ETA: 21:00 - loss: 0.6245 - regression_loss: 0.5054 - classification_loss: 0.1191
 7242/10000 [====================>.........] - ETA: 21:00 - loss: 0.6245 - regression_loss: 0.5054 - classification_loss: 0.1191
 7243/10000 [====================>.........] - ETA: 20:59 - loss: 0.6245 - regression_loss: 0.5054 - classification_loss: 0.1191
 7244/10000 [====================>.........] - ETA: 20:59 - loss: 0.6244 - regression_loss: 0.5053 - classification_loss: 0.1191
 7245/10000 [====================>.........] - ETA: 20:58 - loss: 0.6244 - regression_loss: 0.5053 - classification_loss: 0.1191
 7246/10000 [====================>.........] - ETA: 20:58 - loss: 0.6244 - regression_loss: 0.5053 - classification_loss: 0.1191
 7247/10000 [====================>.........] - ETA: 20:57 - loss: 0.6244 - regression_loss: 0.5053 - classification_loss: 0.1191
 7248/10000 [====================>.........] - ETA: 20:57 - loss: 0.6244 - regression_loss: 0.5053 - classification_loss: 0.1191
 7249/10000 [====================>.........] - ETA: 20:57 - loss: 0.6244 - regression_loss: 0.5053 - classification_loss: 0.1191
 7250/10000 [====================>.........] - ETA: 20:56 - loss: 0.6245 - regression_loss: 0.5053 - classification_loss: 0.1191
 7251/10000 [====================>.........] - ETA: 20:56 - loss: 0.6244 - regression_loss: 0.5053 - classification_loss: 0.1191
 7252/10000 [====================>.........] - ETA: 20:55 - loss: 0.6244 - regression_loss: 0.5053 - classification_loss: 0.1191
 7253/10000 [====================>.........] - ETA: 20:55 - loss: 0.6244 - regression_loss: 0.5053 - classification_loss: 0.1191
 7254/10000 [====================>.........] - ETA: 20:54 - loss: 0.6243 - regression_loss: 0.5052 - classification_loss: 0.1191
 7255/10000 [====================>.........] - ETA: 20:54 - loss: 0.6244 - regression_loss: 0.5053 - classification_loss: 0.1191
 7256/10000 [====================>.........] - ETA: 20:53 - loss: 0.6244 - regression_loss: 0.5053 - classification_loss: 0.1191
 7257/10000 [====================>.........] - ETA: 20:53 - loss: 0.6243 - regression_loss: 0.5053 - classification_loss: 0.1191
 7258/10000 [====================>.........] - ETA: 20:52 - loss: 0.6243 - regression_loss: 0.5052 - classification_loss: 0.1191
 7259/10000 [====================>.........] - ETA: 20:52 - loss: 0.6243 - regression_loss: 0.5052 - classification_loss: 0.1191
 7260/10000 [====================>.........] - ETA: 20:52 - loss: 0.6243 - regression_loss: 0.5052 - classification_loss: 0.1191
 7261/10000 [====================>.........] - ETA: 20:51 - loss: 0.6242 - regression_loss: 0.5052 - classification_loss: 0.1191
 7262/10000 [====================>.........] - ETA: 20:51 - loss: 0.6243 - regression_loss: 0.5052 - classification_loss: 0.1191
 7263/10000 [====================>.........] - ETA: 20:50 - loss: 0.6243 - regression_loss: 0.5052 - classification_loss: 0.1191
 7264/10000 [====================>.........] - ETA: 20:50 - loss: 0.6243 - regression_loss: 0.5052 - classification_loss: 0.1191
 7265/10000 [====================>.........] - ETA: 20:49 - loss: 0.6243 - regression_loss: 0.5052 - classification_loss: 0.1191
 7266/10000 [====================>.........] - ETA: 20:49 - loss: 0.6242 - regression_loss: 0.5052 - classification_loss: 0.1191
 7267/10000 [====================>.........] - ETA: 20:48 - loss: 0.6243 - regression_loss: 0.5052 - classification_loss: 0.1191
 7268/10000 [====================>.........] - ETA: 20:48 - loss: 0.6243 - regression_loss: 0.5052 - classification_loss: 0.1191
 7269/10000 [====================>.........] - ETA: 20:47 - loss: 0.6242 - regression_loss: 0.5052 - classification_loss: 0.1191
 7270/10000 [====================>.........] - ETA: 20:47 - loss: 0.6242 - regression_loss: 0.5051 - classification_loss: 0.1191
 7271/10000 [====================>.........] - ETA: 20:47 - loss: 0.6242 - regression_loss: 0.5051 - classification_loss: 0.1191
 7272/10000 [====================>.........] - ETA: 20:46 - loss: 0.6242 - regression_loss: 0.5051 - classification_loss: 0.1191
 7273/10000 [====================>.........] - ETA: 20:46 - loss: 0.6241 - regression_loss: 0.5050 - classification_loss: 0.1191
 7274/10000 [====================>.........] - ETA: 20:45 - loss: 0.6241 - regression_loss: 0.5051 - classification_loss: 0.1191
 7275/10000 [====================>.........] - ETA: 20:45 - loss: 0.6241 - regression_loss: 0.5051 - classification_loss: 0.1191
 7276/10000 [====================>.........] - ETA: 20:44 - loss: 0.6241 - regression_loss: 0.5050 - classification_loss: 0.1191
 7277/10000 [====================>.........] - ETA: 20:44 - loss: 0.6241 - regression_loss: 0.5050 - classification_loss: 0.1190
 7278/10000 [====================>.........] - ETA: 20:43 - loss: 0.6241 - regression_loss: 0.5051 - classification_loss: 0.1191
 7279/10000 [====================>.........] - ETA: 20:43 - loss: 0.6241 - regression_loss: 0.5051 - classification_loss: 0.1191
 7280/10000 [====================>.........] - ETA: 20:42 - loss: 0.6241 - regression_loss: 0.5050 - classification_loss: 0.1191
 7281/10000 [====================>.........] - ETA: 20:42 - loss: 0.6241 - regression_loss: 0.5051 - classification_loss: 0.1191
 7282/10000 [====================>.........] - ETA: 20:42 - loss: 0.6241 - regression_loss: 0.5051 - classification_loss: 0.1191
 7283/10000 [====================>.........] - ETA: 20:41 - loss: 0.6241 - regression_loss: 0.5051 - classification_loss: 0.1191
 7284/10000 [====================>.........] - ETA: 20:41 - loss: 0.6241 - regression_loss: 0.5050 - classification_loss: 0.1190
 7285/10000 [====================>.........] - ETA: 20:40 - loss: 0.6240 - regression_loss: 0.5050 - classification_loss: 0.1190
 7286/10000 [====================>.........] - ETA: 20:40 - loss: 0.6240 - regression_loss: 0.5050 - classification_loss: 0.1190
 7287/10000 [====================>.........] - ETA: 20:39 - loss: 0.6240 - regression_loss: 0.5049 - classification_loss: 0.1190
 7288/10000 [====================>.........] - ETA: 20:39 - loss: 0.6240 - regression_loss: 0.5050 - classification_loss: 0.1190
 7289/10000 [====================>.........] - ETA: 20:38 - loss: 0.6240 - regression_loss: 0.5050 - classification_loss: 0.1190
 7290/10000 [====================>.........] - ETA: 20:38 - loss: 0.6239 - regression_loss: 0.5049 - classification_loss: 0.1190
 7291/10000 [====================>.........] - ETA: 20:37 - loss: 0.6239 - regression_loss: 0.5049 - classification_loss: 0.1190
 7292/10000 [====================>.........] - ETA: 20:37 - loss: 0.6239 - regression_loss: 0.5049 - classification_loss: 0.1190
 7293/10000 [====================>.........] - ETA: 20:37 - loss: 0.6238 - regression_loss: 0.5048 - classification_loss: 0.1190
 7294/10000 [====================>.........] - ETA: 20:36 - loss: 0.6239 - regression_loss: 0.5049 - classification_loss: 0.1190
 7295/10000 [====================>.........] - ETA: 20:36 - loss: 0.6239 - regression_loss: 0.5049 - classification_loss: 0.1190
 7296/10000 [====================>.........] - ETA: 20:35 - loss: 0.6239 - regression_loss: 0.5049 - classification_loss: 0.1190
 7297/10000 [====================>.........] - ETA: 20:35 - loss: 0.6239 - regression_loss: 0.5049 - classification_loss: 0.1190
 7298/10000 [====================>.........] - ETA: 20:34 - loss: 0.6238 - regression_loss: 0.5048 - classification_loss: 0.1190
 7299/10000 [====================>.........] - ETA: 20:34 - loss: 0.6238 - regression_loss: 0.5048 - classification_loss: 0.1190
 7300/10000 [====================>.........] - ETA: 20:33 - loss: 0.6238 - regression_loss: 0.5048 - classification_loss: 0.1190
 7301/10000 [====================>.........] - ETA: 20:33 - loss: 0.6238 - regression_loss: 0.5048 - classification_loss: 0.1190
 7302/10000 [====================>.........] - ETA: 20:32 - loss: 0.6239 - regression_loss: 0.5049 - classification_loss: 0.1190
 7303/10000 [====================>.........] - ETA: 20:32 - loss: 0.6239 - regression_loss: 0.5049 - classification_loss: 0.1191
 7304/10000 [====================>.........] - ETA: 20:31 - loss: 0.6239 - regression_loss: 0.5049 - classification_loss: 0.1190
 7305/10000 [====================>.........] - ETA: 20:31 - loss: 0.6240 - regression_loss: 0.5049 - classification_loss: 0.1191
 7306/10000 [====================>.........] - ETA: 20:31 - loss: 0.6240 - regression_loss: 0.5049 - classification_loss: 0.1191
 7307/10000 [====================>.........] - ETA: 20:30 - loss: 0.6240 - regression_loss: 0.5049 - classification_loss: 0.1190
 7308/10000 [====================>.........] - ETA: 20:30 - loss: 0.6240 - regression_loss: 0.5049 - classification_loss: 0.1190
 7309/10000 [====================>.........] - ETA: 20:29 - loss: 0.6239 - regression_loss: 0.5049 - classification_loss: 0.1190
 7310/10000 [====================>.........] - ETA: 20:29 - loss: 0.6239 - regression_loss: 0.5048 - classification_loss: 0.1190
 7311/10000 [====================>.........] - ETA: 20:28 - loss: 0.6238 - regression_loss: 0.5048 - classification_loss: 0.1190
 7312/10000 [====================>.........] - ETA: 20:28 - loss: 0.6238 - regression_loss: 0.5047 - classification_loss: 0.1190
 7313/10000 [====================>.........] - ETA: 20:27 - loss: 0.6237 - regression_loss: 0.5047 - classification_loss: 0.1190
 7314/10000 [====================>.........] - ETA: 20:27 - loss: 0.6238 - regression_loss: 0.5047 - classification_loss: 0.1190
 7315/10000 [====================>.........] - ETA: 20:26 - loss: 0.6237 - regression_loss: 0.5047 - classification_loss: 0.1190
 7316/10000 [====================>.........] - ETA: 20:26 - loss: 0.6237 - regression_loss: 0.5047 - classification_loss: 0.1190
 7317/10000 [====================>.........] - ETA: 20:25 - loss: 0.6236 - regression_loss: 0.5046 - classification_loss: 0.1190
 7318/10000 [====================>.........] - ETA: 20:25 - loss: 0.6235 - regression_loss: 0.5046 - classification_loss: 0.1190
 7319/10000 [====================>.........] - ETA: 20:25 - loss: 0.6236 - regression_loss: 0.5046 - classification_loss: 0.1190
 7320/10000 [====================>.........] - ETA: 20:24 - loss: 0.6236 - regression_loss: 0.5046 - classification_loss: 0.1190
 7321/10000 [====================>.........] - ETA: 20:24 - loss: 0.6236 - regression_loss: 0.5047 - classification_loss: 0.1190
 7322/10000 [====================>.........] - ETA: 20:23 - loss: 0.6236 - regression_loss: 0.5046 - classification_loss: 0.1190
 7323/10000 [====================>.........] - ETA: 20:23 - loss: 0.6236 - regression_loss: 0.5046 - classification_loss: 0.1190
 7324/10000 [====================>.........] - ETA: 20:22 - loss: 0.6236 - regression_loss: 0.5046 - classification_loss: 0.1190
 7325/10000 [====================>.........] - ETA: 20:22 - loss: 0.6235 - regression_loss: 0.5046 - classification_loss: 0.1190
 7326/10000 [====================>.........] - ETA: 20:21 - loss: 0.6236 - regression_loss: 0.5046 - classification_loss: 0.1190
 7327/10000 [====================>.........] - ETA: 20:21 - loss: 0.6236 - regression_loss: 0.5046 - classification_loss: 0.1190
 7328/10000 [====================>.........] - ETA: 20:20 - loss: 0.6236 - regression_loss: 0.5046 - classification_loss: 0.1190
 7329/10000 [====================>.........] - ETA: 20:20 - loss: 0.6236 - regression_loss: 0.5046 - classification_loss: 0.1190
 7330/10000 [====================>.........] - ETA: 20:20 - loss: 0.6236 - regression_loss: 0.5046 - classification_loss: 0.1190
 7331/10000 [====================>.........] - ETA: 20:19 - loss: 0.6236 - regression_loss: 0.5046 - classification_loss: 0.1190
 7332/10000 [====================>.........] - ETA: 20:19 - loss: 0.6236 - regression_loss: 0.5046 - classification_loss: 0.1190
 7333/10000 [====================>.........] - ETA: 20:18 - loss: 0.6237 - regression_loss: 0.5047 - classification_loss: 0.1190
 7334/10000 [=====================>........] - ETA: 20:18 - loss: 0.6238 - regression_loss: 0.5047 - classification_loss: 0.1190
 7335/10000 [=====================>........] - ETA: 20:17 - loss: 0.6237 - regression_loss: 0.5047 - classification_loss: 0.1190
 7336/10000 [=====================>........] - ETA: 20:17 - loss: 0.6238 - regression_loss: 0.5047 - classification_loss: 0.1190
 7337/10000 [=====================>........] - ETA: 20:16 - loss: 0.6238 - regression_loss: 0.5048 - classification_loss: 0.1190
 7338/10000 [=====================>........] - ETA: 20:16 - loss: 0.6238 - regression_loss: 0.5047 - classification_loss: 0.1190
 7339/10000 [=====================>........] - ETA: 20:15 - loss: 0.6237 - regression_loss: 0.5047 - classification_loss: 0.1190
 7340/10000 [=====================>........] - ETA: 20:15 - loss: 0.6237 - regression_loss: 0.5047 - classification_loss: 0.1190
 7341/10000 [=====================>........] - ETA: 20:14 - loss: 0.6238 - regression_loss: 0.5047 - classification_loss: 0.1190
 7342/10000 [=====================>........] - ETA: 20:14 - loss: 0.6239 - regression_loss: 0.5049 - classification_loss: 0.1190
 7343/10000 [=====================>........] - ETA: 20:14 - loss: 0.6240 - regression_loss: 0.5049 - classification_loss: 0.1190
 7344/10000 [=====================>........] - ETA: 20:13 - loss: 0.6239 - regression_loss: 0.5049 - classification_loss: 0.1190
 7345/10000 [=====================>........] - ETA: 20:13 - loss: 0.6240 - regression_loss: 0.5049 - classification_loss: 0.1190
 7346/10000 [=====================>........] - ETA: 20:12 - loss: 0.6239 - regression_loss: 0.5049 - classification_loss: 0.1190
 7347/10000 [=====================>........] - ETA: 20:12 - loss: 0.6240 - regression_loss: 0.5049 - classification_loss: 0.1190
 7348/10000 [=====================>........] - ETA: 20:11 - loss: 0.6239 - regression_loss: 0.5049 - classification_loss: 0.1190
 7349/10000 [=====================>........] - ETA: 20:11 - loss: 0.6238 - regression_loss: 0.5048 - classification_loss: 0.1190
 7350/10000 [=====================>........] - ETA: 20:10 - loss: 0.6238 - regression_loss: 0.5048 - classification_loss: 0.1190
 7351/10000 [=====================>........] - ETA: 20:10 - loss: 0.6238 - regression_loss: 0.5048 - classification_loss: 0.1190
 7352/10000 [=====================>........] - ETA: 20:09 - loss: 0.6238 - regression_loss: 0.5048 - classification_loss: 0.1190
 7353/10000 [=====================>........] - ETA: 20:09 - loss: 0.6238 - regression_loss: 0.5048 - classification_loss: 0.1190
 7354/10000 [=====================>........] - ETA: 20:09 - loss: 0.6237 - regression_loss: 0.5047 - classification_loss: 0.1190
 7355/10000 [=====================>........] - ETA: 20:08 - loss: 0.6237 - regression_loss: 0.5047 - classification_loss: 0.1190
 7356/10000 [=====================>........] - ETA: 20:08 - loss: 0.6238 - regression_loss: 0.5048 - classification_loss: 0.1190
 7357/10000 [=====================>........] - ETA: 20:07 - loss: 0.6237 - regression_loss: 0.5047 - classification_loss: 0.1190
 7358/10000 [=====================>........] - ETA: 20:07 - loss: 0.6237 - regression_loss: 0.5047 - classification_loss: 0.1190
 7359/10000 [=====================>........] - ETA: 20:06 - loss: 0.6237 - regression_loss: 0.5047 - classification_loss: 0.1190
 7360/10000 [=====================>........] - ETA: 20:06 - loss: 0.6237 - regression_loss: 0.5047 - classification_loss: 0.1190
 7361/10000 [=====================>........] - ETA: 20:05 - loss: 0.6237 - regression_loss: 0.5047 - classification_loss: 0.1190
 7362/10000 [=====================>........] - ETA: 20:05 - loss: 0.6237 - regression_loss: 0.5047 - classification_loss: 0.1190
 7363/10000 [=====================>........] - ETA: 20:04 - loss: 0.6236 - regression_loss: 0.5047 - classification_loss: 0.1190
 7364/10000 [=====================>........] - ETA: 20:04 - loss: 0.6236 - regression_loss: 0.5046 - classification_loss: 0.1190
 7365/10000 [=====================>........] - ETA: 20:03 - loss: 0.6236 - regression_loss: 0.5046 - classification_loss: 0.1189
 7366/10000 [=====================>........] - ETA: 20:03 - loss: 0.6235 - regression_loss: 0.5046 - classification_loss: 0.1189
 7367/10000 [=====================>........] - ETA: 20:03 - loss: 0.6235 - regression_loss: 0.5046 - classification_loss: 0.1189
 7368/10000 [=====================>........] - ETA: 20:02 - loss: 0.6235 - regression_loss: 0.5046 - classification_loss: 0.1189
 7369/10000 [=====================>........] - ETA: 20:02 - loss: 0.6234 - regression_loss: 0.5045 - classification_loss: 0.1189
 7370/10000 [=====================>........] - ETA: 20:01 - loss: 0.6234 - regression_loss: 0.5045 - classification_loss: 0.1189
 7371/10000 [=====================>........] - ETA: 20:01 - loss: 0.6235 - regression_loss: 0.5045 - classification_loss: 0.1189
 7372/10000 [=====================>........] - ETA: 20:00 - loss: 0.6234 - regression_loss: 0.5045 - classification_loss: 0.1189
 7373/10000 [=====================>........] - ETA: 20:00 - loss: 0.6234 - regression_loss: 0.5044 - classification_loss: 0.1189
 7374/10000 [=====================>........] - ETA: 19:59 - loss: 0.6233 - regression_loss: 0.5044 - classification_loss: 0.1189
 7375/10000 [=====================>........] - ETA: 19:59 - loss: 0.6234 - regression_loss: 0.5045 - classification_loss: 0.1189
 7376/10000 [=====================>........] - ETA: 19:58 - loss: 0.6235 - regression_loss: 0.5045 - classification_loss: 0.1189
 7377/10000 [=====================>........] - ETA: 19:58 - loss: 0.6235 - regression_loss: 0.5045 - classification_loss: 0.1189
 7378/10000 [=====================>........] - ETA: 19:58 - loss: 0.6234 - regression_loss: 0.5045 - classification_loss: 0.1189
 7379/10000 [=====================>........] - ETA: 19:57 - loss: 0.6234 - regression_loss: 0.5045 - classification_loss: 0.1189
 7380/10000 [=====================>........] - ETA: 19:57 - loss: 0.6234 - regression_loss: 0.5045 - classification_loss: 0.1189
 7381/10000 [=====================>........] - ETA: 19:56 - loss: 0.6234 - regression_loss: 0.5045 - classification_loss: 0.1189
 7382/10000 [=====================>........] - ETA: 19:56 - loss: 0.6234 - regression_loss: 0.5045 - classification_loss: 0.1189
 7383/10000 [=====================>........] - ETA: 19:55 - loss: 0.6233 - regression_loss: 0.5044 - classification_loss: 0.1189
 7384/10000 [=====================>........] - ETA: 19:55 - loss: 0.6233 - regression_loss: 0.5044 - classification_loss: 0.1189
 7385/10000 [=====================>........] - ETA: 19:54 - loss: 0.6232 - regression_loss: 0.5043 - classification_loss: 0.1189
 7386/10000 [=====================>........] - ETA: 19:54 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1189
 7387/10000 [=====================>........] - ETA: 19:53 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1189
 7388/10000 [=====================>........] - ETA: 19:53 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7389/10000 [=====================>........] - ETA: 19:53 - loss: 0.6232 - regression_loss: 0.5043 - classification_loss: 0.1188
 7390/10000 [=====================>........] - ETA: 19:52 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7391/10000 [=====================>........] - ETA: 19:52 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7392/10000 [=====================>........] - ETA: 19:51 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7393/10000 [=====================>........] - ETA: 19:51 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7394/10000 [=====================>........] - ETA: 19:50 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7395/10000 [=====================>........] - ETA: 19:50 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7396/10000 [=====================>........] - ETA: 19:49 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7397/10000 [=====================>........] - ETA: 19:49 - loss: 0.6233 - regression_loss: 0.5044 - classification_loss: 0.1188
 7398/10000 [=====================>........] - ETA: 19:48 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7399/10000 [=====================>........] - ETA: 19:48 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1189
 7400/10000 [=====================>........] - ETA: 19:48 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7401/10000 [=====================>........] - ETA: 19:47 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7402/10000 [=====================>........] - ETA: 19:47 - loss: 0.6233 - regression_loss: 0.5044 - classification_loss: 0.1188
 7403/10000 [=====================>........] - ETA: 19:46 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7404/10000 [=====================>........] - ETA: 19:46 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7405/10000 [=====================>........] - ETA: 19:45 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7406/10000 [=====================>........] - ETA: 19:45 - loss: 0.6232 - regression_loss: 0.5043 - classification_loss: 0.1188
 7407/10000 [=====================>........] - ETA: 19:44 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7408/10000 [=====================>........] - ETA: 19:44 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7409/10000 [=====================>........] - ETA: 19:43 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7410/10000 [=====================>........] - ETA: 19:43 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7411/10000 [=====================>........] - ETA: 19:43 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7412/10000 [=====================>........] - ETA: 19:42 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7413/10000 [=====================>........] - ETA: 19:42 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7414/10000 [=====================>........] - ETA: 19:41 - loss: 0.6230 - regression_loss: 0.5042 - classification_loss: 0.1188
 7415/10000 [=====================>........] - ETA: 19:41 - loss: 0.6230 - regression_loss: 0.5043 - classification_loss: 0.1188
 7416/10000 [=====================>........] - ETA: 19:40 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7417/10000 [=====================>........] - ETA: 19:40 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7418/10000 [=====================>........] - ETA: 19:39 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7419/10000 [=====================>........] - ETA: 19:39 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7420/10000 [=====================>........] - ETA: 19:38 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7421/10000 [=====================>........] - ETA: 19:38 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7422/10000 [=====================>........] - ETA: 19:37 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7423/10000 [=====================>........] - ETA: 19:37 - loss: 0.6230 - regression_loss: 0.5042 - classification_loss: 0.1188
 7424/10000 [=====================>........] - ETA: 19:37 - loss: 0.6230 - regression_loss: 0.5042 - classification_loss: 0.1188
 7425/10000 [=====================>........] - ETA: 19:36 - loss: 0.6230 - regression_loss: 0.5042 - classification_loss: 0.1188
 7426/10000 [=====================>........] - ETA: 19:36 - loss: 0.6229 - regression_loss: 0.5042 - classification_loss: 0.1188
 7427/10000 [=====================>........] - ETA: 19:35 - loss: 0.6229 - regression_loss: 0.5041 - classification_loss: 0.1188
 7428/10000 [=====================>........] - ETA: 19:35 - loss: 0.6229 - regression_loss: 0.5041 - classification_loss: 0.1188
 7429/10000 [=====================>........] - ETA: 19:34 - loss: 0.6229 - regression_loss: 0.5041 - classification_loss: 0.1188
 7430/10000 [=====================>........] - ETA: 19:34 - loss: 0.6229 - regression_loss: 0.5041 - classification_loss: 0.1188
 7431/10000 [=====================>........] - ETA: 19:33 - loss: 0.6229 - regression_loss: 0.5042 - classification_loss: 0.1188
 7432/10000 [=====================>........] - ETA: 19:33 - loss: 0.6229 - regression_loss: 0.5042 - classification_loss: 0.1188
 7433/10000 [=====================>........] - ETA: 19:32 - loss: 0.6229 - regression_loss: 0.5041 - classification_loss: 0.1188
 7434/10000 [=====================>........] - ETA: 19:32 - loss: 0.6229 - regression_loss: 0.5041 - classification_loss: 0.1188
 7435/10000 [=====================>........] - ETA: 19:31 - loss: 0.6228 - regression_loss: 0.5040 - classification_loss: 0.1188
 7436/10000 [=====================>........] - ETA: 19:31 - loss: 0.6228 - regression_loss: 0.5040 - classification_loss: 0.1188
 7437/10000 [=====================>........] - ETA: 19:31 - loss: 0.6228 - regression_loss: 0.5041 - classification_loss: 0.1188
 7438/10000 [=====================>........] - ETA: 19:30 - loss: 0.6228 - regression_loss: 0.5041 - classification_loss: 0.1188
 7439/10000 [=====================>........] - ETA: 19:30 - loss: 0.6229 - regression_loss: 0.5041 - classification_loss: 0.1187
 7440/10000 [=====================>........] - ETA: 19:29 - loss: 0.6229 - regression_loss: 0.5041 - classification_loss: 0.1187
 7441/10000 [=====================>........] - ETA: 19:29 - loss: 0.6229 - regression_loss: 0.5041 - classification_loss: 0.1187
 7442/10000 [=====================>........] - ETA: 19:28 - loss: 0.6229 - regression_loss: 0.5042 - classification_loss: 0.1188
 7443/10000 [=====================>........] - ETA: 19:28 - loss: 0.6229 - regression_loss: 0.5042 - classification_loss: 0.1187
 7444/10000 [=====================>........] - ETA: 19:27 - loss: 0.6229 - regression_loss: 0.5042 - classification_loss: 0.1187
 7445/10000 [=====================>........] - ETA: 19:27 - loss: 0.6229 - regression_loss: 0.5041 - classification_loss: 0.1187
 7446/10000 [=====================>........] - ETA: 19:26 - loss: 0.6228 - regression_loss: 0.5041 - classification_loss: 0.1187
 7447/10000 [=====================>........] - ETA: 19:26 - loss: 0.6228 - regression_loss: 0.5041 - classification_loss: 0.1187
 7448/10000 [=====================>........] - ETA: 19:26 - loss: 0.6230 - regression_loss: 0.5042 - classification_loss: 0.1187
 7449/10000 [=====================>........] - ETA: 19:25 - loss: 0.6230 - regression_loss: 0.5042 - classification_loss: 0.1188
 7450/10000 [=====================>........] - ETA: 19:25 - loss: 0.6229 - regression_loss: 0.5042 - classification_loss: 0.1187
 7451/10000 [=====================>........] - ETA: 19:24 - loss: 0.6228 - regression_loss: 0.5041 - classification_loss: 0.1187
 7452/10000 [=====================>........] - ETA: 19:24 - loss: 0.6228 - regression_loss: 0.5041 - classification_loss: 0.1187
 7453/10000 [=====================>........] - ETA: 19:23 - loss: 0.6228 - regression_loss: 0.5041 - classification_loss: 0.1187
 7454/10000 [=====================>........] - ETA: 19:23 - loss: 0.6227 - regression_loss: 0.5040 - classification_loss: 0.1187
 7455/10000 [=====================>........] - ETA: 19:22 - loss: 0.6227 - regression_loss: 0.5040 - classification_loss: 0.1187
 7456/10000 [=====================>........] - ETA: 19:22 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7457/10000 [=====================>........] - ETA: 19:21 - loss: 0.6225 - regression_loss: 0.5039 - classification_loss: 0.1187
 7458/10000 [=====================>........] - ETA: 19:21 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7459/10000 [=====================>........] - ETA: 19:21 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7460/10000 [=====================>........] - ETA: 19:20 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7461/10000 [=====================>........] - ETA: 19:20 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7462/10000 [=====================>........] - ETA: 19:19 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1186
 7463/10000 [=====================>........] - ETA: 19:19 - loss: 0.6225 - regression_loss: 0.5039 - classification_loss: 0.1187
 7464/10000 [=====================>........] - ETA: 19:18 - loss: 0.6227 - regression_loss: 0.5040 - classification_loss: 0.1187
 7465/10000 [=====================>........] - ETA: 19:18 - loss: 0.6227 - regression_loss: 0.5040 - classification_loss: 0.1187
 7466/10000 [=====================>........] - ETA: 19:17 - loss: 0.6227 - regression_loss: 0.5040 - classification_loss: 0.1187
 7467/10000 [=====================>........] - ETA: 19:17 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7468/10000 [=====================>........] - ETA: 19:16 - loss: 0.6228 - regression_loss: 0.5040 - classification_loss: 0.1187
 7469/10000 [=====================>........] - ETA: 19:16 - loss: 0.6227 - regression_loss: 0.5040 - classification_loss: 0.1187
 7470/10000 [=====================>........] - ETA: 19:16 - loss: 0.6227 - regression_loss: 0.5040 - classification_loss: 0.1187
 7471/10000 [=====================>........] - ETA: 19:15 - loss: 0.6228 - regression_loss: 0.5041 - classification_loss: 0.1187
 7472/10000 [=====================>........] - ETA: 19:15 - loss: 0.6228 - regression_loss: 0.5041 - classification_loss: 0.1187
 7473/10000 [=====================>........] - ETA: 19:14 - loss: 0.6229 - regression_loss: 0.5042 - classification_loss: 0.1187
 7474/10000 [=====================>........] - ETA: 19:14 - loss: 0.6229 - regression_loss: 0.5042 - classification_loss: 0.1187
 7475/10000 [=====================>........] - ETA: 19:13 - loss: 0.6230 - regression_loss: 0.5042 - classification_loss: 0.1187
 7476/10000 [=====================>........] - ETA: 19:13 - loss: 0.6229 - regression_loss: 0.5042 - classification_loss: 0.1187
 7477/10000 [=====================>........] - ETA: 19:12 - loss: 0.6229 - regression_loss: 0.5042 - classification_loss: 0.1187
 7478/10000 [=====================>........] - ETA: 19:12 - loss: 0.6229 - regression_loss: 0.5042 - classification_loss: 0.1187
 7479/10000 [=====================>........] - ETA: 19:11 - loss: 0.6229 - regression_loss: 0.5042 - classification_loss: 0.1187
 7480/10000 [=====================>........] - ETA: 19:11 - loss: 0.6228 - regression_loss: 0.5042 - classification_loss: 0.1187
 7481/10000 [=====================>........] - ETA: 19:11 - loss: 0.6228 - regression_loss: 0.5041 - classification_loss: 0.1187
 7482/10000 [=====================>........] - ETA: 19:10 - loss: 0.6229 - regression_loss: 0.5042 - classification_loss: 0.1187
 7483/10000 [=====================>........] - ETA: 19:10 - loss: 0.6230 - regression_loss: 0.5043 - classification_loss: 0.1187
 7484/10000 [=====================>........] - ETA: 19:09 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7485/10000 [=====================>........] - ETA: 19:09 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7486/10000 [=====================>........] - ETA: 19:08 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7487/10000 [=====================>........] - ETA: 19:08 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7488/10000 [=====================>........] - ETA: 19:07 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7489/10000 [=====================>........] - ETA: 19:07 - loss: 0.6233 - regression_loss: 0.5045 - classification_loss: 0.1188
 7490/10000 [=====================>........] - ETA: 19:06 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7491/10000 [=====================>........] - ETA: 19:06 - loss: 0.6231 - regression_loss: 0.5044 - classification_loss: 0.1188
 7492/10000 [=====================>........] - ETA: 19:06 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7493/10000 [=====================>........] - ETA: 19:05 - loss: 0.6231 - regression_loss: 0.5044 - classification_loss: 0.1188
 7494/10000 [=====================>........] - ETA: 19:05 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7495/10000 [=====================>........] - ETA: 19:04 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7496/10000 [=====================>........] - ETA: 19:04 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7497/10000 [=====================>........] - ETA: 19:03 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7498/10000 [=====================>........] - ETA: 19:03 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7499/10000 [=====================>........] - ETA: 19:02 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7500/10000 [=====================>........] - ETA: 19:02 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7501/10000 [=====================>........] - ETA: 19:01 - loss: 0.6231 - regression_loss: 0.5044 - classification_loss: 0.1188
 7502/10000 [=====================>........] - ETA: 19:01 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7503/10000 [=====================>........] - ETA: 19:00 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7504/10000 [=====================>........] - ETA: 19:00 - loss: 0.6233 - regression_loss: 0.5045 - classification_loss: 0.1188
 7505/10000 [=====================>........] - ETA: 19:00 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7506/10000 [=====================>........] - ETA: 18:59 - loss: 0.6233 - regression_loss: 0.5045 - classification_loss: 0.1188
 7507/10000 [=====================>........] - ETA: 18:59 - loss: 0.6233 - regression_loss: 0.5045 - classification_loss: 0.1188
 7508/10000 [=====================>........] - ETA: 18:58 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7509/10000 [=====================>........] - ETA: 18:58 - loss: 0.6234 - regression_loss: 0.5045 - classification_loss: 0.1188
 7510/10000 [=====================>........] - ETA: 18:57 - loss: 0.6233 - regression_loss: 0.5045 - classification_loss: 0.1188
 7511/10000 [=====================>........] - ETA: 18:57 - loss: 0.6233 - regression_loss: 0.5045 - classification_loss: 0.1188
 7512/10000 [=====================>........] - ETA: 18:56 - loss: 0.6233 - regression_loss: 0.5045 - classification_loss: 0.1188
 7513/10000 [=====================>........] - ETA: 18:56 - loss: 0.6233 - regression_loss: 0.5045 - classification_loss: 0.1188
 7514/10000 [=====================>........] - ETA: 18:55 - loss: 0.6233 - regression_loss: 0.5045 - classification_loss: 0.1188
 7515/10000 [=====================>........] - ETA: 18:55 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7516/10000 [=====================>........] - ETA: 18:55 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7517/10000 [=====================>........] - ETA: 18:54 - loss: 0.6233 - regression_loss: 0.5044 - classification_loss: 0.1188
 7518/10000 [=====================>........] - ETA: 18:54 - loss: 0.6234 - regression_loss: 0.5045 - classification_loss: 0.1189
 7519/10000 [=====================>........] - ETA: 18:53 - loss: 0.6233 - regression_loss: 0.5045 - classification_loss: 0.1188
 7520/10000 [=====================>........] - ETA: 18:53 - loss: 0.6233 - regression_loss: 0.5045 - classification_loss: 0.1188
 7521/10000 [=====================>........] - ETA: 18:52 - loss: 0.6233 - regression_loss: 0.5045 - classification_loss: 0.1188
 7522/10000 [=====================>........] - ETA: 18:52 - loss: 0.6233 - regression_loss: 0.5045 - classification_loss: 0.1188
 7523/10000 [=====================>........] - ETA: 18:51 - loss: 0.6233 - regression_loss: 0.5045 - classification_loss: 0.1188
 7524/10000 [=====================>........] - ETA: 18:51 - loss: 0.6233 - regression_loss: 0.5045 - classification_loss: 0.1188
 7525/10000 [=====================>........] - ETA: 18:50 - loss: 0.6233 - regression_loss: 0.5045 - classification_loss: 0.1188
 7526/10000 [=====================>........] - ETA: 18:50 - loss: 0.6233 - regression_loss: 0.5044 - classification_loss: 0.1188
 7527/10000 [=====================>........] - ETA: 18:49 - loss: 0.6233 - regression_loss: 0.5045 - classification_loss: 0.1188
 7528/10000 [=====================>........] - ETA: 18:49 - loss: 0.6233 - regression_loss: 0.5044 - classification_loss: 0.1188
 7529/10000 [=====================>........] - ETA: 18:49 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7530/10000 [=====================>........] - ETA: 18:48 - loss: 0.6232 - regression_loss: 0.5044 - classification_loss: 0.1188
 7531/10000 [=====================>........] - ETA: 18:48 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7532/10000 [=====================>........] - ETA: 18:47 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7533/10000 [=====================>........] - ETA: 18:47 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7534/10000 [=====================>........] - ETA: 18:46 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7535/10000 [=====================>........] - ETA: 18:46 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7536/10000 [=====================>........] - ETA: 18:45 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7537/10000 [=====================>........] - ETA: 18:45 - loss: 0.6230 - regression_loss: 0.5042 - classification_loss: 0.1188
 7538/10000 [=====================>........] - ETA: 18:44 - loss: 0.6230 - regression_loss: 0.5042 - classification_loss: 0.1188
 7539/10000 [=====================>........] - ETA: 18:44 - loss: 0.6230 - regression_loss: 0.5042 - classification_loss: 0.1188
 7540/10000 [=====================>........] - ETA: 18:44 - loss: 0.6230 - regression_loss: 0.5042 - classification_loss: 0.1188
 7541/10000 [=====================>........] - ETA: 18:43 - loss: 0.6230 - regression_loss: 0.5042 - classification_loss: 0.1188
 7542/10000 [=====================>........] - ETA: 18:43 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7543/10000 [=====================>........] - ETA: 18:42 - loss: 0.6231 - regression_loss: 0.5043 - classification_loss: 0.1188
 7544/10000 [=====================>........] - ETA: 18:42 - loss: 0.6231 - regression_loss: 0.5042 - classification_loss: 0.1188
 7545/10000 [=====================>........] - ETA: 18:41 - loss: 0.6231 - regression_loss: 0.5042 - classification_loss: 0.1188
 7546/10000 [=====================>........] - ETA: 18:41 - loss: 0.6230 - regression_loss: 0.5042 - classification_loss: 0.1188
 7547/10000 [=====================>........] - ETA: 18:40 - loss: 0.6230 - regression_loss: 0.5042 - classification_loss: 0.1188
 7548/10000 [=====================>........] - ETA: 18:40 - loss: 0.6230 - regression_loss: 0.5042 - classification_loss: 0.1188
 7549/10000 [=====================>........] - ETA: 18:39 - loss: 0.6230 - regression_loss: 0.5042 - classification_loss: 0.1188
 7550/10000 [=====================>........] - ETA: 18:39 - loss: 0.6229 - regression_loss: 0.5041 - classification_loss: 0.1188
 7551/10000 [=====================>........] - ETA: 18:38 - loss: 0.6229 - regression_loss: 0.5041 - classification_loss: 0.1188
 7552/10000 [=====================>........] - ETA: 18:38 - loss: 0.6229 - regression_loss: 0.5041 - classification_loss: 0.1188
 7553/10000 [=====================>........] - ETA: 18:38 - loss: 0.6229 - regression_loss: 0.5041 - classification_loss: 0.1188
 7554/10000 [=====================>........] - ETA: 18:37 - loss: 0.6229 - regression_loss: 0.5041 - classification_loss: 0.1188
 7555/10000 [=====================>........] - ETA: 18:37 - loss: 0.6228 - regression_loss: 0.5040 - classification_loss: 0.1187
 7556/10000 [=====================>........] - ETA: 18:36 - loss: 0.6228 - regression_loss: 0.5040 - classification_loss: 0.1187
 7557/10000 [=====================>........] - ETA: 18:36 - loss: 0.6228 - regression_loss: 0.5040 - classification_loss: 0.1187
 7558/10000 [=====================>........] - ETA: 18:35 - loss: 0.6227 - regression_loss: 0.5040 - classification_loss: 0.1187
 7559/10000 [=====================>........] - ETA: 18:35 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7560/10000 [=====================>........] - ETA: 18:34 - loss: 0.6227 - regression_loss: 0.5040 - classification_loss: 0.1187
 7561/10000 [=====================>........] - ETA: 18:34 - loss: 0.6227 - regression_loss: 0.5040 - classification_loss: 0.1187
 7562/10000 [=====================>........] - ETA: 18:33 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7563/10000 [=====================>........] - ETA: 18:33 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7564/10000 [=====================>........] - ETA: 18:33 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7565/10000 [=====================>........] - ETA: 18:32 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7566/10000 [=====================>........] - ETA: 18:32 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7567/10000 [=====================>........] - ETA: 18:31 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7568/10000 [=====================>........] - ETA: 18:31 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7569/10000 [=====================>........] - ETA: 18:30 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7570/10000 [=====================>........] - ETA: 18:30 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7571/10000 [=====================>........] - ETA: 18:29 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7572/10000 [=====================>........] - ETA: 18:29 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7573/10000 [=====================>........] - ETA: 18:28 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1187
 7574/10000 [=====================>........] - ETA: 18:28 - loss: 0.6224 - regression_loss: 0.5037 - classification_loss: 0.1187
 7575/10000 [=====================>........] - ETA: 18:28 - loss: 0.6224 - regression_loss: 0.5037 - classification_loss: 0.1187
 7576/10000 [=====================>........] - ETA: 18:27 - loss: 0.6224 - regression_loss: 0.5037 - classification_loss: 0.1187
 7577/10000 [=====================>........] - ETA: 18:27 - loss: 0.6224 - regression_loss: 0.5037 - classification_loss: 0.1187
 7578/10000 [=====================>........] - ETA: 18:26 - loss: 0.6223 - regression_loss: 0.5036 - classification_loss: 0.1187
 7579/10000 [=====================>........] - ETA: 18:26 - loss: 0.6223 - regression_loss: 0.5036 - classification_loss: 0.1187
 7580/10000 [=====================>........] - ETA: 18:25 - loss: 0.6222 - regression_loss: 0.5036 - classification_loss: 0.1187
 7581/10000 [=====================>........] - ETA: 18:25 - loss: 0.6222 - regression_loss: 0.5036 - classification_loss: 0.1187
 7582/10000 [=====================>........] - ETA: 18:24 - loss: 0.6222 - regression_loss: 0.5036 - classification_loss: 0.1187
 7583/10000 [=====================>........] - ETA: 18:24 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1187
 7584/10000 [=====================>........] - ETA: 18:23 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1187
 7585/10000 [=====================>........] - ETA: 18:23 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1187
 7586/10000 [=====================>........] - ETA: 18:23 - loss: 0.6223 - regression_loss: 0.5036 - classification_loss: 0.1187
 7587/10000 [=====================>........] - ETA: 18:22 - loss: 0.6223 - regression_loss: 0.5036 - classification_loss: 0.1187
 7588/10000 [=====================>........] - ETA: 18:22 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1187
 7589/10000 [=====================>........] - ETA: 18:21 - loss: 0.6224 - regression_loss: 0.5037 - classification_loss: 0.1187
 7590/10000 [=====================>........] - ETA: 18:21 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7591/10000 [=====================>........] - ETA: 18:20 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7592/10000 [=====================>........] - ETA: 18:20 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7593/10000 [=====================>........] - ETA: 18:19 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7594/10000 [=====================>........] - ETA: 18:19 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7595/10000 [=====================>........] - ETA: 18:18 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7596/10000 [=====================>........] - ETA: 18:18 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7597/10000 [=====================>........] - ETA: 18:17 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7598/10000 [=====================>........] - ETA: 18:17 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7599/10000 [=====================>........] - ETA: 18:17 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7600/10000 [=====================>........] - ETA: 18:16 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7601/10000 [=====================>........] - ETA: 18:16 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7602/10000 [=====================>........] - ETA: 18:15 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7603/10000 [=====================>........] - ETA: 18:15 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7604/10000 [=====================>........] - ETA: 18:14 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7605/10000 [=====================>........] - ETA: 18:14 - loss: 0.6225 - regression_loss: 0.5039 - classification_loss: 0.1187
 7606/10000 [=====================>........] - ETA: 18:13 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7607/10000 [=====================>........] - ETA: 18:13 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7608/10000 [=====================>........] - ETA: 18:12 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1187
 7609/10000 [=====================>........] - ETA: 18:12 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7610/10000 [=====================>........] - ETA: 18:12 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7611/10000 [=====================>........] - ETA: 18:11 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1187
 7612/10000 [=====================>........] - ETA: 18:11 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7613/10000 [=====================>........] - ETA: 18:10 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7614/10000 [=====================>........] - ETA: 18:10 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7615/10000 [=====================>........] - ETA: 18:09 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1186
 7616/10000 [=====================>........] - ETA: 18:09 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7617/10000 [=====================>........] - ETA: 18:08 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7618/10000 [=====================>........] - ETA: 18:08 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7619/10000 [=====================>........] - ETA: 18:07 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7620/10000 [=====================>........] - ETA: 18:07 - loss: 0.6225 - regression_loss: 0.5039 - classification_loss: 0.1186
 7621/10000 [=====================>........] - ETA: 18:07 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1186
 7622/10000 [=====================>........] - ETA: 18:06 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1187
 7623/10000 [=====================>........] - ETA: 18:06 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7624/10000 [=====================>........] - ETA: 18:05 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1187
 7625/10000 [=====================>........] - ETA: 18:05 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1187
 7626/10000 [=====================>........] - ETA: 18:04 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1187
 7627/10000 [=====================>........] - ETA: 18:04 - loss: 0.6225 - regression_loss: 0.5039 - classification_loss: 0.1187
 7628/10000 [=====================>........] - ETA: 18:03 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7629/10000 [=====================>........] - ETA: 18:03 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7630/10000 [=====================>........] - ETA: 18:02 - loss: 0.6225 - regression_loss: 0.5039 - classification_loss: 0.1187
 7631/10000 [=====================>........] - ETA: 18:02 - loss: 0.6225 - regression_loss: 0.5039 - classification_loss: 0.1186
 7632/10000 [=====================>........] - ETA: 18:02 - loss: 0.6225 - regression_loss: 0.5039 - classification_loss: 0.1186
 7633/10000 [=====================>........] - ETA: 18:01 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1186
 7634/10000 [=====================>........] - ETA: 18:01 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7635/10000 [=====================>........] - ETA: 18:00 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1186
 7636/10000 [=====================>........] - ETA: 18:00 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7637/10000 [=====================>........] - ETA: 17:59 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7638/10000 [=====================>........] - ETA: 17:59 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7639/10000 [=====================>........] - ETA: 17:58 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7640/10000 [=====================>........] - ETA: 17:58 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7641/10000 [=====================>........] - ETA: 17:57 - loss: 0.6222 - regression_loss: 0.5036 - classification_loss: 0.1186
 7642/10000 [=====================>........] - ETA: 17:57 - loss: 0.6222 - regression_loss: 0.5037 - classification_loss: 0.1186
 7643/10000 [=====================>........] - ETA: 17:56 - loss: 0.6222 - regression_loss: 0.5037 - classification_loss: 0.1186
 7644/10000 [=====================>........] - ETA: 17:56 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7645/10000 [=====================>........] - ETA: 17:56 - loss: 0.6222 - regression_loss: 0.5036 - classification_loss: 0.1186
 7646/10000 [=====================>........] - ETA: 17:55 - loss: 0.6222 - regression_loss: 0.5036 - classification_loss: 0.1186
 7647/10000 [=====================>........] - ETA: 17:55 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7648/10000 [=====================>........] - ETA: 17:54 - loss: 0.6222 - regression_loss: 0.5037 - classification_loss: 0.1186
 7649/10000 [=====================>........] - ETA: 17:54 - loss: 0.6222 - regression_loss: 0.5036 - classification_loss: 0.1186
 7650/10000 [=====================>........] - ETA: 17:53 - loss: 0.6221 - regression_loss: 0.5036 - classification_loss: 0.1185
 7651/10000 [=====================>........] - ETA: 17:53 - loss: 0.6221 - regression_loss: 0.5035 - classification_loss: 0.1185
 7652/10000 [=====================>........] - ETA: 17:52 - loss: 0.6220 - regression_loss: 0.5035 - classification_loss: 0.1185
 7653/10000 [=====================>........] - ETA: 17:52 - loss: 0.6220 - regression_loss: 0.5035 - classification_loss: 0.1185
 7654/10000 [=====================>........] - ETA: 17:51 - loss: 0.6220 - regression_loss: 0.5035 - classification_loss: 0.1185
 7655/10000 [=====================>........] - ETA: 17:51 - loss: 0.6220 - regression_loss: 0.5035 - classification_loss: 0.1185
 7656/10000 [=====================>........] - ETA: 17:51 - loss: 0.6220 - regression_loss: 0.5035 - classification_loss: 0.1185
 7657/10000 [=====================>........] - ETA: 17:50 - loss: 0.6221 - regression_loss: 0.5036 - classification_loss: 0.1185
 7658/10000 [=====================>........] - ETA: 17:50 - loss: 0.6221 - regression_loss: 0.5036 - classification_loss: 0.1185
 7659/10000 [=====================>........] - ETA: 17:49 - loss: 0.6221 - regression_loss: 0.5036 - classification_loss: 0.1185
 7660/10000 [=====================>........] - ETA: 17:49 - loss: 0.6221 - regression_loss: 0.5035 - classification_loss: 0.1185
 7661/10000 [=====================>........] - ETA: 17:48 - loss: 0.6221 - regression_loss: 0.5035 - classification_loss: 0.1185
 7662/10000 [=====================>........] - ETA: 17:48 - loss: 0.6221 - regression_loss: 0.5035 - classification_loss: 0.1185
 7663/10000 [=====================>........] - ETA: 17:47 - loss: 0.6222 - regression_loss: 0.5037 - classification_loss: 0.1186
 7664/10000 [=====================>........] - ETA: 17:47 - loss: 0.6222 - regression_loss: 0.5037 - classification_loss: 0.1186
 7665/10000 [=====================>........] - ETA: 17:46 - loss: 0.6225 - regression_loss: 0.5039 - classification_loss: 0.1186
 7666/10000 [=====================>........] - ETA: 17:46 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1186
 7667/10000 [======================>.......] - ETA: 17:46 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1186
 7668/10000 [======================>.......] - ETA: 17:45 - loss: 0.6225 - regression_loss: 0.5039 - classification_loss: 0.1186
 7669/10000 [======================>.......] - ETA: 17:45 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1186
 7670/10000 [======================>.......] - ETA: 17:44 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7671/10000 [======================>.......] - ETA: 17:44 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1187
 7672/10000 [======================>.......] - ETA: 17:43 - loss: 0.6224 - regression_loss: 0.5037 - classification_loss: 0.1186
 7673/10000 [======================>.......] - ETA: 17:43 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7674/10000 [======================>.......] - ETA: 17:42 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7675/10000 [======================>.......] - ETA: 17:42 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7676/10000 [======================>.......] - ETA: 17:41 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1186
 7677/10000 [======================>.......] - ETA: 17:41 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1186
 7678/10000 [======================>.......] - ETA: 17:40 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7679/10000 [======================>.......] - ETA: 17:40 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7680/10000 [======================>.......] - ETA: 17:40 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7681/10000 [======================>.......] - ETA: 17:39 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7682/10000 [======================>.......] - ETA: 17:39 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7683/10000 [======================>.......] - ETA: 17:38 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7684/10000 [======================>.......] - ETA: 17:38 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7685/10000 [======================>.......] - ETA: 17:37 - loss: 0.6225 - regression_loss: 0.5039 - classification_loss: 0.1187
 7686/10000 [======================>.......] - ETA: 17:37 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7687/10000 [======================>.......] - ETA: 17:36 - loss: 0.6226 - regression_loss: 0.5039 - classification_loss: 0.1187
 7688/10000 [======================>.......] - ETA: 17:36 - loss: 0.6225 - regression_loss: 0.5039 - classification_loss: 0.1187
 7689/10000 [======================>.......] - ETA: 17:35 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7690/10000 [======================>.......] - ETA: 17:35 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7691/10000 [======================>.......] - ETA: 17:35 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7692/10000 [======================>.......] - ETA: 17:34 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1186
 7693/10000 [======================>.......] - ETA: 17:34 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7694/10000 [======================>.......] - ETA: 17:33 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7695/10000 [======================>.......] - ETA: 17:33 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1187
 7696/10000 [======================>.......] - ETA: 17:32 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7697/10000 [======================>.......] - ETA: 17:32 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7698/10000 [======================>.......] - ETA: 17:31 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7699/10000 [======================>.......] - ETA: 17:31 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1186
 7700/10000 [======================>.......] - ETA: 17:30 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1186
 7701/10000 [======================>.......] - ETA: 17:30 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1186
 7702/10000 [======================>.......] - ETA: 17:30 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7703/10000 [======================>.......] - ETA: 17:29 - loss: 0.6225 - regression_loss: 0.5039 - classification_loss: 0.1187
 7704/10000 [======================>.......] - ETA: 17:29 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7705/10000 [======================>.......] - ETA: 17:28 - loss: 0.6225 - regression_loss: 0.5039 - classification_loss: 0.1187
 7706/10000 [======================>.......] - ETA: 17:28 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1187
 7707/10000 [======================>.......] - ETA: 17:27 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7708/10000 [======================>.......] - ETA: 17:27 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7709/10000 [======================>.......] - ETA: 17:26 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1186
 7710/10000 [======================>.......] - ETA: 17:26 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7711/10000 [======================>.......] - ETA: 17:25 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1186
 7712/10000 [======================>.......] - ETA: 17:25 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7713/10000 [======================>.......] - ETA: 17:24 - loss: 0.6225 - regression_loss: 0.5038 - classification_loss: 0.1186
 7714/10000 [======================>.......] - ETA: 17:24 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7715/10000 [======================>.......] - ETA: 17:24 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7716/10000 [======================>.......] - ETA: 17:23 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7717/10000 [======================>.......] - ETA: 17:23 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7718/10000 [======================>.......] - ETA: 17:22 - loss: 0.6222 - regression_loss: 0.5036 - classification_loss: 0.1186
 7719/10000 [======================>.......] - ETA: 17:22 - loss: 0.6222 - regression_loss: 0.5036 - classification_loss: 0.1186
 7720/10000 [======================>.......] - ETA: 17:21 - loss: 0.6222 - regression_loss: 0.5036 - classification_loss: 0.1186
 7721/10000 [======================>.......] - ETA: 17:21 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7722/10000 [======================>.......] - ETA: 17:20 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7723/10000 [======================>.......] - ETA: 17:20 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7724/10000 [======================>.......] - ETA: 17:19 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7725/10000 [======================>.......] - ETA: 17:19 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7726/10000 [======================>.......] - ETA: 17:19 - loss: 0.6222 - regression_loss: 0.5037 - classification_loss: 0.1186
 7727/10000 [======================>.......] - ETA: 17:18 - loss: 0.6222 - regression_loss: 0.5036 - classification_loss: 0.1186
 7728/10000 [======================>.......] - ETA: 17:18 - loss: 0.6222 - regression_loss: 0.5036 - classification_loss: 0.1186
 7729/10000 [======================>.......] - ETA: 17:17 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7730/10000 [======================>.......] - ETA: 17:17 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7731/10000 [======================>.......] - ETA: 17:16 - loss: 0.6222 - regression_loss: 0.5036 - classification_loss: 0.1186
 7732/10000 [======================>.......] - ETA: 17:16 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7733/10000 [======================>.......] - ETA: 17:15 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7734/10000 [======================>.......] - ETA: 17:15 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7735/10000 [======================>.......] - ETA: 17:14 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7736/10000 [======================>.......] - ETA: 17:14 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7737/10000 [======================>.......] - ETA: 17:14 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7738/10000 [======================>.......] - ETA: 17:13 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7739/10000 [======================>.......] - ETA: 17:13 - loss: 0.6222 - regression_loss: 0.5036 - classification_loss: 0.1186
 7740/10000 [======================>.......] - ETA: 17:12 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7741/10000 [======================>.......] - ETA: 17:12 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7742/10000 [======================>.......] - ETA: 17:11 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7743/10000 [======================>.......] - ETA: 17:11 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7744/10000 [======================>.......] - ETA: 17:10 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7745/10000 [======================>.......] - ETA: 17:10 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7746/10000 [======================>.......] - ETA: 17:09 - loss: 0.6225 - regression_loss: 0.5039 - classification_loss: 0.1186
 7747/10000 [======================>.......] - ETA: 17:09 - loss: 0.6225 - regression_loss: 0.5039 - classification_loss: 0.1186
 7748/10000 [======================>.......] - ETA: 17:08 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7749/10000 [======================>.......] - ETA: 17:08 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7750/10000 [======================>.......] - ETA: 17:08 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7751/10000 [======================>.......] - ETA: 17:07 - loss: 0.6224 - regression_loss: 0.5037 - classification_loss: 0.1186
 7752/10000 [======================>.......] - ETA: 17:07 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7753/10000 [======================>.......] - ETA: 17:06 - loss: 0.6224 - regression_loss: 0.5037 - classification_loss: 0.1186
 7754/10000 [======================>.......] - ETA: 17:06 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7755/10000 [======================>.......] - ETA: 17:05 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7756/10000 [======================>.......] - ETA: 17:05 - loss: 0.6223 - regression_loss: 0.5038 - classification_loss: 0.1186
 7757/10000 [======================>.......] - ETA: 17:04 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7758/10000 [======================>.......] - ETA: 17:04 - loss: 0.6223 - regression_loss: 0.5038 - classification_loss: 0.1186
 7759/10000 [======================>.......] - ETA: 17:03 - loss: 0.6223 - regression_loss: 0.5038 - classification_loss: 0.1186
 7760/10000 [======================>.......] - ETA: 17:03 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7761/10000 [======================>.......] - ETA: 17:03 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7762/10000 [======================>.......] - ETA: 17:02 - loss: 0.6222 - regression_loss: 0.5037 - classification_loss: 0.1186
 7763/10000 [======================>.......] - ETA: 17:02 - loss: 0.6222 - regression_loss: 0.5037 - classification_loss: 0.1186
 7764/10000 [======================>.......] - ETA: 17:01 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7765/10000 [======================>.......] - ETA: 17:01 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7766/10000 [======================>.......] - ETA: 17:00 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7767/10000 [======================>.......] - ETA: 17:00 - loss: 0.6223 - regression_loss: 0.5038 - classification_loss: 0.1186
 7768/10000 [======================>.......] - ETA: 16:59 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7769/10000 [======================>.......] - ETA: 16:59 - loss: 0.6223 - regression_loss: 0.5038 - classification_loss: 0.1186
 7770/10000 [======================>.......] - ETA: 16:58 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7771/10000 [======================>.......] - ETA: 16:58 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7772/10000 [======================>.......] - ETA: 16:57 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7773/10000 [======================>.......] - ETA: 16:57 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7774/10000 [======================>.......] - ETA: 16:57 - loss: 0.6222 - regression_loss: 0.5037 - classification_loss: 0.1186
 7775/10000 [======================>.......] - ETA: 16:56 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7776/10000 [======================>.......] - ETA: 16:56 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7777/10000 [======================>.......] - ETA: 16:55 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7778/10000 [======================>.......] - ETA: 16:55 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7779/10000 [======================>.......] - ETA: 16:54 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7780/10000 [======================>.......] - ETA: 16:54 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7781/10000 [======================>.......] - ETA: 16:53 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7782/10000 [======================>.......] - ETA: 16:53 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7783/10000 [======================>.......] - ETA: 16:52 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7784/10000 [======================>.......] - ETA: 16:52 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7785/10000 [======================>.......] - ETA: 16:52 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7786/10000 [======================>.......] - ETA: 16:51 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7787/10000 [======================>.......] - ETA: 16:51 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7788/10000 [======================>.......] - ETA: 16:50 - loss: 0.6224 - regression_loss: 0.5038 - classification_loss: 0.1186
 7789/10000 [======================>.......] - ETA: 16:50 - loss: 0.6223 - regression_loss: 0.5037 - classification_loss: 0.1186
 7790/10000 [======================>.......] - ETA: 16:49 - loss: 0.6222 - regression_loss: 0.5037 - classification_loss: 0.1186
 7791/10000 [======================>.......] - ETA: 16:49 - loss: 0.6222 - regression_loss: 0.5036 - classification_loss: 0.1185
 7792/10000 [======================>.......] - ETA: 16:48 - loss: 0.6221 - regression_loss: 0.5036 - classification_loss: 0.1185
 7793/10000 [======================>.......] - ETA: 16:48 - loss: 0.6221 - regression_loss: 0.5036 - classification_loss: 0.1185
 7794/10000 [======================>.......] - ETA: 16:47 - loss: 0.6221 - regression_loss: 0.5036 - classification_loss: 0.1185
 7795/10000 [======================>.......] - ETA: 16:47 - loss: 0.6221 - regression_loss: 0.5035 - classification_loss: 0.1185
 7796/10000 [======================>.......] - ETA: 16:46 - loss: 0.6220 - regression_loss: 0.5035 - classification_loss: 0.1185
 7797/10000 [======================>.......] - ETA: 16:46 - loss: 0.6219 - regression_loss: 0.5034 - classification_loss: 0.1185
 7798/10000 [======================>.......] - ETA: 16:45 - loss: 0.6219 - regression_loss: 0.5034 - classification_loss: 0.1185
 7799/10000 [======================>.......] - ETA: 16:45 - loss: 0.6219 - regression_loss: 0.5034 - classification_loss: 0.1185
 7800/10000 [======================>.......] - ETA: 16:45 - loss: 0.6220 - regression_loss: 0.5035 - classification_loss: 0.1185
 7801/10000 [======================>.......] - ETA: 16:44 - loss: 0.6220 - regression_loss: 0.5035 - classification_loss: 0.1185
 7802/10000 [======================>.......] - ETA: 16:44 - loss: 0.6219 - regression_loss: 0.5034 - classification_loss: 0.1185
 7803/10000 [======================>.......] - ETA: 16:43 - loss: 0.6219 - regression_loss: 0.5034 - classification_loss: 0.1185
 7804/10000 [======================>.......] - ETA: 16:43 - loss: 0.6219 - regression_loss: 0.5034 - classification_loss: 0.1185
 7805/10000 [======================>.......] - ETA: 16:42 - loss: 0.6218 - regression_loss: 0.5034 - classification_loss: 0.1185
 7806/10000 [======================>.......] - ETA: 16:42 - loss: 0.6220 - regression_loss: 0.5035 - classification_loss: 0.1185
 7807/10000 [======================>.......] - ETA: 16:41 - loss: 0.6219 - regression_loss: 0.5034 - classification_loss: 0.1185
 7808/10000 [======================>.......] - ETA: 16:41 - loss: 0.6220 - regression_loss: 0.5035 - classification_loss: 0.1185
 7809/10000 [======================>.......] - ETA: 16:40 - loss: 0.6220 - regression_loss: 0.5035 - classification_loss: 0.1185
 7810/10000 [======================>.......] - ETA: 16:40 - loss: 0.6219 - regression_loss: 0.5034 - classification_loss: 0.1185
 7811/10000 [======================>.......] - ETA: 16:40 - loss: 0.6219 - regression_loss: 0.5034 - classification_loss: 0.1185
 7812/10000 [======================>.......] - ETA: 16:39 - loss: 0.6219 - regression_loss: 0.5034 - classification_loss: 0.1185
 7813/10000 [======================>.......] - ETA: 16:39 - loss: 0.6219 - regression_loss: 0.5034 - classification_loss: 0.1185
 7814/10000 [======================>.......] - ETA: 16:38 - loss: 0.6218 - regression_loss: 0.5034 - classification_loss: 0.1185
 7815/10000 [======================>.......] - ETA: 16:38 - loss: 0.6218 - regression_loss: 0.5034 - classification_loss: 0.1185
 7816/10000 [======================>.......] - ETA: 16:37 - loss: 0.6218 - regression_loss: 0.5034 - classification_loss: 0.1185
 7817/10000 [======================>.......] - ETA: 16:37 - loss: 0.6218 - regression_loss: 0.5033 - classification_loss: 0.1185
 7818/10000 [======================>.......] - ETA: 16:36 - loss: 0.6217 - regression_loss: 0.5033 - classification_loss: 0.1185
 7819/10000 [======================>.......] - ETA: 16:36 - loss: 0.6217 - regression_loss: 0.5033 - classification_loss: 0.1185
 7820/10000 [======================>.......] - ETA: 16:35 - loss: 0.6217 - regression_loss: 0.5033 - classification_loss: 0.1185
 7821/10000 [======================>.......] - ETA: 16:35 - loss: 0.6216 - regression_loss: 0.5032 - classification_loss: 0.1184
 7822/10000 [======================>.......] - ETA: 16:35 - loss: 0.6216 - regression_loss: 0.5031 - classification_loss: 0.1184
 7823/10000 [======================>.......] - ETA: 16:34 - loss: 0.6216 - regression_loss: 0.5031 - classification_loss: 0.1184
 7824/10000 [======================>.......] - ETA: 16:34 - loss: 0.6216 - regression_loss: 0.5032 - classification_loss: 0.1184
 7825/10000 [======================>.......] - ETA: 16:33 - loss: 0.6217 - regression_loss: 0.5032 - classification_loss: 0.1184
 7826/10000 [======================>.......] - ETA: 16:33 - loss: 0.6217 - regression_loss: 0.5032 - classification_loss: 0.1184
 7827/10000 [======================>.......] - ETA: 16:32 - loss: 0.6216 - regression_loss: 0.5032 - classification_loss: 0.1184
 7828/10000 [======================>.......] - ETA: 16:32 - loss: 0.6216 - regression_loss: 0.5032 - classification_loss: 0.1184
 7829/10000 [======================>.......] - ETA: 16:31 - loss: 0.6216 - regression_loss: 0.5032 - classification_loss: 0.1184
 7830/10000 [======================>.......] - ETA: 16:31 - loss: 0.6216 - regression_loss: 0.5032 - classification_loss: 0.1184
 7831/10000 [======================>.......] - ETA: 16:30 - loss: 0.6216 - regression_loss: 0.5032 - classification_loss: 0.1184
 7832/10000 [======================>.......] - ETA: 16:30 - loss: 0.6216 - regression_loss: 0.5032 - classification_loss: 0.1184
 7833/10000 [======================>.......] - ETA: 16:30 - loss: 0.6216 - regression_loss: 0.5032 - classification_loss: 0.1184
 7834/10000 [======================>.......] - ETA: 16:29 - loss: 0.6216 - regression_loss: 0.5032 - classification_loss: 0.1184
 7835/10000 [======================>.......] - ETA: 16:29 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7836/10000 [======================>.......] - ETA: 16:28 - loss: 0.6216 - regression_loss: 0.5032 - classification_loss: 0.1184
 7837/10000 [======================>.......] - ETA: 16:28 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7838/10000 [======================>.......] - ETA: 16:27 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7839/10000 [======================>.......] - ETA: 16:27 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7840/10000 [======================>.......] - ETA: 16:26 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7841/10000 [======================>.......] - ETA: 16:26 - loss: 0.6216 - regression_loss: 0.5032 - classification_loss: 0.1185
 7842/10000 [======================>.......] - ETA: 16:25 - loss: 0.6218 - regression_loss: 0.5033 - classification_loss: 0.1185
 7843/10000 [======================>.......] - ETA: 16:25 - loss: 0.6217 - regression_loss: 0.5032 - classification_loss: 0.1185
 7844/10000 [======================>.......] - ETA: 16:25 - loss: 0.6217 - regression_loss: 0.5032 - classification_loss: 0.1185
 7845/10000 [======================>.......] - ETA: 16:24 - loss: 0.6217 - regression_loss: 0.5032 - classification_loss: 0.1185
 7846/10000 [======================>.......] - ETA: 16:24 - loss: 0.6217 - regression_loss: 0.5032 - classification_loss: 0.1185
 7847/10000 [======================>.......] - ETA: 16:23 - loss: 0.6217 - regression_loss: 0.5032 - classification_loss: 0.1185
 7848/10000 [======================>.......] - ETA: 16:23 - loss: 0.6217 - regression_loss: 0.5033 - classification_loss: 0.1185
 7849/10000 [======================>.......] - ETA: 16:22 - loss: 0.6218 - regression_loss: 0.5033 - classification_loss: 0.1185
 7850/10000 [======================>.......] - ETA: 16:22 - loss: 0.6217 - regression_loss: 0.5032 - classification_loss: 0.1185
 7851/10000 [======================>.......] - ETA: 16:21 - loss: 0.6217 - regression_loss: 0.5033 - classification_loss: 0.1185
 7852/10000 [======================>.......] - ETA: 16:21 - loss: 0.6217 - regression_loss: 0.5032 - classification_loss: 0.1185
 7853/10000 [======================>.......] - ETA: 16:20 - loss: 0.6217 - regression_loss: 0.5032 - classification_loss: 0.1185
 7854/10000 [======================>.......] - ETA: 16:20 - loss: 0.6217 - regression_loss: 0.5032 - classification_loss: 0.1185
 7855/10000 [======================>.......] - ETA: 16:20 - loss: 0.6216 - regression_loss: 0.5031 - classification_loss: 0.1185
 7856/10000 [======================>.......] - ETA: 16:19 - loss: 0.6216 - regression_loss: 0.5031 - classification_loss: 0.1185
 7857/10000 [======================>.......] - ETA: 16:19 - loss: 0.6217 - regression_loss: 0.5032 - classification_loss: 0.1185
 7858/10000 [======================>.......] - ETA: 16:18 - loss: 0.6217 - regression_loss: 0.5032 - classification_loss: 0.1185
 7859/10000 [======================>.......] - ETA: 16:18 - loss: 0.6217 - regression_loss: 0.5032 - classification_loss: 0.1185
 7860/10000 [======================>.......] - ETA: 16:17 - loss: 0.6217 - regression_loss: 0.5032 - classification_loss: 0.1185
 7861/10000 [======================>.......] - ETA: 16:17 - loss: 0.6218 - regression_loss: 0.5033 - classification_loss: 0.1185
 7862/10000 [======================>.......] - ETA: 16:16 - loss: 0.6218 - regression_loss: 0.5033 - classification_loss: 0.1185
 7863/10000 [======================>.......] - ETA: 16:16 - loss: 0.6218 - regression_loss: 0.5033 - classification_loss: 0.1185
 7864/10000 [======================>.......] - ETA: 16:15 - loss: 0.6219 - regression_loss: 0.5034 - classification_loss: 0.1185
 7865/10000 [======================>.......] - ETA: 16:15 - loss: 0.6218 - regression_loss: 0.5033 - classification_loss: 0.1185
 7866/10000 [======================>.......] - ETA: 16:15 - loss: 0.6219 - regression_loss: 0.5034 - classification_loss: 0.1185
 7867/10000 [======================>.......] - ETA: 16:14 - loss: 0.6219 - regression_loss: 0.5034 - classification_loss: 0.1185
 7868/10000 [======================>.......] - ETA: 16:14 - loss: 0.6219 - regression_loss: 0.5034 - classification_loss: 0.1185
 7869/10000 [======================>.......] - ETA: 16:13 - loss: 0.6219 - regression_loss: 0.5034 - classification_loss: 0.1185
 7870/10000 [======================>.......] - ETA: 16:13 - loss: 0.6219 - regression_loss: 0.5034 - classification_loss: 0.1185
 7871/10000 [======================>.......] - ETA: 16:12 - loss: 0.6221 - regression_loss: 0.5035 - classification_loss: 0.1185
 7872/10000 [======================>.......] - ETA: 16:12 - loss: 0.6220 - regression_loss: 0.5035 - classification_loss: 0.1185
 7873/10000 [======================>.......] - ETA: 16:11 - loss: 0.6219 - regression_loss: 0.5034 - classification_loss: 0.1185
 7874/10000 [======================>.......] - ETA: 16:11 - loss: 0.6219 - regression_loss: 0.5034 - classification_loss: 0.1185
 7875/10000 [======================>.......] - ETA: 16:10 - loss: 0.6219 - regression_loss: 0.5034 - classification_loss: 0.1185
 7876/10000 [======================>.......] - ETA: 16:10 - loss: 0.6219 - regression_loss: 0.5034 - classification_loss: 0.1185
 7877/10000 [======================>.......] - ETA: 16:10 - loss: 0.6218 - regression_loss: 0.5034 - classification_loss: 0.1185
 7878/10000 [======================>.......] - ETA: 16:09 - loss: 0.6218 - regression_loss: 0.5033 - classification_loss: 0.1185
 7879/10000 [======================>.......] - ETA: 16:09 - loss: 0.6218 - regression_loss: 0.5033 - classification_loss: 0.1185
 7880/10000 [======================>.......] - ETA: 16:08 - loss: 0.6218 - regression_loss: 0.5033 - classification_loss: 0.1185
 7881/10000 [======================>.......] - ETA: 16:08 - loss: 0.6218 - regression_loss: 0.5033 - classification_loss: 0.1185
 7882/10000 [======================>.......] - ETA: 16:07 - loss: 0.6218 - regression_loss: 0.5033 - classification_loss: 0.1185
 7883/10000 [======================>.......] - ETA: 16:07 - loss: 0.6217 - regression_loss: 0.5033 - classification_loss: 0.1185
 7884/10000 [======================>.......] - ETA: 16:06 - loss: 0.6217 - regression_loss: 0.5032 - classification_loss: 0.1185
 7885/10000 [======================>.......] - ETA: 16:06 - loss: 0.6218 - regression_loss: 0.5033 - classification_loss: 0.1185
 7886/10000 [======================>.......] - ETA: 16:05 - loss: 0.6217 - regression_loss: 0.5032 - classification_loss: 0.1185
 7887/10000 [======================>.......] - ETA: 16:05 - loss: 0.6217 - regression_loss: 0.5032 - classification_loss: 0.1185
 7888/10000 [======================>.......] - ETA: 16:05 - loss: 0.6216 - regression_loss: 0.5032 - classification_loss: 0.1185
 7889/10000 [======================>.......] - ETA: 16:04 - loss: 0.6216 - regression_loss: 0.5031 - classification_loss: 0.1184
 7890/10000 [======================>.......] - ETA: 16:04 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7891/10000 [======================>.......] - ETA: 16:03 - loss: 0.6216 - regression_loss: 0.5031 - classification_loss: 0.1185
 7892/10000 [======================>.......] - ETA: 16:03 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7893/10000 [======================>.......] - ETA: 16:02 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7894/10000 [======================>.......] - ETA: 16:02 - loss: 0.6215 - regression_loss: 0.5030 - classification_loss: 0.1184
 7895/10000 [======================>.......] - ETA: 16:01 - loss: 0.6215 - regression_loss: 0.5030 - classification_loss: 0.1184
 7896/10000 [======================>.......] - ETA: 16:01 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7897/10000 [======================>.......] - ETA: 16:00 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7898/10000 [======================>.......] - ETA: 16:00 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7899/10000 [======================>.......] - ETA: 16:00 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7900/10000 [======================>.......] - ETA: 15:59 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7901/10000 [======================>.......] - ETA: 15:59 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7902/10000 [======================>.......] - ETA: 15:58 - loss: 0.6213 - regression_loss: 0.5030 - classification_loss: 0.1184
 7903/10000 [======================>.......] - ETA: 15:58 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7904/10000 [======================>.......] - ETA: 15:57 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7905/10000 [======================>.......] - ETA: 15:57 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7906/10000 [======================>.......] - ETA: 15:56 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7907/10000 [======================>.......] - ETA: 15:56 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7908/10000 [======================>.......] - ETA: 15:55 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7909/10000 [======================>.......] - ETA: 15:55 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7910/10000 [======================>.......] - ETA: 15:54 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7911/10000 [======================>.......] - ETA: 15:54 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7912/10000 [======================>.......] - ETA: 15:54 - loss: 0.6215 - regression_loss: 0.5030 - classification_loss: 0.1185
 7913/10000 [======================>.......] - ETA: 15:53 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7914/10000 [======================>.......] - ETA: 15:53 - loss: 0.6214 - regression_loss: 0.5029 - classification_loss: 0.1184
 7915/10000 [======================>.......] - ETA: 15:52 - loss: 0.6215 - regression_loss: 0.5030 - classification_loss: 0.1184
 7916/10000 [======================>.......] - ETA: 15:52 - loss: 0.6215 - regression_loss: 0.5030 - classification_loss: 0.1184
 7917/10000 [======================>.......] - ETA: 15:51 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7918/10000 [======================>.......] - ETA: 15:51 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7919/10000 [======================>.......] - ETA: 15:50 - loss: 0.6216 - regression_loss: 0.5031 - classification_loss: 0.1184
 7920/10000 [======================>.......] - ETA: 15:50 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7921/10000 [======================>.......] - ETA: 15:49 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7922/10000 [======================>.......] - ETA: 15:49 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7923/10000 [======================>.......] - ETA: 15:49 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7924/10000 [======================>.......] - ETA: 15:48 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7925/10000 [======================>.......] - ETA: 15:48 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7926/10000 [======================>.......] - ETA: 15:47 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7927/10000 [======================>.......] - ETA: 15:47 - loss: 0.6213 - regression_loss: 0.5030 - classification_loss: 0.1184
 7928/10000 [======================>.......] - ETA: 15:46 - loss: 0.6213 - regression_loss: 0.5030 - classification_loss: 0.1184
 7929/10000 [======================>.......] - ETA: 15:46 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7930/10000 [======================>.......] - ETA: 15:45 - loss: 0.6213 - regression_loss: 0.5030 - classification_loss: 0.1184
 7931/10000 [======================>.......] - ETA: 15:45 - loss: 0.6213 - regression_loss: 0.5029 - classification_loss: 0.1184
 7932/10000 [======================>.......] - ETA: 15:44 - loss: 0.6213 - regression_loss: 0.5029 - classification_loss: 0.1184
 7933/10000 [======================>.......] - ETA: 15:44 - loss: 0.6213 - regression_loss: 0.5029 - classification_loss: 0.1184
 7934/10000 [======================>.......] - ETA: 15:44 - loss: 0.6213 - regression_loss: 0.5029 - classification_loss: 0.1184
 7935/10000 [======================>.......] - ETA: 15:43 - loss: 0.6212 - regression_loss: 0.5028 - classification_loss: 0.1184
 7936/10000 [======================>.......] - ETA: 15:43 - loss: 0.6213 - regression_loss: 0.5030 - classification_loss: 0.1184
 7937/10000 [======================>.......] - ETA: 15:42 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7938/10000 [======================>.......] - ETA: 15:42 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7939/10000 [======================>.......] - ETA: 15:41 - loss: 0.6213 - regression_loss: 0.5029 - classification_loss: 0.1184
 7940/10000 [======================>.......] - ETA: 15:41 - loss: 0.6213 - regression_loss: 0.5029 - classification_loss: 0.1184
 7941/10000 [======================>.......] - ETA: 15:40 - loss: 0.6213 - regression_loss: 0.5029 - classification_loss: 0.1184
 7942/10000 [======================>.......] - ETA: 15:40 - loss: 0.6213 - regression_loss: 0.5029 - classification_loss: 0.1184
 7943/10000 [======================>.......] - ETA: 15:39 - loss: 0.6215 - regression_loss: 0.5030 - classification_loss: 0.1184
 7944/10000 [======================>.......] - ETA: 15:39 - loss: 0.6215 - regression_loss: 0.5030 - classification_loss: 0.1184
 7945/10000 [======================>.......] - ETA: 15:38 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7946/10000 [======================>.......] - ETA: 15:38 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7947/10000 [======================>.......] - ETA: 15:38 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7948/10000 [======================>.......] - ETA: 15:37 - loss: 0.6216 - regression_loss: 0.5031 - classification_loss: 0.1184
 7949/10000 [======================>.......] - ETA: 15:37 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7950/10000 [======================>.......] - ETA: 15:36 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7951/10000 [======================>.......] - ETA: 15:36 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7952/10000 [======================>.......] - ETA: 15:35 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7953/10000 [======================>.......] - ETA: 15:35 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7954/10000 [======================>.......] - ETA: 15:34 - loss: 0.6213 - regression_loss: 0.5030 - classification_loss: 0.1184
 7955/10000 [======================>.......] - ETA: 15:34 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7956/10000 [======================>.......] - ETA: 15:33 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7957/10000 [======================>.......] - ETA: 15:33 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7958/10000 [======================>.......] - ETA: 15:33 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7959/10000 [======================>.......] - ETA: 15:32 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7960/10000 [======================>.......] - ETA: 15:32 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7961/10000 [======================>.......] - ETA: 15:31 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7962/10000 [======================>.......] - ETA: 15:31 - loss: 0.6216 - regression_loss: 0.5031 - classification_loss: 0.1184
 7963/10000 [======================>.......] - ETA: 15:30 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7964/10000 [======================>.......] - ETA: 15:30 - loss: 0.6215 - regression_loss: 0.5030 - classification_loss: 0.1184
 7965/10000 [======================>.......] - ETA: 15:29 - loss: 0.6216 - regression_loss: 0.5031 - classification_loss: 0.1184
 7966/10000 [======================>.......] - ETA: 15:29 - loss: 0.6216 - regression_loss: 0.5031 - classification_loss: 0.1185
 7967/10000 [======================>.......] - ETA: 15:28 - loss: 0.6216 - regression_loss: 0.5031 - classification_loss: 0.1185
 7968/10000 [======================>.......] - ETA: 15:28 - loss: 0.6216 - regression_loss: 0.5031 - classification_loss: 0.1185
 7969/10000 [======================>.......] - ETA: 15:28 - loss: 0.6217 - regression_loss: 0.5032 - classification_loss: 0.1185
 7970/10000 [======================>.......] - ETA: 15:27 - loss: 0.6217 - regression_loss: 0.5032 - classification_loss: 0.1185
 7971/10000 [======================>.......] - ETA: 15:27 - loss: 0.6217 - regression_loss: 0.5032 - classification_loss: 0.1185
 7972/10000 [======================>.......] - ETA: 15:26 - loss: 0.6217 - regression_loss: 0.5033 - classification_loss: 0.1185
 7973/10000 [======================>.......] - ETA: 15:26 - loss: 0.6218 - regression_loss: 0.5033 - classification_loss: 0.1185
 7974/10000 [======================>.......] - ETA: 15:25 - loss: 0.6217 - regression_loss: 0.5032 - classification_loss: 0.1185
 7975/10000 [======================>.......] - ETA: 15:25 - loss: 0.6216 - regression_loss: 0.5032 - classification_loss: 0.1185
 7976/10000 [======================>.......] - ETA: 15:24 - loss: 0.6217 - regression_loss: 0.5032 - classification_loss: 0.1185
 7977/10000 [======================>.......] - ETA: 15:24 - loss: 0.6217 - regression_loss: 0.5032 - classification_loss: 0.1185
 7978/10000 [======================>.......] - ETA: 15:23 - loss: 0.6216 - regression_loss: 0.5032 - classification_loss: 0.1185
 7979/10000 [======================>.......] - ETA: 15:23 - loss: 0.6216 - regression_loss: 0.5031 - classification_loss: 0.1185
 7980/10000 [======================>.......] - ETA: 15:23 - loss: 0.6216 - regression_loss: 0.5031 - classification_loss: 0.1185
 7981/10000 [======================>.......] - ETA: 15:22 - loss: 0.6216 - regression_loss: 0.5031 - classification_loss: 0.1185
 7982/10000 [======================>.......] - ETA: 15:22 - loss: 0.6216 - regression_loss: 0.5031 - classification_loss: 0.1185
 7983/10000 [======================>.......] - ETA: 15:21 - loss: 0.6216 - regression_loss: 0.5032 - classification_loss: 0.1185
 7984/10000 [======================>.......] - ETA: 15:21 - loss: 0.6216 - regression_loss: 0.5031 - classification_loss: 0.1185
 7985/10000 [======================>.......] - ETA: 15:20 - loss: 0.6216 - regression_loss: 0.5031 - classification_loss: 0.1185
 7986/10000 [======================>.......] - ETA: 15:20 - loss: 0.6216 - regression_loss: 0.5031 - classification_loss: 0.1184
 7987/10000 [======================>.......] - ETA: 15:19 - loss: 0.6216 - regression_loss: 0.5032 - classification_loss: 0.1184
 7988/10000 [======================>.......] - ETA: 15:19 - loss: 0.6216 - regression_loss: 0.5032 - classification_loss: 0.1184
 7989/10000 [======================>.......] - ETA: 15:18 - loss: 0.6216 - regression_loss: 0.5031 - classification_loss: 0.1184
 7990/10000 [======================>.......] - ETA: 15:18 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7991/10000 [======================>.......] - ETA: 15:18 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7992/10000 [======================>.......] - ETA: 15:17 - loss: 0.6216 - regression_loss: 0.5031 - classification_loss: 0.1184
 7993/10000 [======================>.......] - ETA: 15:17 - loss: 0.6216 - regression_loss: 0.5031 - classification_loss: 0.1184
 7994/10000 [======================>.......] - ETA: 15:16 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7995/10000 [======================>.......] - ETA: 15:16 - loss: 0.6215 - regression_loss: 0.5031 - classification_loss: 0.1184
 7996/10000 [======================>.......] - ETA: 15:15 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7997/10000 [======================>.......] - ETA: 15:15 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7998/10000 [======================>.......] - ETA: 15:14 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 7999/10000 [======================>.......] - ETA: 15:14 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 8000/10000 [=======================>......] - ETA: 15:13 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 8001/10000 [=======================>......] - ETA: 15:13 - loss: 0.6213 - regression_loss: 0.5029 - classification_loss: 0.1184
 8002/10000 [=======================>......] - ETA: 15:12 - loss: 0.6214 - regression_loss: 0.5030 - classification_loss: 0.1184
 8003/10000 [=======================>......] - ETA: 15:12 - loss: 0.6214 - regression_loss: 0.5029 - classification_loss: 0.1184
 8004/10000 [=======================>......] - ETA: 15:12 - loss: 0.6213 - regression_loss: 0.5029 - classification_loss: 0.1184
 8005/10000 [=======================>......] - ETA: 15:11 - loss: 0.6213 - regression_loss: 0.5029 - classification_loss: 0.1184
 8006/10000 [=======================>......] - ETA: 15:11 - loss: 0.6213 - regression_loss: 0.5029 - classification_loss: 0.1184
 8007/10000 [=======================>......] - ETA: 15:10 - loss: 0.6212 - regression_loss: 0.5028 - classification_loss: 0.1184
 8008/10000 [=======================>......] - ETA: 15:10 - loss: 0.6212 - regression_loss: 0.5028 - classification_loss: 0.1184
 8009/10000 [=======================>......] - ETA: 15:09 - loss: 0.6212 - regression_loss: 0.5028 - classification_loss: 0.1184
 8010/10000 [=======================>......] - ETA: 15:09 - loss: 0.6213 - regression_loss: 0.5029 - classification_loss: 0.1184
 8011/10000 [=======================>......] - ETA: 15:08 - loss: 0.6212 - regression_loss: 0.5028 - classification_loss: 0.1184
 8012/10000 [=======================>......] - ETA: 15:08 - loss: 0.6212 - regression_loss: 0.5028 - classification_loss: 0.1184
 8013/10000 [=======================>......] - ETA: 15:07 - loss: 0.6212 - regression_loss: 0.5028 - classification_loss: 0.1184
 8014/10000 [=======================>......] - ETA: 15:07 - loss: 0.6212 - regression_loss: 0.5028 - classification_loss: 0.1184
 8015/10000 [=======================>......] - ETA: 15:07 - loss: 0.6212 - regression_loss: 0.5028 - classification_loss: 0.1184
 8016/10000 [=======================>......] - ETA: 15:06 - loss: 0.6211 - regression_loss: 0.5027 - classification_loss: 0.1184
 8017/10000 [=======================>......] - ETA: 15:06 - loss: 0.6212 - regression_loss: 0.5028 - classification_loss: 0.1184
 8018/10000 [=======================>......] - ETA: 15:05 - loss: 0.6211 - regression_loss: 0.5028 - classification_loss: 0.1184
 8019/10000 [=======================>......] - ETA: 15:05 - loss: 0.6212 - regression_loss: 0.5028 - classification_loss: 0.1184
 8020/10000 [=======================>......] - ETA: 15:04 - loss: 0.6211 - regression_loss: 0.5028 - classification_loss: 0.1184
 8021/10000 [=======================>......] - ETA: 15:04 - loss: 0.6212 - regression_loss: 0.5028 - classification_loss: 0.1184
 8022/10000 [=======================>......] - ETA: 15:03 - loss: 0.6212 - regression_loss: 0.5029 - classification_loss: 0.1184
 8023/10000 [=======================>......] - ETA: 15:03 - loss: 0.6213 - regression_loss: 0.5029 - classification_loss: 0.1184
 8024/10000 [=======================>......] - ETA: 15:02 - loss: 0.6212 - regression_loss: 0.5029 - classification_loss: 0.1184
 8025/10000 [=======================>......] - ETA: 15:02 - loss: 0.6212 - regression_loss: 0.5029 - classification_loss: 0.1184
 8026/10000 [=======================>......] - ETA: 15:01 - loss: 0.6212 - regression_loss: 0.5029 - classification_loss: 0.1184
 8027/10000 [=======================>......] - ETA: 15:01 - loss: 0.6213 - regression_loss: 0.5029 - classification_loss: 0.1184
 8028/10000 [=======================>......] - ETA: 15:01 - loss: 0.6213 - regression_loss: 0.5029 - classification_loss: 0.1184
 8029/10000 [=======================>......] - ETA: 15:00 - loss: 0.6212 - regression_loss: 0.5028 - classification_loss: 0.1184
 8030/10000 [=======================>......] - ETA: 15:00 - loss: 0.6213 - regression_loss: 0.5029 - classification_loss: 0.1184
 8031/10000 [=======================>......] - ETA: 14:59 - loss: 0.6212 - regression_loss: 0.5029 - classification_loss: 0.1184
 8032/10000 [=======================>......] - ETA: 14:59 - loss: 0.6212 - regression_loss: 0.5028 - classification_loss: 0.1183
 8033/10000 [=======================>......] - ETA: 14:58 - loss: 0.6211 - regression_loss: 0.5028 - classification_loss: 0.1183
 8034/10000 [=======================>......] - ETA: 14:58 - loss: 0.6211 - regression_loss: 0.5028 - classification_loss: 0.1183
 8035/10000 [=======================>......] - ETA: 14:57 - loss: 0.6211 - regression_loss: 0.5028 - classification_loss: 0.1183
 8036/10000 [=======================>......] - ETA: 14:57 - loss: 0.6210 - regression_loss: 0.5027 - classification_loss: 0.1183
 8037/10000 [=======================>......] - ETA: 14:56 - loss: 0.6211 - regression_loss: 0.5028 - classification_loss: 0.1183
 8038/10000 [=======================>......] - ETA: 14:56 - loss: 0.6211 - regression_loss: 0.5027 - classification_loss: 0.1183
 8039/10000 [=======================>......] - ETA: 14:56 - loss: 0.6210 - regression_loss: 0.5027 - classification_loss: 0.1183
 8040/10000 [=======================>......] - ETA: 14:55 - loss: 0.6212 - regression_loss: 0.5028 - classification_loss: 0.1184
 8041/10000 [=======================>......] - ETA: 14:55 - loss: 0.6212 - regression_loss: 0.5028 - classification_loss: 0.1184
 8042/10000 [=======================>......] - ETA: 14:54 - loss: 0.6212 - regression_loss: 0.5028 - classification_loss: 0.1183
 8043/10000 [=======================>......] - ETA: 14:54 - loss: 0.6211 - regression_loss: 0.5028 - classification_loss: 0.1183
 8044/10000 [=======================>......] - ETA: 14:53 - loss: 0.6211 - regression_loss: 0.5028 - classification_loss: 0.1183
 8045/10000 [=======================>......] - ETA: 14:53 - loss: 0.6211 - regression_loss: 0.5028 - classification_loss: 0.1183
 8046/10000 [=======================>......] - ETA: 14:52 - loss: 0.6211 - regression_loss: 0.5028 - classification_loss: 0.1183
 8047/10000 [=======================>......] - ETA: 14:52 - loss: 0.6211 - regression_loss: 0.5028 - classification_loss: 0.1183
 8048/10000 [=======================>......] - ETA: 14:51 - loss: 0.6211 - regression_loss: 0.5027 - classification_loss: 0.1183
 8049/10000 [=======================>......] - ETA: 14:51 - loss: 0.6211 - regression_loss: 0.5027 - classification_loss: 0.1183
 8050/10000 [=======================>......] - ETA: 14:50 - loss: 0.6211 - regression_loss: 0.5028 - classification_loss: 0.1183
 8051/10000 [=======================>......] - ETA: 14:50 - loss: 0.6211 - regression_loss: 0.5028 - classification_loss: 0.1183
 8052/10000 [=======================>......] - ETA: 14:50 - loss: 0.6210 - regression_loss: 0.5027 - classification_loss: 0.1183
 8053/10000 [=======================>......] - ETA: 14:49 - loss: 0.6210 - regression_loss: 0.5027 - classification_loss: 0.1183
 8054/10000 [=======================>......] - ETA: 14:49 - loss: 0.6210 - regression_loss: 0.5027 - classification_loss: 0.1183
 8055/10000 [=======================>......] - ETA: 14:48 - loss: 0.6210 - regression_loss: 0.5027 - classification_loss: 0.1183
 8056/10000 [=======================>......] - ETA: 14:48 - loss: 0.6210 - regression_loss: 0.5027 - classification_loss: 0.1183
 8057/10000 [=======================>......] - ETA: 14:47 - loss: 0.6209 - regression_loss: 0.5026 - classification_loss: 0.1183
 8058/10000 [=======================>......] - ETA: 14:47 - loss: 0.6209 - regression_loss: 0.5026 - classification_loss: 0.1183
 8059/10000 [=======================>......] - ETA: 14:46 - loss: 0.6209 - regression_loss: 0.5026 - classification_loss: 0.1183
 8060/10000 [=======================>......] - ETA: 14:46 - loss: 0.6209 - regression_loss: 0.5027 - classification_loss: 0.1183
 8061/10000 [=======================>......] - ETA: 14:45 - loss: 0.6209 - regression_loss: 0.5026 - classification_loss: 0.1183
 8062/10000 [=======================>......] - ETA: 14:45 - loss: 0.6209 - regression_loss: 0.5026 - classification_loss: 0.1183
 8063/10000 [=======================>......] - ETA: 14:45 - loss: 0.6209 - regression_loss: 0.5026 - classification_loss: 0.1183
 8064/10000 [=======================>......] - ETA: 14:44 - loss: 0.6209 - regression_loss: 0.5026 - classification_loss: 0.1183
 8065/10000 [=======================>......] - ETA: 14:44 - loss: 0.6208 - regression_loss: 0.5026 - classification_loss: 0.1183
 8066/10000 [=======================>......] - ETA: 14:43 - loss: 0.6210 - regression_loss: 0.5027 - classification_loss: 0.1183
 8067/10000 [=======================>......] - ETA: 14:43 - loss: 0.6210 - regression_loss: 0.5027 - classification_loss: 0.1183
 8068/10000 [=======================>......] - ETA: 14:42 - loss: 0.6209 - regression_loss: 0.5026 - classification_loss: 0.1183
 8069/10000 [=======================>......] - ETA: 14:42 - loss: 0.6209 - regression_loss: 0.5026 - classification_loss: 0.1183
 8070/10000 [=======================>......] - ETA: 14:41 - loss: 0.6209 - regression_loss: 0.5026 - classification_loss: 0.1183
 8071/10000 [=======================>......] - ETA: 14:41 - loss: 0.6210 - regression_loss: 0.5027 - classification_loss: 0.1183
 8072/10000 [=======================>......] - ETA: 14:40 - loss: 0.6210 - regression_loss: 0.5027 - classification_loss: 0.1183
 8073/10000 [=======================>......] - ETA: 14:40 - loss: 0.6210 - regression_loss: 0.5027 - classification_loss: 0.1183
 8074/10000 [=======================>......] - ETA: 14:39 - loss: 0.6211 - regression_loss: 0.5028 - classification_loss: 0.1183
 8075/10000 [=======================>......] - ETA: 14:39 - loss: 0.6211 - regression_loss: 0.5028 - classification_loss: 0.1183
 8076/10000 [=======================>......] - ETA: 14:39 - loss: 0.6210 - regression_loss: 0.5027 - classification_loss: 0.1183
 8077/10000 [=======================>......] - ETA: 14:38 - loss: 0.6210 - regression_loss: 0.5027 - classification_loss: 0.1183
 8078/10000 [=======================>......] - ETA: 14:38 - loss: 0.6210 - regression_loss: 0.5027 - classification_loss: 0.1183
 8079/10000 [=======================>......] - ETA: 14:37 - loss: 0.6209 - regression_loss: 0.5027 - classification_loss: 0.1183
 8080/10000 [=======================>......] - ETA: 14:37 - loss: 0.6209 - regression_loss: 0.5026 - classification_loss: 0.1183
 8081/10000 [=======================>......] - ETA: 14:36 - loss: 0.6208 - regression_loss: 0.5026 - classification_loss: 0.1183
 8082/10000 [=======================>......] - ETA: 14:36 - loss: 0.6208 - regression_loss: 0.5025 - classification_loss: 0.1183
 8083/10000 [=======================>......] - ETA: 14:35 - loss: 0.6209 - regression_loss: 0.5026 - classification_loss: 0.1183
 8084/10000 [=======================>......] - ETA: 14:35 - loss: 0.6208 - regression_loss: 0.5025 - classification_loss: 0.1183
 8085/10000 [=======================>......] - ETA: 14:34 - loss: 0.6208 - regression_loss: 0.5025 - classification_loss: 0.1183
 8086/10000 [=======================>......] - ETA: 14:34 - loss: 0.6208 - regression_loss: 0.5025 - classification_loss: 0.1183
 8087/10000 [=======================>......] - ETA: 14:34 - loss: 0.6208 - regression_loss: 0.5025 - classification_loss: 0.1183
 8088/10000 [=======================>......] - ETA: 14:33 - loss: 0.6207 - regression_loss: 0.5025 - classification_loss: 0.1183
 8089/10000 [=======================>......] - ETA: 14:33 - loss: 0.6208 - regression_loss: 0.5025 - classification_loss: 0.1183
 8090/10000 [=======================>......] - ETA: 14:32 - loss: 0.6207 - regression_loss: 0.5025 - classification_loss: 0.1183
 8091/10000 [=======================>......] - ETA: 14:32 - loss: 0.6207 - regression_loss: 0.5024 - classification_loss: 0.1182
 8092/10000 [=======================>......] - ETA: 14:31 - loss: 0.6207 - regression_loss: 0.5024 - classification_loss: 0.1182
 8093/10000 [=======================>......] - ETA: 14:31 - loss: 0.6206 - regression_loss: 0.5024 - classification_loss: 0.1182
 8094/10000 [=======================>......] - ETA: 14:30 - loss: 0.6207 - regression_loss: 0.5025 - classification_loss: 0.1182
 8095/10000 [=======================>......] - ETA: 14:30 - loss: 0.6208 - regression_loss: 0.5025 - classification_loss: 0.1183
 8096/10000 [=======================>......] - ETA: 14:29 - loss: 0.6207 - regression_loss: 0.5025 - classification_loss: 0.1182
 8097/10000 [=======================>......] - ETA: 14:29 - loss: 0.6207 - regression_loss: 0.5025 - classification_loss: 0.1182
 8098/10000 [=======================>......] - ETA: 14:29 - loss: 0.6207 - regression_loss: 0.5025 - classification_loss: 0.1182
 8099/10000 [=======================>......] - ETA: 14:28 - loss: 0.6207 - regression_loss: 0.5025 - classification_loss: 0.1182
 8100/10000 [=======================>......] - ETA: 14:28 - loss: 0.6207 - regression_loss: 0.5025 - classification_loss: 0.1182
 8101/10000 [=======================>......] - ETA: 14:27 - loss: 0.6207 - regression_loss: 0.5025 - classification_loss: 0.1182
 8102/10000 [=======================>......] - ETA: 14:27 - loss: 0.6207 - regression_loss: 0.5025 - classification_loss: 0.1182
 8103/10000 [=======================>......] - ETA: 14:26 - loss: 0.6208 - regression_loss: 0.5025 - classification_loss: 0.1182
 8104/10000 [=======================>......] - ETA: 14:26 - loss: 0.6207 - regression_loss: 0.5025 - classification_loss: 0.1182
 8105/10000 [=======================>......] - ETA: 14:25 - loss: 0.6207 - regression_loss: 0.5025 - classification_loss: 0.1182
 8106/10000 [=======================>......] - ETA: 14:25 - loss: 0.6207 - regression_loss: 0.5025 - classification_loss: 0.1183
 8107/10000 [=======================>......] - ETA: 14:24 - loss: 0.6207 - regression_loss: 0.5024 - classification_loss: 0.1183
 8108/10000 [=======================>......] - ETA: 14:24 - loss: 0.6206 - regression_loss: 0.5024 - classification_loss: 0.1182
 8109/10000 [=======================>......] - ETA: 14:24 - loss: 0.6206 - regression_loss: 0.5024 - classification_loss: 0.1182
 8110/10000 [=======================>......] - ETA: 14:23 - loss: 0.6206 - regression_loss: 0.5024 - classification_loss: 0.1182
 8111/10000 [=======================>......] - ETA: 14:23 - loss: 0.6206 - regression_loss: 0.5024 - classification_loss: 0.1182
 8112/10000 [=======================>......] - ETA: 14:22 - loss: 0.6206 - regression_loss: 0.5024 - classification_loss: 0.1182
 8113/10000 [=======================>......] - ETA: 14:22 - loss: 0.6205 - regression_loss: 0.5023 - classification_loss: 0.1182
 8114/10000 [=======================>......] - ETA: 14:21 - loss: 0.6206 - regression_loss: 0.5024 - classification_loss: 0.1182
 8115/10000 [=======================>......] - ETA: 14:21 - loss: 0.6205 - regression_loss: 0.5023 - classification_loss: 0.1182
 8116/10000 [=======================>......] - ETA: 14:20 - loss: 0.6205 - regression_loss: 0.5023 - classification_loss: 0.1182
 8117/10000 [=======================>......] - ETA: 14:20 - loss: 0.6204 - regression_loss: 0.5022 - classification_loss: 0.1182
 8118/10000 [=======================>......] - ETA: 14:19 - loss: 0.6204 - regression_loss: 0.5022 - classification_loss: 0.1182
 8119/10000 [=======================>......] - ETA: 14:19 - loss: 0.6204 - regression_loss: 0.5022 - classification_loss: 0.1182
 8120/10000 [=======================>......] - ETA: 14:19 - loss: 0.6205 - regression_loss: 0.5023 - classification_loss: 0.1182
 8121/10000 [=======================>......] - ETA: 14:18 - loss: 0.6205 - regression_loss: 0.5023 - classification_loss: 0.1182
 8122/10000 [=======================>......] - ETA: 14:18 - loss: 0.6206 - regression_loss: 0.5024 - classification_loss: 0.1182
 8123/10000 [=======================>......] - ETA: 14:17 - loss: 0.6205 - regression_loss: 0.5023 - classification_loss: 0.1182
 8124/10000 [=======================>......] - ETA: 14:17 - loss: 0.6205 - regression_loss: 0.5023 - classification_loss: 0.1182
 8125/10000 [=======================>......] - ETA: 14:16 - loss: 0.6205 - regression_loss: 0.5023 - classification_loss: 0.1182
 8126/10000 [=======================>......] - ETA: 14:16 - loss: 0.6205 - regression_loss: 0.5023 - classification_loss: 0.1182
 8127/10000 [=======================>......] - ETA: 14:15 - loss: 0.6206 - regression_loss: 0.5024 - classification_loss: 0.1182
 8128/10000 [=======================>......] - ETA: 14:15 - loss: 0.6205 - regression_loss: 0.5023 - classification_loss: 0.1182
 8129/10000 [=======================>......] - ETA: 14:14 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1182
 8130/10000 [=======================>......] - ETA: 14:14 - loss: 0.6205 - regression_loss: 0.5023 - classification_loss: 0.1182
 8131/10000 [=======================>......] - ETA: 14:14 - loss: 0.6205 - regression_loss: 0.5023 - classification_loss: 0.1182
 8132/10000 [=======================>......] - ETA: 14:13 - loss: 0.6205 - regression_loss: 0.5023 - classification_loss: 0.1182
 8133/10000 [=======================>......] - ETA: 14:13 - loss: 0.6205 - regression_loss: 0.5023 - classification_loss: 0.1182
 8134/10000 [=======================>......] - ETA: 14:12 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1182
 8135/10000 [=======================>......] - ETA: 14:12 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1182
 8136/10000 [=======================>......] - ETA: 14:11 - loss: 0.6205 - regression_loss: 0.5023 - classification_loss: 0.1182
 8137/10000 [=======================>......] - ETA: 14:11 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1182
 8138/10000 [=======================>......] - ETA: 14:10 - loss: 0.6206 - regression_loss: 0.5024 - classification_loss: 0.1182
 8139/10000 [=======================>......] - ETA: 14:10 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1182
 8140/10000 [=======================>......] - ETA: 14:09 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1182
 8141/10000 [=======================>......] - ETA: 14:09 - loss: 0.6206 - regression_loss: 0.5024 - classification_loss: 0.1182
 8142/10000 [=======================>......] - ETA: 14:08 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1182
 8143/10000 [=======================>......] - ETA: 14:08 - loss: 0.6206 - regression_loss: 0.5024 - classification_loss: 0.1182
 8144/10000 [=======================>......] - ETA: 14:08 - loss: 0.6206 - regression_loss: 0.5024 - classification_loss: 0.1182
 8145/10000 [=======================>......] - ETA: 14:07 - loss: 0.6206 - regression_loss: 0.5024 - classification_loss: 0.1182
 8146/10000 [=======================>......] - ETA: 14:07 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1182
 8147/10000 [=======================>......] - ETA: 14:06 - loss: 0.6205 - regression_loss: 0.5023 - classification_loss: 0.1182
 8148/10000 [=======================>......] - ETA: 14:06 - loss: 0.6205 - regression_loss: 0.5023 - classification_loss: 0.1181
 8149/10000 [=======================>......] - ETA: 14:05 - loss: 0.6205 - regression_loss: 0.5023 - classification_loss: 0.1181
 8150/10000 [=======================>......] - ETA: 14:05 - loss: 0.6205 - regression_loss: 0.5023 - classification_loss: 0.1181
 8151/10000 [=======================>......] - ETA: 14:04 - loss: 0.6205 - regression_loss: 0.5023 - classification_loss: 0.1181
 8152/10000 [=======================>......] - ETA: 14:04 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1181
 8153/10000 [=======================>......] - ETA: 14:03 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1181
 8154/10000 [=======================>......] - ETA: 14:03 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1181
 8155/10000 [=======================>......] - ETA: 14:02 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8156/10000 [=======================>......] - ETA: 14:02 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8157/10000 [=======================>......] - ETA: 14:02 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8158/10000 [=======================>......] - ETA: 14:01 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1181
 8159/10000 [=======================>......] - ETA: 14:01 - loss: 0.6206 - regression_loss: 0.5024 - classification_loss: 0.1181
 8160/10000 [=======================>......] - ETA: 14:00 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8161/10000 [=======================>......] - ETA: 14:00 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8162/10000 [=======================>......] - ETA: 13:59 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1181
 8163/10000 [=======================>......] - ETA: 13:59 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8164/10000 [=======================>......] - ETA: 13:58 - loss: 0.6205 - regression_loss: 0.5023 - classification_loss: 0.1181
 8165/10000 [=======================>......] - ETA: 13:58 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8166/10000 [=======================>......] - ETA: 13:57 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8167/10000 [=======================>......] - ETA: 13:57 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1181
 8168/10000 [=======================>......] - ETA: 13:57 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8169/10000 [=======================>......] - ETA: 13:56 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8170/10000 [=======================>......] - ETA: 13:56 - loss: 0.6206 - regression_loss: 0.5024 - classification_loss: 0.1182
 8171/10000 [=======================>......] - ETA: 13:55 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8172/10000 [=======================>......] - ETA: 13:55 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8173/10000 [=======================>......] - ETA: 13:54 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8174/10000 [=======================>......] - ETA: 13:54 - loss: 0.6205 - regression_loss: 0.5023 - classification_loss: 0.1181
 8175/10000 [=======================>......] - ETA: 13:53 - loss: 0.6205 - regression_loss: 0.5023 - classification_loss: 0.1181
 8176/10000 [=======================>......] - ETA: 13:53 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1182
 8177/10000 [=======================>......] - ETA: 13:52 - loss: 0.6205 - regression_loss: 0.5023 - classification_loss: 0.1181
 8178/10000 [=======================>......] - ETA: 13:52 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1181
 8179/10000 [=======================>......] - ETA: 13:52 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1181
 8180/10000 [=======================>......] - ETA: 13:51 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8181/10000 [=======================>......] - ETA: 13:51 - loss: 0.6204 - regression_loss: 0.5022 - classification_loss: 0.1182
 8182/10000 [=======================>......] - ETA: 13:50 - loss: 0.6204 - regression_loss: 0.5022 - classification_loss: 0.1181
 8183/10000 [=======================>......] - ETA: 13:50 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8184/10000 [=======================>......] - ETA: 13:49 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8185/10000 [=======================>......] - ETA: 13:49 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8186/10000 [=======================>......] - ETA: 13:48 - loss: 0.6203 - regression_loss: 0.5021 - classification_loss: 0.1181
 8187/10000 [=======================>......] - ETA: 13:48 - loss: 0.6202 - regression_loss: 0.5021 - classification_loss: 0.1181
 8188/10000 [=======================>......] - ETA: 13:47 - loss: 0.6202 - regression_loss: 0.5021 - classification_loss: 0.1181
 8189/10000 [=======================>......] - ETA: 13:47 - loss: 0.6202 - regression_loss: 0.5021 - classification_loss: 0.1181
 8190/10000 [=======================>......] - ETA: 13:47 - loss: 0.6202 - regression_loss: 0.5021 - classification_loss: 0.1181
 8191/10000 [=======================>......] - ETA: 13:46 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8192/10000 [=======================>......] - ETA: 13:46 - loss: 0.6202 - regression_loss: 0.5021 - classification_loss: 0.1181
 8193/10000 [=======================>......] - ETA: 13:45 - loss: 0.6202 - regression_loss: 0.5021 - classification_loss: 0.1181
 8194/10000 [=======================>......] - ETA: 13:45 - loss: 0.6202 - regression_loss: 0.5021 - classification_loss: 0.1181
 8195/10000 [=======================>......] - ETA: 13:44 - loss: 0.6202 - regression_loss: 0.5021 - classification_loss: 0.1181
 8196/10000 [=======================>......] - ETA: 13:44 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1181
 8197/10000 [=======================>......] - ETA: 13:43 - loss: 0.6201 - regression_loss: 0.5020 - classification_loss: 0.1181
 8198/10000 [=======================>......] - ETA: 13:43 - loss: 0.6201 - regression_loss: 0.5020 - classification_loss: 0.1181
 8199/10000 [=======================>......] - ETA: 13:42 - loss: 0.6201 - regression_loss: 0.5020 - classification_loss: 0.1181
 8200/10000 [=======================>......] - ETA: 13:42 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1181
 8201/10000 [=======================>......] - ETA: 13:42 - loss: 0.6200 - regression_loss: 0.5019 - classification_loss: 0.1180
 8202/10000 [=======================>......] - ETA: 13:41 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8203/10000 [=======================>......] - ETA: 13:41 - loss: 0.6201 - regression_loss: 0.5020 - classification_loss: 0.1181
 8204/10000 [=======================>......] - ETA: 13:40 - loss: 0.6201 - regression_loss: 0.5020 - classification_loss: 0.1181
 8205/10000 [=======================>......] - ETA: 13:40 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8206/10000 [=======================>......] - ETA: 13:39 - loss: 0.6200 - regression_loss: 0.5019 - classification_loss: 0.1180
 8207/10000 [=======================>......] - ETA: 13:39 - loss: 0.6199 - regression_loss: 0.5019 - classification_loss: 0.1180
 8208/10000 [=======================>......] - ETA: 13:38 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8209/10000 [=======================>......] - ETA: 13:38 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8210/10000 [=======================>......] - ETA: 13:37 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8211/10000 [=======================>......] - ETA: 13:37 - loss: 0.6199 - regression_loss: 0.5019 - classification_loss: 0.1180
 8212/10000 [=======================>......] - ETA: 13:36 - loss: 0.6199 - regression_loss: 0.5019 - classification_loss: 0.1180
 8213/10000 [=======================>......] - ETA: 13:36 - loss: 0.6199 - regression_loss: 0.5019 - classification_loss: 0.1180
 8214/10000 [=======================>......] - ETA: 13:36 - loss: 0.6199 - regression_loss: 0.5019 - classification_loss: 0.1180
 8215/10000 [=======================>......] - ETA: 13:35 - loss: 0.6199 - regression_loss: 0.5019 - classification_loss: 0.1180
 8216/10000 [=======================>......] - ETA: 13:35 - loss: 0.6199 - regression_loss: 0.5019 - classification_loss: 0.1180
 8217/10000 [=======================>......] - ETA: 13:34 - loss: 0.6199 - regression_loss: 0.5019 - classification_loss: 0.1180
 8218/10000 [=======================>......] - ETA: 13:34 - loss: 0.6200 - regression_loss: 0.5019 - classification_loss: 0.1181
 8219/10000 [=======================>......] - ETA: 13:33 - loss: 0.6200 - regression_loss: 0.5019 - classification_loss: 0.1181
 8220/10000 [=======================>......] - ETA: 13:33 - loss: 0.6200 - regression_loss: 0.5019 - classification_loss: 0.1181
 8221/10000 [=======================>......] - ETA: 13:32 - loss: 0.6199 - regression_loss: 0.5019 - classification_loss: 0.1180
 8222/10000 [=======================>......] - ETA: 13:32 - loss: 0.6200 - regression_loss: 0.5019 - classification_loss: 0.1181
 8223/10000 [=======================>......] - ETA: 13:31 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1181
 8224/10000 [=======================>......] - ETA: 13:31 - loss: 0.6200 - regression_loss: 0.5019 - classification_loss: 0.1181
 8225/10000 [=======================>......] - ETA: 13:31 - loss: 0.6200 - regression_loss: 0.5019 - classification_loss: 0.1181
 8226/10000 [=======================>......] - ETA: 13:30 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1181
 8227/10000 [=======================>......] - ETA: 13:30 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8228/10000 [=======================>......] - ETA: 13:29 - loss: 0.6200 - regression_loss: 0.5019 - classification_loss: 0.1180
 8229/10000 [=======================>......] - ETA: 13:29 - loss: 0.6200 - regression_loss: 0.5019 - classification_loss: 0.1180
 8230/10000 [=======================>......] - ETA: 13:28 - loss: 0.6201 - regression_loss: 0.5020 - classification_loss: 0.1181
 8231/10000 [=======================>......] - ETA: 13:28 - loss: 0.6201 - regression_loss: 0.5020 - classification_loss: 0.1180
 8232/10000 [=======================>......] - ETA: 13:27 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8233/10000 [=======================>......] - ETA: 13:27 - loss: 0.6201 - regression_loss: 0.5020 - classification_loss: 0.1180
 8234/10000 [=======================>......] - ETA: 13:26 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8235/10000 [=======================>......] - ETA: 13:26 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8236/10000 [=======================>......] - ETA: 13:26 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8237/10000 [=======================>......] - ETA: 13:25 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8238/10000 [=======================>......] - ETA: 13:25 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8239/10000 [=======================>......] - ETA: 13:24 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8240/10000 [=======================>......] - ETA: 13:24 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8241/10000 [=======================>......] - ETA: 13:23 - loss: 0.6201 - regression_loss: 0.5020 - classification_loss: 0.1180
 8242/10000 [=======================>......] - ETA: 13:23 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8243/10000 [=======================>......] - ETA: 13:22 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1180
 8244/10000 [=======================>......] - ETA: 13:22 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8245/10000 [=======================>......] - ETA: 13:21 - loss: 0.6199 - regression_loss: 0.5019 - classification_loss: 0.1180
 8246/10000 [=======================>......] - ETA: 13:21 - loss: 0.6199 - regression_loss: 0.5019 - classification_loss: 0.1180
 8247/10000 [=======================>......] - ETA: 13:21 - loss: 0.6199 - regression_loss: 0.5019 - classification_loss: 0.1180
 8248/10000 [=======================>......] - ETA: 13:20 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8249/10000 [=======================>......] - ETA: 13:20 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8250/10000 [=======================>......] - ETA: 13:19 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8251/10000 [=======================>......] - ETA: 13:19 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8252/10000 [=======================>......] - ETA: 13:18 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8253/10000 [=======================>......] - ETA: 13:18 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8254/10000 [=======================>......] - ETA: 13:17 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8255/10000 [=======================>......] - ETA: 13:17 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8256/10000 [=======================>......] - ETA: 13:16 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8257/10000 [=======================>......] - ETA: 13:16 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8258/10000 [=======================>......] - ETA: 13:16 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8259/10000 [=======================>......] - ETA: 13:15 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1180
 8260/10000 [=======================>......] - ETA: 13:15 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8261/10000 [=======================>......] - ETA: 13:14 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8262/10000 [=======================>......] - ETA: 13:14 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8263/10000 [=======================>......] - ETA: 13:13 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8264/10000 [=======================>......] - ETA: 13:13 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8265/10000 [=======================>......] - ETA: 13:12 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1180
 8266/10000 [=======================>......] - ETA: 13:12 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1180
 8267/10000 [=======================>......] - ETA: 13:11 - loss: 0.6202 - regression_loss: 0.5021 - classification_loss: 0.1180
 8268/10000 [=======================>......] - ETA: 13:11 - loss: 0.6202 - regression_loss: 0.5021 - classification_loss: 0.1180
 8269/10000 [=======================>......] - ETA: 13:11 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8270/10000 [=======================>......] - ETA: 13:10 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8271/10000 [=======================>......] - ETA: 13:10 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8272/10000 [=======================>......] - ETA: 13:09 - loss: 0.6202 - regression_loss: 0.5021 - classification_loss: 0.1180
 8273/10000 [=======================>......] - ETA: 13:09 - loss: 0.6202 - regression_loss: 0.5021 - classification_loss: 0.1180
 8274/10000 [=======================>......] - ETA: 13:08 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8275/10000 [=======================>......] - ETA: 13:08 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8276/10000 [=======================>......] - ETA: 13:07 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8277/10000 [=======================>......] - ETA: 13:07 - loss: 0.6203 - regression_loss: 0.5021 - classification_loss: 0.1181
 8278/10000 [=======================>......] - ETA: 13:06 - loss: 0.6202 - regression_loss: 0.5021 - classification_loss: 0.1181
 8279/10000 [=======================>......] - ETA: 13:06 - loss: 0.6202 - regression_loss: 0.5021 - classification_loss: 0.1181
 8280/10000 [=======================>......] - ETA: 13:05 - loss: 0.6202 - regression_loss: 0.5021 - classification_loss: 0.1181
 8281/10000 [=======================>......] - ETA: 13:05 - loss: 0.6202 - regression_loss: 0.5021 - classification_loss: 0.1181
 8282/10000 [=======================>......] - ETA: 13:05 - loss: 0.6201 - regression_loss: 0.5020 - classification_loss: 0.1181
 8283/10000 [=======================>......] - ETA: 13:04 - loss: 0.6201 - regression_loss: 0.5020 - classification_loss: 0.1181
 8284/10000 [=======================>......] - ETA: 13:04 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1181
 8285/10000 [=======================>......] - ETA: 13:03 - loss: 0.6201 - regression_loss: 0.5020 - classification_loss: 0.1181
 8286/10000 [=======================>......] - ETA: 13:03 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1181
 8287/10000 [=======================>......] - ETA: 13:02 - loss: 0.6201 - regression_loss: 0.5020 - classification_loss: 0.1181
 8288/10000 [=======================>......] - ETA: 13:02 - loss: 0.6201 - regression_loss: 0.5020 - classification_loss: 0.1181
 8289/10000 [=======================>......] - ETA: 13:01 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1181
 8290/10000 [=======================>......] - ETA: 13:01 - loss: 0.6202 - regression_loss: 0.5021 - classification_loss: 0.1181
 8291/10000 [=======================>......] - ETA: 13:00 - loss: 0.6202 - regression_loss: 0.5021 - classification_loss: 0.1181
 8292/10000 [=======================>......] - ETA: 13:00 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8293/10000 [=======================>......] - ETA: 13:00 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8294/10000 [=======================>......] - ETA: 12:59 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1181
 8295/10000 [=======================>......] - ETA: 12:59 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8296/10000 [=======================>......] - ETA: 12:58 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8297/10000 [=======================>......] - ETA: 12:58 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8298/10000 [=======================>......] - ETA: 12:57 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8299/10000 [=======================>......] - ETA: 12:57 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8300/10000 [=======================>......] - ETA: 12:56 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8301/10000 [=======================>......] - ETA: 12:56 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1181
 8302/10000 [=======================>......] - ETA: 12:55 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1181
 8303/10000 [=======================>......] - ETA: 12:55 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8304/10000 [=======================>......] - ETA: 12:54 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8305/10000 [=======================>......] - ETA: 12:54 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1181
 8306/10000 [=======================>......] - ETA: 12:54 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8307/10000 [=======================>......] - ETA: 12:53 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8308/10000 [=======================>......] - ETA: 12:53 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8309/10000 [=======================>......] - ETA: 12:52 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1181
 8310/10000 [=======================>......] - ETA: 12:52 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8311/10000 [=======================>......] - ETA: 12:51 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1182
 8312/10000 [=======================>......] - ETA: 12:51 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1182
 8313/10000 [=======================>......] - ETA: 12:50 - loss: 0.6206 - regression_loss: 0.5024 - classification_loss: 0.1182
 8314/10000 [=======================>......] - ETA: 12:50 - loss: 0.6206 - regression_loss: 0.5024 - classification_loss: 0.1182
 8315/10000 [=======================>......] - ETA: 12:49 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1182
 8316/10000 [=======================>......] - ETA: 12:49 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8317/10000 [=======================>......] - ETA: 12:49 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8318/10000 [=======================>......] - ETA: 12:48 - loss: 0.6206 - regression_loss: 0.5024 - classification_loss: 0.1181
 8319/10000 [=======================>......] - ETA: 12:48 - loss: 0.6207 - regression_loss: 0.5025 - classification_loss: 0.1182
 8320/10000 [=======================>......] - ETA: 12:47 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1182
 8321/10000 [=======================>......] - ETA: 12:47 - loss: 0.6207 - regression_loss: 0.5025 - classification_loss: 0.1182
 8322/10000 [=======================>......] - ETA: 12:46 - loss: 0.6207 - regression_loss: 0.5025 - classification_loss: 0.1182
 8323/10000 [=======================>......] - ETA: 12:46 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8324/10000 [=======================>......] - ETA: 12:45 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1182
 8325/10000 [=======================>......] - ETA: 12:45 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8326/10000 [=======================>......] - ETA: 12:44 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8327/10000 [=======================>......] - ETA: 12:44 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8328/10000 [=======================>......] - ETA: 12:44 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8329/10000 [=======================>......] - ETA: 12:43 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8330/10000 [=======================>......] - ETA: 12:43 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8331/10000 [=======================>......] - ETA: 12:42 - loss: 0.6206 - regression_loss: 0.5024 - classification_loss: 0.1181
 8332/10000 [=======================>......] - ETA: 12:42 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8333/10000 [=======================>......] - ETA: 12:41 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8334/10000 [========================>.....] - ETA: 12:41 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8335/10000 [========================>.....] - ETA: 12:40 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8336/10000 [========================>.....] - ETA: 12:40 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8337/10000 [========================>.....] - ETA: 12:39 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8338/10000 [========================>.....] - ETA: 12:39 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8339/10000 [========================>.....] - ETA: 12:38 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8340/10000 [========================>.....] - ETA: 12:38 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1181
 8341/10000 [========================>.....] - ETA: 12:38 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8342/10000 [========================>.....] - ETA: 12:37 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8343/10000 [========================>.....] - ETA: 12:37 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1181
 8344/10000 [========================>.....] - ETA: 12:36 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8345/10000 [========================>.....] - ETA: 12:36 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8346/10000 [========================>.....] - ETA: 12:35 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8347/10000 [========================>.....] - ETA: 12:35 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8348/10000 [========================>.....] - ETA: 12:34 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8349/10000 [========================>.....] - ETA: 12:34 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8350/10000 [========================>.....] - ETA: 12:33 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8351/10000 [========================>.....] - ETA: 12:33 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1181
 8352/10000 [========================>.....] - ETA: 12:33 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8353/10000 [========================>.....] - ETA: 12:32 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8354/10000 [========================>.....] - ETA: 12:32 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1181
 8355/10000 [========================>.....] - ETA: 12:31 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8356/10000 [========================>.....] - ETA: 12:31 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8357/10000 [========================>.....] - ETA: 12:30 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1181
 8358/10000 [========================>.....] - ETA: 12:30 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8359/10000 [========================>.....] - ETA: 12:29 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1181
 8360/10000 [========================>.....] - ETA: 12:29 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8361/10000 [========================>.....] - ETA: 12:28 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8362/10000 [========================>.....] - ETA: 12:28 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8363/10000 [========================>.....] - ETA: 12:28 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8364/10000 [========================>.....] - ETA: 12:27 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8365/10000 [========================>.....] - ETA: 12:27 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8366/10000 [========================>.....] - ETA: 12:26 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1180
 8367/10000 [========================>.....] - ETA: 12:26 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1180
 8368/10000 [========================>.....] - ETA: 12:25 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8369/10000 [========================>.....] - ETA: 12:25 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8370/10000 [========================>.....] - ETA: 12:24 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8371/10000 [========================>.....] - ETA: 12:24 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8372/10000 [========================>.....] - ETA: 12:23 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8373/10000 [========================>.....] - ETA: 12:23 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8374/10000 [========================>.....] - ETA: 12:22 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1180
 8375/10000 [========================>.....] - ETA: 12:22 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8376/10000 [========================>.....] - ETA: 12:22 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8377/10000 [========================>.....] - ETA: 12:21 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8378/10000 [========================>.....] - ETA: 12:21 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8379/10000 [========================>.....] - ETA: 12:20 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8380/10000 [========================>.....] - ETA: 12:20 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8381/10000 [========================>.....] - ETA: 12:19 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8382/10000 [========================>.....] - ETA: 12:19 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8383/10000 [========================>.....] - ETA: 12:18 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8384/10000 [========================>.....] - ETA: 12:18 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8385/10000 [========================>.....] - ETA: 12:17 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8386/10000 [========================>.....] - ETA: 12:17 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8387/10000 [========================>.....] - ETA: 12:17 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8388/10000 [========================>.....] - ETA: 12:16 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8389/10000 [========================>.....] - ETA: 12:16 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8390/10000 [========================>.....] - ETA: 12:15 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8391/10000 [========================>.....] - ETA: 12:15 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8392/10000 [========================>.....] - ETA: 12:14 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8393/10000 [========================>.....] - ETA: 12:14 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8394/10000 [========================>.....] - ETA: 12:13 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8395/10000 [========================>.....] - ETA: 12:13 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8396/10000 [========================>.....] - ETA: 12:12 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8397/10000 [========================>.....] - ETA: 12:12 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8398/10000 [========================>.....] - ETA: 12:12 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1180
 8399/10000 [========================>.....] - ETA: 12:11 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8400/10000 [========================>.....] - ETA: 12:11 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8401/10000 [========================>.....] - ETA: 12:10 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8402/10000 [========================>.....] - ETA: 12:10 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8403/10000 [========================>.....] - ETA: 12:09 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8404/10000 [========================>.....] - ETA: 12:09 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8405/10000 [========================>.....] - ETA: 12:08 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8406/10000 [========================>.....] - ETA: 12:08 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8407/10000 [========================>.....] - ETA: 12:07 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8408/10000 [========================>.....] - ETA: 12:07 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8409/10000 [========================>.....] - ETA: 12:07 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8410/10000 [========================>.....] - ETA: 12:06 - loss: 0.6205 - regression_loss: 0.5026 - classification_loss: 0.1180
 8411/10000 [========================>.....] - ETA: 12:06 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8412/10000 [========================>.....] - ETA: 12:05 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8413/10000 [========================>.....] - ETA: 12:05 - loss: 0.6205 - regression_loss: 0.5026 - classification_loss: 0.1180
 8414/10000 [========================>.....] - ETA: 12:04 - loss: 0.6205 - regression_loss: 0.5026 - classification_loss: 0.1180
 8415/10000 [========================>.....] - ETA: 12:04 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8416/10000 [========================>.....] - ETA: 12:03 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8417/10000 [========================>.....] - ETA: 12:03 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1179
 8418/10000 [========================>.....] - ETA: 12:02 - loss: 0.6205 - regression_loss: 0.5026 - classification_loss: 0.1179
 8419/10000 [========================>.....] - ETA: 12:02 - loss: 0.6205 - regression_loss: 0.5026 - classification_loss: 0.1179
 8420/10000 [========================>.....] - ETA: 12:02 - loss: 0.6205 - regression_loss: 0.5026 - classification_loss: 0.1179
 8421/10000 [========================>.....] - ETA: 12:01 - loss: 0.6205 - regression_loss: 0.5026 - classification_loss: 0.1179
 8422/10000 [========================>.....] - ETA: 12:01 - loss: 0.6205 - regression_loss: 0.5026 - classification_loss: 0.1179
 8423/10000 [========================>.....] - ETA: 12:00 - loss: 0.6205 - regression_loss: 0.5026 - classification_loss: 0.1179
 8424/10000 [========================>.....] - ETA: 12:00 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1179
 8425/10000 [========================>.....] - ETA: 11:59 - loss: 0.6204 - regression_loss: 0.5025 - classification_loss: 0.1179
 8426/10000 [========================>.....] - ETA: 11:59 - loss: 0.6204 - regression_loss: 0.5025 - classification_loss: 0.1179
 8427/10000 [========================>.....] - ETA: 11:58 - loss: 0.6204 - regression_loss: 0.5025 - classification_loss: 0.1179
 8428/10000 [========================>.....] - ETA: 11:58 - loss: 0.6204 - regression_loss: 0.5025 - classification_loss: 0.1179
 8429/10000 [========================>.....] - ETA: 11:57 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 8430/10000 [========================>.....] - ETA: 11:57 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 8431/10000 [========================>.....] - ETA: 11:57 - loss: 0.6204 - regression_loss: 0.5025 - classification_loss: 0.1179
 8432/10000 [========================>.....] - ETA: 11:56 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 8433/10000 [========================>.....] - ETA: 11:56 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 8434/10000 [========================>.....] - ETA: 11:55 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 8435/10000 [========================>.....] - ETA: 11:55 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1179
 8436/10000 [========================>.....] - ETA: 11:54 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 8437/10000 [========================>.....] - ETA: 11:54 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 8438/10000 [========================>.....] - ETA: 11:53 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 8439/10000 [========================>.....] - ETA: 11:53 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 8440/10000 [========================>.....] - ETA: 11:52 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 8441/10000 [========================>.....] - ETA: 11:52 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 8442/10000 [========================>.....] - ETA: 11:51 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 8443/10000 [========================>.....] - ETA: 11:51 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 8444/10000 [========================>.....] - ETA: 11:51 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8445/10000 [========================>.....] - ETA: 11:50 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8446/10000 [========================>.....] - ETA: 11:50 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8447/10000 [========================>.....] - ETA: 11:49 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8448/10000 [========================>.....] - ETA: 11:49 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8449/10000 [========================>.....] - ETA: 11:48 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8450/10000 [========================>.....] - ETA: 11:48 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8451/10000 [========================>.....] - ETA: 11:47 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8452/10000 [========================>.....] - ETA: 11:47 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8453/10000 [========================>.....] - ETA: 11:46 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8454/10000 [========================>.....] - ETA: 11:46 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8455/10000 [========================>.....] - ETA: 11:46 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8456/10000 [========================>.....] - ETA: 11:45 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1180
 8457/10000 [========================>.....] - ETA: 11:45 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8458/10000 [========================>.....] - ETA: 11:44 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8459/10000 [========================>.....] - ETA: 11:44 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8460/10000 [========================>.....] - ETA: 11:43 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8461/10000 [========================>.....] - ETA: 11:43 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8462/10000 [========================>.....] - ETA: 11:42 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8463/10000 [========================>.....] - ETA: 11:42 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8464/10000 [========================>.....] - ETA: 11:41 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8465/10000 [========================>.....] - ETA: 11:41 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8466/10000 [========================>.....] - ETA: 11:40 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1180
 8467/10000 [========================>.....] - ETA: 11:40 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8468/10000 [========================>.....] - ETA: 11:40 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8469/10000 [========================>.....] - ETA: 11:39 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8470/10000 [========================>.....] - ETA: 11:39 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8471/10000 [========================>.....] - ETA: 11:38 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8472/10000 [========================>.....] - ETA: 11:38 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8473/10000 [========================>.....] - ETA: 11:37 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8474/10000 [========================>.....] - ETA: 11:37 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8475/10000 [========================>.....] - ETA: 11:36 - loss: 0.6204 - regression_loss: 0.5025 - classification_loss: 0.1180
 8476/10000 [========================>.....] - ETA: 11:36 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8477/10000 [========================>.....] - ETA: 11:35 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8478/10000 [========================>.....] - ETA: 11:35 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8479/10000 [========================>.....] - ETA: 11:35 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8480/10000 [========================>.....] - ETA: 11:34 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8481/10000 [========================>.....] - ETA: 11:34 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8482/10000 [========================>.....] - ETA: 11:33 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8483/10000 [========================>.....] - ETA: 11:33 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1180
 8484/10000 [========================>.....] - ETA: 11:32 - loss: 0.6208 - regression_loss: 0.5027 - classification_loss: 0.1181
 8485/10000 [========================>.....] - ETA: 11:32 - loss: 0.6207 - regression_loss: 0.5027 - classification_loss: 0.1181
 8486/10000 [========================>.....] - ETA: 11:31 - loss: 0.6208 - regression_loss: 0.5028 - classification_loss: 0.1181
 8487/10000 [========================>.....] - ETA: 11:31 - loss: 0.6208 - regression_loss: 0.5028 - classification_loss: 0.1181
 8488/10000 [========================>.....] - ETA: 11:30 - loss: 0.6208 - regression_loss: 0.5028 - classification_loss: 0.1181
 8489/10000 [========================>.....] - ETA: 11:30 - loss: 0.6208 - regression_loss: 0.5028 - classification_loss: 0.1181
 8490/10000 [========================>.....] - ETA: 11:30 - loss: 0.6208 - regression_loss: 0.5027 - classification_loss: 0.1181
 8491/10000 [========================>.....] - ETA: 11:29 - loss: 0.6208 - regression_loss: 0.5027 - classification_loss: 0.1181
 8492/10000 [========================>.....] - ETA: 11:29 - loss: 0.6208 - regression_loss: 0.5027 - classification_loss: 0.1181
 8493/10000 [========================>.....] - ETA: 11:28 - loss: 0.6209 - regression_loss: 0.5029 - classification_loss: 0.1181
 8494/10000 [========================>.....] - ETA: 11:28 - loss: 0.6209 - regression_loss: 0.5029 - classification_loss: 0.1181
 8495/10000 [========================>.....] - ETA: 11:27 - loss: 0.6209 - regression_loss: 0.5029 - classification_loss: 0.1181
 8496/10000 [========================>.....] - ETA: 11:27 - loss: 0.6209 - regression_loss: 0.5029 - classification_loss: 0.1181
 8497/10000 [========================>.....] - ETA: 11:26 - loss: 0.6210 - regression_loss: 0.5029 - classification_loss: 0.1181
 8498/10000 [========================>.....] - ETA: 11:26 - loss: 0.6210 - regression_loss: 0.5029 - classification_loss: 0.1181
 8499/10000 [========================>.....] - ETA: 11:25 - loss: 0.6210 - regression_loss: 0.5029 - classification_loss: 0.1181
 8500/10000 [========================>.....] - ETA: 11:25 - loss: 0.6210 - regression_loss: 0.5030 - classification_loss: 0.1181
 8501/10000 [========================>.....] - ETA: 11:24 - loss: 0.6210 - regression_loss: 0.5029 - classification_loss: 0.1181
 8502/10000 [========================>.....] - ETA: 11:24 - loss: 0.6210 - regression_loss: 0.5030 - classification_loss: 0.1181
 8503/10000 [========================>.....] - ETA: 11:24 - loss: 0.6210 - regression_loss: 0.5029 - classification_loss: 0.1181
 8504/10000 [========================>.....] - ETA: 11:23 - loss: 0.6210 - regression_loss: 0.5029 - classification_loss: 0.1181
 8505/10000 [========================>.....] - ETA: 11:23 - loss: 0.6210 - regression_loss: 0.5029 - classification_loss: 0.1180
 8506/10000 [========================>.....] - ETA: 11:22 - loss: 0.6209 - regression_loss: 0.5029 - classification_loss: 0.1180
 8507/10000 [========================>.....] - ETA: 11:22 - loss: 0.6209 - regression_loss: 0.5028 - classification_loss: 0.1180
 8508/10000 [========================>.....] - ETA: 11:21 - loss: 0.6208 - regression_loss: 0.5028 - classification_loss: 0.1180
 8509/10000 [========================>.....] - ETA: 11:21 - loss: 0.6208 - regression_loss: 0.5028 - classification_loss: 0.1180
 8510/10000 [========================>.....] - ETA: 11:20 - loss: 0.6208 - regression_loss: 0.5028 - classification_loss: 0.1180
 8511/10000 [========================>.....] - ETA: 11:20 - loss: 0.6208 - regression_loss: 0.5028 - classification_loss: 0.1180
 8512/10000 [========================>.....] - ETA: 11:19 - loss: 0.6208 - regression_loss: 0.5027 - classification_loss: 0.1180
 8513/10000 [========================>.....] - ETA: 11:19 - loss: 0.6209 - regression_loss: 0.5029 - classification_loss: 0.1181
 8514/10000 [========================>.....] - ETA: 11:19 - loss: 0.6208 - regression_loss: 0.5028 - classification_loss: 0.1180
 8515/10000 [========================>.....] - ETA: 11:18 - loss: 0.6208 - regression_loss: 0.5028 - classification_loss: 0.1180
 8516/10000 [========================>.....] - ETA: 11:18 - loss: 0.6208 - regression_loss: 0.5028 - classification_loss: 0.1180
 8517/10000 [========================>.....] - ETA: 11:17 - loss: 0.6208 - regression_loss: 0.5028 - classification_loss: 0.1180
 8518/10000 [========================>.....] - ETA: 11:17 - loss: 0.6209 - regression_loss: 0.5028 - classification_loss: 0.1180
 8519/10000 [========================>.....] - ETA: 11:16 - loss: 0.6208 - regression_loss: 0.5028 - classification_loss: 0.1180
 8520/10000 [========================>.....] - ETA: 11:16 - loss: 0.6209 - regression_loss: 0.5028 - classification_loss: 0.1180
 8521/10000 [========================>.....] - ETA: 11:15 - loss: 0.6208 - regression_loss: 0.5028 - classification_loss: 0.1180
 8522/10000 [========================>.....] - ETA: 11:15 - loss: 0.6209 - regression_loss: 0.5028 - classification_loss: 0.1180
 8523/10000 [========================>.....] - ETA: 11:14 - loss: 0.6209 - regression_loss: 0.5028 - classification_loss: 0.1180
 8524/10000 [========================>.....] - ETA: 11:14 - loss: 0.6209 - regression_loss: 0.5028 - classification_loss: 0.1180
 8525/10000 [========================>.....] - ETA: 11:13 - loss: 0.6208 - regression_loss: 0.5028 - classification_loss: 0.1180
 8526/10000 [========================>.....] - ETA: 11:13 - loss: 0.6208 - regression_loss: 0.5028 - classification_loss: 0.1180
 8527/10000 [========================>.....] - ETA: 11:13 - loss: 0.6209 - regression_loss: 0.5029 - classification_loss: 0.1180
 8528/10000 [========================>.....] - ETA: 11:12 - loss: 0.6209 - regression_loss: 0.5029 - classification_loss: 0.1180
 8529/10000 [========================>.....] - ETA: 11:12 - loss: 0.6209 - regression_loss: 0.5028 - classification_loss: 0.1180
 8530/10000 [========================>.....] - ETA: 11:11 - loss: 0.6208 - regression_loss: 0.5028 - classification_loss: 0.1180
 8531/10000 [========================>.....] - ETA: 11:11 - loss: 0.6208 - regression_loss: 0.5028 - classification_loss: 0.1180
 8532/10000 [========================>.....] - ETA: 11:10 - loss: 0.6208 - regression_loss: 0.5028 - classification_loss: 0.1180
 8533/10000 [========================>.....] - ETA: 11:10 - loss: 0.6207 - regression_loss: 0.5027 - classification_loss: 0.1180
 8534/10000 [========================>.....] - ETA: 11:09 - loss: 0.6208 - regression_loss: 0.5028 - classification_loss: 0.1180
 8535/10000 [========================>.....] - ETA: 11:09 - loss: 0.6208 - regression_loss: 0.5028 - classification_loss: 0.1180
 8536/10000 [========================>.....] - ETA: 11:08 - loss: 0.6209 - regression_loss: 0.5028 - classification_loss: 0.1181
 8537/10000 [========================>.....] - ETA: 11:08 - loss: 0.6208 - regression_loss: 0.5028 - classification_loss: 0.1180
 8538/10000 [========================>.....] - ETA: 11:08 - loss: 0.6208 - regression_loss: 0.5028 - classification_loss: 0.1180
 8539/10000 [========================>.....] - ETA: 11:07 - loss: 0.6207 - regression_loss: 0.5027 - classification_loss: 0.1180
 8540/10000 [========================>.....] - ETA: 11:07 - loss: 0.6207 - regression_loss: 0.5027 - classification_loss: 0.1180
 8541/10000 [========================>.....] - ETA: 11:06 - loss: 0.6207 - regression_loss: 0.5027 - classification_loss: 0.1180
 8542/10000 [========================>.....] - ETA: 11:06 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8543/10000 [========================>.....] - ETA: 11:05 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8544/10000 [========================>.....] - ETA: 11:05 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8545/10000 [========================>.....] - ETA: 11:04 - loss: 0.6205 - regression_loss: 0.5026 - classification_loss: 0.1180
 8546/10000 [========================>.....] - ETA: 11:04 - loss: 0.6207 - regression_loss: 0.5027 - classification_loss: 0.1180
 8547/10000 [========================>.....] - ETA: 11:03 - loss: 0.6207 - regression_loss: 0.5027 - classification_loss: 0.1180
 8548/10000 [========================>.....] - ETA: 11:03 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8549/10000 [========================>.....] - ETA: 11:03 - loss: 0.6207 - regression_loss: 0.5027 - classification_loss: 0.1180
 8550/10000 [========================>.....] - ETA: 11:02 - loss: 0.6206 - regression_loss: 0.5027 - classification_loss: 0.1180
 8551/10000 [========================>.....] - ETA: 11:02 - loss: 0.6207 - regression_loss: 0.5027 - classification_loss: 0.1180
 8552/10000 [========================>.....] - ETA: 11:01 - loss: 0.6207 - regression_loss: 0.5027 - classification_loss: 0.1180
 8553/10000 [========================>.....] - ETA: 11:01 - loss: 0.6207 - regression_loss: 0.5027 - classification_loss: 0.1180
 8554/10000 [========================>.....] - ETA: 11:00 - loss: 0.6206 - regression_loss: 0.5027 - classification_loss: 0.1180
 8555/10000 [========================>.....] - ETA: 11:00 - loss: 0.6206 - regression_loss: 0.5027 - classification_loss: 0.1180
 8556/10000 [========================>.....] - ETA: 10:59 - loss: 0.6206 - regression_loss: 0.5027 - classification_loss: 0.1180
 8557/10000 [========================>.....] - ETA: 10:59 - loss: 0.6207 - regression_loss: 0.5027 - classification_loss: 0.1180
 8558/10000 [========================>.....] - ETA: 10:58 - loss: 0.6206 - regression_loss: 0.5027 - classification_loss: 0.1180
 8559/10000 [========================>.....] - ETA: 10:58 - loss: 0.6207 - regression_loss: 0.5027 - classification_loss: 0.1180
 8560/10000 [========================>.....] - ETA: 10:58 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8561/10000 [========================>.....] - ETA: 10:57 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8562/10000 [========================>.....] - ETA: 10:57 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8563/10000 [========================>.....] - ETA: 10:56 - loss: 0.6205 - regression_loss: 0.5026 - classification_loss: 0.1180
 8564/10000 [========================>.....] - ETA: 10:56 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8565/10000 [========================>.....] - ETA: 10:55 - loss: 0.6204 - regression_loss: 0.5025 - classification_loss: 0.1180
 8566/10000 [========================>.....] - ETA: 10:55 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8567/10000 [========================>.....] - ETA: 10:54 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8568/10000 [========================>.....] - ETA: 10:54 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1180
 8569/10000 [========================>.....] - ETA: 10:53 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8570/10000 [========================>.....] - ETA: 10:53 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8571/10000 [========================>.....] - ETA: 10:52 - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1180
 8572/10000 [========================>.....] - ETA: 10:52 - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 8573/10000 [========================>.....] - ETA: 10:52 - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 8574/10000 [========================>.....] - ETA: 10:51 - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 8575/10000 [========================>.....] - ETA: 10:51 - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 8576/10000 [========================>.....] - ETA: 10:50 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8577/10000 [========================>.....] - ETA: 10:50 - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1180
 8578/10000 [========================>.....] - ETA: 10:49 - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 8579/10000 [========================>.....] - ETA: 10:49 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8580/10000 [========================>.....] - ETA: 10:48 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1180
 8581/10000 [========================>.....] - ETA: 10:48 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8582/10000 [========================>.....] - ETA: 10:47 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8583/10000 [========================>.....] - ETA: 10:47 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8584/10000 [========================>.....] - ETA: 10:46 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8585/10000 [========================>.....] - ETA: 10:46 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8586/10000 [========================>.....] - ETA: 10:46 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8587/10000 [========================>.....] - ETA: 10:45 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8588/10000 [========================>.....] - ETA: 10:45 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8589/10000 [========================>.....] - ETA: 10:44 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8590/10000 [========================>.....] - ETA: 10:44 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8591/10000 [========================>.....] - ETA: 10:43 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8592/10000 [========================>.....] - ETA: 10:43 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1181
 8593/10000 [========================>.....] - ETA: 10:42 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8594/10000 [========================>.....] - ETA: 10:42 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8595/10000 [========================>.....] - ETA: 10:41 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8596/10000 [========================>.....] - ETA: 10:41 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8597/10000 [========================>.....] - ETA: 10:41 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8598/10000 [========================>.....] - ETA: 10:40 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8599/10000 [========================>.....] - ETA: 10:40 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8600/10000 [========================>.....] - ETA: 10:39 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8601/10000 [========================>.....] - ETA: 10:39 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8602/10000 [========================>.....] - ETA: 10:38 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8603/10000 [========================>.....] - ETA: 10:38 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8604/10000 [========================>.....] - ETA: 10:37 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8605/10000 [========================>.....] - ETA: 10:37 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1181
 8606/10000 [========================>.....] - ETA: 10:36 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8607/10000 [========================>.....] - ETA: 10:36 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8608/10000 [========================>.....] - ETA: 10:35 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8609/10000 [========================>.....] - ETA: 10:35 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1180
 8610/10000 [========================>.....] - ETA: 10:35 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8611/10000 [========================>.....] - ETA: 10:34 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1180
 8612/10000 [========================>.....] - ETA: 10:34 - loss: 0.6202 - regression_loss: 0.5021 - classification_loss: 0.1180
 8613/10000 [========================>.....] - ETA: 10:33 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1181
 8614/10000 [========================>.....] - ETA: 10:33 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8615/10000 [========================>.....] - ETA: 10:32 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8616/10000 [========================>.....] - ETA: 10:32 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8617/10000 [========================>.....] - ETA: 10:31 - loss: 0.6201 - regression_loss: 0.5020 - classification_loss: 0.1180
 8618/10000 [========================>.....] - ETA: 10:31 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8619/10000 [========================>.....] - ETA: 10:30 - loss: 0.6201 - regression_loss: 0.5020 - classification_loss: 0.1180
 8620/10000 [========================>.....] - ETA: 10:30 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8621/10000 [========================>.....] - ETA: 10:30 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8622/10000 [========================>.....] - ETA: 10:29 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1180
 8623/10000 [========================>.....] - ETA: 10:29 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1180
 8624/10000 [========================>.....] - ETA: 10:28 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8625/10000 [========================>.....] - ETA: 10:28 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8626/10000 [========================>.....] - ETA: 10:27 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8627/10000 [========================>.....] - ETA: 10:27 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1180
 8628/10000 [========================>.....] - ETA: 10:26 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1180
 8629/10000 [========================>.....] - ETA: 10:26 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8630/10000 [========================>.....] - ETA: 10:25 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8631/10000 [========================>.....] - ETA: 10:25 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8632/10000 [========================>.....] - ETA: 10:25 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8633/10000 [========================>.....] - ETA: 10:24 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8634/10000 [========================>.....] - ETA: 10:24 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8635/10000 [========================>.....] - ETA: 10:23 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8636/10000 [========================>.....] - ETA: 10:23 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8637/10000 [========================>.....] - ETA: 10:22 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8638/10000 [========================>.....] - ETA: 10:22 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8639/10000 [========================>.....] - ETA: 10:21 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8640/10000 [========================>.....] - ETA: 10:21 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8641/10000 [========================>.....] - ETA: 10:20 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8642/10000 [========================>.....] - ETA: 10:20 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8643/10000 [========================>.....] - ETA: 10:20 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8644/10000 [========================>.....] - ETA: 10:19 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8645/10000 [========================>.....] - ETA: 10:19 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8646/10000 [========================>.....] - ETA: 10:18 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8647/10000 [========================>.....] - ETA: 10:18 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8648/10000 [========================>.....] - ETA: 10:17 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1181
 8649/10000 [========================>.....] - ETA: 10:17 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8650/10000 [========================>.....] - ETA: 10:16 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8651/10000 [========================>.....] - ETA: 10:16 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1181
 8652/10000 [========================>.....] - ETA: 10:15 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8653/10000 [========================>.....] - ETA: 10:15 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1181
 8654/10000 [========================>.....] - ETA: 10:14 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8655/10000 [========================>.....] - ETA: 10:14 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1181
 8656/10000 [========================>.....] - ETA: 10:14 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8657/10000 [========================>.....] - ETA: 10:13 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8658/10000 [========================>.....] - ETA: 10:13 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8659/10000 [========================>.....] - ETA: 10:12 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8660/10000 [========================>.....] - ETA: 10:12 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1181
 8661/10000 [========================>.....] - ETA: 10:11 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8662/10000 [========================>.....] - ETA: 10:11 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8663/10000 [========================>.....] - ETA: 10:10 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1180
 8664/10000 [========================>.....] - ETA: 10:10 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1180
 8665/10000 [========================>.....] - ETA: 10:09 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1180
 8666/10000 [========================>.....] - ETA: 10:09 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8667/10000 [=========================>....] - ETA: 10:09 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8668/10000 [=========================>....] - ETA: 10:08 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8669/10000 [=========================>....] - ETA: 10:08 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8670/10000 [=========================>....] - ETA: 10:07 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8671/10000 [=========================>....] - ETA: 10:07 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8672/10000 [=========================>....] - ETA: 10:06 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8673/10000 [=========================>....] - ETA: 10:06 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8674/10000 [=========================>....] - ETA: 10:05 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8675/10000 [=========================>....] - ETA: 10:05 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8676/10000 [=========================>....] - ETA: 10:04 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8677/10000 [=========================>....] - ETA: 10:04 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8678/10000 [=========================>....] - ETA: 10:03 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8679/10000 [=========================>....] - ETA: 10:03 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8680/10000 [=========================>....] - ETA: 10:03 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8681/10000 [=========================>....] - ETA: 10:02 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8682/10000 [=========================>....] - ETA: 10:02 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8683/10000 [=========================>....] - ETA: 10:01 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8684/10000 [=========================>....] - ETA: 10:01 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8685/10000 [=========================>....] - ETA: 10:00 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1181
 8686/10000 [=========================>....] - ETA: 10:00 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8687/10000 [=========================>....] - ETA: 9:59 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181 
 8688/10000 [=========================>....] - ETA: 9:59 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1181
 8689/10000 [=========================>....] - ETA: 9:58 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8690/10000 [=========================>....] - ETA: 9:58 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8691/10000 [=========================>....] - ETA: 9:58 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8692/10000 [=========================>....] - ETA: 9:57 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8693/10000 [=========================>....] - ETA: 9:57 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8694/10000 [=========================>....] - ETA: 9:56 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8695/10000 [=========================>....] - ETA: 9:56 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8696/10000 [=========================>....] - ETA: 9:55 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8697/10000 [=========================>....] - ETA: 9:55 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8698/10000 [=========================>....] - ETA: 9:54 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8699/10000 [=========================>....] - ETA: 9:54 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8700/10000 [=========================>....] - ETA: 9:53 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8701/10000 [=========================>....] - ETA: 9:53 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8702/10000 [=========================>....] - ETA: 9:53 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8703/10000 [=========================>....] - ETA: 9:52 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8704/10000 [=========================>....] - ETA: 9:52 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8705/10000 [=========================>....] - ETA: 9:51 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8706/10000 [=========================>....] - ETA: 9:51 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8707/10000 [=========================>....] - ETA: 9:50 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8708/10000 [=========================>....] - ETA: 9:50 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8709/10000 [=========================>....] - ETA: 9:49 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8710/10000 [=========================>....] - ETA: 9:49 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8711/10000 [=========================>....] - ETA: 9:48 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8712/10000 [=========================>....] - ETA: 9:48 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8713/10000 [=========================>....] - ETA: 9:47 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8714/10000 [=========================>....] - ETA: 9:47 - loss: 0.6208 - regression_loss: 0.5027 - classification_loss: 0.1181
 8715/10000 [=========================>....] - ETA: 9:47 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8716/10000 [=========================>....] - ETA: 9:46 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8717/10000 [=========================>....] - ETA: 9:46 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8718/10000 [=========================>....] - ETA: 9:45 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8719/10000 [=========================>....] - ETA: 9:45 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8720/10000 [=========================>....] - ETA: 9:44 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8721/10000 [=========================>....] - ETA: 9:44 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1181
 8722/10000 [=========================>....] - ETA: 9:43 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8723/10000 [=========================>....] - ETA: 9:43 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8724/10000 [=========================>....] - ETA: 9:42 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8725/10000 [=========================>....] - ETA: 9:42 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1181
 8726/10000 [=========================>....] - ETA: 9:42 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8727/10000 [=========================>....] - ETA: 9:41 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8728/10000 [=========================>....] - ETA: 9:41 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1181
 8729/10000 [=========================>....] - ETA: 9:40 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8730/10000 [=========================>....] - ETA: 9:40 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1181
 8731/10000 [=========================>....] - ETA: 9:39 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8732/10000 [=========================>....] - ETA: 9:39 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1181
 8733/10000 [=========================>....] - ETA: 9:38 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8734/10000 [=========================>....] - ETA: 9:38 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8735/10000 [=========================>....] - ETA: 9:37 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1181
 8736/10000 [=========================>....] - ETA: 9:37 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1181
 8737/10000 [=========================>....] - ETA: 9:37 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8738/10000 [=========================>....] - ETA: 9:36 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1181
 8739/10000 [=========================>....] - ETA: 9:36 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8740/10000 [=========================>....] - ETA: 9:35 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8741/10000 [=========================>....] - ETA: 9:35 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8742/10000 [=========================>....] - ETA: 9:34 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8743/10000 [=========================>....] - ETA: 9:34 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1181
 8744/10000 [=========================>....] - ETA: 9:33 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8745/10000 [=========================>....] - ETA: 9:33 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8746/10000 [=========================>....] - ETA: 9:32 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8747/10000 [=========================>....] - ETA: 9:32 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8748/10000 [=========================>....] - ETA: 9:32 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1181
 8749/10000 [=========================>....] - ETA: 9:31 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8750/10000 [=========================>....] - ETA: 9:31 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8751/10000 [=========================>....] - ETA: 9:30 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8752/10000 [=========================>....] - ETA: 9:30 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8753/10000 [=========================>....] - ETA: 9:29 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8754/10000 [=========================>....] - ETA: 9:29 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8755/10000 [=========================>....] - ETA: 9:28 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1181
 8756/10000 [=========================>....] - ETA: 9:28 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8757/10000 [=========================>....] - ETA: 9:27 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1181
 8758/10000 [=========================>....] - ETA: 9:27 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1181
 8759/10000 [=========================>....] - ETA: 9:26 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8760/10000 [=========================>....] - ETA: 9:26 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8761/10000 [=========================>....] - ETA: 9:26 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1181
 8762/10000 [=========================>....] - ETA: 9:25 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1181
 8763/10000 [=========================>....] - ETA: 9:25 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8764/10000 [=========================>....] - ETA: 9:24 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8765/10000 [=========================>....] - ETA: 9:24 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8766/10000 [=========================>....] - ETA: 9:23 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8767/10000 [=========================>....] - ETA: 9:23 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8768/10000 [=========================>....] - ETA: 9:22 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8769/10000 [=========================>....] - ETA: 9:22 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8770/10000 [=========================>....] - ETA: 9:21 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8771/10000 [=========================>....] - ETA: 9:21 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8772/10000 [=========================>....] - ETA: 9:21 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8773/10000 [=========================>....] - ETA: 9:20 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1181
 8774/10000 [=========================>....] - ETA: 9:20 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1181
 8775/10000 [=========================>....] - ETA: 9:19 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1181
 8776/10000 [=========================>....] - ETA: 9:19 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1180
 8777/10000 [=========================>....] - ETA: 9:18 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8778/10000 [=========================>....] - ETA: 9:18 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8779/10000 [=========================>....] - ETA: 9:17 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8780/10000 [=========================>....] - ETA: 9:17 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8781/10000 [=========================>....] - ETA: 9:16 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8782/10000 [=========================>....] - ETA: 9:16 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8783/10000 [=========================>....] - ETA: 9:15 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8784/10000 [=========================>....] - ETA: 9:15 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8785/10000 [=========================>....] - ETA: 9:15 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8786/10000 [=========================>....] - ETA: 9:14 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8787/10000 [=========================>....] - ETA: 9:14 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8788/10000 [=========================>....] - ETA: 9:13 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8789/10000 [=========================>....] - ETA: 9:13 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8790/10000 [=========================>....] - ETA: 9:12 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1181
 8791/10000 [=========================>....] - ETA: 9:12 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8792/10000 [=========================>....] - ETA: 9:11 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8793/10000 [=========================>....] - ETA: 9:11 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1181
 8794/10000 [=========================>....] - ETA: 9:10 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1181
 8795/10000 [=========================>....] - ETA: 9:10 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1181
 8796/10000 [=========================>....] - ETA: 9:10 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1181
 8797/10000 [=========================>....] - ETA: 9:09 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1181
 8798/10000 [=========================>....] - ETA: 9:09 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1181
 8799/10000 [=========================>....] - ETA: 9:08 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8800/10000 [=========================>....] - ETA: 9:08 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1180
 8801/10000 [=========================>....] - ETA: 9:07 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1180
 8802/10000 [=========================>....] - ETA: 9:07 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1180
 8803/10000 [=========================>....] - ETA: 9:06 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1181
 8804/10000 [=========================>....] - ETA: 9:06 - loss: 0.6203 - regression_loss: 0.5022 - classification_loss: 0.1181
 8805/10000 [=========================>....] - ETA: 9:05 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1180
 8806/10000 [=========================>....] - ETA: 9:05 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1180
 8807/10000 [=========================>....] - ETA: 9:05 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1180
 8808/10000 [=========================>....] - ETA: 9:04 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1180
 8809/10000 [=========================>....] - ETA: 9:04 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8810/10000 [=========================>....] - ETA: 9:03 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8811/10000 [=========================>....] - ETA: 9:03 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1180
 8812/10000 [=========================>....] - ETA: 9:02 - loss: 0.6202 - regression_loss: 0.5021 - classification_loss: 0.1180
 8813/10000 [=========================>....] - ETA: 9:02 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8814/10000 [=========================>....] - ETA: 9:01 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1180
 8815/10000 [=========================>....] - ETA: 9:01 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1180
 8816/10000 [=========================>....] - ETA: 9:00 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1180
 8817/10000 [=========================>....] - ETA: 9:00 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8818/10000 [=========================>....] - ETA: 8:59 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8819/10000 [=========================>....] - ETA: 8:59 - loss: 0.6200 - regression_loss: 0.5021 - classification_loss: 0.1180
 8820/10000 [=========================>....] - ETA: 8:59 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8821/10000 [=========================>....] - ETA: 8:58 - loss: 0.6200 - regression_loss: 0.5021 - classification_loss: 0.1180
 8822/10000 [=========================>....] - ETA: 8:58 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8823/10000 [=========================>....] - ETA: 8:57 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8824/10000 [=========================>....] - ETA: 8:57 - loss: 0.6199 - regression_loss: 0.5019 - classification_loss: 0.1180
 8825/10000 [=========================>....] - ETA: 8:56 - loss: 0.6199 - regression_loss: 0.5019 - classification_loss: 0.1180
 8826/10000 [=========================>....] - ETA: 8:56 - loss: 0.6199 - regression_loss: 0.5019 - classification_loss: 0.1180
 8827/10000 [=========================>....] - ETA: 8:55 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8828/10000 [=========================>....] - ETA: 8:55 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8829/10000 [=========================>....] - ETA: 8:54 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8830/10000 [=========================>....] - ETA: 8:54 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8831/10000 [=========================>....] - ETA: 8:54 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8832/10000 [=========================>....] - ETA: 8:53 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1180
 8833/10000 [=========================>....] - ETA: 8:53 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8834/10000 [=========================>....] - ETA: 8:52 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8835/10000 [=========================>....] - ETA: 8:52 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8836/10000 [=========================>....] - ETA: 8:51 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8837/10000 [=========================>....] - ETA: 8:51 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8838/10000 [=========================>....] - ETA: 8:50 - loss: 0.6201 - regression_loss: 0.5022 - classification_loss: 0.1180
 8839/10000 [=========================>....] - ETA: 8:50 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8840/10000 [=========================>....] - ETA: 8:49 - loss: 0.6200 - regression_loss: 0.5021 - classification_loss: 0.1180
 8841/10000 [=========================>....] - ETA: 8:49 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8842/10000 [=========================>....] - ETA: 8:49 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1180
 8843/10000 [=========================>....] - ETA: 8:48 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8844/10000 [=========================>....] - ETA: 8:48 - loss: 0.6201 - regression_loss: 0.5022 - classification_loss: 0.1180
 8845/10000 [=========================>....] - ETA: 8:47 - loss: 0.6201 - regression_loss: 0.5022 - classification_loss: 0.1180
 8846/10000 [=========================>....] - ETA: 8:47 - loss: 0.6201 - regression_loss: 0.5022 - classification_loss: 0.1180
 8847/10000 [=========================>....] - ETA: 8:46 - loss: 0.6201 - regression_loss: 0.5022 - classification_loss: 0.1180
 8848/10000 [=========================>....] - ETA: 8:46 - loss: 0.6201 - regression_loss: 0.5022 - classification_loss: 0.1180
 8849/10000 [=========================>....] - ETA: 8:45 - loss: 0.6201 - regression_loss: 0.5022 - classification_loss: 0.1180
 8850/10000 [=========================>....] - ETA: 8:45 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8851/10000 [=========================>....] - ETA: 8:44 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1180
 8852/10000 [=========================>....] - ETA: 8:44 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1179
 8853/10000 [=========================>....] - ETA: 8:43 - loss: 0.6201 - regression_loss: 0.5021 - classification_loss: 0.1179
 8854/10000 [=========================>....] - ETA: 8:43 - loss: 0.6201 - regression_loss: 0.5022 - classification_loss: 0.1179
 8855/10000 [=========================>....] - ETA: 8:43 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1179
 8856/10000 [=========================>....] - ETA: 8:42 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8857/10000 [=========================>....] - ETA: 8:42 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8858/10000 [=========================>....] - ETA: 8:41 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8859/10000 [=========================>....] - ETA: 8:41 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8860/10000 [=========================>....] - ETA: 8:40 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8861/10000 [=========================>....] - ETA: 8:40 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8862/10000 [=========================>....] - ETA: 8:39 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8863/10000 [=========================>....] - ETA: 8:39 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8864/10000 [=========================>....] - ETA: 8:38 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1180
 8865/10000 [=========================>....] - ETA: 8:38 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8866/10000 [=========================>....] - ETA: 8:38 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8867/10000 [=========================>....] - ETA: 8:37 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8868/10000 [=========================>....] - ETA: 8:37 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1181
 8869/10000 [=========================>....] - ETA: 8:36 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1181
 8870/10000 [=========================>....] - ETA: 8:36 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1180
 8871/10000 [=========================>....] - ETA: 8:35 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8872/10000 [=========================>....] - ETA: 8:35 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1180
 8873/10000 [=========================>....] - ETA: 8:34 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8874/10000 [=========================>....] - ETA: 8:34 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8875/10000 [=========================>....] - ETA: 8:33 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8876/10000 [=========================>....] - ETA: 8:33 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8877/10000 [=========================>....] - ETA: 8:33 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8878/10000 [=========================>....] - ETA: 8:32 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1180
 8879/10000 [=========================>....] - ETA: 8:32 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8880/10000 [=========================>....] - ETA: 8:31 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8881/10000 [=========================>....] - ETA: 8:31 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1181
 8882/10000 [=========================>....] - ETA: 8:30 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1180
 8883/10000 [=========================>....] - ETA: 8:30 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1180
 8884/10000 [=========================>....] - ETA: 8:29 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1181
 8885/10000 [=========================>....] - ETA: 8:29 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8886/10000 [=========================>....] - ETA: 8:28 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8887/10000 [=========================>....] - ETA: 8:28 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8888/10000 [=========================>....] - ETA: 8:28 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8889/10000 [=========================>....] - ETA: 8:27 - loss: 0.6207 - regression_loss: 0.5027 - classification_loss: 0.1181
 8890/10000 [=========================>....] - ETA: 8:27 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8891/10000 [=========================>....] - ETA: 8:26 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8892/10000 [=========================>....] - ETA: 8:26 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8893/10000 [=========================>....] - ETA: 8:25 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8894/10000 [=========================>....] - ETA: 8:25 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1181
 8895/10000 [=========================>....] - ETA: 8:24 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8896/10000 [=========================>....] - ETA: 8:24 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8897/10000 [=========================>....] - ETA: 8:23 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1180
 8898/10000 [=========================>....] - ETA: 8:23 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1180
 8899/10000 [=========================>....] - ETA: 8:22 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8900/10000 [=========================>....] - ETA: 8:22 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8901/10000 [=========================>....] - ETA: 8:22 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1180
 8902/10000 [=========================>....] - ETA: 8:21 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8903/10000 [=========================>....] - ETA: 8:21 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8904/10000 [=========================>....] - ETA: 8:20 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8905/10000 [=========================>....] - ETA: 8:20 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8906/10000 [=========================>....] - ETA: 8:19 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8907/10000 [=========================>....] - ETA: 8:19 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8908/10000 [=========================>....] - ETA: 8:18 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8909/10000 [=========================>....] - ETA: 8:18 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1180
 8910/10000 [=========================>....] - ETA: 8:17 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1180
 8911/10000 [=========================>....] - ETA: 8:17 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1181
 8912/10000 [=========================>....] - ETA: 8:17 - loss: 0.6207 - regression_loss: 0.5027 - classification_loss: 0.1181
 8913/10000 [=========================>....] - ETA: 8:16 - loss: 0.6207 - regression_loss: 0.5027 - classification_loss: 0.1180
 8914/10000 [=========================>....] - ETA: 8:16 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1180
 8915/10000 [=========================>....] - ETA: 8:15 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8916/10000 [=========================>....] - ETA: 8:15 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8917/10000 [=========================>....] - ETA: 8:14 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8918/10000 [=========================>....] - ETA: 8:14 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8919/10000 [=========================>....] - ETA: 8:13 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8920/10000 [=========================>....] - ETA: 8:13 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8921/10000 [=========================>....] - ETA: 8:12 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8922/10000 [=========================>....] - ETA: 8:12 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8923/10000 [=========================>....] - ETA: 8:11 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8924/10000 [=========================>....] - ETA: 8:11 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1180
 8925/10000 [=========================>....] - ETA: 8:11 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8926/10000 [=========================>....] - ETA: 8:10 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8927/10000 [=========================>....] - ETA: 8:10 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1180
 8928/10000 [=========================>....] - ETA: 8:09 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8929/10000 [=========================>....] - ETA: 8:09 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1180
 8930/10000 [=========================>....] - ETA: 8:08 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8931/10000 [=========================>....] - ETA: 8:08 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8932/10000 [=========================>....] - ETA: 8:07 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8933/10000 [=========================>....] - ETA: 8:07 - loss: 0.6207 - regression_loss: 0.5026 - classification_loss: 0.1180
 8934/10000 [=========================>....] - ETA: 8:06 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8935/10000 [=========================>....] - ETA: 8:06 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8936/10000 [=========================>....] - ETA: 8:06 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8937/10000 [=========================>....] - ETA: 8:05 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8938/10000 [=========================>....] - ETA: 8:05 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8939/10000 [=========================>....] - ETA: 8:04 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1180
 8940/10000 [=========================>....] - ETA: 8:04 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8941/10000 [=========================>....] - ETA: 8:03 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1180
 8942/10000 [=========================>....] - ETA: 8:03 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8943/10000 [=========================>....] - ETA: 8:02 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8944/10000 [=========================>....] - ETA: 8:02 - loss: 0.6204 - regression_loss: 0.5025 - classification_loss: 0.1180
 8945/10000 [=========================>....] - ETA: 8:01 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8946/10000 [=========================>....] - ETA: 8:01 - loss: 0.6204 - regression_loss: 0.5025 - classification_loss: 0.1180
 8947/10000 [=========================>....] - ETA: 8:01 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8948/10000 [=========================>....] - ETA: 8:00 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8949/10000 [=========================>....] - ETA: 8:00 - loss: 0.6204 - regression_loss: 0.5025 - classification_loss: 0.1180
 8950/10000 [=========================>....] - ETA: 7:59 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8951/10000 [=========================>....] - ETA: 7:59 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8952/10000 [=========================>....] - ETA: 7:58 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1180
 8953/10000 [=========================>....] - ETA: 7:58 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 8954/10000 [=========================>....] - ETA: 7:57 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8955/10000 [=========================>....] - ETA: 7:57 - loss: 0.6204 - regression_loss: 0.5025 - classification_loss: 0.1180
 8956/10000 [=========================>....] - ETA: 7:56 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8957/10000 [=========================>....] - ETA: 7:56 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8958/10000 [=========================>....] - ETA: 7:55 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8959/10000 [=========================>....] - ETA: 7:55 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8960/10000 [=========================>....] - ETA: 7:55 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8961/10000 [=========================>....] - ETA: 7:54 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8962/10000 [=========================>....] - ETA: 7:54 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8963/10000 [=========================>....] - ETA: 7:53 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8964/10000 [=========================>....] - ETA: 7:53 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8965/10000 [=========================>....] - ETA: 7:52 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8966/10000 [=========================>....] - ETA: 7:52 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1180
 8967/10000 [=========================>....] - ETA: 7:51 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1180
 8968/10000 [=========================>....] - ETA: 7:51 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8969/10000 [=========================>....] - ETA: 7:50 - loss: 0.6205 - regression_loss: 0.5024 - classification_loss: 0.1180
 8970/10000 [=========================>....] - ETA: 7:50 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8971/10000 [=========================>....] - ETA: 7:50 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8972/10000 [=========================>....] - ETA: 7:49 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8973/10000 [=========================>....] - ETA: 7:49 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8974/10000 [=========================>....] - ETA: 7:48 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8975/10000 [=========================>....] - ETA: 7:48 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8976/10000 [=========================>....] - ETA: 7:47 - loss: 0.6204 - regression_loss: 0.5023 - classification_loss: 0.1180
 8977/10000 [=========================>....] - ETA: 7:47 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8978/10000 [=========================>....] - ETA: 7:46 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8979/10000 [=========================>....] - ETA: 7:46 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8980/10000 [=========================>....] - ETA: 7:45 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8981/10000 [=========================>....] - ETA: 7:45 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8982/10000 [=========================>....] - ETA: 7:45 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8983/10000 [=========================>....] - ETA: 7:44 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8984/10000 [=========================>....] - ETA: 7:44 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 8985/10000 [=========================>....] - ETA: 7:43 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8986/10000 [=========================>....] - ETA: 7:43 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8987/10000 [=========================>....] - ETA: 7:42 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8988/10000 [=========================>....] - ETA: 7:42 - loss: 0.6206 - regression_loss: 0.5025 - classification_loss: 0.1180
 8989/10000 [=========================>....] - ETA: 7:41 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8990/10000 [=========================>....] - ETA: 7:41 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8991/10000 [=========================>....] - ETA: 7:40 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8992/10000 [=========================>....] - ETA: 7:40 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8993/10000 [=========================>....] - ETA: 7:39 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8994/10000 [=========================>....] - ETA: 7:39 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8995/10000 [=========================>....] - ETA: 7:39 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8996/10000 [=========================>....] - ETA: 7:38 - loss: 0.6206 - regression_loss: 0.5026 - classification_loss: 0.1180
 8997/10000 [=========================>....] - ETA: 7:38 - loss: 0.6205 - regression_loss: 0.5026 - classification_loss: 0.1180
 8998/10000 [=========================>....] - ETA: 7:37 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 8999/10000 [=========================>....] - ETA: 7:37 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 9000/10000 [==========================>...] - ETA: 7:36 - loss: 0.6204 - regression_loss: 0.5025 - classification_loss: 0.1180
 9001/10000 [==========================>...] - ETA: 7:36 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 9002/10000 [==========================>...] - ETA: 7:35 - loss: 0.6204 - regression_loss: 0.5025 - classification_loss: 0.1180
 9003/10000 [==========================>...] - ETA: 7:35 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 9004/10000 [==========================>...] - ETA: 7:34 - loss: 0.6205 - regression_loss: 0.5025 - classification_loss: 0.1180
 9005/10000 [==========================>...] - ETA: 7:34 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 9006/10000 [==========================>...] - ETA: 7:34 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 9007/10000 [==========================>...] - ETA: 7:33 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1180
 9008/10000 [==========================>...] - ETA: 7:33 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 9009/10000 [==========================>...] - ETA: 7:32 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1180
 9010/10000 [==========================>...] - ETA: 7:32 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 9011/10000 [==========================>...] - ETA: 7:31 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 9012/10000 [==========================>...] - ETA: 7:31 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 9013/10000 [==========================>...] - ETA: 7:30 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 9014/10000 [==========================>...] - ETA: 7:30 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 9015/10000 [==========================>...] - ETA: 7:29 - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1180
 9016/10000 [==========================>...] - ETA: 7:29 - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1180
 9017/10000 [==========================>...] - ETA: 7:29 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1180
 9018/10000 [==========================>...] - ETA: 7:28 - loss: 0.6201 - regression_loss: 0.5022 - classification_loss: 0.1179
 9019/10000 [==========================>...] - ETA: 7:28 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1179
 9020/10000 [==========================>...] - ETA: 7:27 - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1180
 9021/10000 [==========================>...] - ETA: 7:27 - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1180
 9022/10000 [==========================>...] - ETA: 7:26 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1180
 9023/10000 [==========================>...] - ETA: 7:26 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1180
 9024/10000 [==========================>...] - ETA: 7:25 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1180
 9025/10000 [==========================>...] - ETA: 7:25 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 9026/10000 [==========================>...] - ETA: 7:24 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 9027/10000 [==========================>...] - ETA: 7:24 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 9028/10000 [==========================>...] - ETA: 7:23 - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1180
 9029/10000 [==========================>...] - ETA: 7:23 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 9030/10000 [==========================>...] - ETA: 7:23 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 9031/10000 [==========================>...] - ETA: 7:22 - loss: 0.6204 - regression_loss: 0.5024 - classification_loss: 0.1180
 9032/10000 [==========================>...] - ETA: 7:22 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1180
 9033/10000 [==========================>...] - ETA: 7:21 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1180
 9034/10000 [==========================>...] - ETA: 7:21 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1180
 9035/10000 [==========================>...] - ETA: 7:20 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1180
 9036/10000 [==========================>...] - ETA: 7:20 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1180
 9037/10000 [==========================>...] - ETA: 7:19 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 9038/10000 [==========================>...] - ETA: 7:19 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1179
 9039/10000 [==========================>...] - ETA: 7:18 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1179
 9040/10000 [==========================>...] - ETA: 7:18 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1179
 9041/10000 [==========================>...] - ETA: 7:18 - loss: 0.6203 - regression_loss: 0.5023 - classification_loss: 0.1179
 9042/10000 [==========================>...] - ETA: 7:17 - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9043/10000 [==========================>...] - ETA: 7:17 - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9044/10000 [==========================>...] - ETA: 7:16 - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9045/10000 [==========================>...] - ETA: 7:16 - loss: 0.6202 - regression_loss: 0.5022 - classification_loss: 0.1179
 9046/10000 [==========================>...] - ETA: 7:15 - loss: 0.6201 - regression_loss: 0.5022 - classification_loss: 0.1179
 9047/10000 [==========================>...] - ETA: 7:15 - loss: 0.6201 - regression_loss: 0.5022 - classification_loss: 0.1179
 9048/10000 [==========================>...] - ETA: 7:14 - loss: 0.6200 - regression_loss: 0.5021 - classification_loss: 0.1179
 9049/10000 [==========================>...] - ETA: 7:14 - loss: 0.6200 - regression_loss: 0.5021 - classification_loss: 0.1179
 9050/10000 [==========================>...] - ETA: 7:13 - loss: 0.6200 - regression_loss: 0.5021 - classification_loss: 0.1179
 9051/10000 [==========================>...] - ETA: 7:13 - loss: 0.6200 - regression_loss: 0.5021 - classification_loss: 0.1179
 9052/10000 [==========================>...] - ETA: 7:13 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9053/10000 [==========================>...] - ETA: 7:12 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9054/10000 [==========================>...] - ETA: 7:12 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9055/10000 [==========================>...] - ETA: 7:11 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9056/10000 [==========================>...] - ETA: 7:11 - loss: 0.6198 - regression_loss: 0.5020 - classification_loss: 0.1179
 9057/10000 [==========================>...] - ETA: 7:10 - loss: 0.6198 - regression_loss: 0.5020 - classification_loss: 0.1179
 9058/10000 [==========================>...] - ETA: 7:10 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9059/10000 [==========================>...] - ETA: 7:09 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9060/10000 [==========================>...] - ETA: 7:09 - loss: 0.6197 - regression_loss: 0.5019 - classification_loss: 0.1179
 9061/10000 [==========================>...] - ETA: 7:08 - loss: 0.6197 - regression_loss: 0.5018 - classification_loss: 0.1179
 9062/10000 [==========================>...] - ETA: 7:08 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9063/10000 [==========================>...] - ETA: 7:07 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9064/10000 [==========================>...] - ETA: 7:07 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9065/10000 [==========================>...] - ETA: 7:07 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9066/10000 [==========================>...] - ETA: 7:06 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9067/10000 [==========================>...] - ETA: 7:06 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9068/10000 [==========================>...] - ETA: 7:05 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9069/10000 [==========================>...] - ETA: 7:05 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9070/10000 [==========================>...] - ETA: 7:04 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9071/10000 [==========================>...] - ETA: 7:04 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9072/10000 [==========================>...] - ETA: 7:03 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9073/10000 [==========================>...] - ETA: 7:03 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9074/10000 [==========================>...] - ETA: 7:02 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9075/10000 [==========================>...] - ETA: 7:02 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9076/10000 [==========================>...] - ETA: 7:02 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9077/10000 [==========================>...] - ETA: 7:01 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9078/10000 [==========================>...] - ETA: 7:01 - loss: 0.6197 - regression_loss: 0.5019 - classification_loss: 0.1179
 9079/10000 [==========================>...] - ETA: 7:00 - loss: 0.6197 - regression_loss: 0.5018 - classification_loss: 0.1179
 9080/10000 [==========================>...] - ETA: 7:00 - loss: 0.6197 - regression_loss: 0.5018 - classification_loss: 0.1179
 9081/10000 [==========================>...] - ETA: 6:59 - loss: 0.6197 - regression_loss: 0.5018 - classification_loss: 0.1179
 9082/10000 [==========================>...] - ETA: 6:59 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9083/10000 [==========================>...] - ETA: 6:58 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9084/10000 [==========================>...] - ETA: 6:58 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9085/10000 [==========================>...] - ETA: 6:57 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9086/10000 [==========================>...] - ETA: 6:57 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9087/10000 [==========================>...] - ETA: 6:57 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9088/10000 [==========================>...] - ETA: 6:56 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9089/10000 [==========================>...] - ETA: 6:56 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9090/10000 [==========================>...] - ETA: 6:55 - loss: 0.6197 - regression_loss: 0.5019 - classification_loss: 0.1179
 9091/10000 [==========================>...] - ETA: 6:55 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9092/10000 [==========================>...] - ETA: 6:54 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9093/10000 [==========================>...] - ETA: 6:54 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9094/10000 [==========================>...] - ETA: 6:53 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9095/10000 [==========================>...] - ETA: 6:53 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9096/10000 [==========================>...] - ETA: 6:52 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9097/10000 [==========================>...] - ETA: 6:52 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9098/10000 [==========================>...] - ETA: 6:52 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9099/10000 [==========================>...] - ETA: 6:51 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9100/10000 [==========================>...] - ETA: 6:51 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9101/10000 [==========================>...] - ETA: 6:50 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9102/10000 [==========================>...] - ETA: 6:50 - loss: 0.6197 - regression_loss: 0.5018 - classification_loss: 0.1179
 9103/10000 [==========================>...] - ETA: 6:49 - loss: 0.6197 - regression_loss: 0.5018 - classification_loss: 0.1179
 9104/10000 [==========================>...] - ETA: 6:49 - loss: 0.6197 - regression_loss: 0.5018 - classification_loss: 0.1179
 9105/10000 [==========================>...] - ETA: 6:48 - loss: 0.6197 - regression_loss: 0.5018 - classification_loss: 0.1179
 9106/10000 [==========================>...] - ETA: 6:48 - loss: 0.6197 - regression_loss: 0.5018 - classification_loss: 0.1179
 9107/10000 [==========================>...] - ETA: 6:47 - loss: 0.6197 - regression_loss: 0.5018 - classification_loss: 0.1179
 9108/10000 [==========================>...] - ETA: 6:47 - loss: 0.6196 - regression_loss: 0.5018 - classification_loss: 0.1179
 9109/10000 [==========================>...] - ETA: 6:47 - loss: 0.6197 - regression_loss: 0.5018 - classification_loss: 0.1179
 9110/10000 [==========================>...] - ETA: 6:46 - loss: 0.6197 - regression_loss: 0.5018 - classification_loss: 0.1179
 9111/10000 [==========================>...] - ETA: 6:46 - loss: 0.6197 - regression_loss: 0.5018 - classification_loss: 0.1179
 9112/10000 [==========================>...] - ETA: 6:45 - loss: 0.6197 - regression_loss: 0.5018 - classification_loss: 0.1179
 9113/10000 [==========================>...] - ETA: 6:45 - loss: 0.6196 - regression_loss: 0.5017 - classification_loss: 0.1179
 9114/10000 [==========================>...] - ETA: 6:44 - loss: 0.6196 - regression_loss: 0.5017 - classification_loss: 0.1179
 9115/10000 [==========================>...] - ETA: 6:44 - loss: 0.6196 - regression_loss: 0.5017 - classification_loss: 0.1179
 9116/10000 [==========================>...] - ETA: 6:43 - loss: 0.6195 - regression_loss: 0.5017 - classification_loss: 0.1178
 9117/10000 [==========================>...] - ETA: 6:43 - loss: 0.6196 - regression_loss: 0.5017 - classification_loss: 0.1179
 9118/10000 [==========================>...] - ETA: 6:42 - loss: 0.6196 - regression_loss: 0.5018 - classification_loss: 0.1179
 9119/10000 [==========================>...] - ETA: 6:42 - loss: 0.6196 - regression_loss: 0.5018 - classification_loss: 0.1179
 9120/10000 [==========================>...] - ETA: 6:41 - loss: 0.6196 - regression_loss: 0.5018 - classification_loss: 0.1179
 9121/10000 [==========================>...] - ETA: 6:41 - loss: 0.6196 - regression_loss: 0.5018 - classification_loss: 0.1178
 9122/10000 [==========================>...] - ETA: 6:41 - loss: 0.6197 - regression_loss: 0.5019 - classification_loss: 0.1179
 9123/10000 [==========================>...] - ETA: 6:40 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9124/10000 [==========================>...] - ETA: 6:40 - loss: 0.6198 - regression_loss: 0.5020 - classification_loss: 0.1179
 9125/10000 [==========================>...] - ETA: 6:39 - loss: 0.6200 - regression_loss: 0.5020 - classification_loss: 0.1179
 9126/10000 [==========================>...] - ETA: 6:39 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9127/10000 [==========================>...] - ETA: 6:38 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9128/10000 [==========================>...] - ETA: 6:38 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9129/10000 [==========================>...] - ETA: 6:37 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9130/10000 [==========================>...] - ETA: 6:37 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9131/10000 [==========================>...] - ETA: 6:36 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9132/10000 [==========================>...] - ETA: 6:36 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9133/10000 [==========================>...] - ETA: 6:36 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9134/10000 [==========================>...] - ETA: 6:35 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9135/10000 [==========================>...] - ETA: 6:35 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9136/10000 [==========================>...] - ETA: 6:34 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9137/10000 [==========================>...] - ETA: 6:34 - loss: 0.6198 - regression_loss: 0.5020 - classification_loss: 0.1179
 9138/10000 [==========================>...] - ETA: 6:33 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9139/10000 [==========================>...] - ETA: 6:33 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9140/10000 [==========================>...] - ETA: 6:32 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9141/10000 [==========================>...] - ETA: 6:32 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9142/10000 [==========================>...] - ETA: 6:31 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9143/10000 [==========================>...] - ETA: 6:31 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9144/10000 [==========================>...] - ETA: 6:31 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9145/10000 [==========================>...] - ETA: 6:30 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9146/10000 [==========================>...] - ETA: 6:30 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9147/10000 [==========================>...] - ETA: 6:29 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9148/10000 [==========================>...] - ETA: 6:29 - loss: 0.6199 - regression_loss: 0.5020 - classification_loss: 0.1179
 9149/10000 [==========================>...] - ETA: 6:28 - loss: 0.6198 - regression_loss: 0.5020 - classification_loss: 0.1179
 9150/10000 [==========================>...] - ETA: 6:28 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9151/10000 [==========================>...] - ETA: 6:27 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9152/10000 [==========================>...] - ETA: 6:27 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9153/10000 [==========================>...] - ETA: 6:26 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9154/10000 [==========================>...] - ETA: 6:26 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9155/10000 [==========================>...] - ETA: 6:25 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9156/10000 [==========================>...] - ETA: 6:25 - loss: 0.6197 - regression_loss: 0.5019 - classification_loss: 0.1179
 9157/10000 [==========================>...] - ETA: 6:25 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1179
 9158/10000 [==========================>...] - ETA: 6:24 - loss: 0.6197 - regression_loss: 0.5019 - classification_loss: 0.1179
 9159/10000 [==========================>...] - ETA: 6:24 - loss: 0.6197 - regression_loss: 0.5018 - classification_loss: 0.1179
 9160/10000 [==========================>...] - ETA: 6:23 - loss: 0.6197 - regression_loss: 0.5018 - classification_loss: 0.1179
 9161/10000 [==========================>...] - ETA: 6:23 - loss: 0.6197 - regression_loss: 0.5018 - classification_loss: 0.1179
 9162/10000 [==========================>...] - ETA: 6:22 - loss: 0.6197 - regression_loss: 0.5018 - classification_loss: 0.1179
 9163/10000 [==========================>...] - ETA: 6:22 - loss: 0.6197 - regression_loss: 0.5018 - classification_loss: 0.1179
 9164/10000 [==========================>...] - ETA: 6:21 - loss: 0.6197 - regression_loss: 0.5018 - classification_loss: 0.1179
 9165/10000 [==========================>...] - ETA: 6:21 - loss: 0.6197 - regression_loss: 0.5018 - classification_loss: 0.1179
 9166/10000 [==========================>...] - ETA: 6:20 - loss: 0.6196 - regression_loss: 0.5018 - classification_loss: 0.1178
 9167/10000 [==========================>...] - ETA: 6:20 - loss: 0.6197 - regression_loss: 0.5018 - classification_loss: 0.1178
 9168/10000 [==========================>...] - ETA: 6:20 - loss: 0.6196 - regression_loss: 0.5018 - classification_loss: 0.1178
 9169/10000 [==========================>...] - ETA: 6:19 - loss: 0.6196 - regression_loss: 0.5018 - classification_loss: 0.1178
 9170/10000 [==========================>...] - ETA: 6:19 - loss: 0.6198 - regression_loss: 0.5019 - classification_loss: 0.1178
 9171/10000 [==========================>...] - ETA: 6:18 - loss: 0.6198 - regression_loss: 0.5020 - classification_loss: 0.1178
 9172/10000 [==========================>...] - ETA: 6:18 - loss: 0.6198 - regression_loss: 0.5020 - classification_loss: 0.1178
 9173/10000 [==========================>...] - ETA: 6:17 - loss: 0.6200 - regression_loss: 0.5021 - classification_loss: 0.1179
 9174/10000 [==========================>...] - ETA: 6:17 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1179
 9175/10000 [==========================>...] - ETA: 6:16 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9176/10000 [==========================>...] - ETA: 6:16 - loss: 0.6200 - regression_loss: 0.5021 - classification_loss: 0.1178
 9177/10000 [==========================>...] - ETA: 6:15 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9178/10000 [==========================>...] - ETA: 6:15 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9179/10000 [==========================>...] - ETA: 6:15 - loss: 0.6200 - regression_loss: 0.5021 - classification_loss: 0.1178
 9180/10000 [==========================>...] - ETA: 6:14 - loss: 0.6199 - regression_loss: 0.5021 - classification_loss: 0.1178
 9181/10000 [==========================>...] - ETA: 6:14 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9182/10000 [==========================>...] - ETA: 6:13 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9183/10000 [==========================>...] - ETA: 6:13 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9184/10000 [==========================>...] - ETA: 6:12 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9185/10000 [==========================>...] - ETA: 6:12 - loss: 0.6200 - regression_loss: 0.5021 - classification_loss: 0.1178
 9186/10000 [==========================>...] - ETA: 6:11 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9187/10000 [==========================>...] - ETA: 6:11 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9188/10000 [==========================>...] - ETA: 6:10 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9189/10000 [==========================>...] - ETA: 6:10 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9190/10000 [==========================>...] - ETA: 6:10 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9191/10000 [==========================>...] - ETA: 6:09 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9192/10000 [==========================>...] - ETA: 6:09 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9193/10000 [==========================>...] - ETA: 6:08 - loss: 0.6201 - regression_loss: 0.5022 - classification_loss: 0.1178
 9194/10000 [==========================>...] - ETA: 6:08 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9195/10000 [==========================>...] - ETA: 6:07 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9196/10000 [==========================>...] - ETA: 6:07 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9197/10000 [==========================>...] - ETA: 6:06 - loss: 0.6201 - regression_loss: 0.5022 - classification_loss: 0.1178
 9198/10000 [==========================>...] - ETA: 6:06 - loss: 0.6201 - regression_loss: 0.5022 - classification_loss: 0.1178
 9199/10000 [==========================>...] - ETA: 6:05 - loss: 0.6201 - regression_loss: 0.5022 - classification_loss: 0.1178
 9200/10000 [==========================>...] - ETA: 6:05 - loss: 0.6201 - regression_loss: 0.5022 - classification_loss: 0.1178
 9201/10000 [==========================>...] - ETA: 6:04 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1179
 9202/10000 [==========================>...] - ETA: 6:04 - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9203/10000 [==========================>...] - ETA: 6:04 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1179
 9204/10000 [==========================>...] - ETA: 6:03 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9205/10000 [==========================>...] - ETA: 6:03 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9206/10000 [==========================>...] - ETA: 6:02 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9207/10000 [==========================>...] - ETA: 6:02 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9208/10000 [==========================>...] - ETA: 6:01 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9209/10000 [==========================>...] - ETA: 6:01 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9210/10000 [==========================>...] - ETA: 6:00 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9211/10000 [==========================>...] - ETA: 6:00 - loss: 0.6201 - regression_loss: 0.5022 - classification_loss: 0.1178
 9212/10000 [==========================>...] - ETA: 5:59 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9213/10000 [==========================>...] - ETA: 5:59 - loss: 0.6201 - regression_loss: 0.5022 - classification_loss: 0.1178
 9214/10000 [==========================>...] - ETA: 5:59 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9215/10000 [==========================>...] - ETA: 5:58 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9216/10000 [==========================>...] - ETA: 5:58 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9217/10000 [==========================>...] - ETA: 5:57 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9218/10000 [==========================>...] - ETA: 5:57 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9219/10000 [==========================>...] - ETA: 5:56 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9220/10000 [==========================>...] - ETA: 5:56 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9221/10000 [==========================>...] - ETA: 5:55 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9222/10000 [==========================>...] - ETA: 5:55 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9223/10000 [==========================>...] - ETA: 5:54 - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1178
 9224/10000 [==========================>...] - ETA: 5:54 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9225/10000 [==========================>...] - ETA: 5:54 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9226/10000 [==========================>...] - ETA: 5:53 - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1178
 9227/10000 [==========================>...] - ETA: 5:53 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9228/10000 [==========================>...] - ETA: 5:52 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9229/10000 [==========================>...] - ETA: 5:52 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9230/10000 [==========================>...] - ETA: 5:51 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9231/10000 [==========================>...] - ETA: 5:51 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9232/10000 [==========================>...] - ETA: 5:50 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9233/10000 [==========================>...] - ETA: 5:50 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9234/10000 [==========================>...] - ETA: 5:49 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9235/10000 [==========================>...] - ETA: 5:49 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9236/10000 [==========================>...] - ETA: 5:49 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9237/10000 [==========================>...] - ETA: 5:48 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1178
 9238/10000 [==========================>...] - ETA: 5:48 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9239/10000 [==========================>...] - ETA: 5:47 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9240/10000 [==========================>...] - ETA: 5:47 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9241/10000 [==========================>...] - ETA: 5:46 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9242/10000 [==========================>...] - ETA: 5:46 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9243/10000 [==========================>...] - ETA: 5:45 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9244/10000 [==========================>...] - ETA: 5:45 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9245/10000 [==========================>...] - ETA: 5:44 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9246/10000 [==========================>...] - ETA: 5:44 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1178
 9247/10000 [==========================>...] - ETA: 5:44 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9248/10000 [==========================>...] - ETA: 5:43 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9249/10000 [==========================>...] - ETA: 5:43 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9250/10000 [==========================>...] - ETA: 5:42 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9251/10000 [==========================>...] - ETA: 5:42 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9252/10000 [==========================>...] - ETA: 5:41 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9253/10000 [==========================>...] - ETA: 5:41 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9254/10000 [==========================>...] - ETA: 5:40 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9255/10000 [==========================>...] - ETA: 5:40 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9256/10000 [==========================>...] - ETA: 5:39 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9257/10000 [==========================>...] - ETA: 5:39 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1178
 9258/10000 [==========================>...] - ETA: 5:38 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9259/10000 [==========================>...] - ETA: 5:38 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9260/10000 [==========================>...] - ETA: 5:38 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9261/10000 [==========================>...] - ETA: 5:37 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1178
 9262/10000 [==========================>...] - ETA: 5:37 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9263/10000 [==========================>...] - ETA: 5:36 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9264/10000 [==========================>...] - ETA: 5:36 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9265/10000 [==========================>...] - ETA: 5:35 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1178
 9266/10000 [==========================>...] - ETA: 5:35 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1178
 9267/10000 [==========================>...] - ETA: 5:34 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9268/10000 [==========================>...] - ETA: 5:34 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9269/10000 [==========================>...] - ETA: 5:33 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9270/10000 [==========================>...] - ETA: 5:33 - loss: 0.6204 - regression_loss: 0.5025 - classification_loss: 0.1178
 9271/10000 [==========================>...] - ETA: 5:33 - loss: 0.6204 - regression_loss: 0.5025 - classification_loss: 0.1178
 9272/10000 [==========================>...] - ETA: 5:32 - loss: 0.6204 - regression_loss: 0.5025 - classification_loss: 0.1178
 9273/10000 [==========================>...] - ETA: 5:32 - loss: 0.6204 - regression_loss: 0.5025 - classification_loss: 0.1178
 9274/10000 [==========================>...] - ETA: 5:31 - loss: 0.6204 - regression_loss: 0.5025 - classification_loss: 0.1178
 9275/10000 [==========================>...] - ETA: 5:31 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9276/10000 [==========================>...] - ETA: 5:30 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9277/10000 [==========================>...] - ETA: 5:30 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9278/10000 [==========================>...] - ETA: 5:29 - loss: 0.6205 - regression_loss: 0.5026 - classification_loss: 0.1178
 9279/10000 [==========================>...] - ETA: 5:29 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9280/10000 [==========================>...] - ETA: 5:28 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9281/10000 [==========================>...] - ETA: 5:28 - loss: 0.6204 - regression_loss: 0.5025 - classification_loss: 0.1178
 9282/10000 [==========================>...] - ETA: 5:27 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9283/10000 [==========================>...] - ETA: 5:27 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9284/10000 [==========================>...] - ETA: 5:27 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9285/10000 [==========================>...] - ETA: 5:26 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9286/10000 [==========================>...] - ETA: 5:26 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9287/10000 [==========================>...] - ETA: 5:25 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9288/10000 [==========================>...] - ETA: 5:25 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9289/10000 [==========================>...] - ETA: 5:24 - loss: 0.6205 - regression_loss: 0.5026 - classification_loss: 0.1178
 9290/10000 [==========================>...] - ETA: 5:24 - loss: 0.6205 - regression_loss: 0.5027 - classification_loss: 0.1178
 9291/10000 [==========================>...] - ETA: 5:23 - loss: 0.6205 - regression_loss: 0.5026 - classification_loss: 0.1178
 9292/10000 [==========================>...] - ETA: 5:23 - loss: 0.6205 - regression_loss: 0.5026 - classification_loss: 0.1178
 9293/10000 [==========================>...] - ETA: 5:22 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9294/10000 [==========================>...] - ETA: 5:22 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9295/10000 [==========================>...] - ETA: 5:22 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9296/10000 [==========================>...] - ETA: 5:21 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9297/10000 [==========================>...] - ETA: 5:21 - loss: 0.6205 - regression_loss: 0.5026 - classification_loss: 0.1178
 9298/10000 [==========================>...] - ETA: 5:20 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9299/10000 [==========================>...] - ETA: 5:20 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9300/10000 [==========================>...] - ETA: 5:19 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9301/10000 [==========================>...] - ETA: 5:19 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9302/10000 [==========================>...] - ETA: 5:18 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9303/10000 [==========================>...] - ETA: 5:18 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1178
 9304/10000 [==========================>...] - ETA: 5:17 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9305/10000 [==========================>...] - ETA: 5:17 - loss: 0.6205 - regression_loss: 0.5027 - classification_loss: 0.1178
 9306/10000 [==========================>...] - ETA: 5:17 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9307/10000 [==========================>...] - ETA: 5:16 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9308/10000 [==========================>...] - ETA: 5:16 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1178
 9309/10000 [==========================>...] - ETA: 5:15 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1178
 9310/10000 [==========================>...] - ETA: 5:15 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9311/10000 [==========================>...] - ETA: 5:14 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9312/10000 [==========================>...] - ETA: 5:14 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9313/10000 [==========================>...] - ETA: 5:13 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9314/10000 [==========================>...] - ETA: 5:13 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9315/10000 [==========================>...] - ETA: 5:12 - loss: 0.6204 - regression_loss: 0.5025 - classification_loss: 0.1178
 9316/10000 [==========================>...] - ETA: 5:12 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9317/10000 [==========================>...] - ETA: 5:12 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9318/10000 [==========================>...] - ETA: 5:11 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9319/10000 [==========================>...] - ETA: 5:11 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9320/10000 [==========================>...] - ETA: 5:10 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9321/10000 [==========================>...] - ETA: 5:10 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9322/10000 [==========================>...] - ETA: 5:09 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9323/10000 [==========================>...] - ETA: 5:09 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9324/10000 [==========================>...] - ETA: 5:08 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9325/10000 [==========================>...] - ETA: 5:08 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9326/10000 [==========================>...] - ETA: 5:07 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9327/10000 [==========================>...] - ETA: 5:07 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9328/10000 [==========================>...] - ETA: 5:06 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9329/10000 [==========================>...] - ETA: 5:06 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9330/10000 [==========================>...] - ETA: 5:06 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9331/10000 [==========================>...] - ETA: 5:05 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9332/10000 [==========================>...] - ETA: 5:05 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9333/10000 [==========================>...] - ETA: 5:04 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1178
 9334/10000 [===========================>..] - ETA: 5:04 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9335/10000 [===========================>..] - ETA: 5:03 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9336/10000 [===========================>..] - ETA: 5:03 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9337/10000 [===========================>..] - ETA: 5:02 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9338/10000 [===========================>..] - ETA: 5:02 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9339/10000 [===========================>..] - ETA: 5:01 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9340/10000 [===========================>..] - ETA: 5:01 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1178
 9341/10000 [===========================>..] - ETA: 5:01 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9342/10000 [===========================>..] - ETA: 5:00 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9343/10000 [===========================>..] - ETA: 5:00 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9344/10000 [===========================>..] - ETA: 4:59 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9345/10000 [===========================>..] - ETA: 4:59 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9346/10000 [===========================>..] - ETA: 4:58 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9347/10000 [===========================>..] - ETA: 4:58 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9348/10000 [===========================>..] - ETA: 4:57 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9349/10000 [===========================>..] - ETA: 4:57 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9350/10000 [===========================>..] - ETA: 4:56 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1178
 9351/10000 [===========================>..] - ETA: 4:56 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9352/10000 [===========================>..] - ETA: 4:56 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9353/10000 [===========================>..] - ETA: 4:55 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9354/10000 [===========================>..] - ETA: 4:55 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9355/10000 [===========================>..] - ETA: 4:54 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9356/10000 [===========================>..] - ETA: 4:54 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9357/10000 [===========================>..] - ETA: 4:53 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9358/10000 [===========================>..] - ETA: 4:53 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9359/10000 [===========================>..] - ETA: 4:52 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1178
 9360/10000 [===========================>..] - ETA: 4:52 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1178
 9361/10000 [===========================>..] - ETA: 4:51 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9362/10000 [===========================>..] - ETA: 4:51 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9363/10000 [===========================>..] - ETA: 4:51 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9364/10000 [===========================>..] - ETA: 4:50 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9365/10000 [===========================>..] - ETA: 4:50 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1178
 9366/10000 [===========================>..] - ETA: 4:49 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9367/10000 [===========================>..] - ETA: 4:49 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9368/10000 [===========================>..] - ETA: 4:48 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9369/10000 [===========================>..] - ETA: 4:48 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9370/10000 [===========================>..] - ETA: 4:47 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9371/10000 [===========================>..] - ETA: 4:47 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9372/10000 [===========================>..] - ETA: 4:46 - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9373/10000 [===========================>..] - ETA: 4:46 - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1178
 9374/10000 [===========================>..] - ETA: 4:45 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9375/10000 [===========================>..] - ETA: 4:45 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9376/10000 [===========================>..] - ETA: 4:45 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9377/10000 [===========================>..] - ETA: 4:44 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9378/10000 [===========================>..] - ETA: 4:44 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9379/10000 [===========================>..] - ETA: 4:43 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9380/10000 [===========================>..] - ETA: 4:43 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9381/10000 [===========================>..] - ETA: 4:42 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9382/10000 [===========================>..] - ETA: 4:42 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9383/10000 [===========================>..] - ETA: 4:41 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1178
 9384/10000 [===========================>..] - ETA: 4:41 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9385/10000 [===========================>..] - ETA: 4:40 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9386/10000 [===========================>..] - ETA: 4:40 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9387/10000 [===========================>..] - ETA: 4:40 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1178
 9388/10000 [===========================>..] - ETA: 4:39 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1178
 9389/10000 [===========================>..] - ETA: 4:39 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1177
 9390/10000 [===========================>..] - ETA: 4:38 - loss: 0.6199 - regression_loss: 0.5022 - classification_loss: 0.1177
 9391/10000 [===========================>..] - ETA: 4:38 - loss: 0.6199 - regression_loss: 0.5022 - classification_loss: 0.1177
 9392/10000 [===========================>..] - ETA: 4:37 - loss: 0.6199 - regression_loss: 0.5022 - classification_loss: 0.1177
 9393/10000 [===========================>..] - ETA: 4:37 - loss: 0.6199 - regression_loss: 0.5021 - classification_loss: 0.1177
 9394/10000 [===========================>..] - ETA: 4:36 - loss: 0.6199 - regression_loss: 0.5022 - classification_loss: 0.1178
 9395/10000 [===========================>..] - ETA: 4:36 - loss: 0.6199 - regression_loss: 0.5022 - classification_loss: 0.1178
 9396/10000 [===========================>..] - ETA: 4:35 - loss: 0.6199 - regression_loss: 0.5021 - classification_loss: 0.1177
 9397/10000 [===========================>..] - ETA: 4:35 - loss: 0.6199 - regression_loss: 0.5021 - classification_loss: 0.1177
 9398/10000 [===========================>..] - ETA: 4:35 - loss: 0.6198 - regression_loss: 0.5021 - classification_loss: 0.1177
 9399/10000 [===========================>..] - ETA: 4:34 - loss: 0.6198 - regression_loss: 0.5021 - classification_loss: 0.1177
 9400/10000 [===========================>..] - ETA: 4:34 - loss: 0.6197 - regression_loss: 0.5020 - classification_loss: 0.1177
 9401/10000 [===========================>..] - ETA: 4:33 - loss: 0.6198 - regression_loss: 0.5020 - classification_loss: 0.1177
 9402/10000 [===========================>..] - ETA: 4:33 - loss: 0.6197 - regression_loss: 0.5020 - classification_loss: 0.1177
 9403/10000 [===========================>..] - ETA: 4:32 - loss: 0.6197 - regression_loss: 0.5020 - classification_loss: 0.1177
 9404/10000 [===========================>..] - ETA: 4:32 - loss: 0.6197 - regression_loss: 0.5020 - classification_loss: 0.1177
 9405/10000 [===========================>..] - ETA: 4:31 - loss: 0.6197 - regression_loss: 0.5020 - classification_loss: 0.1177
 9406/10000 [===========================>..] - ETA: 4:31 - loss: 0.6197 - regression_loss: 0.5020 - classification_loss: 0.1177
 9407/10000 [===========================>..] - ETA: 4:30 - loss: 0.6198 - regression_loss: 0.5020 - classification_loss: 0.1177
 9408/10000 [===========================>..] - ETA: 4:30 - loss: 0.6197 - regression_loss: 0.5020 - classification_loss: 0.1177
 9409/10000 [===========================>..] - ETA: 4:29 - loss: 0.6198 - regression_loss: 0.5021 - classification_loss: 0.1177
 9410/10000 [===========================>..] - ETA: 4:29 - loss: 0.6197 - regression_loss: 0.5020 - classification_loss: 0.1177
 9411/10000 [===========================>..] - ETA: 4:29 - loss: 0.6198 - regression_loss: 0.5021 - classification_loss: 0.1177
 9412/10000 [===========================>..] - ETA: 4:28 - loss: 0.6197 - regression_loss: 0.5020 - classification_loss: 0.1177
 9413/10000 [===========================>..] - ETA: 4:28 - loss: 0.6197 - regression_loss: 0.5020 - classification_loss: 0.1177
 9414/10000 [===========================>..] - ETA: 4:27 - loss: 0.6197 - regression_loss: 0.5020 - classification_loss: 0.1177
 9415/10000 [===========================>..] - ETA: 4:27 - loss: 0.6198 - regression_loss: 0.5021 - classification_loss: 0.1177
 9416/10000 [===========================>..] - ETA: 4:26 - loss: 0.6199 - regression_loss: 0.5021 - classification_loss: 0.1177
 9417/10000 [===========================>..] - ETA: 4:26 - loss: 0.6199 - regression_loss: 0.5021 - classification_loss: 0.1177
 9418/10000 [===========================>..] - ETA: 4:25 - loss: 0.6198 - regression_loss: 0.5021 - classification_loss: 0.1177
 9419/10000 [===========================>..] - ETA: 4:25 - loss: 0.6198 - regression_loss: 0.5021 - classification_loss: 0.1177
 9420/10000 [===========================>..] - ETA: 4:24 - loss: 0.6198 - regression_loss: 0.5021 - classification_loss: 0.1177
 9421/10000 [===========================>..] - ETA: 4:24 - loss: 0.6198 - regression_loss: 0.5021 - classification_loss: 0.1177
 9422/10000 [===========================>..] - ETA: 4:24 - loss: 0.6199 - regression_loss: 0.5021 - classification_loss: 0.1177
 9423/10000 [===========================>..] - ETA: 4:23 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1177
 9424/10000 [===========================>..] - ETA: 4:23 - loss: 0.6199 - regression_loss: 0.5022 - classification_loss: 0.1177
 9425/10000 [===========================>..] - ETA: 4:22 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9426/10000 [===========================>..] - ETA: 4:22 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1178
 9427/10000 [===========================>..] - ETA: 4:21 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9428/10000 [===========================>..] - ETA: 4:21 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1177
 9429/10000 [===========================>..] - ETA: 4:20 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1178
 9430/10000 [===========================>..] - ETA: 4:20 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9431/10000 [===========================>..] - ETA: 4:19 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9432/10000 [===========================>..] - ETA: 4:19 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9433/10000 [===========================>..] - ETA: 4:19 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9434/10000 [===========================>..] - ETA: 4:18 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1178
 9435/10000 [===========================>..] - ETA: 4:18 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9436/10000 [===========================>..] - ETA: 4:17 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9437/10000 [===========================>..] - ETA: 4:17 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9438/10000 [===========================>..] - ETA: 4:16 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9439/10000 [===========================>..] - ETA: 4:16 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9440/10000 [===========================>..] - ETA: 4:15 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9441/10000 [===========================>..] - ETA: 4:15 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9442/10000 [===========================>..] - ETA: 4:14 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9443/10000 [===========================>..] - ETA: 4:14 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1178
 9444/10000 [===========================>..] - ETA: 4:14 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9445/10000 [===========================>..] - ETA: 4:13 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9446/10000 [===========================>..] - ETA: 4:13 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9447/10000 [===========================>..] - ETA: 4:12 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9448/10000 [===========================>..] - ETA: 4:12 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9449/10000 [===========================>..] - ETA: 4:11 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9450/10000 [===========================>..] - ETA: 4:11 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1178
 9451/10000 [===========================>..] - ETA: 4:10 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9452/10000 [===========================>..] - ETA: 4:10 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1177
 9453/10000 [===========================>..] - ETA: 4:09 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9454/10000 [===========================>..] - ETA: 4:09 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1177
 9455/10000 [===========================>..] - ETA: 4:08 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1177
 9456/10000 [===========================>..] - ETA: 4:08 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9457/10000 [===========================>..] - ETA: 4:08 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9458/10000 [===========================>..] - ETA: 4:07 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9459/10000 [===========================>..] - ETA: 4:07 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1177
 9460/10000 [===========================>..] - ETA: 4:06 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9461/10000 [===========================>..] - ETA: 4:06 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9462/10000 [===========================>..] - ETA: 4:05 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9463/10000 [===========================>..] - ETA: 4:05 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9464/10000 [===========================>..] - ETA: 4:04 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9465/10000 [===========================>..] - ETA: 4:04 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9466/10000 [===========================>..] - ETA: 4:03 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9467/10000 [===========================>..] - ETA: 4:03 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9468/10000 [===========================>..] - ETA: 4:03 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9469/10000 [===========================>..] - ETA: 4:02 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9470/10000 [===========================>..] - ETA: 4:02 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9471/10000 [===========================>..] - ETA: 4:01 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9472/10000 [===========================>..] - ETA: 4:01 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9473/10000 [===========================>..] - ETA: 4:00 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9474/10000 [===========================>..] - ETA: 4:00 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9475/10000 [===========================>..] - ETA: 3:59 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9476/10000 [===========================>..] - ETA: 3:59 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9477/10000 [===========================>..] - ETA: 3:58 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9478/10000 [===========================>..] - ETA: 3:58 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9479/10000 [===========================>..] - ETA: 3:58 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9480/10000 [===========================>..] - ETA: 3:57 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9481/10000 [===========================>..] - ETA: 3:57 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1177
 9482/10000 [===========================>..] - ETA: 3:56 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9483/10000 [===========================>..] - ETA: 3:56 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9484/10000 [===========================>..] - ETA: 3:55 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9485/10000 [===========================>..] - ETA: 3:55 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9486/10000 [===========================>..] - ETA: 3:54 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9487/10000 [===========================>..] - ETA: 3:54 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9488/10000 [===========================>..] - ETA: 3:53 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9489/10000 [===========================>..] - ETA: 3:53 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9490/10000 [===========================>..] - ETA: 3:52 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1177
 9491/10000 [===========================>..] - ETA: 3:52 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9492/10000 [===========================>..] - ETA: 3:52 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9493/10000 [===========================>..] - ETA: 3:51 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9494/10000 [===========================>..] - ETA: 3:51 - loss: 0.6199 - regression_loss: 0.5022 - classification_loss: 0.1177
 9495/10000 [===========================>..] - ETA: 3:50 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9496/10000 [===========================>..] - ETA: 3:50 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9497/10000 [===========================>..] - ETA: 3:49 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9498/10000 [===========================>..] - ETA: 3:49 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9499/10000 [===========================>..] - ETA: 3:48 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9500/10000 [===========================>..] - ETA: 3:48 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9501/10000 [===========================>..] - ETA: 3:47 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9502/10000 [===========================>..] - ETA: 3:47 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9503/10000 [===========================>..] - ETA: 3:47 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1177
 9504/10000 [===========================>..] - ETA: 3:46 - loss: 0.6199 - regression_loss: 0.5022 - classification_loss: 0.1177
 9505/10000 [===========================>..] - ETA: 3:46 - loss: 0.6199 - regression_loss: 0.5022 - classification_loss: 0.1177
 9506/10000 [===========================>..] - ETA: 3:45 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1177
 9507/10000 [===========================>..] - ETA: 3:45 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1177
 9508/10000 [===========================>..] - ETA: 3:44 - loss: 0.6199 - regression_loss: 0.5022 - classification_loss: 0.1177
 9509/10000 [===========================>..] - ETA: 3:44 - loss: 0.6199 - regression_loss: 0.5022 - classification_loss: 0.1177
 9510/10000 [===========================>..] - ETA: 3:43 - loss: 0.6198 - regression_loss: 0.5022 - classification_loss: 0.1177
 9511/10000 [===========================>..] - ETA: 3:43 - loss: 0.6198 - regression_loss: 0.5021 - classification_loss: 0.1177
 9512/10000 [===========================>..] - ETA: 3:42 - loss: 0.6198 - regression_loss: 0.5021 - classification_loss: 0.1177
 9513/10000 [===========================>..] - ETA: 3:42 - loss: 0.6197 - regression_loss: 0.5021 - classification_loss: 0.1176
 9514/10000 [===========================>..] - ETA: 3:42 - loss: 0.6198 - regression_loss: 0.5021 - classification_loss: 0.1177
 9515/10000 [===========================>..] - ETA: 3:41 - loss: 0.6198 - regression_loss: 0.5021 - classification_loss: 0.1177
 9516/10000 [===========================>..] - ETA: 3:41 - loss: 0.6198 - regression_loss: 0.5022 - classification_loss: 0.1177
 9517/10000 [===========================>..] - ETA: 3:40 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1177
 9518/10000 [===========================>..] - ETA: 3:40 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9519/10000 [===========================>..] - ETA: 3:39 - loss: 0.6200 - regression_loss: 0.5022 - classification_loss: 0.1177
 9520/10000 [===========================>..] - ETA: 3:39 - loss: 0.6199 - regression_loss: 0.5022 - classification_loss: 0.1177
 9521/10000 [===========================>..] - ETA: 3:38 - loss: 0.6199 - regression_loss: 0.5022 - classification_loss: 0.1177
 9522/10000 [===========================>..] - ETA: 3:38 - loss: 0.6199 - regression_loss: 0.5022 - classification_loss: 0.1177
 9523/10000 [===========================>..] - ETA: 3:37 - loss: 0.6199 - regression_loss: 0.5022 - classification_loss: 0.1177
 9524/10000 [===========================>..] - ETA: 3:37 - loss: 0.6199 - regression_loss: 0.5022 - classification_loss: 0.1177
 9525/10000 [===========================>..] - ETA: 3:36 - loss: 0.6199 - regression_loss: 0.5022 - classification_loss: 0.1177
 9526/10000 [===========================>..] - ETA: 3:36 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9527/10000 [===========================>..] - ETA: 3:36 - loss: 0.6199 - regression_loss: 0.5022 - classification_loss: 0.1177
 9528/10000 [===========================>..] - ETA: 3:35 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9529/10000 [===========================>..] - ETA: 3:35 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9530/10000 [===========================>..] - ETA: 3:34 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9531/10000 [===========================>..] - ETA: 3:34 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9532/10000 [===========================>..] - ETA: 3:33 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9533/10000 [===========================>..] - ETA: 3:33 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9534/10000 [===========================>..] - ETA: 3:32 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9535/10000 [===========================>..] - ETA: 3:32 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9536/10000 [===========================>..] - ETA: 3:31 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9537/10000 [===========================>..] - ETA: 3:31 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9538/10000 [===========================>..] - ETA: 3:31 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9539/10000 [===========================>..] - ETA: 3:30 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9540/10000 [===========================>..] - ETA: 3:30 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9541/10000 [===========================>..] - ETA: 3:29 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9542/10000 [===========================>..] - ETA: 3:29 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9543/10000 [===========================>..] - ETA: 3:28 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9544/10000 [===========================>..] - ETA: 3:28 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1177
 9545/10000 [===========================>..] - ETA: 3:27 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9546/10000 [===========================>..] - ETA: 3:27 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1177
 9547/10000 [===========================>..] - ETA: 3:26 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1177
 9548/10000 [===========================>..] - ETA: 3:26 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9549/10000 [===========================>..] - ETA: 3:26 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9550/10000 [===========================>..] - ETA: 3:25 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1177
 9551/10000 [===========================>..] - ETA: 3:25 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1176
 9552/10000 [===========================>..] - ETA: 3:24 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9553/10000 [===========================>..] - ETA: 3:24 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9554/10000 [===========================>..] - ETA: 3:23 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1177
 9555/10000 [===========================>..] - ETA: 3:23 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9556/10000 [===========================>..] - ETA: 3:22 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1177
 9557/10000 [===========================>..] - ETA: 3:22 - loss: 0.6199 - regression_loss: 0.5022 - classification_loss: 0.1177
 9558/10000 [===========================>..] - ETA: 3:21 - loss: 0.6198 - regression_loss: 0.5022 - classification_loss: 0.1176
 9559/10000 [===========================>..] - ETA: 3:21 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9560/10000 [===========================>..] - ETA: 3:21 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9561/10000 [===========================>..] - ETA: 3:20 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1177
 9562/10000 [===========================>..] - ETA: 3:20 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1177
 9563/10000 [===========================>..] - ETA: 3:19 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1177
 9564/10000 [===========================>..] - ETA: 3:19 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9565/10000 [===========================>..] - ETA: 3:18 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9566/10000 [===========================>..] - ETA: 3:18 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1177
 9567/10000 [===========================>..] - ETA: 3:17 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9568/10000 [===========================>..] - ETA: 3:17 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1177
 9569/10000 [===========================>..] - ETA: 3:16 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9570/10000 [===========================>..] - ETA: 3:16 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9571/10000 [===========================>..] - ETA: 3:15 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9572/10000 [===========================>..] - ETA: 3:15 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9573/10000 [===========================>..] - ETA: 3:15 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1177
 9574/10000 [===========================>..] - ETA: 3:14 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9575/10000 [===========================>..] - ETA: 3:14 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9576/10000 [===========================>..] - ETA: 3:13 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1177
 9577/10000 [===========================>..] - ETA: 3:13 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1176
 9578/10000 [===========================>..] - ETA: 3:12 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1177
 9579/10000 [===========================>..] - ETA: 3:12 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9580/10000 [===========================>..] - ETA: 3:11 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1176
 9581/10000 [===========================>..] - ETA: 3:11 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1176
 9582/10000 [===========================>..] - ETA: 3:10 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1176
 9583/10000 [===========================>..] - ETA: 3:10 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9584/10000 [===========================>..] - ETA: 3:10 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1176
 9585/10000 [===========================>..] - ETA: 3:09 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9586/10000 [===========================>..] - ETA: 3:09 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9587/10000 [===========================>..] - ETA: 3:08 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1176
 9588/10000 [===========================>..] - ETA: 3:08 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9589/10000 [===========================>..] - ETA: 3:07 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9590/10000 [===========================>..] - ETA: 3:07 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9591/10000 [===========================>..] - ETA: 3:06 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9592/10000 [===========================>..] - ETA: 3:06 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9593/10000 [===========================>..] - ETA: 3:05 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9594/10000 [===========================>..] - ETA: 3:05 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9595/10000 [===========================>..] - ETA: 3:05 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9596/10000 [===========================>..] - ETA: 3:04 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9597/10000 [===========================>..] - ETA: 3:04 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1176
 9598/10000 [===========================>..] - ETA: 3:03 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9599/10000 [===========================>..] - ETA: 3:03 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9600/10000 [===========================>..] - ETA: 3:02 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1176
 9601/10000 [===========================>..] - ETA: 3:02 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9602/10000 [===========================>..] - ETA: 3:01 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1176
 9603/10000 [===========================>..] - ETA: 3:01 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9604/10000 [===========================>..] - ETA: 3:00 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9605/10000 [===========================>..] - ETA: 3:00 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9606/10000 [===========================>..] - ETA: 2:59 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9607/10000 [===========================>..] - ETA: 2:59 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9608/10000 [===========================>..] - ETA: 2:59 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1176
 9609/10000 [===========================>..] - ETA: 2:58 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9610/10000 [===========================>..] - ETA: 2:58 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9611/10000 [===========================>..] - ETA: 2:57 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9612/10000 [===========================>..] - ETA: 2:57 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9613/10000 [===========================>..] - ETA: 2:56 - loss: 0.6199 - regression_loss: 0.5024 - classification_loss: 0.1176
 9614/10000 [===========================>..] - ETA: 2:56 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1176
 9615/10000 [===========================>..] - ETA: 2:55 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9616/10000 [===========================>..] - ETA: 2:55 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1176
 9617/10000 [===========================>..] - ETA: 2:54 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1176
 9618/10000 [===========================>..] - ETA: 2:54 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1176
 9619/10000 [===========================>..] - ETA: 2:54 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1176
 9620/10000 [===========================>..] - ETA: 2:53 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1176
 9621/10000 [===========================>..] - ETA: 2:53 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1176
 9622/10000 [===========================>..] - ETA: 2:52 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9623/10000 [===========================>..] - ETA: 2:52 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9624/10000 [===========================>..] - ETA: 2:51 - loss: 0.6202 - regression_loss: 0.5026 - classification_loss: 0.1177
 9625/10000 [===========================>..] - ETA: 2:51 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9626/10000 [===========================>..] - ETA: 2:50 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9627/10000 [===========================>..] - ETA: 2:50 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9628/10000 [===========================>..] - ETA: 2:49 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9629/10000 [===========================>..] - ETA: 2:49 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9630/10000 [===========================>..] - ETA: 2:49 - loss: 0.6202 - regression_loss: 0.5026 - classification_loss: 0.1177
 9631/10000 [===========================>..] - ETA: 2:48 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9632/10000 [===========================>..] - ETA: 2:48 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9633/10000 [===========================>..] - ETA: 2:47 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1176
 9634/10000 [===========================>..] - ETA: 2:47 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1176
 9635/10000 [===========================>..] - ETA: 2:46 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1176
 9636/10000 [===========================>..] - ETA: 2:46 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9637/10000 [===========================>..] - ETA: 2:45 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9638/10000 [===========================>..] - ETA: 2:45 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1176
 9639/10000 [===========================>..] - ETA: 2:44 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1176
 9640/10000 [===========================>..] - ETA: 2:44 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1176
 9641/10000 [===========================>..] - ETA: 2:43 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1176
 9642/10000 [===========================>..] - ETA: 2:43 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1176
 9643/10000 [===========================>..] - ETA: 2:43 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9644/10000 [===========================>..] - ETA: 2:42 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9645/10000 [===========================>..] - ETA: 2:42 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9646/10000 [===========================>..] - ETA: 2:41 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9647/10000 [===========================>..] - ETA: 2:41 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9648/10000 [===========================>..] - ETA: 2:40 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9649/10000 [===========================>..] - ETA: 2:40 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9650/10000 [===========================>..] - ETA: 2:39 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9651/10000 [===========================>..] - ETA: 2:39 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9652/10000 [===========================>..] - ETA: 2:38 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9653/10000 [===========================>..] - ETA: 2:38 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9654/10000 [===========================>..] - ETA: 2:38 - loss: 0.6203 - regression_loss: 0.5027 - classification_loss: 0.1177
 9655/10000 [===========================>..] - ETA: 2:37 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9656/10000 [===========================>..] - ETA: 2:37 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9657/10000 [===========================>..] - ETA: 2:36 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9658/10000 [===========================>..] - ETA: 2:36 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9659/10000 [===========================>..] - ETA: 2:35 - loss: 0.6202 - regression_loss: 0.5026 - classification_loss: 0.1177
 9660/10000 [===========================>..] - ETA: 2:35 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9661/10000 [===========================>..] - ETA: 2:34 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1177
 9662/10000 [===========================>..] - ETA: 2:34 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1177
 9663/10000 [===========================>..] - ETA: 2:33 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9664/10000 [===========================>..] - ETA: 2:33 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1177
 9665/10000 [===========================>..] - ETA: 2:33 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1176
 9666/10000 [===========================>..] - ETA: 2:32 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1176
 9667/10000 [============================>.] - ETA: 2:32 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9668/10000 [============================>.] - ETA: 2:31 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1177
 9669/10000 [============================>.] - ETA: 2:31 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9670/10000 [============================>.] - ETA: 2:30 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9671/10000 [============================>.] - ETA: 2:30 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9672/10000 [============================>.] - ETA: 2:29 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9673/10000 [============================>.] - ETA: 2:29 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9674/10000 [============================>.] - ETA: 2:28 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9675/10000 [============================>.] - ETA: 2:28 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9676/10000 [============================>.] - ETA: 2:28 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1177
 9677/10000 [============================>.] - ETA: 2:27 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9678/10000 [============================>.] - ETA: 2:27 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9679/10000 [============================>.] - ETA: 2:26 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9680/10000 [============================>.] - ETA: 2:26 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9681/10000 [============================>.] - ETA: 2:25 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9682/10000 [============================>.] - ETA: 2:25 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9683/10000 [============================>.] - ETA: 2:24 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1177
 9684/10000 [============================>.] - ETA: 2:24 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9685/10000 [============================>.] - ETA: 2:23 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9686/10000 [============================>.] - ETA: 2:23 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9687/10000 [============================>.] - ETA: 2:22 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9688/10000 [============================>.] - ETA: 2:22 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9689/10000 [============================>.] - ETA: 2:22 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9690/10000 [============================>.] - ETA: 2:21 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9691/10000 [============================>.] - ETA: 2:21 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9692/10000 [============================>.] - ETA: 2:20 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9693/10000 [============================>.] - ETA: 2:20 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9694/10000 [============================>.] - ETA: 2:19 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9695/10000 [============================>.] - ETA: 2:19 - loss: 0.6202 - regression_loss: 0.5026 - classification_loss: 0.1177
 9696/10000 [============================>.] - ETA: 2:18 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9697/10000 [============================>.] - ETA: 2:18 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9698/10000 [============================>.] - ETA: 2:17 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9699/10000 [============================>.] - ETA: 2:17 - loss: 0.6202 - regression_loss: 0.5026 - classification_loss: 0.1177
 9700/10000 [============================>.] - ETA: 2:17 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9701/10000 [============================>.] - ETA: 2:16 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9702/10000 [============================>.] - ETA: 2:16 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9703/10000 [============================>.] - ETA: 2:15 - loss: 0.6202 - regression_loss: 0.5026 - classification_loss: 0.1177
 9704/10000 [============================>.] - ETA: 2:15 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9705/10000 [============================>.] - ETA: 2:14 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1177
 9706/10000 [============================>.] - ETA: 2:14 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1177
 9707/10000 [============================>.] - ETA: 2:13 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9708/10000 [============================>.] - ETA: 2:13 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1176
 9709/10000 [============================>.] - ETA: 2:12 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9710/10000 [============================>.] - ETA: 2:12 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9711/10000 [============================>.] - ETA: 2:12 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9712/10000 [============================>.] - ETA: 2:11 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1177
 9713/10000 [============================>.] - ETA: 2:11 - loss: 0.6201 - regression_loss: 0.5025 - classification_loss: 0.1177
 9714/10000 [============================>.] - ETA: 2:10 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9715/10000 [============================>.] - ETA: 2:10 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9716/10000 [============================>.] - ETA: 2:09 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9717/10000 [============================>.] - ETA: 2:09 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9718/10000 [============================>.] - ETA: 2:08 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9719/10000 [============================>.] - ETA: 2:08 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9720/10000 [============================>.] - ETA: 2:07 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9721/10000 [============================>.] - ETA: 2:07 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9722/10000 [============================>.] - ETA: 2:06 - loss: 0.6202 - regression_loss: 0.5026 - classification_loss: 0.1177
 9723/10000 [============================>.] - ETA: 2:06 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9724/10000 [============================>.] - ETA: 2:06 - loss: 0.6202 - regression_loss: 0.5026 - classification_loss: 0.1177
 9725/10000 [============================>.] - ETA: 2:05 - loss: 0.6202 - regression_loss: 0.5026 - classification_loss: 0.1176
 9726/10000 [============================>.] - ETA: 2:05 - loss: 0.6202 - regression_loss: 0.5026 - classification_loss: 0.1177
 9727/10000 [============================>.] - ETA: 2:04 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9728/10000 [============================>.] - ETA: 2:04 - loss: 0.6204 - regression_loss: 0.5027 - classification_loss: 0.1177
 9729/10000 [============================>.] - ETA: 2:03 - loss: 0.6204 - regression_loss: 0.5027 - classification_loss: 0.1177
 9730/10000 [============================>.] - ETA: 2:03 - loss: 0.6204 - regression_loss: 0.5027 - classification_loss: 0.1177
 9731/10000 [============================>.] - ETA: 2:02 - loss: 0.6203 - regression_loss: 0.5027 - classification_loss: 0.1177
 9732/10000 [============================>.] - ETA: 2:02 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9733/10000 [============================>.] - ETA: 2:01 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9734/10000 [============================>.] - ETA: 2:01 - loss: 0.6204 - regression_loss: 0.5027 - classification_loss: 0.1177
 9735/10000 [============================>.] - ETA: 2:01 - loss: 0.6204 - regression_loss: 0.5027 - classification_loss: 0.1177
 9736/10000 [============================>.] - ETA: 2:00 - loss: 0.6204 - regression_loss: 0.5027 - classification_loss: 0.1177
 9737/10000 [============================>.] - ETA: 2:00 - loss: 0.6204 - regression_loss: 0.5027 - classification_loss: 0.1177
 9738/10000 [============================>.] - ETA: 1:59 - loss: 0.6204 - regression_loss: 0.5028 - classification_loss: 0.1177
 9739/10000 [============================>.] - ETA: 1:59 - loss: 0.6204 - regression_loss: 0.5027 - classification_loss: 0.1177
 9740/10000 [============================>.] - ETA: 1:58 - loss: 0.6204 - regression_loss: 0.5027 - classification_loss: 0.1177
 9741/10000 [============================>.] - ETA: 1:58 - loss: 0.6205 - regression_loss: 0.5028 - classification_loss: 0.1177
 9742/10000 [============================>.] - ETA: 1:57 - loss: 0.6205 - regression_loss: 0.5028 - classification_loss: 0.1177
 9743/10000 [============================>.] - ETA: 1:57 - loss: 0.6206 - regression_loss: 0.5028 - classification_loss: 0.1178
 9744/10000 [============================>.] - ETA: 1:56 - loss: 0.6206 - regression_loss: 0.5028 - classification_loss: 0.1178
 9745/10000 [============================>.] - ETA: 1:56 - loss: 0.6205 - regression_loss: 0.5028 - classification_loss: 0.1178
 9746/10000 [============================>.] - ETA: 1:56 - loss: 0.6206 - regression_loss: 0.5028 - classification_loss: 0.1178
 9747/10000 [============================>.] - ETA: 1:55 - loss: 0.6206 - regression_loss: 0.5029 - classification_loss: 0.1178
 9748/10000 [============================>.] - ETA: 1:55 - loss: 0.6206 - regression_loss: 0.5028 - classification_loss: 0.1178
 9749/10000 [============================>.] - ETA: 1:54 - loss: 0.6206 - regression_loss: 0.5028 - classification_loss: 0.1178
 9750/10000 [============================>.] - ETA: 1:54 - loss: 0.6206 - regression_loss: 0.5028 - classification_loss: 0.1178
 9751/10000 [============================>.] - ETA: 1:53 - loss: 0.6205 - regression_loss: 0.5028 - classification_loss: 0.1178
 9752/10000 [============================>.] - ETA: 1:53 - loss: 0.6206 - regression_loss: 0.5028 - classification_loss: 0.1178
 9753/10000 [============================>.] - ETA: 1:52 - loss: 0.6206 - regression_loss: 0.5028 - classification_loss: 0.1178
 9754/10000 [============================>.] - ETA: 1:52 - loss: 0.6206 - regression_loss: 0.5028 - classification_loss: 0.1178
 9755/10000 [============================>.] - ETA: 1:51 - loss: 0.6207 - regression_loss: 0.5029 - classification_loss: 0.1178
 9756/10000 [============================>.] - ETA: 1:51 - loss: 0.6206 - regression_loss: 0.5029 - classification_loss: 0.1178
 9757/10000 [============================>.] - ETA: 1:51 - loss: 0.6206 - regression_loss: 0.5028 - classification_loss: 0.1178
 9758/10000 [============================>.] - ETA: 1:50 - loss: 0.6206 - regression_loss: 0.5029 - classification_loss: 0.1178
 9759/10000 [============================>.] - ETA: 1:50 - loss: 0.6206 - regression_loss: 0.5028 - classification_loss: 0.1178
 9760/10000 [============================>.] - ETA: 1:49 - loss: 0.6206 - regression_loss: 0.5028 - classification_loss: 0.1178
 9761/10000 [============================>.] - ETA: 1:49 - loss: 0.6205 - regression_loss: 0.5028 - classification_loss: 0.1178
 9762/10000 [============================>.] - ETA: 1:48 - loss: 0.6205 - regression_loss: 0.5028 - classification_loss: 0.1178
 9763/10000 [============================>.] - ETA: 1:48 - loss: 0.6205 - regression_loss: 0.5028 - classification_loss: 0.1178
 9764/10000 [============================>.] - ETA: 1:47 - loss: 0.6206 - regression_loss: 0.5028 - classification_loss: 0.1178
 9765/10000 [============================>.] - ETA: 1:47 - loss: 0.6206 - regression_loss: 0.5028 - classification_loss: 0.1178
 9766/10000 [============================>.] - ETA: 1:46 - loss: 0.6206 - regression_loss: 0.5028 - classification_loss: 0.1178
 9767/10000 [============================>.] - ETA: 1:46 - loss: 0.6206 - regression_loss: 0.5029 - classification_loss: 0.1178
 9768/10000 [============================>.] - ETA: 1:45 - loss: 0.6206 - regression_loss: 0.5028 - classification_loss: 0.1178
 9769/10000 [============================>.] - ETA: 1:45 - loss: 0.6206 - regression_loss: 0.5028 - classification_loss: 0.1178
 9770/10000 [============================>.] - ETA: 1:45 - loss: 0.6205 - regression_loss: 0.5028 - classification_loss: 0.1178
 9771/10000 [============================>.] - ETA: 1:44 - loss: 0.6205 - regression_loss: 0.5027 - classification_loss: 0.1178
 9772/10000 [============================>.] - ETA: 1:44 - loss: 0.6205 - regression_loss: 0.5027 - classification_loss: 0.1178
 9773/10000 [============================>.] - ETA: 1:43 - loss: 0.6205 - regression_loss: 0.5027 - classification_loss: 0.1178
 9774/10000 [============================>.] - ETA: 1:43 - loss: 0.6204 - regression_loss: 0.5027 - classification_loss: 0.1177
 9775/10000 [============================>.] - ETA: 1:42 - loss: 0.6204 - regression_loss: 0.5027 - classification_loss: 0.1177
 9776/10000 [============================>.] - ETA: 1:42 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9777/10000 [============================>.] - ETA: 1:41 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9778/10000 [============================>.] - ETA: 1:41 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9779/10000 [============================>.] - ETA: 1:40 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9780/10000 [============================>.] - ETA: 1:40 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9781/10000 [============================>.] - ETA: 1:40 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9782/10000 [============================>.] - ETA: 1:39 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9783/10000 [============================>.] - ETA: 1:39 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9784/10000 [============================>.] - ETA: 1:38 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9785/10000 [============================>.] - ETA: 1:38 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9786/10000 [============================>.] - ETA: 1:37 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9787/10000 [============================>.] - ETA: 1:37 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9788/10000 [============================>.] - ETA: 1:36 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9789/10000 [============================>.] - ETA: 1:36 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9790/10000 [============================>.] - ETA: 1:35 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9791/10000 [============================>.] - ETA: 1:35 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9792/10000 [============================>.] - ETA: 1:35 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9793/10000 [============================>.] - ETA: 1:34 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1177
 9794/10000 [============================>.] - ETA: 1:34 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9795/10000 [============================>.] - ETA: 1:33 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9796/10000 [============================>.] - ETA: 1:33 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9797/10000 [============================>.] - ETA: 1:32 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9798/10000 [============================>.] - ETA: 1:32 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1177
 9799/10000 [============================>.] - ETA: 1:31 - loss: 0.6199 - regression_loss: 0.5022 - classification_loss: 0.1177
 9800/10000 [============================>.] - ETA: 1:31 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1177
 9801/10000 [============================>.] - ETA: 1:30 - loss: 0.6199 - regression_loss: 0.5022 - classification_loss: 0.1176
 9802/10000 [============================>.] - ETA: 1:30 - loss: 0.6199 - regression_loss: 0.5022 - classification_loss: 0.1176
 9803/10000 [============================>.] - ETA: 1:29 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1176
 9804/10000 [============================>.] - ETA: 1:29 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1176
 9805/10000 [============================>.] - ETA: 1:29 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1176
 9806/10000 [============================>.] - ETA: 1:28 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1176
 9807/10000 [============================>.] - ETA: 1:28 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1176
 9808/10000 [============================>.] - ETA: 1:27 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1176
 9809/10000 [============================>.] - ETA: 1:27 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1176
 9810/10000 [============================>.] - ETA: 1:26 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1176
 9811/10000 [============================>.] - ETA: 1:26 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1177
 9812/10000 [============================>.] - ETA: 1:25 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9813/10000 [============================>.] - ETA: 1:25 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1176
 9814/10000 [============================>.] - ETA: 1:24 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1176
 9815/10000 [============================>.] - ETA: 1:24 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9816/10000 [============================>.] - ETA: 1:24 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1176
 9817/10000 [============================>.] - ETA: 1:23 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1176
 9818/10000 [============================>.] - ETA: 1:23 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1176
 9819/10000 [============================>.] - ETA: 1:22 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1176
 9820/10000 [============================>.] - ETA: 1:22 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1176
 9821/10000 [============================>.] - ETA: 1:21 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1177
 9822/10000 [============================>.] - ETA: 1:21 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1176
 9823/10000 [============================>.] - ETA: 1:20 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1176
 9824/10000 [============================>.] - ETA: 1:20 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1176
 9825/10000 [============================>.] - ETA: 1:19 - loss: 0.6199 - regression_loss: 0.5022 - classification_loss: 0.1177
 9826/10000 [============================>.] - ETA: 1:19 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9827/10000 [============================>.] - ETA: 1:19 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1177
 9828/10000 [============================>.] - ETA: 1:18 - loss: 0.6199 - regression_loss: 0.5023 - classification_loss: 0.1177
 9829/10000 [============================>.] - ETA: 1:18 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9830/10000 [============================>.] - ETA: 1:17 - loss: 0.6199 - regression_loss: 0.5022 - classification_loss: 0.1177
 9831/10000 [============================>.] - ETA: 1:17 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9832/10000 [============================>.] - ETA: 1:16 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9833/10000 [============================>.] - ETA: 1:16 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9834/10000 [============================>.] - ETA: 1:15 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1177
 9835/10000 [============================>.] - ETA: 1:15 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1177
 9836/10000 [============================>.] - ETA: 1:14 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9837/10000 [============================>.] - ETA: 1:14 - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1177
 9838/10000 [============================>.] - ETA: 1:13 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9839/10000 [============================>.] - ETA: 1:13 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9840/10000 [============================>.] - ETA: 1:13 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9841/10000 [============================>.] - ETA: 1:12 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9842/10000 [============================>.] - ETA: 1:12 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9843/10000 [============================>.] - ETA: 1:11 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9844/10000 [============================>.] - ETA: 1:11 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9845/10000 [============================>.] - ETA: 1:10 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9846/10000 [============================>.] - ETA: 1:10 - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1177
 9847/10000 [============================>.] - ETA: 1:09 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9848/10000 [============================>.] - ETA: 1:09 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9849/10000 [============================>.] - ETA: 1:08 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9850/10000 [============================>.] - ETA: 1:08 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9851/10000 [============================>.] - ETA: 1:08 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9852/10000 [============================>.] - ETA: 1:07 - loss: 0.6200 - regression_loss: 0.5023 - classification_loss: 0.1177
 9853/10000 [============================>.] - ETA: 1:07 - loss: 0.6200 - regression_loss: 0.5024 - classification_loss: 0.1177
 9854/10000 [============================>.] - ETA: 1:06 - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9855/10000 [============================>.] - ETA: 1:06 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1177
 9856/10000 [============================>.] - ETA: 1:05 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1177
 9857/10000 [============================>.] - ETA: 1:05 - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1177
 9858/10000 [============================>.] - ETA: 1:04 - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1177
 9859/10000 [============================>.] - ETA: 1:04 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9860/10000 [============================>.] - ETA: 1:03 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9861/10000 [============================>.] - ETA: 1:03 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9862/10000 [============================>.] - ETA: 1:03 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9863/10000 [============================>.] - ETA: 1:02 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9864/10000 [============================>.] - ETA: 1:02 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9865/10000 [============================>.] - ETA: 1:01 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9866/10000 [============================>.] - ETA: 1:01 - loss: 0.6204 - regression_loss: 0.5027 - classification_loss: 0.1178
 9867/10000 [============================>.] - ETA: 1:00 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9868/10000 [============================>.] - ETA: 1:00 - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9869/10000 [============================>.] - ETA: 59s - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178 
 9870/10000 [============================>.] - ETA: 59s - loss: 0.6203 - regression_loss: 0.5026 - classification_loss: 0.1178
 9871/10000 [============================>.] - ETA: 58s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9872/10000 [============================>.] - ETA: 58s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9873/10000 [============================>.] - ETA: 58s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9874/10000 [============================>.] - ETA: 57s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9875/10000 [============================>.] - ETA: 57s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9876/10000 [============================>.] - ETA: 56s - loss: 0.6204 - regression_loss: 0.5025 - classification_loss: 0.1178
 9877/10000 [============================>.] - ETA: 56s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9878/10000 [============================>.] - ETA: 55s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9879/10000 [============================>.] - ETA: 55s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9880/10000 [============================>.] - ETA: 54s - loss: 0.6204 - regression_loss: 0.5025 - classification_loss: 0.1178
 9881/10000 [============================>.] - ETA: 54s - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9882/10000 [============================>.] - ETA: 53s - loss: 0.6205 - regression_loss: 0.5026 - classification_loss: 0.1178
 9883/10000 [============================>.] - ETA: 53s - loss: 0.6205 - regression_loss: 0.5026 - classification_loss: 0.1178
 9884/10000 [============================>.] - ETA: 52s - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9885/10000 [============================>.] - ETA: 52s - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9886/10000 [============================>.] - ETA: 52s - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9887/10000 [============================>.] - ETA: 51s - loss: 0.6205 - regression_loss: 0.5027 - classification_loss: 0.1178
 9888/10000 [============================>.] - ETA: 51s - loss: 0.6205 - regression_loss: 0.5027 - classification_loss: 0.1178
 9889/10000 [============================>.] - ETA: 50s - loss: 0.6205 - regression_loss: 0.5027 - classification_loss: 0.1178
 9890/10000 [============================>.] - ETA: 50s - loss: 0.6205 - regression_loss: 0.5027 - classification_loss: 0.1178
 9891/10000 [============================>.] - ETA: 49s - loss: 0.6205 - regression_loss: 0.5026 - classification_loss: 0.1178
 9892/10000 [============================>.] - ETA: 49s - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9893/10000 [============================>.] - ETA: 48s - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9894/10000 [============================>.] - ETA: 48s - loss: 0.6204 - regression_loss: 0.5026 - classification_loss: 0.1178
 9895/10000 [============================>.] - ETA: 47s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9896/10000 [============================>.] - ETA: 47s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9897/10000 [============================>.] - ETA: 47s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9898/10000 [============================>.] - ETA: 46s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9899/10000 [============================>.] - ETA: 46s - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9900/10000 [============================>.] - ETA: 45s - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9901/10000 [============================>.] - ETA: 45s - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9902/10000 [============================>.] - ETA: 44s - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9903/10000 [============================>.] - ETA: 44s - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1178
 9904/10000 [============================>.] - ETA: 43s - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1178
 9905/10000 [============================>.] - ETA: 43s - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9906/10000 [============================>.] - ETA: 42s - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1177
 9907/10000 [============================>.] - ETA: 42s - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1178
 9908/10000 [============================>.] - ETA: 42s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9909/10000 [============================>.] - ETA: 41s - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9910/10000 [============================>.] - ETA: 41s - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9911/10000 [============================>.] - ETA: 40s - loss: 0.6201 - regression_loss: 0.5024 - classification_loss: 0.1178
 9912/10000 [============================>.] - ETA: 40s - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9913/10000 [============================>.] - ETA: 39s - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9914/10000 [============================>.] - ETA: 39s - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9915/10000 [============================>.] - ETA: 38s - loss: 0.6202 - regression_loss: 0.5025 - classification_loss: 0.1178
 9916/10000 [============================>.] - ETA: 38s - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9917/10000 [============================>.] - ETA: 37s - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9918/10000 [============================>.] - ETA: 37s - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9919/10000 [============================>.] - ETA: 37s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9920/10000 [============================>.] - ETA: 36s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9921/10000 [============================>.] - ETA: 36s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9922/10000 [============================>.] - ETA: 35s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9923/10000 [============================>.] - ETA: 35s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9924/10000 [============================>.] - ETA: 34s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9925/10000 [============================>.] - ETA: 34s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9926/10000 [============================>.] - ETA: 33s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9927/10000 [============================>.] - ETA: 33s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9928/10000 [============================>.] - ETA: 32s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9929/10000 [============================>.] - ETA: 32s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9930/10000 [============================>.] - ETA: 31s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1178
 9931/10000 [============================>.] - ETA: 31s - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1178
 9932/10000 [============================>.] - ETA: 31s - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1178
 9933/10000 [============================>.] - ETA: 30s - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1178
 9934/10000 [============================>.] - ETA: 30s - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1178
 9935/10000 [============================>.] - ETA: 29s - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1179
 9936/10000 [============================>.] - ETA: 29s - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 9937/10000 [============================>.] - ETA: 28s - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 9938/10000 [============================>.] - ETA: 28s - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1179
 9939/10000 [============================>.] - ETA: 27s - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 9940/10000 [============================>.] - ETA: 27s - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 9941/10000 [============================>.] - ETA: 26s - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 9942/10000 [============================>.] - ETA: 26s - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1179
 9943/10000 [============================>.] - ETA: 26s - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 9944/10000 [============================>.] - ETA: 25s - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 9945/10000 [============================>.] - ETA: 25s - loss: 0.6204 - regression_loss: 0.5025 - classification_loss: 0.1179
 9946/10000 [============================>.] - ETA: 24s - loss: 0.6204 - regression_loss: 0.5025 - classification_loss: 0.1179
 9947/10000 [============================>.] - ETA: 24s - loss: 0.6203 - regression_loss: 0.5025 - classification_loss: 0.1179
 9948/10000 [============================>.] - ETA: 23s - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 9949/10000 [============================>.] - ETA: 23s - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 9950/10000 [============================>.] - ETA: 22s - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1179
 9951/10000 [============================>.] - ETA: 22s - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 9952/10000 [============================>.] - ETA: 21s - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1179
 9953/10000 [============================>.] - ETA: 21s - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 9954/10000 [============================>.] - ETA: 21s - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1179
 9955/10000 [============================>.] - ETA: 20s - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1179
 9956/10000 [============================>.] - ETA: 20s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9957/10000 [============================>.] - ETA: 19s - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 9958/10000 [============================>.] - ETA: 19s - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 9959/10000 [============================>.] - ETA: 18s - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1179
 9960/10000 [============================>.] - ETA: 18s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9961/10000 [============================>.] - ETA: 17s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9962/10000 [============================>.] - ETA: 17s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9963/10000 [============================>.] - ETA: 16s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9964/10000 [============================>.] - ETA: 16s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9965/10000 [============================>.] - ETA: 15s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9966/10000 [============================>.] - ETA: 15s - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1179
 9967/10000 [============================>.] - ETA: 15s - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1179
 9968/10000 [============================>.] - ETA: 14s - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1179
 9969/10000 [============================>.] - ETA: 14s - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9970/10000 [============================>.] - ETA: 13s - loss: 0.6201 - regression_loss: 0.5022 - classification_loss: 0.1178
 9971/10000 [============================>.] - ETA: 13s - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9972/10000 [============================>.] - ETA: 12s - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1178
 9973/10000 [============================>.] - ETA: 12s - loss: 0.6201 - regression_loss: 0.5022 - classification_loss: 0.1179
 9974/10000 [============================>.] - ETA: 11s - loss: 0.6201 - regression_loss: 0.5022 - classification_loss: 0.1179
 9975/10000 [============================>.] - ETA: 11s - loss: 0.6201 - regression_loss: 0.5022 - classification_loss: 0.1179
 9976/10000 [============================>.] - ETA: 10s - loss: 0.6201 - regression_loss: 0.5022 - classification_loss: 0.1179
 9977/10000 [============================>.] - ETA: 10s - loss: 0.6201 - regression_loss: 0.5023 - classification_loss: 0.1179
 9978/10000 [============================>.] - ETA: 10s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9979/10000 [============================>.] - ETA: 9s - loss: 0.6202 - regression_loss: 0.5024 - classification_loss: 0.1179 
 9980/10000 [============================>.] - ETA: 9s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9981/10000 [============================>.] - ETA: 8s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9982/10000 [============================>.] - ETA: 8s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9983/10000 [============================>.] - ETA: 7s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9984/10000 [============================>.] - ETA: 7s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9985/10000 [============================>.] - ETA: 6s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9986/10000 [============================>.] - ETA: 6s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9987/10000 [============================>.] - ETA: 5s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9988/10000 [============================>.] - ETA: 5s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9989/10000 [============================>.] - ETA: 5s - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 9990/10000 [============================>.] - ETA: 4s - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 9991/10000 [============================>.] - ETA: 4s - loss: 0.6203 - regression_loss: 0.5024 - classification_loss: 0.1179
 9992/10000 [============================>.] - ETA: 3s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9993/10000 [============================>.] - ETA: 3s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9994/10000 [============================>.] - ETA: 2s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9995/10000 [============================>.] - ETA: 2s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9996/10000 [============================>.] - ETA: 1s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9997/10000 [============================>.] - ETA: 1s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9998/10000 [============================>.] - ETA: 0s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
 9999/10000 [============================>.] - ETA: 0s - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
10000/10000 [==============================] - 4568s 457ms/step - loss: 0.6202 - regression_loss: 0.5023 - classification_loss: 0.1179
1/49522/49523/49524/49525/49526/49527/49528/49529/495210/495211/495212/495213/495214/495215/495216/495217/495218/495219/495220/495221/495222/495223/495224/495225/495226/495227/495228/495229/495230/495231/495232/495233/495234/495235/495236/495237/495238/495239/495240/495241/495242/495243/495244/495245/495246/495247/495248/495249/495250/495251/495252/495253/495254/495255/495256/495257/495258/495259/495260/495261/495262/495263/495264/495265/495266/495267/495268/495269/495270/495271/495272/495273/495274/495275/495276/495277/495278/495279/495280/495281/495282/495283/495284/495285/495286/495287/495288/495289/495290/495291/495292/495293/495294/495295/495296/495297/495298/495299/4952100/4952101/4952102/4952103/4952104/4952105/4952106/4952107/4952108/4952109/4952110/4952111/4952112/4952113/4952114/4952115/4952116/4952117/4952118/4952119/4952120/4952121/4952122/4952123/4952124/4952125/4952126/4952127/4952128/4952129/4952130/4952131/4952132/4952133/4952134/4952135/4952136/4952137/4952138/4952139/4952140/4952141/4952142/4952143/4952144/4952145/4952146/4952147/4952148/4952149/4952150/4952151/4952152/4952153/4952154/4952155/4952156/4952157/4952158/4952159/4952160/4952161/4952162/4952163/4952164/4952165/4952166/4952167/4952168/4952169/4952170/4952171/4952172/4952173/4952174/4952175/4952176/4952177/4952178/4952179/4952180/4952181/4952182/4952183/4952184/4952185/4952186/4952187/4952188/4952189/4952190/4952191/4952192/4952193/4952194/4952195/4952196/4952197/4952198/4952199/4952200/4952201/4952202/4952203/4952204/4952205/4952206/4952207/4952208/4952209/4952210/4952211/4952212/4952213/4952214/4952215/4952216/4952217/4952218/4952219/4952220/4952221/4952222/4952223/4952224/4952225/4952226/4952227/4952228/4952229/4952230/4952231/4952232/4952233/4952234/4952235/4952236/4952237/4952238/4952239/4952240/4952241/4952242/4952243/4952244/4952245/4952246/4952247/4952248/4952249/4952250/4952251/4952252/4952253/4952254/4952255/4952256/4952257/4952258/4952259/4952260/4952261/4952262/4952263/4952264/4952265/4952266/4952267/4952268/4952269/4952270/4952271/4952272/4952273/4952274/4952275/4952276/4952277/4952278/4952279/4952280/4952281/4952282/4952283/4952284/4952285/4952286/4952287/4952288/4952289/4952290/4952291/4952292/4952293/4952294/4952295/4952296/4952297/4952298/4952299/4952300/4952301/4952302/4952303/4952304/4952305/4952306/4952307/4952308/4952309/4952310/4952311/4952312/4952313/4952314/4952315/4952316/4952317/4952318/4952319/4952320/4952321/4952322/4952323/4952324/4952325/4952326/4952327/4952328/4952329/4952330/4952331/4952332/4952333/4952334/4952335/4952336/4952337/4952338/4952339/4952340/4952341/4952342/4952343/4952344/4952345/4952346/4952347/4952348/4952349/4952350/4952351/4952352/4952353/4952354/4952355/4952356/4952357/4952358/4952359/4952360/4952361/4952362/4952363/4952364/4952365/4952366/4952367/4952368/4952369/4952370/4952371/4952372/4952373/4952374/4952375/4952376/4952377/4952378/4952379/4952380/4952381/4952382/4952383/4952384/4952385/4952386/4952387/4952388/4952389/4952390/4952391/4952392/4952393/4952394/4952395/4952396/4952397/4952398/4952399/4952400/4952401/4952402/4952403/4952404/4952405/4952406/4952407/4952408/4952409/4952410/4952411/4952412/4952413/4952414/4952415/4952416/4952417/4952418/4952419/4952420/4952421/4952422/4952423/4952424/4952425/4952426/4952427/4952428/4952429/4952430/4952431/4952432/4952433/4952434/4952435/4952436/4952437/4952438/4952439/4952440/4952441/4952442/4952443/4952444/4952445/4952446/4952447/4952448/4952449/4952450/4952451/4952452/4952453/4952454/4952455/4952456/4952457/4952458/4952459/4952460/4952461/4952462/4952463/4952464/4952465/4952466/4952467/4952468/4952469/4952470/4952471/4952472/4952473/4952474/4952475/4952476/4952477/4952478/4952479/4952480/4952481/4952482/4952483/4952484/4952485/4952486/4952487/4952488/4952489/4952490/4952491/4952492/4952493/4952494/4952495/4952496/4952497/4952498/4952499/4952500/4952501/4952502/4952503/4952504/4952505/4952506/4952507/4952508/4952509/4952510/4952511/4952512/4952513/4952514/4952515/4952516/4952517/4952518/4952519/4952520/4952521/4952522/4952523/4952524/4952525/4952526/4952527/4952528/4952529/4952530/4952531/4952532/4952533/4952534/4952535/4952536/4952537/4952538/4952539/4952540/4952541/4952542/4952543/4952544/4952545/4952546/4952547/4952548/4952549/4952550/4952551/4952552/4952553/4952554/4952555/4952556/4952557/4952558/4952559/4952560/4952561/4952562/4952563/4952564/4952565/4952566/4952567/4952568/4952569/4952570/4952571/4952572/4952573/4952574/4952575/4952576/4952577/4952578/4952579/4952580/4952581/4952582/4952583/4952584/4952585/4952586/4952587/4952588/4952589/4952590/4952591/4952592/4952593/4952594/4952595/4952596/4952597/4952598/4952599/4952600/4952601/4952602/4952603/4952604/4952605/4952606/4952607/4952608/4952609/4952610/4952611/4952612/4952613/4952614/4952615/4952616/4952617/4952618/4952619/4952620/4952621/4952622/4952623/4952624/4952625/4952626/4952627/4952628/4952629/4952630/4952631/4952632/4952633/4952634/4952635/4952636/4952637/4952638/4952639/4952640/4952641/4952642/4952643/4952644/4952645/4952646/4952647/4952648/4952649/4952650/4952651/4952652/4952653/4952654/4952655/4952656/4952657/4952658/4952659/4952660/4952661/4952662/4952663/4952664/4952665/4952666/4952667/4952668/4952669/4952670/4952671/4952672/4952673/4952674/4952675/4952676/4952677/4952678/4952679/4952680/4952681/4952682/4952683/4952684/4952685/4952686/4952687/4952688/4952689/4952690/4952691/4952692/4952693/4952694/4952695/4952696/4952697/4952698/4952699/4952700/4952701/4952702/4952703/4952704/4952705/4952706/4952707/4952708/4952709/4952710/4952711/4952712/4952713/4952714/4952715/4952716/4952717/4952718/4952719/4952720/4952721/4952722/4952723/4952724/4952725/4952726/4952727/4952728/4952729/4952730/4952731/4952732/4952733/4952734/4952735/4952736/4952737/4952738/4952739/4952740/4952741/4952742/4952743/4952744/4952745/4952746/4952747/4952748/4952749/4952750/4952751/4952752/4952753/4952754/4952755/4952756/4952757/4952758/4952759/4952760/4952761/4952762/4952763/4952764/4952765/4952766/4952767/4952768/4952769/4952770/4952771/4952772/4952773/4952774/4952775/4952776/4952777/4952778/4952779/4952780/4952781/4952782/4952783/4952784/4952785/4952786/4952787/4952788/4952789/4952790/4952791/4952792/4952793/4952794/4952795/4952796/4952797/4952798/4952799/4952800/4952801/4952802/4952803/4952804/4952805/4952806/4952807/4952808/4952809/4952810/4952811/4952812/4952813/4952814/4952815/4952816/4952817/4952818/4952819/4952820/4952821/4952822/4952823/4952824/4952825/4952826/4952827/4952828/4952829/4952830/4952831/4952832/4952833/4952834/4952835/4952836/4952837/4952838/4952839/4952840/4952841/4952842/4952843/4952844/4952845/4952846/4952847/4952848/4952849/4952850/4952851/4952852/4952853/4952854/4952855/4952856/4952857/4952858/4952859/4952860/4952861/4952862/4952863/4952864/4952865/4952866/4952867/4952868/4952869/4952870/4952871/4952872/4952873/4952874/4952875/4952876/4952877/4952878/4952879/4952880/4952881/4952882/4952883/4952884/4952885/4952886/4952887/4952888/4952889/4952890/4952891/4952892/4952893/4952894/4952895/4952896/4952897/4952898/4952899/4952900/4952901/4952902/4952903/4952904/4952905/4952906/4952907/4952908/4952909/4952910/4952911/4952912/4952913/4952914/4952915/4952916/4952917/4952918/4952919/4952920/4952921/4952922/4952923/4952924/4952925/4952926/4952927/4952928/4952929/4952930/4952931/4952932/4952933/4952934/4952935/4952936/4952937/4952938/4952939/4952940/4952941/4952942/4952943/4952944/4952945/4952946/4952947/4952948/4952949/4952950/4952951/4952952/4952953/4952954/4952955/4952956/4952957/4952958/4952959/4952960/4952961/4952962/4952963/4952964/4952965/4952966/4952967/4952968/4952969/4952970/4952971/4952972/4952973/4952974/4952975/4952976/4952977/4952978/4952979/4952980/4952981/4952982/4952983/4952984/4952985/4952986/4952987/4952988/4952989/4952990/4952991/4952992/4952993/4952994/4952995/4952996/4952997/4952998/4952999/49521000/49521001/49521002/49521003/49521004/49521005/49521006/49521007/49521008/49521009/49521010/49521011/49521012/49521013/49521014/49521015/49521016/49521017/49521018/49521019/49521020/49521021/49521022/49521023/49521024/49521025/49521026/49521027/49521028/49521029/49521030/49521031/49521032/49521033/49521034/49521035/49521036/49521037/49521038/49521039/49521040/49521041/49521042/49521043/49521044/49521045/49521046/49521047/49521048/49521049/49521050/49521051/49521052/49521053/49521054/49521055/49521056/49521057/49521058/49521059/49521060/49521061/49521062/49521063/49521064/49521065/49521066/49521067/49521068/49521069/49521070/49521071/49521072/49521073/49521074/49521075/49521076/49521077/49521078/49521079/49521080/49521081/49521082/49521083/49521084/49521085/49521086/49521087/49521088/49521089/49521090/49521091/49521092/49521093/49521094/49521095/49521096/49521097/49521098/49521099/49521100/49521101/49521102/49521103/49521104/49521105/49521106/49521107/49521108/49521109/49521110/49521111/49521112/49521113/49521114/49521115/49521116/49521117/49521118/49521119/49521120/49521121/49521122/49521123/49521124/49521125/49521126/49521127/49521128/49521129/49521130/49521131/49521132/49521133/49521134/49521135/49521136/49521137/49521138/49521139/49521140/49521141/49521142/49521143/49521144/49521145/49521146/49521147/49521148/49521149/49521150/49521151/49521152/49521153/49521154/49521155/49521156/49521157/49521158/49521159/49521160/49521161/49521162/49521163/49521164/49521165/49521166/49521167/49521168/49521169/49521170/49521171/49521172/49521173/49521174/49521175/49521176/49521177/49521178/49521179/49521180/49521181/49521182/49521183/49521184/49521185/49521186/49521187/49521188/49521189/49521190/49521191/49521192/49521193/49521194/49521195/49521196/49521197/49521198/49521199/49521200/49521201/49521202/49521203/49521204/49521205/49521206/49521207/49521208/49521209/49521210/49521211/49521212/49521213/49521214/49521215/49521216/49521217/49521218/49521219/49521220/49521221/49521222/49521223/49521224/49521225/49521226/49521227/49521228/49521229/49521230/49521231/49521232/49521233/49521234/49521235/49521236/49521237/49521238/49521239/49521240/49521241/49521242/49521243/49521244/49521245/49521246/49521247/49521248/49521249/49521250/49521251/49521252/49521253/49521254/49521255/49521256/49521257/49521258/49521259/49521260/49521261/49521262/49521263/49521264/49521265/49521266/49521267/49521268/49521269/49521270/49521271/49521272/49521273/49521274/49521275/49521276/49521277/49521278/49521279/49521280/49521281/49521282/49521283/49521284/49521285/49521286/49521287/49521288/49521289/49521290/49521291/49521292/49521293/49521294/49521295/49521296/49521297/49521298/49521299/49521300/49521301/49521302/49521303/49521304/49521305/49521306/49521307/49521308/49521309/49521310/49521311/49521312/49521313/49521314/49521315/49521316/49521317/49521318/49521319/49521320/49521321/49521322/49521323/49521324/49521325/49521326/49521327/49521328/49521329/49521330/49521331/49521332/49521333/49521334/49521335/49521336/49521337/49521338/49521339/49521340/49521341/49521342/49521343/49521344/49521345/49521346/49521347/49521348/49521349/49521350/49521351/49521352/49521353/49521354/49521355/49521356/49521357/49521358/49521359/49521360/49521361/49521362/49521363/49521364/49521365/49521366/49521367/49521368/49521369/49521370/49521371/49521372/49521373/49521374/49521375/49521376/49521377/49521378/49521379/49521380/49521381/49521382/49521383/49521384/49521385/49521386/49521387/49521388/49521389/49521390/49521391/49521392/49521393/49521394/49521395/49521396/49521397/49521398/49521399/49521400/49521401/49521402/49521403/49521404/49521405/49521406/49521407/49521408/49521409/49521410/49521411/49521412/49521413/49521414/49521415/49521416/49521417/49521418/49521419/49521420/49521421/49521422/49521423/49521424/49521425/49521426/49521427/49521428/49521429/49521430/49521431/49521432/49521433/49521434/49521435/49521436/49521437/49521438/49521439/49521440/49521441/49521442/49521443/49521444/49521445/49521446/49521447/49521448/49521449/49521450/49521451/49521452/49521453/49521454/49521455/49521456/49521457/49521458/49521459/49521460/49521461/49521462/49521463/49521464/49521465/49521466/49521467/49521468/49521469/49521470/49521471/49521472/49521473/49521474/49521475/49521476/49521477/49521478/49521479/49521480/49521481/49521482/49521483/49521484/49521485/49521486/49521487/49521488/49521489/49521490/49521491/49521492/49521493/49521494/49521495/49521496/49521497/49521498/49521499/49521500/49521501/49521502/49521503/49521504/49521505/49521506/49521507/49521508/49521509/49521510/49521511/49521512/49521513/49521514/49521515/49521516/49521517/49521518/49521519/49521520/49521521/49521522/49521523/49521524/49521525/49521526/49521527/49521528/49521529/49521530/49521531/49521532/49521533/49521534/49521535/49521536/49521537/49521538/49521539/49521540/49521541/49521542/49521543/49521544/49521545/49521546/49521547/49521548/49521549/49521550/49521551/49521552/49521553/49521554/49521555/49521556/49521557/49521558/49521559/49521560/49521561/49521562/49521563/49521564/49521565/49521566/49521567/49521568/49521569/49521570/49521571/49521572/49521573/49521574/49521575/49521576/49521577/49521578/49521579/49521580/49521581/49521582/49521583/49521584/49521585/49521586/49521587/49521588/49521589/49521590/49521591/49521592/49521593/49521594/49521595/49521596/49521597/49521598/49521599/49521600/49521601/49521602/49521603/49521604/49521605/49521606/49521607/49521608/49521609/49521610/49521611/49521612/49521613/49521614/49521615/49521616/49521617/49521618/49521619/49521620/49521621/49521622/49521623/49521624/49521625/49521626/49521627/49521628/49521629/49521630/49521631/49521632/49521633/49521634/49521635/49521636/49521637/49521638/49521639/49521640/49521641/49521642/49521643/49521644/49521645/49521646/49521647/49521648/49521649/49521650/49521651/49521652/49521653/49521654/49521655/49521656/49521657/49521658/49521659/49521660/49521661/49521662/49521663/49521664/49521665/49521666/49521667/49521668/49521669/49521670/49521671/49521672/49521673/49521674/49521675/49521676/49521677/49521678/49521679/49521680/49521681/49521682/49521683/49521684/49521685/49521686/49521687/49521688/49521689/49521690/49521691/49521692/49521693/49521694/49521695/49521696/49521697/49521698/49521699/49521700/49521701/49521702/49521703/49521704/49521705/49521706/49521707/49521708/49521709/49521710/49521711/49521712/49521713/49521714/49521715/49521716/49521717/49521718/49521719/49521720/49521721/49521722/49521723/49521724/49521725/49521726/49521727/49521728/49521729/49521730/49521731/49521732/49521733/49521734/49521735/49521736/49521737/49521738/49521739/49521740/49521741/49521742/49521743/49521744/49521745/49521746/49521747/49521748/49521749/49521750/49521751/49521752/49521753/49521754/49521755/49521756/49521757/49521758/49521759/49521760/49521761/49521762/49521763/49521764/49521765/49521766/49521767/49521768/49521769/49521770/49521771/49521772/49521773/49521774/49521775/49521776/49521777/49521778/49521779/49521780/49521781/49521782/49521783/49521784/49521785/49521786/49521787/49521788/49521789/49521790/49521791/49521792/49521793/49521794/49521795/49521796/49521797/49521798/49521799/49521800/49521801/49521802/49521803/49521804/49521805/49521806/49521807/49521808/49521809/49521810/49521811/49521812/49521813/49521814/49521815/49521816/49521817/49521818/49521819/49521820/49521821/49521822/49521823/49521824/49521825/49521826/49521827/49521828/49521829/49521830/49521831/49521832/49521833/49521834/49521835/49521836/49521837/49521838/49521839/49521840/49521841/49521842/49521843/49521844/49521845/49521846/49521847/49521848/49521849/49521850/49521851/49521852/49521853/49521854/49521855/49521856/49521857/49521858/49521859/49521860/49521861/49521862/49521863/49521864/49521865/49521866/49521867/49521868/49521869/49521870/49521871/49521872/49521873/49521874/49521875/49521876/49521877/49521878/49521879/49521880/49521881/49521882/49521883/49521884/49521885/49521886/49521887/49521888/49521889/49521890/49521891/49521892/49521893/49521894/49521895/49521896/49521897/49521898/49521899/49521900/49521901/49521902/49521903/49521904/49521905/49521906/49521907/49521908/49521909/49521910/49521911/49521912/49521913/49521914/49521915/49521916/49521917/49521918/49521919/49521920/49521921/49521922/49521923/49521924/49521925/49521926/49521927/49521928/49521929/49521930/49521931/49521932/49521933/49521934/49521935/49521936/49521937/49521938/49521939/49521940/49521941/49521942/49521943/49521944/49521945/49521946/49521947/49521948/49521949/49521950/49521951/49521952/49521953/49521954/49521955/49521956/49521957/49521958/49521959/49521960/49521961/49521962/49521963/49521964/49521965/49521966/49521967/49521968/49521969/49521970/49521971/49521972/49521973/49521974/49521975/49521976/49521977/49521978/49521979/49521980/49521981/49521982/49521983/49521984/49521985/49521986/49521987/49521988/49521989/49521990/49521991/49521992/49521993/49521994/49521995/49521996/49521997/49521998/49521999/49522000/49522001/49522002/49522003/49522004/49522005/49522006/49522007/49522008/49522009/49522010/49522011/49522012/49522013/49522014/49522015/49522016/49522017/49522018/49522019/49522020/49522021/49522022/49522023/49522024/49522025/49522026/49522027/49522028/49522029/49522030/49522031/49522032/49522033/49522034/49522035/49522036/49522037/49522038/49522039/49522040/49522041/49522042/49522043/49522044/49522045/49522046/49522047/49522048/49522049/49522050/49522051/49522052/49522053/49522054/49522055/49522056/49522057/49522058/49522059/49522060/49522061/49522062/49522063/49522064/49522065/49522066/49522067/49522068/49522069/49522070/49522071/49522072/49522073/49522074/49522075/49522076/49522077/49522078/49522079/49522080/49522081/49522082/49522083/49522084/49522085/49522086/49522087/49522088/49522089/49522090/49522091/49522092/49522093/49522094/49522095/49522096/49522097/49522098/49522099/49522100/49522101/49522102/49522103/49522104/49522105/49522106/49522107/49522108/49522109/49522110/49522111/49522112/49522113/49522114/49522115/49522116/49522117/49522118/49522119/49522120/49522121/49522122/49522123/49522124/49522125/49522126/49522127/49522128/49522129/49522130/49522131/49522132/49522133/49522134/49522135/49522136/49522137/49522138/49522139/49522140/49522141/49522142/49522143/49522144/49522145/49522146/49522147/49522148/49522149/49522150/49522151/49522152/49522153/49522154/49522155/49522156/49522157/49522158/49522159/49522160/49522161/49522162/49522163/49522164/49522165/49522166/49522167/49522168/49522169/49522170/49522171/49522172/49522173/49522174/49522175/49522176/49522177/49522178/49522179/49522180/49522181/49522182/49522183/49522184/49522185/49522186/49522187/49522188/49522189/49522190/49522191/49522192/49522193/49522194/49522195/49522196/49522197/49522198/49522199/49522200/49522201/49522202/49522203/49522204/49522205/49522206/49522207/49522208/49522209/49522210/49522211/49522212/49522213/49522214/49522215/49522216/49522217/49522218/49522219/49522220/49522221/49522222/49522223/49522224/49522225/49522226/49522227/49522228/49522229/49522230/49522231/49522232/49522233/49522234/49522235/49522236/49522237/49522238/49522239/49522240/49522241/49522242/49522243/49522244/49522245/49522246/49522247/49522248/49522249/49522250/49522251/49522252/49522253/49522254/49522255/49522256/49522257/49522258/49522259/49522260/49522261/49522262/49522263/49522264/49522265/49522266/49522267/49522268/49522269/49522270/49522271/49522272/49522273/49522274/49522275/49522276/49522277/49522278/49522279/49522280/49522281/49522282/49522283/49522284/49522285/49522286/49522287/49522288/49522289/49522290/49522291/49522292/49522293/49522294/49522295/49522296/49522297/49522298/49522299/49522300/49522301/49522302/49522303/49522304/49522305/49522306/49522307/49522308/49522309/49522310/49522311/49522312/49522313/49522314/49522315/49522316/49522317/49522318/49522319/49522320/49522321/49522322/49522323/49522324/49522325/49522326/49522327/49522328/49522329/49522330/49522331/49522332/49522333/49522334/49522335/49522336/49522337/49522338/49522339/49522340/49522341/49522342/49522343/49522344/49522345/49522346/49522347/49522348/49522349/49522350/49522351/49522352/49522353/49522354/49522355/49522356/49522357/49522358/49522359/49522360/49522361/49522362/49522363/49522364/49522365/49522366/49522367/49522368/49522369/49522370/49522371/49522372/49522373/49522374/49522375/49522376/49522377/49522378/49522379/49522380/49522381/49522382/49522383/49522384/49522385/49522386/49522387/49522388/49522389/49522390/49522391/49522392/49522393/49522394/49522395/49522396/49522397/49522398/49522399/49522400/49522401/49522402/49522403/49522404/49522405/49522406/49522407/49522408/49522409/49522410/49522411/49522412/49522413/49522414/49522415/49522416/49522417/49522418/49522419/49522420/49522421/49522422/49522423/49522424/49522425/49522426/49522427/49522428/49522429/49522430/49522431/49522432/49522433/49522434/49522435/49522436/49522437/49522438/49522439/49522440/49522441/49522442/49522443/49522444/49522445/49522446/49522447/49522448/49522449/49522450/49522451/49522452/49522453/49522454/49522455/49522456/49522457/49522458/49522459/49522460/49522461/49522462/49522463/49522464/49522465/49522466/49522467/49522468/49522469/49522470/49522471/49522472/49522473/49522474/49522475/49522476/49522477/49522478/49522479/49522480/49522481/49522482/49522483/49522484/49522485/49522486/49522487/49522488/49522489/49522490/49522491/49522492/49522493/49522494/49522495/49522496/49522497/49522498/49522499/49522500/49522501/49522502/49522503/49522504/49522505/49522506/49522507/49522508/49522509/49522510/49522511/49522512/49522513/49522514/49522515/49522516/49522517/49522518/49522519/49522520/49522521/49522522/49522523/49522524/49522525/49522526/49522527/49522528/49522529/49522530/49522531/49522532/49522533/49522534/49522535/49522536/49522537/49522538/49522539/49522540/49522541/49522542/49522543/49522544/49522545/49522546/49522547/49522548/49522549/49522550/49522551/49522552/49522553/49522554/49522555/49522556/49522557/49522558/49522559/49522560/49522561/49522562/49522563/49522564/49522565/49522566/49522567/49522568/49522569/49522570/49522571/49522572/49522573/49522574/49522575/49522576/49522577/49522578/49522579/49522580/49522581/49522582/49522583/49522584/49522585/49522586/49522587/49522588/49522589/49522590/49522591/49522592/49522593/49522594/49522595/49522596/49522597/49522598/49522599/49522600/49522601/49522602/49522603/49522604/49522605/49522606/49522607/49522608/49522609/49522610/49522611/49522612/49522613/49522614/49522615/49522616/49522617/49522618/49522619/49522620/49522621/49522622/49522623/49522624/49522625/49522626/49522627/49522628/49522629/49522630/49522631/49522632/49522633/49522634/49522635/49522636/49522637/49522638/49522639/49522640/49522641/49522642/49522643/49522644/49522645/49522646/49522647/49522648/49522649/49522650/49522651/49522652/49522653/49522654/49522655/49522656/49522657/49522658/49522659/49522660/49522661/49522662/49522663/49522664/49522665/49522666/49522667/49522668/49522669/49522670/49522671/49522672/49522673/49522674/49522675/49522676/49522677/49522678/49522679/49522680/49522681/49522682/49522683/49522684/49522685/49522686/49522687/49522688/49522689/49522690/49522691/49522692/49522693/49522694/49522695/49522696/49522697/49522698/49522699/49522700/49522701/49522702/49522703/49522704/49522705/49522706/49522707/49522708/49522709/49522710/49522711/49522712/49522713/49522714/49522715/49522716/49522717/49522718/49522719/49522720/49522721/49522722/49522723/49522724/49522725/49522726/49522727/49522728/49522729/49522730/49522731/49522732/49522733/49522734/49522735/49522736/49522737/49522738/49522739/49522740/49522741/49522742/49522743/49522744/49522745/49522746/49522747/49522748/49522749/49522750/49522751/49522752/49522753/49522754/49522755/49522756/49522757/49522758/49522759/49522760/49522761/49522762/49522763/49522764/49522765/49522766/49522767/49522768/49522769/49522770/49522771/49522772/49522773/49522774/49522775/49522776/49522777/49522778/49522779/49522780/49522781/49522782/49522783/49522784/49522785/49522786/49522787/49522788/49522789/49522790/49522791/49522792/49522793/49522794/49522795/49522796/49522797/49522798/49522799/49522800/49522801/49522802/49522803/49522804/49522805/49522806/49522807/49522808/49522809/49522810/49522811/49522812/49522813/49522814/49522815/49522816/49522817/49522818/49522819/49522820/49522821/49522822/49522823/49522824/49522825/49522826/49522827/49522828/49522829/49522830/49522831/49522832/49522833/49522834/49522835/49522836/49522837/49522838/49522839/49522840/49522841/49522842/49522843/49522844/49522845/49522846/49522847/49522848/49522849/49522850/49522851/49522852/49522853/49522854/49522855/49522856/49522857/49522858/49522859/49522860/49522861/49522862/49522863/49522864/49522865/49522866/49522867/49522868/49522869/49522870/49522871/49522872/49522873/49522874/49522875/49522876/49522877/49522878/49522879/49522880/49522881/49522882/49522883/49522884/49522885/49522886/49522887/49522888/49522889/49522890/49522891/49522892/49522893/49522894/49522895/49522896/49522897/49522898/49522899/49522900/49522901/49522902/49522903/49522904/49522905/49522906/49522907/49522908/49522909/49522910/49522911/49522912/49522913/49522914/49522915/49522916/49522917/49522918/49522919/49522920/49522921/49522922/49522923/49522924/49522925/49522926/49522927/49522928/49522929/49522930/49522931/49522932/49522933/49522934/49522935/49522936/49522937/49522938/49522939/49522940/49522941/49522942/49522943/49522944/49522945/49522946/49522947/49522948/49522949/49522950/49522951/49522952/49522953/49522954/49522955/49522956/49522957/49522958/49522959/49522960/49522961/49522962/49522963/49522964/49522965/49522966/49522967/49522968/49522969/49522970/49522971/49522972/49522973/49522974/49522975/49522976/49522977/49522978/49522979/49522980/49522981/49522982/49522983/49522984/49522985/49522986/49522987/49522988/49522989/49522990/49522991/49522992/49522993/49522994/49522995/49522996/49522997/49522998/49522999/49523000/49523001/49523002/49523003/49523004/49523005/49523006/49523007/49523008/49523009/49523010/49523011/49523012/49523013/49523014/49523015/49523016/49523017/49523018/49523019/49523020/49523021/49523022/49523023/49523024/49523025/49523026/49523027/49523028/49523029/49523030/49523031/49523032/49523033/49523034/49523035/49523036/49523037/49523038/49523039/49523040/49523041/49523042/49523043/49523044/49523045/49523046/49523047/49523048/49523049/49523050/49523051/49523052/49523053/49523054/49523055/49523056/49523057/49523058/49523059/49523060/49523061/49523062/49523063/49523064/49523065/49523066/49523067/49523068/49523069/49523070/49523071/49523072/49523073/49523074/49523075/49523076/49523077/49523078/49523079/49523080/49523081/49523082/49523083/49523084/49523085/49523086/49523087/49523088/49523089/49523090/49523091/49523092/49523093/49523094/49523095/49523096/49523097/49523098/49523099/49523100/49523101/49523102/49523103/49523104/49523105/49523106/49523107/49523108/49523109/49523110/49523111/49523112/49523113/49523114/49523115/49523116/49523117/49523118/49523119/49523120/49523121/49523122/49523123/49523124/49523125/49523126/49523127/49523128/49523129/49523130/49523131/49523132/49523133/49523134/49523135/49523136/49523137/49523138/49523139/49523140/49523141/49523142/49523143/49523144/49523145/49523146/49523147/49523148/49523149/49523150/49523151/49523152/49523153/49523154/49523155/49523156/49523157/49523158/49523159/49523160/49523161/49523162/49523163/49523164/49523165/49523166/49523167/49523168/49523169/49523170/49523171/49523172/49523173/49523174/49523175/49523176/49523177/49523178/49523179/49523180/49523181/49523182/49523183/49523184/49523185/49523186/49523187/49523188/49523189/49523190/49523191/49523192/49523193/49523194/49523195/49523196/49523197/49523198/49523199/49523200/49523201/49523202/49523203/49523204/49523205/49523206/49523207/49523208/49523209/49523210/49523211/49523212/49523213/49523214/49523215/49523216/49523217/49523218/49523219/49523220/49523221/49523222/49523223/49523224/49523225/49523226/49523227/49523228/49523229/49523230/49523231/49523232/49523233/49523234/49523235/49523236/49523237/49523238/49523239/49523240/49523241/49523242/49523243/49523244/49523245/49523246/49523247/49523248/49523249/49523250/49523251/49523252/49523253/49523254/49523255/49523256/49523257/49523258/49523259/49523260/49523261/49523262/49523263/49523264/49523265/49523266/49523267/49523268/49523269/49523270/49523271/49523272/49523273/49523274/49523275/49523276/49523277/49523278/49523279/49523280/49523281/49523282/49523283/49523284/49523285/49523286/49523287/49523288/49523289/49523290/49523291/49523292/49523293/49523294/49523295/49523296/49523297/49523298/49523299/49523300/49523301/49523302/49523303/49523304/49523305/49523306/49523307/49523308/49523309/49523310/49523311/49523312/49523313/49523314/49523315/49523316/49523317/49523318/49523319/49523320/49523321/49523322/49523323/49523324/49523325/49523326/49523327/49523328/49523329/49523330/49523331/49523332/49523333/49523334/49523335/49523336/49523337/49523338/49523339/49523340/49523341/49523342/49523343/49523344/49523345/49523346/49523347/49523348/49523349/49523350/49523351/49523352/49523353/49523354/49523355/49523356/49523357/49523358/49523359/49523360/49523361/49523362/49523363/49523364/49523365/49523366/49523367/49523368/49523369/49523370/49523371/49523372/49523373/49523374/49523375/49523376/49523377/49523378/49523379/49523380/49523381/49523382/49523383/49523384/49523385/49523386/49523387/49523388/49523389/49523390/49523391/49523392/49523393/49523394/49523395/49523396/49523397/49523398/49523399/49523400/49523401/49523402/49523403/49523404/49523405/49523406/49523407/49523408/49523409/49523410/49523411/49523412/49523413/49523414/49523415/49523416/49523417/49523418/49523419/49523420/49523421/49523422/49523423/49523424/49523425/49523426/49523427/49523428/49523429/49523430/49523431/49523432/49523433/49523434/49523435/49523436/49523437/49523438/49523439/49523440/49523441/49523442/49523443/49523444/49523445/49523446/49523447/49523448/49523449/49523450/49523451/49523452/49523453/49523454/49523455/49523456/49523457/49523458/49523459/49523460/49523461/49523462/49523463/49523464/49523465/49523466/49523467/49523468/49523469/49523470/49523471/49523472/49523473/49523474/49523475/49523476/49523477/49523478/49523479/49523480/49523481/49523482/49523483/49523484/49523485/49523486/49523487/49523488/49523489/49523490/49523491/49523492/49523493/49523494/49523495/49523496/49523497/49523498/49523499/49523500/49523501/49523502/49523503/49523504/49523505/49523506/49523507/49523508/49523509/49523510/49523511/49523512/49523513/49523514/49523515/49523516/49523517/49523518/49523519/49523520/49523521/49523522/49523523/49523524/49523525/49523526/49523527/49523528/49523529/49523530/49523531/49523532/49523533/49523534/49523535/49523536/49523537/49523538/49523539/49523540/49523541/49523542/49523543/49523544/49523545/49523546/49523547/49523548/49523549/49523550/49523551/49523552/49523553/49523554/49523555/49523556/49523557/49523558/49523559/49523560/49523561/49523562/49523563/49523564/49523565/49523566/49523567/49523568/49523569/49523570/49523571/49523572/49523573/49523574/49523575/49523576/49523577/49523578/49523579/49523580/49523581/49523582/49523583/49523584/49523585/49523586/49523587/49523588/49523589/49523590/49523591/49523592/49523593/49523594/49523595/49523596/49523597/49523598/49523599/49523600/49523601/49523602/49523603/49523604/49523605/49523606/49523607/49523608/49523609/49523610/49523611/49523612/49523613/49523614/49523615/49523616/49523617/49523618/49523619/49523620/49523621/49523622/49523623/49523624/49523625/49523626/49523627/49523628/49523629/49523630/49523631/49523632/49523633/49523634/49523635/49523636/49523637/49523638/49523639/49523640/49523641/49523642/49523643/49523644/49523645/49523646/49523647/49523648/49523649/49523650/49523651/49523652/49523653/49523654/49523655/49523656/49523657/49523658/49523659/49523660/49523661/49523662/49523663/49523664/49523665/49523666/49523667/49523668/49523669/49523670/49523671/49523672/49523673/49523674/49523675/49523676/49523677/49523678/49523679/49523680/49523681/49523682/49523683/49523684/49523685/49523686/49523687/49523688/49523689/49523690/49523691/49523692/49523693/49523694/49523695/49523696/49523697/49523698/49523699/49523700/49523701/49523702/49523703/49523704/49523705/49523706/49523707/49523708/49523709/49523710/49523711/49523712/49523713/49523714/49523715/49523716/49523717/49523718/49523719/49523720/49523721/49523722/49523723/49523724/49523725/49523726/49523727/49523728/49523729/49523730/49523731/49523732/49523733/49523734/49523735/49523736/49523737/49523738/49523739/49523740/49523741/49523742/49523743/49523744/49523745/49523746/49523747/49523748/49523749/49523750/49523751/49523752/49523753/49523754/49523755/49523756/49523757/49523758/49523759/49523760/49523761/49523762/49523763/49523764/49523765/49523766/49523767/49523768/49523769/49523770/49523771/49523772/49523773/49523774/49523775/49523776/49523777/49523778/49523779/49523780/49523781/49523782/49523783/49523784/49523785/49523786/49523787/49523788/49523789/49523790/49523791/49523792/49523793/49523794/49523795/49523796/49523797/49523798/49523799/49523800/49523801/49523802/49523803/49523804/49523805/49523806/49523807/49523808/49523809/49523810/49523811/49523812/49523813/49523814/49523815/49523816/49523817/49523818/49523819/49523820/49523821/49523822/49523823/49523824/49523825/49523826/49523827/49523828/49523829/49523830/49523831/49523832/49523833/49523834/49523835/49523836/49523837/49523838/49523839/49523840/49523841/49523842/49523843/49523844/49523845/49523846/49523847/49523848/49523849/49523850/49523851/49523852/49523853/49523854/49523855/49523856/49523857/49523858/49523859/49523860/49523861/49523862/49523863/49523864/49523865/49523866/49523867/49523868/49523869/49523870/49523871/49523872/49523873/49523874/49523875/49523876/49523877/49523878/49523879/49523880/49523881/49523882/49523883/49523884/49523885/49523886/49523887/49523888/49523889/49523890/49523891/49523892/49523893/49523894/49523895/49523896/49523897/49523898/49523899/49523900/49523901/49523902/49523903/49523904/49523905/49523906/49523907/49523908/49523909/49523910/49523911/49523912/49523913/49523914/49523915/49523916/49523917/49523918/49523919/49523920/49523921/49523922/49523923/49523924/49523925/49523926/49523927/49523928/49523929/49523930/49523931/49523932/49523933/49523934/49523935/49523936/49523937/49523938/49523939/49523940/49523941/49523942/49523943/49523944/49523945/49523946/49523947/49523948/49523949/49523950/49523951/49523952/49523953/49523954/49523955/49523956/49523957/49523958/49523959/49523960/49523961/49523962/49523963/49523964/49523965/49523966/49523967/49523968/49523969/49523970/49523971/49523972/49523973/49523974/49523975/49523976/49523977/49523978/49523979/49523980/49523981/49523982/49523983/49523984/49523985/49523986/49523987/49523988/49523989/49523990/49523991/49523992/49523993/49523994/49523995/49523996/49523997/49523998/49523999/49524000/49524001/49524002/49524003/49524004/49524005/49524006/49524007/49524008/49524009/49524010/49524011/49524012/49524013/49524014/49524015/49524016/49524017/49524018/49524019/49524020/49524021/49524022/49524023/49524024/49524025/49524026/49524027/49524028/49524029/49524030/49524031/49524032/49524033/49524034/49524035/49524036/49524037/49524038/49524039/49524040/49524041/49524042/49524043/49524044/49524045/49524046/49524047/49524048/49524049/49524050/49524051/49524052/49524053/49524054/49524055/49524056/49524057/49524058/49524059/49524060/49524061/49524062/49524063/49524064/49524065/49524066/49524067/49524068/49524069/49524070/49524071/49524072/49524073/49524074/49524075/49524076/49524077/49524078/49524079/49524080/49524081/49524082/49524083/49524084/49524085/49524086/49524087/49524088/49524089/49524090/49524091/49524092/49524093/49524094/49524095/49524096/49524097/49524098/49524099/49524100/49524101/49524102/49524103/49524104/49524105/49524106/49524107/49524108/49524109/49524110/49524111/49524112/49524113/49524114/49524115/49524116/49524117/49524118/49524119/49524120/49524121/49524122/49524123/49524124/49524125/49524126/49524127/49524128/49524129/49524130/49524131/49524132/49524133/49524134/49524135/49524136/49524137/49524138/49524139/49524140/49524141/49524142/49524143/49524144/49524145/49524146/49524147/49524148/49524149/49524150/49524151/49524152/49524153/49524154/49524155/49524156/49524157/49524158/49524159/49524160/49524161/49524162/49524163/49524164/49524165/49524166/49524167/49524168/49524169/49524170/49524171/49524172/49524173/49524174/49524175/49524176/49524177/49524178/49524179/49524180/49524181/49524182/49524183/49524184/49524185/49524186/49524187/49524188/49524189/49524190/49524191/49524192/49524193/49524194/49524195/49524196/49524197/49524198/49524199/49524200/49524201/49524202/49524203/49524204/49524205/49524206/49524207/49524208/49524209/49524210/49524211/49524212/49524213/49524214/49524215/49524216/49524217/49524218/49524219/49524220/49524221/49524222/49524223/49524224/49524225/49524226/49524227/49524228/49524229/49524230/49524231/49524232/49524233/49524234/49524235/49524236/49524237/49524238/49524239/49524240/49524241/49524242/49524243/49524244/49524245/49524246/49524247/49524248/49524249/49524250/49524251/49524252/49524253/49524254/49524255/49524256/49524257/49524258/49524259/49524260/49524261/49524262/49524263/49524264/49524265/49524266/49524267/49524268/49524269/49524270/49524271/49524272/49524273/49524274/49524275/49524276/49524277/49524278/49524279/49524280/49524281/49524282/49524283/49524284/49524285/49524286/49524287/49524288/49524289/49524290/49524291/49524292/49524293/49524294/49524295/49524296/49524297/49524298/49524299/49524300/49524301/49524302/49524303/49524304/49524305/49524306/49524307/49524308/49524309/49524310/49524311/49524312/49524313/49524314/49524315/49524316/49524317/49524318/49524319/49524320/49524321/49524322/49524323/49524324/49524325/49524326/49524327/49524328/49524329/49524330/49524331/49524332/49524333/49524334/49524335/49524336/49524337/49524338/49524339/49524340/49524341/49524342/49524343/49524344/49524345/49524346/49524347/49524348/49524349/49524350/49524351/49524352/49524353/49524354/49524355/49524356/49524357/49524358/49524359/49524360/49524361/49524362/49524363/49524364/49524365/49524366/49524367/49524368/49524369/49524370/49524371/49524372/49524373/49524374/49524375/49524376/49524377/49524378/49524379/49524380/49524381/49524382/49524383/49524384/49524385/49524386/49524387/49524388/49524389/49524390/49524391/49524392/49524393/49524394/49524395/49524396/49524397/49524398/49524399/49524400/49524401/49524402/49524403/49524404/49524405/49524406/49524407/49524408/49524409/49524410/49524411/49524412/49524413/49524414/49524415/49524416/49524417/49524418/49524419/49524420/49524421/49524422/49524423/49524424/49524425/49524426/49524427/49524428/49524429/49524430/49524431/49524432/49524433/49524434/49524435/49524436/49524437/49524438/49524439/49524440/49524441/49524442/49524443/49524444/49524445/49524446/49524447/49524448/49524449/49524450/49524451/49524452/49524453/49524454/49524455/49524456/49524457/49524458/49524459/49524460/49524461/49524462/49524463/49524464/49524465/49524466/49524467/49524468/49524469/49524470/49524471/49524472/49524473/49524474/49524475/49524476/49524477/49524478/49524479/49524480/49524481/49524482/49524483/49524484/49524485/49524486/49524487/49524488/49524489/49524490/49524491/49524492/49524493/49524494/49524495/49524496/49524497/49524498/49524499/49524500/49524501/49524502/49524503/49524504/49524505/49524506/49524507/49524508/49524509/49524510/49524511/49524512/49524513/49524514/49524515/49524516/49524517/49524518/49524519/49524520/49524521/49524522/49524523/49524524/49524525/49524526/49524527/49524528/49524529/49524530/49524531/49524532/49524533/49524534/49524535/49524536/49524537/49524538/49524539/49524540/49524541/49524542/49524543/49524544/49524545/49524546/49524547/49524548/49524549/49524550/49524551/49524552/49524553/49524554/49524555/49524556/49524557/49524558/49524559/49524560/49524561/49524562/49524563/49524564/49524565/49524566/49524567/49524568/49524569/49524570/49524571/49524572/49524573/49524574/49524575/49524576/49524577/49524578/49524579/49524580/49524581/49524582/49524583/49524584/49524585/49524586/49524587/49524588/49524589/49524590/49524591/49524592/49524593/49524594/49524595/49524596/49524597/49524598/49524599/49524600/49524601/49524602/49524603/49524604/49524605/49524606/49524607/49524608/49524609/49524610/49524611/49524612/49524613/49524614/49524615/49524616/49524617/49524618/49524619/49524620/49524621/49524622/49524623/49524624/49524625/49524626/49524627/49524628/49524629/49524630/49524631/49524632/49524633/49524634/49524635/49524636/49524637/49524638/49524639/49524640/49524641/49524642/49524643/49524644/49524645/49524646/49524647/49524648/49524649/49524650/49524651/49524652/49524653/49524654/49524655/49524656/49524657/49524658/49524659/49524660/49524661/49524662/49524663/49524664/49524665/49524666/49524667/49524668/49524669/49524670/49524671/49524672/49524673/49524674/49524675/49524676/49524677/49524678/49524679/49524680/49524681/49524682/49524683/49524684/49524685/49524686/49524687/49524688/49524689/49524690/49524691/49524692/49524693/49524694/49524695/49524696/49524697/49524698/49524699/49524700/49524701/49524702/49524703/49524704/49524705/49524706/49524707/49524708/49524709/49524710/49524711/49524712/49524713/49524714/49524715/49524716/49524717/49524718/49524719/49524720/49524721/49524722/49524723/49524724/49524725/49524726/49524727/49524728/49524729/49524730/49524731/49524732/49524733/49524734/49524735/49524736/49524737/49524738/49524739/49524740/49524741/49524742/49524743/49524744/49524745/49524746/49524747/49524748/49524749/49524750/49524751/49524752/49524753/49524754/49524755/49524756/49524757/49524758/49524759/49524760/49524761/49524762/49524763/49524764/49524765/49524766/49524767/49524768/49524769/49524770/49524771/49524772/49524773/49524774/49524775/49524776/49524777/49524778/49524779/49524780/49524781/49524782/49524783/49524784/49524785/49524786/49524787/49524788/49524789/49524790/49524791/49524792/49524793/49524794/49524795/49524796/49524797/49524798/49524799/49524800/49524801/49524802/49524803/49524804/49524805/49524806/49524807/49524808/49524809/49524810/49524811/49524812/49524813/49524814/49524815/49524816/49524817/49524818/49524819/49524820/49524821/49524822/49524823/49524824/49524825/49524826/49524827/49524828/49524829/49524830/49524831/49524832/49524833/49524834/49524835/49524836/49524837/49524838/49524839/49524840/49524841/49524842/49524843/49524844/49524845/49524846/49524847/49524848/49524849/49524850/49524851/49524852/49524853/49524854/49524855/49524856/49524857/49524858/49524859/49524860/49524861/49524862/49524863/49524864/49524865/49524866/49524867/49524868/49524869/49524870/49524871/49524872/49524873/49524874/49524875/49524876/49524877/49524878/49524879/49524880/49524881/49524882/49524883/49524884/49524885/49524886/49524887/49524888/49524889/49524890/49524891/49524892/49524893/49524894/49524895/49524896/49524897/49524898/49524899/49524900/49524901/49524902/49524903/49524904/49524905/49524906/49524907/49524908/49524909/49524910/49524911/49524912/49524913/49524914/49524915/49524916/49524917/49524918/49524919/49524920/49524921/49524922/49524923/49524924/49524925/49524926/49524927/49524928/49524929/49524930/49524931/49524932/49524933/49524934/49524935/49524936/49524937/49524938/49524939/49524940/49524941/49524942/49524943/49524944/49524945/49524946/49524947/49524948/49524949/49524950/49524951/49524952/49521/49522/49523/49524/49525/49526/49527/49528/49529/495210/495211/495212/495213/495214/495215/495216/495217/495218/495219/495220/495221/495222/495223/495224/495225/495226/495227/495228/495229/495230/495231/495232/495233/495234/495235/495236/495237/495238/495239/495240/495241/495242/495243/495244/495245/495246/495247/495248/495249/495250/495251/495252/495253/495254/495255/495256/495257/495258/495259/495260/495261/495262/495263/495264/495265/495266/495267/495268/495269/495270/495271/495272/495273/495274/495275/495276/495277/495278/495279/495280/495281/495282/495283/495284/495285/495286/495287/495288/495289/495290/495291/495292/495293/495294/495295/495296/495297/495298/495299/4952100/4952101/4952102/4952103/4952104/4952105/4952106/4952107/4952108/4952109/4952110/4952111/4952112/4952113/4952114/4952115/4952116/4952117/4952118/4952119/4952120/4952121/4952122/4952123/4952124/4952125/4952126/4952127/4952128/4952129/4952130/4952131/4952132/4952133/4952134/4952135/4952136/4952137/4952138/4952139/4952140/4952141/4952142/4952143/4952144/4952145/4952146/4952147/4952148/4952149/4952150/4952151/4952152/4952153/4952154/4952155/4952156/4952157/4952158/4952159/4952160/4952161/4952162/4952163/4952164/4952165/4952166/4952167/4952168/4952169/4952170/4952171/4952172/4952173/4952174/4952175/4952176/4952177/4952178/4952179/4952180/4952181/4952182/4952183/4952184/4952185/4952186/4952187/4952188/4952189/4952190/4952191/4952192/4952193/4952194/4952195/4952196/4952197/4952198/4952199/4952200/4952201/4952202/4952203/4952204/4952205/4952206/4952207/4952208/4952209/4952210/4952211/4952212/4952213/4952214/4952215/4952216/4952217/4952218/4952219/4952220/4952221/4952222/4952223/4952224/4952225/4952226/4952227/4952228/4952229/4952230/4952231/4952232/4952233/4952234/4952235/4952236/4952237/4952238/4952239/4952240/4952241/4952242/4952243/4952244/4952245/4952246/4952247/4952248/4952249/4952250/4952251/4952252/4952253/4952254/4952255/4952256/4952257/4952258/4952259/4952260/4952261/4952262/4952263/4952264/4952265/4952266/4952267/4952268/4952269/4952270/4952271/4952272/4952273/4952274/4952275/4952276/4952277/4952278/4952279/4952280/4952281/4952282/4952283/4952284/4952285/4952286/4952287/4952288/4952289/4952290/4952291/4952292/4952293/4952294/4952295/4952296/4952297/4952298/4952299/4952300/4952301/4952302/4952303/4952304/4952305/4952306/4952307/4952308/4952309/4952310/4952311/4952312/4952313/4952314/4952315/4952316/4952317/4952318/4952319/4952320/4952321/4952322/4952323/4952324/4952325/4952326/4952327/4952328/4952329/4952330/4952331/4952332/4952333/4952334/4952335/4952336/4952337/4952338/4952339/4952340/4952341/4952342/4952343/4952344/4952345/4952346/4952347/4952348/4952349/4952350/4952351/4952352/4952353/4952354/4952355/4952356/4952357/4952358/4952359/4952360/4952361/4952362/4952363/4952364/4952365/4952366/4952367/4952368/4952369/4952370/4952371/4952372/4952373/4952374/4952375/4952376/4952377/4952378/4952379/4952380/4952381/4952382/4952383/4952384/4952385/4952386/4952387/4952388/4952389/4952390/4952391/4952392/4952393/4952394/4952395/4952396/4952397/4952398/4952399/4952400/4952401/4952402/4952403/4952404/4952405/4952406/4952407/4952408/4952409/4952410/4952411/4952412/4952413/4952414/4952415/4952416/4952417/4952418/4952419/4952420/4952421/4952422/4952423/4952424/4952425/4952426/4952427/4952428/4952429/4952430/4952431/4952432/4952433/4952434/4952435/4952436/4952437/4952438/4952439/4952440/4952441/4952442/4952443/4952444/4952445/4952446/4952447/4952448/4952449/4952450/4952451/4952452/4952453/4952454/4952455/4952456/4952457/4952458/4952459/4952460/4952461/4952462/4952463/4952464/4952465/4952466/4952467/4952468/4952469/4952470/4952471/4952472/4952473/4952474/4952475/4952476/4952477/4952478/4952479/4952480/4952481/4952482/4952483/4952484/4952485/4952486/4952487/4952488/4952489/4952490/4952491/4952492/4952493/4952494/4952495/4952496/4952497/4952498/4952499/4952500/4952501/4952502/4952503/4952504/4952505/4952506/4952507/4952508/4952509/4952510/4952511/4952512/4952513/4952514/4952515/4952516/4952517/4952518/4952519/4952520/4952521/4952522/4952523/4952524/4952525/4952526/4952527/4952528/4952529/4952530/4952531/4952532/4952533/4952534/4952535/4952536/4952537/4952538/4952539/4952540/4952541/4952542/4952543/4952544/4952545/4952546/4952547/4952548/4952549/4952550/4952551/4952552/4952553/4952554/4952555/4952556/4952557/4952558/4952559/4952560/4952561/4952562/4952563/4952564/4952565/4952566/4952567/4952568/4952569/4952570/4952571/4952572/4952573/4952574/4952575/4952576/4952577/4952578/4952579/4952580/4952581/4952582/4952583/4952584/4952585/4952586/4952587/4952588/4952589/4952590/4952591/4952592/4952593/4952594/4952595/4952596/4952597/4952598/4952599/4952600/4952601/4952602/4952603/4952604/4952605/4952606/4952607/4952608/4952609/4952610/4952611/4952612/4952613/4952614/4952615/4952616/4952617/4952618/4952619/4952620/4952621/4952622/4952623/4952624/4952625/4952626/4952627/4952628/4952629/4952630/4952631/4952632/4952633/4952634/4952635/4952636/4952637/4952638/4952639/4952640/4952641/4952642/4952643/4952644/4952645/4952646/4952647/4952648/4952649/4952650/4952651/4952652/4952653/4952654/4952655/4952656/4952657/4952658/4952659/4952660/4952661/4952662/4952663/4952664/4952665/4952666/4952667/4952668/4952669/4952670/4952671/4952672/4952673/4952674/4952675/4952676/4952677/4952678/4952679/4952680/4952681/4952682/4952683/4952684/4952685/4952686/4952687/4952688/4952689/4952690/4952691/4952692/4952693/4952694/4952695/4952696/4952697/4952698/4952699/4952700/4952701/4952702/4952703/4952704/4952705/4952706/4952707/4952708/4952709/4952710/4952711/4952712/4952713/4952714/4952715/4952716/4952717/4952718/4952719/4952720/4952721/4952722/4952723/4952724/4952725/4952726/4952727/4952728/4952729/4952730/4952731/4952732/4952733/4952734/4952735/4952736/4952737/4952738/4952739/4952740/4952741/4952742/4952743/4952744/4952745/4952746/4952747/4952748/4952749/4952750/4952751/4952752/4952753/4952754/4952755/4952756/4952757/4952758/4952759/4952760/4952761/4952762/4952763/4952764/4952765/4952766/4952767/4952768/4952769/4952770/4952771/4952772/4952773/4952774/4952775/4952776/4952777/4952778/4952779/4952780/4952781/4952782/4952783/4952784/4952785/4952786/4952787/4952788/4952789/4952790/4952791/4952792/4952793/4952794/4952795/4952796/4952797/4952798/4952799/4952800/4952801/4952802/4952803/4952804/4952805/4952806/4952807/4952808/4952809/4952810/4952811/4952812/4952813/4952814/4952815/4952816/4952817/4952818/4952819/4952820/4952821/4952822/4952823/4952824/4952825/4952826/4952827/4952828/4952829/4952830/4952831/4952832/4952833/4952834/4952835/4952836/4952837/4952838/4952839/4952840/4952841/4952842/4952843/4952844/4952845/4952846/4952847/4952848/4952849/4952850/4952851/4952852/4952853/4952854/4952855/4952856/4952857/4952858/4952859/4952860/4952861/4952862/4952863/4952864/4952865/4952866/4952867/4952868/4952869/4952870/4952871/4952872/4952873/4952874/4952875/4952876/4952877/4952878/4952879/4952880/4952881/4952882/4952883/4952884/4952885/4952886/4952887/4952888/4952889/4952890/4952891/4952892/4952893/4952894/4952895/4952896/4952897/4952898/4952899/4952900/4952901/4952902/4952903/4952904/4952905/4952906/4952907/4952908/4952909/4952910/4952911/4952912/4952913/4952914/4952915/4952916/4952917/4952918/4952919/4952920/4952921/4952922/4952923/4952924/4952925/4952926/4952927/4952928/4952929/4952930/4952931/4952932/4952933/4952934/4952935/4952936/4952937/4952938/4952939/4952940/4952941/4952942/4952943/4952944/4952945/4952946/4952947/4952948/4952949/4952950/4952951/4952952/4952953/4952954/4952955/4952956/4952957/4952958/4952959/4952960/4952961/4952962/4952963/4952964/4952965/4952966/4952967/4952968/4952969/4952970/4952971/4952972/4952973/4952974/4952975/4952976/4952977/4952978/4952979/4952980/4952981/4952982/4952983/4952984/4952985/4952986/4952987/4952988/4952989/4952990/4952991/4952992/4952993/4952994/4952995/4952996/4952997/4952998/4952999/49521000/49521001/49521002/49521003/49521004/49521005/49521006/49521007/49521008/49521009/49521010/49521011/49521012/49521013/49521014/49521015/49521016/49521017/49521018/49521019/49521020/49521021/49521022/49521023/49521024/49521025/49521026/49521027/49521028/49521029/49521030/49521031/49521032/49521033/49521034/49521035/49521036/49521037/49521038/49521039/49521040/49521041/49521042/49521043/49521044/49521045/49521046/49521047/49521048/49521049/49521050/49521051/49521052/49521053/49521054/49521055/49521056/49521057/49521058/49521059/49521060/49521061/49521062/49521063/49521064/49521065/49521066/49521067/49521068/49521069/49521070/49521071/49521072/49521073/49521074/49521075/49521076/49521077/49521078/49521079/49521080/49521081/49521082/49521083/49521084/49521085/49521086/49521087/49521088/49521089/49521090/49521091/49521092/49521093/49521094/49521095/49521096/49521097/49521098/49521099/49521100/49521101/49521102/49521103/49521104/49521105/49521106/49521107/49521108/49521109/49521110/49521111/49521112/49521113/49521114/49521115/49521116/49521117/49521118/49521119/49521120/49521121/49521122/49521123/49521124/49521125/49521126/49521127/49521128/49521129/49521130/49521131/49521132/49521133/49521134/49521135/49521136/49521137/49521138/49521139/49521140/49521141/49521142/49521143/49521144/49521145/49521146/49521147/49521148/49521149/49521150/49521151/49521152/49521153/49521154/49521155/49521156/49521157/49521158/49521159/49521160/49521161/49521162/49521163/49521164/49521165/49521166/49521167/49521168/49521169/49521170/49521171/49521172/49521173/49521174/49521175/49521176/49521177/49521178/49521179/49521180/49521181/49521182/49521183/49521184/49521185/49521186/49521187/49521188/49521189/49521190/49521191/49521192/49521193/49521194/49521195/49521196/49521197/49521198/49521199/49521200/49521201/49521202/49521203/49521204/49521205/49521206/49521207/49521208/49521209/49521210/49521211/49521212/49521213/49521214/49521215/49521216/49521217/49521218/49521219/49521220/49521221/49521222/49521223/49521224/49521225/49521226/49521227/49521228/49521229/49521230/49521231/49521232/49521233/49521234/49521235/49521236/49521237/49521238/49521239/49521240/49521241/49521242/49521243/49521244/49521245/49521246/49521247/49521248/49521249/49521250/49521251/49521252/49521253/49521254/49521255/49521256/49521257/49521258/49521259/49521260/49521261/49521262/49521263/49521264/49521265/49521266/49521267/49521268/49521269/49521270/49521271/49521272/49521273/49521274/49521275/49521276/49521277/49521278/49521279/49521280/49521281/49521282/49521283/49521284/49521285/49521286/49521287/49521288/49521289/49521290/49521291/49521292/49521293/49521294/49521295/49521296/49521297/49521298/49521299/49521300/49521301/49521302/49521303/49521304/49521305/49521306/49521307/49521308/49521309/49521310/49521311/49521312/49521313/49521314/49521315/49521316/49521317/49521318/49521319/49521320/49521321/49521322/49521323/49521324/49521325/49521326/49521327/49521328/49521329/49521330/49521331/49521332/49521333/49521334/49521335/49521336/49521337/49521338/49521339/49521340/49521341/49521342/49521343/49521344/49521345/49521346/49521347/49521348/49521349/49521350/49521351/49521352/49521353/49521354/49521355/49521356/49521357/49521358/49521359/49521360/49521361/49521362/49521363/49521364/49521365/49521366/49521367/49521368/49521369/49521370/49521371/49521372/49521373/49521374/49521375/49521376/49521377/49521378/49521379/49521380/49521381/49521382/49521383/49521384/49521385/49521386/49521387/49521388/49521389/49521390/49521391/49521392/49521393/49521394/49521395/49521396/49521397/49521398/49521399/49521400/49521401/49521402/49521403/49521404/49521405/49521406/49521407/49521408/49521409/49521410/49521411/49521412/49521413/49521414/49521415/49521416/49521417/49521418/49521419/49521420/49521421/49521422/49521423/49521424/49521425/49521426/49521427/49521428/49521429/49521430/49521431/49521432/49521433/49521434/49521435/49521436/49521437/49521438/49521439/49521440/49521441/49521442/49521443/49521444/49521445/49521446/49521447/49521448/49521449/49521450/49521451/49521452/49521453/49521454/49521455/49521456/49521457/49521458/49521459/49521460/49521461/49521462/49521463/49521464/49521465/49521466/49521467/49521468/49521469/49521470/49521471/49521472/49521473/49521474/49521475/49521476/49521477/49521478/49521479/49521480/49521481/49521482/49521483/49521484/49521485/49521486/49521487/49521488/49521489/49521490/49521491/49521492/49521493/49521494/49521495/49521496/49521497/49521498/49521499/49521500/49521501/49521502/49521503/49521504/49521505/49521506/49521507/49521508/49521509/49521510/49521511/49521512/49521513/49521514/49521515/49521516/49521517/49521518/49521519/49521520/49521521/49521522/49521523/49521524/49521525/49521526/49521527/49521528/49521529/49521530/49521531/49521532/49521533/49521534/49521535/49521536/49521537/49521538/49521539/49521540/49521541/49521542/49521543/49521544/49521545/49521546/49521547/49521548/49521549/49521550/49521551/49521552/49521553/49521554/49521555/49521556/49521557/49521558/49521559/49521560/49521561/49521562/49521563/49521564/49521565/49521566/49521567/49521568/49521569/49521570/49521571/49521572/49521573/49521574/49521575/49521576/49521577/49521578/49521579/49521580/49521581/49521582/49521583/49521584/49521585/49521586/49521587/49521588/49521589/49521590/49521591/49521592/49521593/49521594/49521595/49521596/49521597/49521598/49521599/49521600/49521601/49521602/49521603/49521604/49521605/49521606/49521607/49521608/49521609/49521610/49521611/49521612/49521613/49521614/49521615/49521616/49521617/49521618/49521619/49521620/49521621/49521622/49521623/49521624/49521625/49521626/49521627/49521628/49521629/49521630/49521631/49521632/49521633/49521634/49521635/49521636/49521637/49521638/49521639/49521640/49521641/49521642/49521643/49521644/49521645/49521646/49521647/49521648/49521649/49521650/49521651/49521652/49521653/49521654/49521655/49521656/49521657/49521658/49521659/49521660/49521661/49521662/49521663/49521664/49521665/49521666/49521667/49521668/49521669/49521670/49521671/49521672/49521673/49521674/49521675/49521676/49521677/49521678/49521679/49521680/49521681/49521682/49521683/49521684/49521685/49521686/49521687/49521688/49521689/49521690/49521691/49521692/49521693/49521694/49521695/49521696/49521697/49521698/49521699/49521700/49521701/49521702/49521703/49521704/49521705/49521706/49521707/49521708/49521709/49521710/49521711/49521712/49521713/49521714/49521715/49521716/49521717/49521718/49521719/49521720/49521721/49521722/49521723/49521724/49521725/49521726/49521727/49521728/49521729/49521730/49521731/49521732/49521733/49521734/49521735/49521736/49521737/49521738/49521739/49521740/49521741/49521742/49521743/49521744/49521745/49521746/49521747/49521748/49521749/49521750/49521751/49521752/49521753/49521754/49521755/49521756/49521757/49521758/49521759/49521760/49521761/49521762/49521763/49521764/49521765/49521766/49521767/49521768/49521769/49521770/49521771/49521772/49521773/49521774/49521775/49521776/49521777/49521778/49521779/49521780/49521781/49521782/49521783/49521784/49521785/49521786/49521787/49521788/49521789/49521790/49521791/49521792/49521793/49521794/49521795/49521796/49521797/49521798/49521799/49521800/49521801/49521802/49521803/49521804/49521805/49521806/49521807/49521808/49521809/49521810/49521811/49521812/49521813/49521814/49521815/49521816/49521817/49521818/49521819/49521820/49521821/49521822/49521823/49521824/49521825/49521826/49521827/49521828/49521829/49521830/49521831/49521832/49521833/49521834/49521835/49521836/49521837/49521838/49521839/49521840/49521841/49521842/49521843/49521844/49521845/49521846/49521847/49521848/49521849/49521850/49521851/49521852/49521853/49521854/49521855/49521856/49521857/49521858/49521859/49521860/49521861/49521862/49521863/49521864/49521865/49521866/49521867/49521868/49521869/49521870/49521871/49521872/49521873/49521874/49521875/49521876/49521877/49521878/49521879/49521880/49521881/49521882/49521883/49521884/49521885/49521886/49521887/49521888/49521889/49521890/49521891/49521892/49521893/49521894/49521895/49521896/49521897/49521898/49521899/49521900/49521901/49521902/49521903/49521904/49521905/49521906/49521907/49521908/49521909/49521910/49521911/49521912/49521913/49521914/49521915/49521916/49521917/49521918/49521919/49521920/49521921/49521922/49521923/49521924/49521925/49521926/49521927/49521928/49521929/49521930/49521931/49521932/49521933/49521934/49521935/49521936/49521937/49521938/49521939/49521940/49521941/49521942/49521943/49521944/49521945/49521946/49521947/49521948/49521949/49521950/49521951/49521952/49521953/49521954/49521955/49521956/49521957/49521958/49521959/49521960/49521961/49521962/49521963/49521964/49521965/49521966/49521967/49521968/49521969/49521970/49521971/49521972/49521973/49521974/49521975/49521976/49521977/49521978/49521979/49521980/49521981/49521982/49521983/49521984/49521985/49521986/49521987/49521988/49521989/49521990/49521991/49521992/49521993/49521994/49521995/49521996/49521997/49521998/49521999/49522000/49522001/49522002/49522003/49522004/49522005/49522006/49522007/49522008/49522009/49522010/49522011/49522012/49522013/49522014/49522015/49522016/49522017/49522018/49522019/49522020/49522021/49522022/49522023/49522024/49522025/49522026/49522027/49522028/49522029/49522030/49522031/49522032/49522033/49522034/49522035/49522036/49522037/49522038/49522039/49522040/49522041/49522042/49522043/49522044/49522045/49522046/49522047/49522048/49522049/49522050/49522051/49522052/49522053/49522054/49522055/49522056/49522057/49522058/49522059/49522060/49522061/49522062/49522063/49522064/49522065/49522066/49522067/49522068/49522069/49522070/49522071/49522072/49522073/49522074/49522075/49522076/49522077/49522078/49522079/49522080/49522081/49522082/49522083/49522084/49522085/49522086/49522087/49522088/49522089/49522090/49522091/49522092/49522093/49522094/49522095/49522096/49522097/49522098/49522099/49522100/49522101/49522102/49522103/49522104/49522105/49522106/49522107/49522108/49522109/49522110/49522111/49522112/49522113/49522114/49522115/49522116/49522117/49522118/49522119/49522120/49522121/49522122/49522123/49522124/49522125/49522126/49522127/49522128/49522129/49522130/49522131/49522132/49522133/49522134/49522135/49522136/49522137/49522138/49522139/49522140/49522141/49522142/49522143/49522144/49522145/49522146/49522147/49522148/49522149/49522150/49522151/49522152/49522153/49522154/49522155/49522156/49522157/49522158/49522159/49522160/49522161/49522162/49522163/49522164/49522165/49522166/49522167/49522168/49522169/49522170/49522171/49522172/49522173/49522174/49522175/49522176/49522177/49522178/49522179/49522180/49522181/49522182/49522183/49522184/49522185/49522186/49522187/49522188/49522189/49522190/49522191/49522192/49522193/49522194/49522195/49522196/49522197/49522198/49522199/49522200/49522201/49522202/49522203/49522204/49522205/49522206/49522207/49522208/49522209/49522210/49522211/49522212/49522213/49522214/49522215/49522216/49522217/49522218/49522219/49522220/49522221/49522222/49522223/49522224/49522225/49522226/49522227/49522228/49522229/49522230/49522231/49522232/49522233/49522234/49522235/49522236/49522237/49522238/49522239/49522240/49522241/49522242/49522243/49522244/49522245/49522246/49522247/49522248/49522249/49522250/49522251/49522252/49522253/49522254/49522255/49522256/49522257/49522258/49522259/49522260/49522261/49522262/49522263/49522264/49522265/49522266/49522267/49522268/49522269/49522270/49522271/49522272/49522273/49522274/49522275/49522276/49522277/49522278/49522279/49522280/49522281/49522282/49522283/49522284/49522285/49522286/49522287/49522288/49522289/49522290/49522291/49522292/49522293/49522294/49522295/49522296/49522297/49522298/49522299/49522300/49522301/49522302/49522303/49522304/49522305/49522306/49522307/49522308/49522309/49522310/49522311/49522312/49522313/49522314/49522315/49522316/49522317/49522318/49522319/49522320/49522321/49522322/49522323/49522324/49522325/49522326/49522327/49522328/49522329/49522330/49522331/49522332/49522333/49522334/49522335/49522336/49522337/49522338/49522339/49522340/49522341/49522342/49522343/49522344/49522345/49522346/49522347/49522348/49522349/49522350/49522351/49522352/49522353/49522354/49522355/49522356/49522357/49522358/49522359/49522360/49522361/49522362/49522363/49522364/49522365/49522366/49522367/49522368/49522369/49522370/49522371/49522372/49522373/49522374/49522375/49522376/49522377/49522378/49522379/49522380/49522381/49522382/49522383/49522384/49522385/49522386/49522387/49522388/49522389/49522390/49522391/49522392/49522393/49522394/49522395/49522396/49522397/49522398/49522399/49522400/49522401/49522402/49522403/49522404/49522405/49522406/49522407/49522408/49522409/49522410/49522411/49522412/49522413/49522414/49522415/49522416/49522417/49522418/49522419/49522420/49522421/49522422/49522423/49522424/49522425/49522426/49522427/49522428/49522429/49522430/49522431/49522432/49522433/49522434/49522435/49522436/49522437/49522438/49522439/49522440/49522441/49522442/49522443/49522444/49522445/49522446/49522447/49522448/49522449/49522450/49522451/49522452/49522453/49522454/49522455/49522456/49522457/49522458/49522459/49522460/49522461/49522462/49522463/49522464/49522465/49522466/49522467/49522468/49522469/49522470/49522471/49522472/49522473/49522474/49522475/49522476/49522477/49522478/49522479/49522480/49522481/49522482/49522483/49522484/49522485/49522486/49522487/49522488/49522489/49522490/49522491/49522492/49522493/49522494/49522495/49522496/49522497/49522498/49522499/49522500/49522501/49522502/49522503/49522504/49522505/49522506/49522507/49522508/49522509/49522510/49522511/49522512/49522513/49522514/49522515/49522516/49522517/49522518/49522519/49522520/49522521/49522522/49522523/49522524/49522525/49522526/49522527/49522528/49522529/49522530/49522531/49522532/49522533/49522534/49522535/49522536/49522537/49522538/49522539/49522540/49522541/49522542/49522543/49522544/49522545/49522546/49522547/49522548/49522549/49522550/49522551/49522552/49522553/49522554/49522555/49522556/49522557/49522558/49522559/49522560/49522561/49522562/49522563/49522564/49522565/49522566/49522567/49522568/49522569/49522570/49522571/49522572/49522573/49522574/49522575/49522576/49522577/49522578/49522579/49522580/49522581/49522582/49522583/49522584/49522585/49522586/49522587/49522588/49522589/49522590/49522591/49522592/49522593/49522594/49522595/49522596/49522597/49522598/49522599/49522600/49522601/49522602/49522603/49522604/49522605/49522606/49522607/49522608/49522609/49522610/49522611/49522612/49522613/49522614/49522615/49522616/49522617/49522618/49522619/49522620/49522621/49522622/49522623/49522624/49522625/49522626/49522627/49522628/49522629/49522630/49522631/49522632/49522633/49522634/49522635/49522636/49522637/49522638/49522639/49522640/49522641/49522642/49522643/49522644/49522645/49522646/49522647/49522648/49522649/49522650/49522651/49522652/49522653/49522654/49522655/49522656/49522657/49522658/49522659/49522660/49522661/49522662/49522663/49522664/49522665/49522666/49522667/49522668/49522669/49522670/49522671/49522672/49522673/49522674/49522675/49522676/49522677/49522678/49522679/49522680/49522681/49522682/49522683/49522684/49522685/49522686/49522687/49522688/49522689/49522690/49522691/49522692/49522693/49522694/49522695/49522696/49522697/49522698/49522699/49522700/49522701/49522702/49522703/49522704/49522705/49522706/49522707/49522708/49522709/49522710/49522711/49522712/49522713/49522714/49522715/49522716/49522717/49522718/49522719/49522720/49522721/49522722/49522723/49522724/49522725/49522726/49522727/49522728/49522729/49522730/49522731/49522732/49522733/49522734/49522735/49522736/49522737/49522738/49522739/49522740/49522741/49522742/49522743/49522744/49522745/49522746/49522747/49522748/49522749/49522750/49522751/49522752/49522753/49522754/49522755/49522756/49522757/49522758/49522759/49522760/49522761/49522762/49522763/49522764/49522765/49522766/49522767/49522768/49522769/49522770/49522771/49522772/49522773/49522774/49522775/49522776/49522777/49522778/49522779/49522780/49522781/49522782/49522783/49522784/49522785/49522786/49522787/49522788/49522789/49522790/49522791/49522792/49522793/49522794/49522795/49522796/49522797/49522798/49522799/49522800/49522801/49522802/49522803/49522804/49522805/49522806/49522807/49522808/49522809/49522810/49522811/49522812/49522813/49522814/49522815/49522816/49522817/49522818/49522819/49522820/49522821/49522822/49522823/49522824/49522825/49522826/49522827/49522828/49522829/49522830/49522831/49522832/49522833/49522834/49522835/49522836/49522837/49522838/49522839/49522840/49522841/49522842/49522843/49522844/49522845/49522846/49522847/49522848/49522849/49522850/49522851/49522852/49522853/49522854/49522855/49522856/49522857/49522858/49522859/49522860/49522861/49522862/49522863/49522864/49522865/49522866/49522867/49522868/49522869/49522870/49522871/49522872/49522873/49522874/49522875/49522876/49522877/49522878/49522879/49522880/49522881/49522882/49522883/49522884/49522885/49522886/49522887/49522888/49522889/49522890/49522891/49522892/49522893/49522894/49522895/49522896/49522897/49522898/49522899/49522900/49522901/49522902/49522903/49522904/49522905/49522906/49522907/49522908/49522909/49522910/49522911/49522912/49522913/49522914/49522915/49522916/49522917/49522918/49522919/49522920/49522921/49522922/49522923/49522924/49522925/49522926/49522927/49522928/49522929/49522930/49522931/49522932/49522933/49522934/49522935/49522936/49522937/49522938/49522939/49522940/49522941/49522942/49522943/49522944/49522945/49522946/49522947/49522948/49522949/49522950/49522951/49522952/49522953/49522954/49522955/49522956/49522957/49522958/49522959/49522960/49522961/49522962/49522963/49522964/49522965/49522966/49522967/49522968/49522969/49522970/49522971/49522972/49522973/49522974/49522975/49522976/49522977/49522978/49522979/49522980/49522981/49522982/49522983/49522984/49522985/49522986/49522987/49522988/49522989/49522990/49522991/49522992/49522993/49522994/49522995/49522996/49522997/49522998/49522999/49523000/49523001/49523002/49523003/49523004/49523005/49523006/49523007/49523008/49523009/49523010/49523011/49523012/49523013/49523014/49523015/49523016/49523017/49523018/49523019/49523020/49523021/49523022/49523023/49523024/49523025/49523026/49523027/49523028/49523029/49523030/49523031/49523032/49523033/49523034/49523035/49523036/49523037/49523038/49523039/49523040/49523041/49523042/49523043/49523044/49523045/49523046/49523047/49523048/49523049/49523050/49523051/49523052/49523053/49523054/49523055/49523056/49523057/49523058/49523059/49523060/49523061/49523062/49523063/49523064/49523065/49523066/49523067/49523068/49523069/49523070/49523071/49523072/49523073/49523074/49523075/49523076/49523077/49523078/49523079/49523080/49523081/49523082/49523083/49523084/49523085/49523086/49523087/49523088/49523089/49523090/49523091/49523092/49523093/49523094/49523095/49523096/49523097/49523098/49523099/49523100/49523101/49523102/49523103/49523104/49523105/49523106/49523107/49523108/49523109/49523110/49523111/49523112/49523113/49523114/49523115/49523116/49523117/49523118/49523119/49523120/49523121/49523122/49523123/49523124/49523125/49523126/49523127/49523128/49523129/49523130/49523131/49523132/49523133/49523134/49523135/49523136/49523137/49523138/49523139/49523140/49523141/49523142/49523143/49523144/49523145/49523146/49523147/49523148/49523149/49523150/49523151/49523152/49523153/49523154/49523155/49523156/49523157/49523158/49523159/49523160/49523161/49523162/49523163/49523164/49523165/49523166/49523167/49523168/49523169/49523170/49523171/49523172/49523173/49523174/49523175/49523176/49523177/49523178/49523179/49523180/49523181/49523182/49523183/49523184/49523185/49523186/49523187/49523188/49523189/49523190/49523191/49523192/49523193/49523194/49523195/49523196/49523197/49523198/49523199/49523200/49523201/49523202/49523203/49523204/49523205/49523206/49523207/49523208/49523209/49523210/49523211/49523212/49523213/49523214/49523215/49523216/49523217/49523218/49523219/49523220/49523221/49523222/49523223/49523224/49523225/49523226/49523227/49523228/49523229/49523230/49523231/49523232/49523233/49523234/49523235/49523236/49523237/49523238/49523239/49523240/49523241/49523242/49523243/49523244/49523245/49523246/49523247/49523248/49523249/49523250/49523251/49523252/49523253/49523254/49523255/49523256/49523257/49523258/49523259/49523260/49523261/49523262/49523263/49523264/49523265/49523266/49523267/49523268/49523269/49523270/49523271/49523272/49523273/49523274/49523275/49523276/49523277/49523278/49523279/49523280/49523281/49523282/49523283/49523284/49523285/49523286/49523287/49523288/49523289/49523290/49523291/49523292/49523293/49523294/49523295/49523296/49523297/49523298/49523299/49523300/49523301/49523302/49523303/49523304/49523305/49523306/49523307/49523308/49523309/49523310/49523311/49523312/49523313/49523314/49523315/49523316/49523317/49523318/49523319/49523320/49523321/49523322/49523323/49523324/49523325/49523326/49523327/49523328/49523329/49523330/49523331/49523332/49523333/49523334/49523335/49523336/49523337/49523338/49523339/49523340/49523341/49523342/49523343/49523344/49523345/49523346/49523347/49523348/49523349/49523350/49523351/49523352/49523353/49523354/49523355/49523356/49523357/49523358/49523359/49523360/49523361/49523362/49523363/49523364/49523365/49523366/49523367/49523368/49523369/49523370/49523371/49523372/49523373/49523374/49523375/49523376/49523377/49523378/49523379/49523380/49523381/49523382/49523383/49523384/49523385/49523386/49523387/49523388/49523389/49523390/49523391/49523392/49523393/49523394/49523395/49523396/49523397/49523398/49523399/49523400/49523401/49523402/49523403/49523404/49523405/49523406/49523407/49523408/49523409/49523410/49523411/49523412/49523413/49523414/49523415/49523416/49523417/49523418/49523419/49523420/49523421/49523422/49523423/49523424/49523425/49523426/49523427/49523428/49523429/49523430/49523431/49523432/49523433/49523434/49523435/49523436/49523437/49523438/49523439/49523440/49523441/49523442/49523443/49523444/49523445/49523446/49523447/49523448/49523449/49523450/49523451/49523452/49523453/49523454/49523455/49523456/49523457/49523458/49523459/49523460/49523461/49523462/49523463/49523464/49523465/49523466/49523467/49523468/49523469/49523470/49523471/49523472/49523473/49523474/49523475/49523476/49523477/49523478/49523479/49523480/49523481/49523482/49523483/49523484/49523485/49523486/49523487/49523488/49523489/49523490/49523491/49523492/49523493/49523494/49523495/49523496/49523497/49523498/49523499/49523500/49523501/49523502/49523503/49523504/49523505/49523506/49523507/49523508/49523509/49523510/49523511/49523512/49523513/49523514/49523515/49523516/49523517/49523518/49523519/49523520/49523521/49523522/49523523/49523524/49523525/49523526/49523527/49523528/49523529/49523530/49523531/49523532/49523533/49523534/49523535/49523536/49523537/49523538/49523539/49523540/49523541/49523542/49523543/49523544/49523545/49523546/49523547/49523548/49523549/49523550/49523551/49523552/49523553/49523554/49523555/49523556/49523557/49523558/49523559/49523560/49523561/49523562/49523563/49523564/49523565/49523566/49523567/49523568/49523569/49523570/49523571/49523572/49523573/49523574/49523575/49523576/49523577/49523578/49523579/49523580/49523581/49523582/49523583/49523584/49523585/49523586/49523587/49523588/49523589/49523590/49523591/49523592/49523593/49523594/49523595/49523596/49523597/49523598/49523599/49523600/49523601/49523602/49523603/49523604/49523605/49523606/49523607/49523608/49523609/49523610/49523611/49523612/49523613/49523614/49523615/49523616/49523617/49523618/49523619/49523620/49523621/49523622/49523623/49523624/49523625/49523626/49523627/49523628/49523629/49523630/49523631/49523632/49523633/49523634/49523635/49523636/49523637/49523638/49523639/49523640/49523641/49523642/49523643/49523644/49523645/49523646/49523647/49523648/49523649/49523650/49523651/49523652/49523653/49523654/49523655/49523656/49523657/49523658/49523659/49523660/49523661/49523662/49523663/49523664/49523665/49523666/49523667/49523668/49523669/49523670/49523671/49523672/49523673/49523674/49523675/49523676/49523677/49523678/49523679/49523680/49523681/49523682/49523683/49523684/49523685/49523686/49523687/49523688/49523689/49523690/49523691/49523692/49523693/49523694/49523695/49523696/49523697/49523698/49523699/49523700/49523701/49523702/49523703/49523704/49523705/49523706/49523707/49523708/49523709/49523710/49523711/49523712/49523713/49523714/49523715/49523716/49523717/49523718/49523719/49523720/49523721/49523722/49523723/49523724/49523725/49523726/49523727/49523728/49523729/49523730/49523731/49523732/49523733/49523734/49523735/49523736/49523737/49523738/49523739/49523740/49523741/49523742/49523743/49523744/49523745/49523746/49523747/49523748/49523749/49523750/49523751/49523752/49523753/49523754/49523755/49523756/49523757/49523758/49523759/49523760/49523761/49523762/49523763/49523764/49523765/49523766/49523767/49523768/49523769/49523770/49523771/49523772/49523773/49523774/49523775/49523776/49523777/49523778/49523779/49523780/49523781/49523782/49523783/49523784/49523785/49523786/49523787/49523788/49523789/49523790/49523791/49523792/49523793/49523794/49523795/49523796/49523797/49523798/49523799/49523800/49523801/49523802/49523803/49523804/49523805/49523806/49523807/49523808/49523809/49523810/49523811/49523812/49523813/49523814/49523815/49523816/49523817/49523818/49523819/49523820/49523821/49523822/49523823/49523824/49523825/49523826/49523827/49523828/49523829/49523830/49523831/49523832/49523833/49523834/49523835/49523836/49523837/49523838/49523839/49523840/49523841/49523842/49523843/49523844/49523845/49523846/49523847/49523848/49523849/49523850/49523851/49523852/49523853/49523854/49523855/49523856/49523857/49523858/49523859/49523860/49523861/49523862/49523863/49523864/49523865/49523866/49523867/49523868/49523869/49523870/49523871/49523872/49523873/49523874/49523875/49523876/49523877/49523878/49523879/49523880/49523881/49523882/49523883/49523884/49523885/49523886/49523887/49523888/49523889/49523890/49523891/49523892/49523893/49523894/49523895/49523896/49523897/49523898/49523899/49523900/49523901/49523902/49523903/49523904/49523905/49523906/49523907/49523908/49523909/49523910/49523911/49523912/49523913/49523914/49523915/49523916/49523917/49523918/49523919/49523920/49523921/49523922/49523923/49523924/49523925/49523926/49523927/49523928/49523929/49523930/49523931/49523932/49523933/49523934/49523935/49523936/49523937/49523938/49523939/49523940/49523941/49523942/49523943/49523944/49523945/49523946/49523947/49523948/49523949/49523950/49523951/49523952/49523953/49523954/49523955/49523956/49523957/49523958/49523959/49523960/49523961/49523962/49523963/49523964/49523965/49523966/49523967/49523968/49523969/49523970/49523971/49523972/49523973/49523974/49523975/49523976/49523977/49523978/49523979/49523980/49523981/49523982/49523983/49523984/49523985/49523986/49523987/49523988/49523989/49523990/49523991/49523992/49523993/49523994/49523995/49523996/49523997/49523998/49523999/49524000/49524001/49524002/49524003/49524004/49524005/49524006/49524007/49524008/49524009/49524010/49524011/49524012/49524013/49524014/49524015/49524016/49524017/49524018/49524019/49524020/49524021/49524022/49524023/49524024/49524025/49524026/49524027/49524028/49524029/49524030/49524031/49524032/49524033/49524034/49524035/49524036/49524037/49524038/49524039/49524040/49524041/49524042/49524043/49524044/49524045/49524046/49524047/49524048/49524049/49524050/49524051/49524052/49524053/49524054/49524055/49524056/49524057/49524058/49524059/49524060/49524061/49524062/49524063/49524064/49524065/49524066/49524067/49524068/49524069/49524070/49524071/49524072/49524073/49524074/49524075/49524076/49524077/49524078/49524079/49524080/49524081/49524082/49524083/49524084/49524085/49524086/49524087/49524088/49524089/49524090/49524091/49524092/49524093/49524094/49524095/49524096/49524097/49524098/49524099/49524100/49524101/49524102/49524103/49524104/49524105/49524106/49524107/49524108/49524109/49524110/49524111/49524112/49524113/49524114/49524115/49524116/49524117/49524118/49524119/49524120/49524121/49524122/49524123/49524124/49524125/49524126/49524127/49524128/49524129/49524130/49524131/49524132/49524133/49524134/49524135/49524136/49524137/49524138/49524139/49524140/49524141/49524142/49524143/49524144/49524145/49524146/49524147/49524148/49524149/49524150/49524151/49524152/49524153/49524154/49524155/49524156/49524157/49524158/49524159/49524160/49524161/49524162/49524163/49524164/49524165/49524166/49524167/49524168/49524169/49524170/49524171/49524172/49524173/49524174/49524175/49524176/49524177/49524178/49524179/49524180/49524181/49524182/49524183/49524184/49524185/49524186/49524187/49524188/49524189/49524190/49524191/49524192/49524193/49524194/49524195/49524196/49524197/49524198/49524199/49524200/49524201/49524202/49524203/49524204/49524205/49524206/49524207/49524208/49524209/49524210/49524211/49524212/49524213/49524214/49524215/49524216/49524217/49524218/49524219/49524220/49524221/49524222/49524223/49524224/49524225/49524226/49524227/49524228/49524229/49524230/49524231/49524232/49524233/49524234/49524235/49524236/49524237/49524238/49524239/49524240/49524241/49524242/49524243/49524244/49524245/49524246/49524247/49524248/49524249/49524250/49524251/49524252/49524253/49524254/49524255/49524256/49524257/49524258/49524259/49524260/49524261/49524262/49524263/49524264/49524265/49524266/49524267/49524268/49524269/49524270/49524271/49524272/49524273/49524274/49524275/49524276/49524277/49524278/49524279/49524280/49524281/49524282/49524283/49524284/49524285/49524286/49524287/49524288/49524289/49524290/49524291/49524292/49524293/49524294/49524295/49524296/49524297/49524298/49524299/49524300/49524301/49524302/49524303/49524304/49524305/49524306/49524307/49524308/49524309/49524310/49524311/49524312/49524313/49524314/49524315/49524316/49524317/49524318/49524319/49524320/49524321/49524322/49524323/49524324/49524325/49524326/49524327/49524328/49524329/49524330/49524331/49524332/49524333/49524334/49524335/49524336/49524337/49524338/49524339/49524340/49524341/49524342/49524343/49524344/49524345/49524346/49524347/49524348/49524349/49524350/49524351/49524352/49524353/49524354/49524355/49524356/49524357/49524358/49524359/49524360/49524361/49524362/49524363/49524364/49524365/49524366/49524367/49524368/49524369/49524370/49524371/49524372/49524373/49524374/49524375/49524376/49524377/49524378/49524379/49524380/49524381/49524382/49524383/49524384/49524385/49524386/49524387/49524388/49524389/49524390/49524391/49524392/49524393/49524394/49524395/49524396/49524397/49524398/49524399/49524400/49524401/49524402/49524403/49524404/49524405/49524406/49524407/49524408/49524409/49524410/49524411/49524412/49524413/49524414/49524415/49524416/49524417/49524418/49524419/49524420/49524421/49524422/49524423/49524424/49524425/49524426/49524427/49524428/49524429/49524430/49524431/49524432/49524433/49524434/49524435/49524436/49524437/49524438/49524439/49524440/49524441/49524442/49524443/49524444/49524445/49524446/49524447/49524448/49524449/49524450/49524451/49524452/49524453/49524454/49524455/49524456/49524457/49524458/49524459/49524460/49524461/49524462/49524463/49524464/49524465/49524466/49524467/49524468/49524469/49524470/49524471/49524472/49524473/49524474/49524475/49524476/49524477/49524478/49524479/49524480/49524481/49524482/49524483/49524484/49524485/49524486/49524487/49524488/49524489/49524490/49524491/49524492/49524493/49524494/49524495/49524496/49524497/49524498/49524499/49524500/49524501/49524502/49524503/49524504/49524505/49524506/49524507/49524508/49524509/49524510/49524511/49524512/49524513/49524514/49524515/49524516/49524517/49524518/49524519/49524520/49524521/49524522/49524523/49524524/49524525/49524526/49524527/49524528/49524529/49524530/49524531/49524532/49524533/49524534/49524535/49524536/49524537/49524538/49524539/49524540/49524541/49524542/49524543/49524544/49524545/49524546/49524547/49524548/49524549/49524550/49524551/49524552/49524553/49524554/49524555/49524556/49524557/49524558/49524559/49524560/49524561/49524562/49524563/49524564/49524565/49524566/49524567/49524568/49524569/49524570/49524571/49524572/49524573/49524574/49524575/49524576/49524577/49524578/49524579/49524580/49524581/49524582/49524583/49524584/49524585/49524586/49524587/49524588/49524589/49524590/49524591/49524592/49524593/49524594/49524595/49524596/49524597/49524598/49524599/49524600/49524601/49524602/49524603/49524604/49524605/49524606/49524607/49524608/49524609/49524610/49524611/49524612/49524613/49524614/49524615/49524616/49524617/49524618/49524619/49524620/49524621/49524622/49524623/49524624/49524625/49524626/49524627/49524628/49524629/49524630/49524631/49524632/49524633/49524634/49524635/49524636/49524637/49524638/49524639/49524640/49524641/49524642/49524643/49524644/49524645/49524646/49524647/49524648/49524649/49524650/49524651/49524652/49524653/49524654/49524655/49524656/49524657/49524658/49524659/49524660/49524661/49524662/49524663/49524664/49524665/49524666/49524667/49524668/49524669/49524670/49524671/49524672/49524673/49524674/49524675/49524676/49524677/49524678/49524679/49524680/49524681/49524682/49524683/49524684/49524685/49524686/49524687/49524688/49524689/49524690/49524691/49524692/49524693/49524694/49524695/49524696/49524697/49524698/49524699/49524700/49524701/49524702/49524703/49524704/49524705/49524706/49524707/49524708/49524709/49524710/49524711/49524712/49524713/49524714/49524715/49524716/49524717/49524718/49524719/49524720/49524721/49524722/49524723/49524724/49524725/49524726/49524727/49524728/49524729/49524730/49524731/49524732/49524733/49524734/49524735/49524736/49524737/49524738/49524739/49524740/49524741/49524742/49524743/49524744/49524745/49524746/49524747/49524748/49524749/49524750/49524751/49524752/49524753/49524754/49524755/49524756/49524757/49524758/49524759/49524760/49524761/49524762/49524763/49524764/49524765/49524766/49524767/49524768/49524769/49524770/49524771/49524772/49524773/49524774/49524775/49524776/49524777/49524778/49524779/49524780/49524781/49524782/49524783/49524784/49524785/49524786/49524787/49524788/49524789/49524790/49524791/49524792/49524793/49524794/49524795/49524796/49524797/49524798/49524799/49524800/49524801/49524802/49524803/49524804/49524805/49524806/49524807/49524808/49524809/49524810/49524811/49524812/49524813/49524814/49524815/49524816/49524817/49524818/49524819/49524820/49524821/49524822/49524823/49524824/49524825/49524826/49524827/49524828/49524829/49524830/49524831/49524832/49524833/49524834/49524835/49524836/49524837/49524838/49524839/49524840/49524841/49524842/49524843/49524844/49524845/49524846/49524847/49524848/49524849/49524850/49524851/49524852/49524853/49524854/49524855/49524856/49524857/49524858/49524859/49524860/49524861/49524862/49524863/49524864/49524865/49524866/49524867/49524868/49524869/49524870/49524871/49524872/49524873/49524874/49524875/49524876/49524877/49524878/49524879/49524880/49524881/49524882/49524883/49524884/49524885/49524886/49524887/49524888/49524889/49524890/49524891/49524892/49524893/49524894/49524895/49524896/49524897/49524898/49524899/49524900/49524901/49524902/49524903/49524904/49524905/49524906/49524907/49524908/49524909/49524910/49524911/49524912/49524913/49524914/49524915/49524916/49524917/49524918/49524919/49524920/49524921/49524922/49524923/49524924/49524925/49524926/49524927/49524928/49524929/49524930/49524931/49524932/49524933/49524934/49524935/49524936/49524937/49524938/49524939/49524940/49524941/49524942/49524943/49524944/49524945/49524946/49524947/49524948/49524949/49524950/49524951/49524952/4952aeroplane 0.6851
bicycle 0.7057
bird 0.6182
boat 0.4137
bottle 0.4291
bus 0.6889
car 0.7739
cat 0.8028
chair 0.3963
cow 0.5987
diningtable 0.4844
dog 0.7188
horse 0.7263
motorbike 0.7196
person 0.7670
pottedplant 0.3360
sheep 0.5448
sofa 0.5591
train 0.7155
tvmonitor 0.6790
mAP: 0.6181

Epoch 00002: saving model to ./snapshots\resnet50_pascal_02.h5
